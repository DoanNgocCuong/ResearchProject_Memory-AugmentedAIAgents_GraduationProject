### **üìÑ ƒê·ªì √°n nghi√™n c·ª©u: ƒê·ªÅ t√†i: "Long-Term Memory Augmentation for Conversational Question Answering Systems"** _**(TƒÉng c∆∞·ªùng tr√≠ nh·ªõ d√†i h·∫°n cho c√°c h·ªá th·ªëng h·ªèi ƒë√°p h·ªôi tho·∫°i)**_

  

üìù **T√°c gi·∫£:**

üè´ **ƒê∆°n v·ªã nghi√™n c·ª©u:**

üìÖ **Ng√†y th·ª±c hi·ªán:**

  

---

  

# **üìå 1. Gi·ªõi thi·ªáu (Introduction)**

  

## 1.1. ƒê·∫∑t v·∫•n ƒë·ªÅ:

### üî• **ƒê·ªông l·ª±c ch√≠nh:**

1. **C√° nh√¢n h√≥a s√¢u s·∫Øc** Ghi nh·ªõ th√¥ng tin, s·ªü th√≠ch v√† h√†nh vi ng∆∞·ªùi d√πng gi√∫p chatbot/robot ph·∫£n h·ªìi ph√π h·ª£p, t·∫°o c·∫£m gi√°c th√¢n quen v√† tƒÉng s·ª± h√†i l√≤ng. üëâ **V√≠ d·ª•**: M·ªôt ng∆∞·ªùi d√πng th∆∞·ªùng t√¨m c√¥ng th·ª©c ƒÉn chay. Chatbot ghi nh·ªõ ƒëi·ªÅu n√†y v√† lu√¥n g·ª£i √Ω m√≥n chay, thay v√¨ c√°c c√¥ng th·ª©c ng·∫´u nhi√™n. üëâ _N·∫øu kh√¥ng c√≥ ƒëi·ªÅu n√†y_: Chatbot c√≥ th·ªÉ li√™n t·ª•c ƒë·ªÅ xu·∫•t m√≥n m·∫∑n, g√¢y phi·ªÅn to√°i v√† l√†m ng∆∞·ªùi d√πng m·∫•t thi·ªán c·∫£m, t·ª´ ƒë√≥ gi·∫£m kh·∫£ nƒÉng quay l·∫°i s·ª≠ d·ª•ng.
    
2. **Duy tr√¨ v√† t√°i s·ª≠ d·ª•ng ng·ªØ c·∫£nh** Tr√≠ nh·ªõ d√†i h·∫°n cho ph√©p hi·ªÉu ƒë∆∞·ª£c l·ªãch s·ª≠ h·ªôi tho·∫°i qua nhi·ªÅu phi√™n l√†m vi·ªác, gi·ªØ m·∫°ch logic v√† tr√°nh y√™u c·∫ßu ng∆∞·ªùi d√πng ph·∫£i l·∫∑p l·∫°i th√¥ng tin. üëâ **V√≠ d·ª•**: Trong m·ªôt chu·ªói h·ªôi tho·∫°i v·ªÅ ƒë∆°n h√†ng, chatbot nh·ªõ r·∫±ng ng∆∞·ªùi d√πng ƒëang khi·∫øu n·∫°i v·ªÅ s·∫£n ph·∫©m X t·ª´ phi√™n tr∆∞·ªõc v√† ti·∫øp t·ª•c h·ªó tr·ª£ ngay ·ªü phi√™n sau. üëâ _N·∫øu kh√¥ng c√≥ ƒëi·ªÅu n√†y_: Ng∆∞·ªùi d√πng s·∫Ω ph·∫£i l·∫∑p l·∫°i to√†n b·ªô th√¥ng tin khi quay l·∫°i, g√¢y b·ª±c b·ªôi v√† t·∫°o c·∫£m gi√°c chatbot thi·∫øu chuy√™n nghi·ªáp.
    
3. **H·ªó tr·ª£ t√°c v·ª• d√†i h·∫°n** C√°c ·ª©ng d·ª•ng nh∆∞ theo d√µi h·ªçc t·∫≠p, chƒÉm s√≥c s·ª©c kh·ªèe ho·∫∑c h√†nh tr√¨nh kh√°ch h√†ng y√™u c·∫ßu chatbot ghi nh·ªõ v√† c·∫≠p nh·∫≠t ti·∫øn tr√¨nh li√™n t·ª•c. üëâ **V√≠ d·ª•**: M·ªôt chatbot h·ªçc t·∫≠p ghi nh·ªõ r·∫±ng h·ªçc sinh ƒë√£ y·∫øu ·ªü ph·∫ßn th√¨ qu√° kh·ª© ho√†n th√†nh v√† ti·∫øp t·ª•c luy·ªán t·∫≠p ƒëi·ªÉm n√†y trong c√°c bu·ªïi sau. üëâ _N·∫øu kh√¥ng c√≥ ƒëi·ªÅu n√†y_: Chatbot s·∫Ω l·∫∑p l·∫°i nh·ªØng n·ªôi dung h·ªçc ƒë√£ ·ªïn, b·ªè s√≥t ƒëi·ªÉm y·∫øu c·ªßa h·ªçc sinh, l√†m gi·∫£m hi·ªáu qu·∫£ h·ªçc t·∫≠p v√† c·∫£m gi√°c ‚Äúƒë∆∞·ª£c ƒë·ªìng h√†nh‚Äù c·ªßa ng∆∞·ªùi h·ªçc.
    

---

## **1.2. C√°c gi·∫£i ph√°p hi·ªán t·∫°i v√† h·∫°n ch·∫ø**

Trong nh·ªØng nƒÉm g·∫ßn ƒë√¢y, ƒë·ªÉ x√¢y d·ª±ng c√°c tr·ª£ l√Ω h·ªôi tho·∫°i c√≥ kh·∫£ nƒÉng ghi nh·ªõ d√†i h·∫°n, c·ªông ƒë·ªìng nghi√™n c·ª©u ƒë√£ ph√°t tri·ªÉn ba h∆∞·ªõng ti·∫øp c·∫≠n ch√≠nh, m·ªói h∆∞·ªõng ƒë·∫°i di·ªán cho m·ªôt t∆∞ duy ki·∫øn tr√∫c kh√°c nhau v·ªÅ c√°ch h·ªá th·ªëng l∆∞u tr·ªØ, c·∫≠p nh·∫≠t v√† truy xu·∫•t th√¥ng tin t·ª´ l·ªãch s·ª≠ h·ªôi tho·∫°i.

#### üìå **(1) X·ª≠ l√Ω tr·ª±c ti·∫øp to√†n b·ªô ng·ªØ c·∫£nh d√†i (Long-context Input)**

C√°ch ti·∫øp c·∫≠n ƒë·∫ßu ti√™n l√† cung c·∫•p **to√†n b·ªô l·ªãch s·ª≠ h·ªôi tho·∫°i** tr∆∞·ªõc ƒë√≥ v√†o ph·∫ßn **ng·ªØ c·∫£nh ƒë·∫ßu v√†o c·ªßa LLM** d∆∞·ªõi d·∫°ng m·ªôt chu·ªói duy nh·∫•t. ƒêi·ªÅu n√†y cho ph√©p m√¥ h√¨nh ti·∫øp c·∫≠n to√†n b·ªô d·ªØ li·ªáu tr∆∞·ªõc ƒë√≥ trong m·ªôt l·∫ßn t√≠nh to√°n, t·ª´ ƒë√≥ ƒë∆∞a ra ph·∫£n h·ªìi ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh h·ªôi tho·∫°i k√©o d√†i.

- **∆Øu ƒëi·ªÉm**:
    
    - Kh√¥ng y√™u c·∫ßu thay ƒë·ªïi ki·∫øn tr√∫c m√¥ h√¨nh.
        
    - D·ªÖ tri·ªÉn khai, c√≥ th·ªÉ √°p d·ª•ng tr·ª±c ti·∫øp cho c√°c LLM hi·ªán c√≥ (GPT-3/4, Claude...).
        
- **H·∫°n ch·∫ø**:
    
    - T·ªën t√†i nguy√™n, do ƒë·ªô d√†i ƒë·∫ßu v√†o tƒÉng m·∫°nh.
        
    - D·ªÖ g·∫∑p hi·ªán t∆∞·ª£ng **‚Äúlost-in-the-middle‚Äù**, khi m√¥ h√¨nh **kh√¥ng c√≤n truy c·∫≠p hi·ªáu qu·∫£** t·ªõi c√°c th√¥ng tin n·∫±m gi·ªØa ƒëo·∫°n vƒÉn b·∫£n d√†i ‚Äì d·∫´n ƒë·∫øn vi·ªác qu√™n m·∫•t th√¥ng tin quan tr·ªçng ho·∫∑c tr·∫£ l·ªùi sai l·ªách (Beltagy et al., 2020; Shi et al., 2023).
        
    - B·ªã gi·ªõi h·∫°n b·ªüi ƒë·ªô d√†i context window c·ªßa m√¥ h√¨nh (~8k‚Äì100k tokens t√πy phi√™n b·∫£n).
        

#### üìå **(2) T√≠ch h·ª£p m√¥-ƒëun tr√≠ nh·ªõ kh·∫£ vi (Differentiable Memory Modules)**

H∆∞·ªõng ti·∫øp c·∫≠n th·ª© hai l√† **thi·∫øt k·∫ø l·∫°i ki·∫øn tr√∫c m·∫°ng n∆°-ron**, t√≠ch h·ª£p m·ªôt th√†nh ph·∫ßn **b·ªô nh·ªõ h·ªçc ƒë∆∞·ª£c (learnable memory)** ‚Äì cho ph√©p m√¥ h√¨nh ghi nh·ªõ v√† truy xu·∫•t th√¥ng tin th√¥ng qua c√°c c∆° ch·∫ø nh∆∞ attention ho·∫∑c ƒë·ªçc/ghi kh·∫£ vi.

- **Ti√™u bi·ªÉu**: Memory Networks (Weston et al., 2014), Dynamic Memory Network (Kumar et al., 2016), MemGPT (Wu et al., 2022).
    
- **∆Øu ƒëi·ªÉm**:
    
    - C√≥ kh·∫£ nƒÉng l∆∞u gi·ªØ th√¥ng tin l√¢u d√†i.
        
    - H·ªó tr·ª£ suy lu·∫≠n nhi·ªÅu b∆∞·ªõc d·ª±a tr√™n b·ªô nh·ªõ (multi-hop reasoning).
        
- **H·∫°n ch·∫ø**:
    
    - ƒê√≤i h·ªèi hu·∫•n luy·ªán l·∫°i t·ª´ ƒë·∫ßu, kh√¥ng d·ªÖ √°p d·ª•ng cho c√°c m√¥ h√¨nh LLM th∆∞∆°ng m·∫°i d·∫°ng API.
        
    - Kh√≥ t·ªëi ∆∞u h√≥a hi·ªáu qu·∫£ khi hu·∫•n luy·ªán tr√™n d·ªØ li·ªáu h·ªôi tho·∫°i t·ª± nhi√™n, ƒë·∫∑c bi·ªát n·∫øu h·ªôi tho·∫°i d√†i, nhi·ªÅu bi·∫øn th·ªÉ ng·ªØ nghƒ©a.
        

#### (3) X·ª≠ l√Ω ng·ªØ c·∫£nh v√† truy xu·∫•t khi c·∫ßn (Context Compression & Retrieval)

C√≥ 3 method x·ª≠ l√Ω ch√≠nh:

##### 3.1 L·∫•y to√†n b·ªô th√¥ng tin h·ªôi tho·∫°i:

- **M√¥ t·∫£:**
    
    - Ph∆∞∆°ng ph√°p n√†y l∆∞u tr·ªØ v√† s·ª≠ d·ª•ng to√†n b·ªô n·ªôi dung c·ªßa phi√™n h·ªôi tho·∫°i ƒë·ªÉ x·ª≠ l√Ω v√† truy xu·∫•t th√¥ng tin khi c·∫ßn thi·∫øt.
        
- **∆Øu ƒëi·ªÉm:**
    
    - B·∫£o to√†n ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh v√† chi ti·∫øt c·ªßa cu·ªôc tr√≤ chuy·ªán.
        
    - H·ªØu √≠ch trong c√°c t√¨nh hu·ªëng y√™u c·∫ßu ph√¢n t√≠ch to√†n di·ªán ho·∫∑c khi c·∫ßn truy xu·∫•t th√¥ng tin c·ª• th·ªÉ t·ª´ b·∫•t k·ª≥ ph·∫ßn n√†o c·ªßa h·ªôi tho·∫°i.
        
- **Nh∆∞·ª£c ƒëi·ªÉm:**
    
    - D·ªÖ d·∫´n ƒë·∫øn qu√° t·∫£i b·ªô nh·ªõ v√† gi·∫£m hi·ªáu su·∫•t do l∆∞·ª£ng d·ªØ li·ªáu l·ªõn.
        
    - Kh√≥ khƒÉn trong vi·ªác x√°c ƒë·ªãnh v√† truy xu·∫•t th√¥ng tin quan tr·ªçng do thi·∫øu c·∫•u tr√∫c ph√¢n c·∫•p.
        

##### 3.2 X·ª≠ l√Ω ng·ªØ c·∫£nh theo **round**

- ƒê√¢y l√† ƒë∆°n v·ªã nh·ªè nh·∫•t trong m·ªôt phi√™n h·ªôi tho·∫°i: **m·ªói l∆∞·ª£t h·ªèi‚Äìƒë√°p** ƒë∆∞·ª£c l∆∞u v√† x·ª≠ l√Ω ri√™ng bi·ªát.
    
- K·∫øt qu·∫£ th·ª±c nghi·ªám cho th·∫•y:
    

> - "**Decomposing sessions into rounds significantly enhances reading performance**" ‚Äì ƒë·∫∑c bi·ªát v·ªõi c√°c m√¥ h√¨nh m·∫°nh nh∆∞ GPT-4o.
>     

- **∆Øu ƒëi·ªÉm:**
    
    - Gi·∫£m nhi·ªÖu, tƒÉng ƒë·ªô ch√≠nh x√°c khi truy xu·∫•t.
        
    - Cho ph√©p granularity cao ‚Üí d·ªÖ chunk, ƒë√°nh index, v√† l·ªçc th√¥ng tin.
        
- **Nh∆∞·ª£c ƒëi·ªÉm:**
    
    - D·ªÖ m·∫•t m·∫°ch h·ªôi tho·∫°i n·∫øu kh√¥ng c√≥ chi·∫øn l∆∞·ª£c ƒë·ªçc (reading strategy) h·ª£p l√Ω nh∆∞ Chain-of-Note.
        
- **D·∫´n ch·ª©ng:**
    

> - ‚ÄúUsing LONGMEMEVALM, we compare different value choices‚Ä¶ Decomposing sessions into rounds significantly enhances reading performance‚ÄùRead2 - LONGMEMEVAL - B‚Ä¶.
>     

---

##### 3.3 X·ª≠ l√Ω ng·ªØ c·∫£nh theo **phi√™n** (session)

- M·ªói phi√™n h·ªôi tho·∫°i g·ªìm nhi·ªÅu round li√™n ti·∫øp s·∫Ω ƒë∆∞·ª£c l∆∞u nguy√™n kh·ªëi.
    
- ƒê√¢y l√† baseline ph·ªï bi·∫øn c·ªßa nhi·ªÅu h·ªá th·ªëng hi·ªán nay.
    
- **∆Øu ƒëi·ªÉm:**
    
    - B·∫£o to√†n m·∫°ch h·ªôi tho·∫°i, ƒë·∫∑c bi·ªát h·ªØu √≠ch trong c√°c truy v·∫•n c·∫ßn t·ªïng h·ª£p theo di·ªÖn ti·∫øn (e.g. "trong l·∫ßn tr√≤ chuy·ªán h√¥m tr∆∞·ªõc").
        
- **Nh∆∞·ª£c ƒëi·ªÉm:**
    
    - D·ªÖ g√¢y qu√° t·∫£i context window.
        
    - Kh√≥ truy xu·∫•t ch√≠nh x√°c n·∫øu kh√¥ng c√≥ indexing t·ªët.
        
- **D·∫´n ch·ª©ng:**
    

> - ‚ÄúStoring each session as a single item can hinder effective retrieval and reading‚Äù
>     

  

---

### ∆Øu nh∆∞·ª£c ƒëi·ªÉm c·ªßa c√°c gi·∫£i ph√°p hi·ªán t·∫°i:

1. **Granularity ch∆∞a t·ªëi ∆∞u:**
    
    1. **V·∫•n ƒë·ªÅ:**
        
        - Tr√≠ch xu·∫•t th√¥ng tin t·ª´ to√†n b·ªô session ho·∫∑c t·ª´ng round ri√™ng l·∫ª c√≥ th·ªÉ d·∫´n ƒë·∫øn:
            
            - ƒêo·∫°n qu√° ng·∫Øn: Thi·∫øu ng·ªØ c·∫£nh ƒë·ªÉ m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) tr√≠ch xu·∫•t th√¥ng tin √Ω nghƒ©a.
                
            - ƒêo·∫°n qu√° d√†i: G√¢y nhi·ªÖu, kh√≥ t√≥m t·∫Øt ch√≠nh x√°c v√† d·ªÖ m·∫•t chi ti·∫øt quan tr·ªçng.
                
        - Kh√¥ng ki·ªÉm so√°t ƒë∆∞·ª£c m·ª©c ƒë·ªô li√™n k·∫øt ho·∫∑c chuy·ªÉn ƒë·ªïi ch·ªß ƒë·ªÅ trong c√°c session d√†i.
            
2. **Ch·ªâ s·ª≠ d·ª•ng m·ªôt lo·∫°i kh√≥a (key) duy nh·∫•t cho vi·ªác l·∫≠p ch·ªâ m·ª•c (indexing):**
    
    2. **V·∫•n ƒë·ªÅ:**
        
        - S·ª≠ d·ª•ng m·ªôt lo·∫°i kh√≥a nh∆∞ `K = V + fact` ho·∫∑c `K = V + summary` c√≥ nh·ªØng h·∫°n ch·∫ø:
            
            - **Summary:** T·ªët cho vi·ªác kh·ªõp ng·ªØ nghƒ©a t·ªïng th·ªÉ.
                
            - **Keyphrase:** B·∫Øt ƒë∆∞·ª£c c√°c t·ª´ kh√≥a c·ª• th·ªÉ.
                
            - **Fact:** Truy xu·∫•t ch√≠nh x√°c c√°c th·ª±c th·ªÉ, s·ªë li·ªáu, m·ªëc th·ªùi gian.
                
        - Kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c hi·ªáu ·ª©ng **k·∫øt h·ª£p (ensemble)** gi·ªØa c√°c lo·∫°i kh√≥a kh√°c nhau.
            
3. **Thi·∫øu c·∫•u tr√∫c trong vi·ªác l·∫≠p ch·ªâ m·ª•c:**
    
    1. **V·∫•n ƒë·ªÅ:**
        
        - L·∫≠p ch·ªâ m·ª•c d·∫°ng ph·∫≥ng (flat) kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c c·∫•u tr√∫c ph√¢n c·∫•p c·ªßa vƒÉn b·∫£n h·ªôi tho·∫°i: ƒëo·∫°n ‚Äì session ‚Äì d√≤ng th·ªùi gian.
            
        - Thi·∫øu kh·∫£ nƒÉng ƒëi·ªÅu h∆∞·ªõng m∆∞·ª£t m√† gi·ªØa c√°c m·ª©c ƒë·ªô kh√°i qu√°t (coarse) v√† chi ti·∫øt (fine).
            

---

## 1.3 Dataset:

![](https://csg2ej4iz2hz.sg.larksuite.com/space/api/box/stream/download/asynccode/?code=OThlMzI3OGJmNTllZjA2YmRlMzY0MzFmZjA0MmM4YTlfMDNnY1BSWDhEMzBkSXdQaURZSzBLS1FxOW04MWxHQ01fVG9rZW46VGtBa2I5aVFmb21HckZ4aFAxZ2xUQ2tJZ2owXzE3NDQ5MDA1NTU6MTc0NDkwNDE1NV9WNA)

  

LongMemEval l√† m·ªôt b·ªô d·ªØ li·ªáu to√†n di·ªán, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng ghi nh·ªõ d√†i h·∫°n c·ªßa c√°c tr·ª£ l√Ω tr√≤ chuy·ªán. B·ªô d·ªØ li·ªáu n√†y bao g·ªìm 500 c√¢u h·ªèi ch·∫•t l∆∞·ª£ng cao, t·∫≠p trung v√†o nƒÉm kh·∫£ nƒÉng c·ªët l√µi:ÓàÜ

  

1. **Tr√≠ch xu·∫•t th√¥ng tin (Information Extraction):** Kh·∫£ nƒÉng nh·ªõ l·∫°i th√¥ng tin c·ª• th·ªÉ t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c d√†i, bao g·ªìm c·∫£ chi ti·∫øt do ng∆∞·ªùi d√πng ho·∫∑c tr·ª£ l√Ω cung c·∫•p.ÓàÜ
    
2. **L√Ω lu·∫≠n ƒëa phi√™n (Multi-Session Reasoning):** Kh·∫£ nƒÉng t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu phi√™n tr√≤ chuy·ªán ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi ph·ª©c t·∫°p y√™u c·∫ßu s·ª± t·ªïng h·ª£p v√† so s√°nh.ÓàÜ
    
3. **C·∫≠p nh·∫≠t ki·∫øn th·ª©c (Knowledge Updates):** Kh·∫£ nƒÉng nh·∫≠n bi·∫øt v√† c·∫≠p nh·∫≠t th√¥ng tin c√° nh√¢n c·ªßa ng∆∞·ªùi d√πng theo th·ªùi gian.ÓàÜ
    
4. **L√Ω lu·∫≠n th·ªùi gian (Temporal Reasoning):** Nh·∫≠n th·ª©c v·ªÅ c√°c kh√≠a c·∫°nh th·ªùi gian c·ªßa th√¥ng tin ng∆∞·ªùi d√πng, bao g·ªìm c·∫£ th·ªùi gian ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p r√µ r√†ng v√† si√™u d·ªØ li·ªáu th·ªùi gian trong c√°c t∆∞∆°ng t√°c.ÓàÜ
    
5. **T·ª´ ch·ªëi tr·∫£ l·ªùi (Abstention):** Kh·∫£ nƒÉng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn th√¥ng tin kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong l·ªãch s·ª≠ t∆∞∆°ng t√°c.ÓàÜ
    

### H·ªèi:

- Open Domain v·ªõi Personal kh√°c nhau nh∆∞ n√†o nh·ªâ: **b·ªô d·ªØ li·ªáu Personal t·∫≠p trung v√†o l·ªãch s·ª≠ t∆∞∆°ng t√°c v√† th√¥ng tin c√° nh√¢n c·ªßa m·ªôt ng∆∞·ªùi d√πng v·ªõi AI ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng c√° nh√¢n h√≥a v√† tr√≠ nh·ªõ d√†i h·∫°n li√™n quan ƒë·∫øn ng∆∞·ªùi ƒë√≥**, trong khi **b·ªô d·ªØ li·ªáu Open-Domain ch·ª©a ƒë·ª±ng c√°c cu·ªôc h·ªôi tho·∫°i ƒëa d·∫°ng v·ªÅ ch·ªß ƒë·ªÅ v√† th∆∞·ªùng kh√¥ng t·∫≠p trung v√†o m·ªôt ng∆∞·ªùi d√πng c·ª• th·ªÉ ho·∫∑c l·ªãch s·ª≠ t∆∞∆°ng t√°c c√° nh√¢n k√©o d√†i.**
    
- LongMemEval:
    
    - ƒê·ªëi v·ªõi thi·∫øt l·∫≠p **LONGMEMEVALS**, l·ªãch s·ª≠ tr√≤ chuy·ªán cho m·ªói c√¢u h·ªèi c√≥ ƒë·ªô d√†i kho·∫£ng **115 ngh√¨n token. Thi·∫øt l·∫≠p LONGMEMEVALS c√≥ th·ªÉ bao g·ªìm kho·∫£ng t·ª´ 30 ƒë·∫øn 60 phi√™n. kho·∫£ng 1900 ƒë·∫øn 3800 token/phi√™n.**
        
    - ƒê·ªëi v·ªõi thi·∫øt l·∫≠p **LONGMEMEVALM**, l·ªãch s·ª≠ tr√≤ chuy·ªán cho m·ªói c√¢u h·ªèi **c√≥ t·ªïng ƒë·ªô d√†i kho·∫£ng 1,5 tri·ªáu token. B**ao g·ªìm **500 phi√™n, kho·∫£ng 1900 ƒë·∫øn 3800 token/phi√™n.**
        
    
      
    
    > **Tr∆∞·ªùng h·ª£p phi√™n ng·∫Øn nh·∫•t (1900 token) v√† l∆∞·ª£t t∆∞∆°ng t√°c d√†i nh·∫•t (150 token/l∆∞·ª£t)**: 1900 / 150 ‚âà **12.7 l∆∞·ª£t**.
    > 
    > **Tr∆∞·ªùng h·ª£p phi√™n ng·∫Øn nh·∫•t (1900 token) v√† l∆∞·ª£t t∆∞∆°ng t√°c ng·∫Øn nh·∫•t (50 token/l∆∞·ª£t)**: 1900 / 50 = **38 l∆∞·ª£t**.
    > 
    > **Tr∆∞·ªùng h·ª£p phi√™n d√†i nh·∫•t (3800 token) v√† l∆∞·ª£t t∆∞∆°ng t√°c d√†i nh·∫•t (150 token/l∆∞·ª£t)**: 3800 / 150 ‚âà **25.3 l∆∞·ª£t**.
    > 
    > **Tr∆∞·ªùng h·ª£p phi√™n d√†i nh·∫•t (3800 token) v√† l∆∞·ª£t t∆∞∆°ng t√°c ng·∫Øn nh·∫•t (50 token/l∆∞·ª£t)**: 3800 / 50 = **76 l∆∞·ª£t**.
    > 
    > Nh∆∞ v·∫≠y, d·ª±a tr√™n ∆∞·ªõc t√≠nh v·ªÅ ƒë·ªô d√†i l∆∞·ª£t t∆∞∆°ng t√°c, m·ªôt phi√™n c√≥ ƒë·ªô d√†i t·ª´ **1900 ƒë·∫øn 3800 token** c√≥ th·ªÉ bao g·ªìm **kho·∫£ng t·ª´ 13 ƒë·∫øn 76 l∆∞·ª£t t∆∞∆°ng t√°c**
    

### MSC

1 d√≤ng test s·∫Ω g·ªìm: 1 c√¢u h·ªèi, 1 c√¢u tr·∫£ l·ªùi, v√† 1 l·ªãch s·ª≠ g·ªìm 4 phi√™n h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥.

M·ªói tr∆∞·ªùng h·ª£p ki·ªÉm th·ª≠ (test instance) th∆∞·ªùng bao g·ªìm:

- **L·ªãch s·ª≠ h·ªôi tho·∫°i:** Bao g·ªìm nhi·ªÅu phi√™n (v√≠ d·ª•, 4 phi√™n ƒë·∫ßu ti√™n c·ªßa m·ªôt cu·ªôc ƒë·ªëi tho·∫°i) ƒë·ªÉ x√¢y d·ª±ng ng·ªØ c·∫£nh v√† l∆∞u tr·ªØ th√¥ng tin, th√¥ng tin c√° nh√¢n (persona) c·ªßa c√°c b√™n tham gia.
    
- **C√¢u h·ªèi (query):** L√† l∆∞·ª£t n√≥i hi·ªán t·∫°i trong phi√™n cu·ªëi (v√≠ d·ª•, phi√™n th·ª© 5) m√† m√¥ h√¨nh c·∫ßn tr·∫£ l·ªùi.
    
- **C√¢u tr·∫£ l·ªùi chu·∫©n (gold response):** L√† ƒë√°p √°n ƒë∆∞·ª£c g√°n s·∫µn ƒë·ªÉ so s√°nh v·ªõi ph·∫£n h·ªìi c·ªßa m√¥ h√¨nh.
    

D·∫´n ch·ª©ng b√†i: https://neurips2023-enlsp.github.io/papers/paper_38.pdf

## 1.4 **Gi·∫£i ph√°p ƒë·ªÅ xu·∫•t: K·∫øt h·ª£p LLMs + RAPTOR + Multi-Key Embedding + Hierarchical Indexing**

### 1.4.1 **Ph√¢n ƒëo·∫°n h·ªôi tho·∫°i theo ng·ªØ c·∫£nh tr∆∞·ªõc khi tr√≠ch xu·∫•t:**

#### 1.1‚úÇÔ∏è **Ph√¢n ƒëo·∫°n b·∫±ng LLM (LLM-based Chunking):**

¬†¬†**Ph∆∞∆°ng ph√°p:** S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n chia session th√†nh c√°c ƒëo·∫°n nh·ªè (chunk) d·ª±a tr√™n:

- Chuy·ªÉn ƒë·ªïi ch·ªß ƒë·ªÅ.
    
- M·ª•c ƒë√≠ch c√¢u h·ªèi.
    
- H√†nh vi ng∆∞·ªùi d√πng.
    

¬†¬†**L·ª£i √≠ch:**

¬†¬†¬†¬†T√°ch ƒë∆∞·ª£c c√°c segment theo ch·ªß ƒë·ªÅ.

¬†¬†¬†¬†Gi·ªØ ƒë∆∞·ª£c t√≠nh li√™n k·∫øt b√™n trong m·ªói chunk.

#### **1.2. Ph√¢n ƒëo·∫°n b·∫±ng RAPTOR (RAPTOR Chunking):**

¬†¬†**Ph∆∞∆°ng ph√°p:**

- S·ª≠ d·ª•ng RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) ƒë·ªÉ t·∫°o c√¢y ph√¢n c·∫•p cho t·ª´ng session.
    
- M·ªói node l√† m·ªôt chunk ho·∫∑c t√≥m t·∫Øt c·ªßa c√°c chunk con, ph·ª•c v·ª• cho vi·ªác truy xu·∫•t ph√¢n c·∫•p (hierarchical retrieval).
    

¬†¬†**L·ª£i √≠ch:**

- T·∫°o c·∫•u tr√∫c c√¢y gi√∫p truy xu·∫•t th√¥ng tin hi·ªáu qu·∫£ h∆°n.
    
- T·∫≠n d·ª•ng ƒë∆∞·ª£c c·∫£ th√¥ng tin chi ti·∫øt v√† t·ªïng quan.
    

**K·∫øt qu·∫£:** Thay v√¨ s·ª≠ d·ª•ng `K = Session + fact`, ta c√≥ `K = Session1.i + Fact` (v·ªõi `i` l√† c√°c chunk nh·ªè ƒë∆∞·ª£c t·∫°o t·ª´ session ban ƒë·∫ßu).

### 1.4.2 **Embedding: L√†m ph·∫≥ng v√† l·∫≠p ch·ªâ m·ª•c (Flatten & Index):**

#### üßæ **2.1. Embedding ph·∫≥ng b·∫±ng RAPTOR (RAPTOR Flat Embedding):**

- **Ph∆∞∆°ng ph√°p:**
    
    - ƒê∆∞a t·ª´ng chunk (bao g·ªìm chunk ban ƒë·∫ßu, chunk ƒë∆∞·ª£c t·∫°o b·ªüi LLM, v√† summary chunk) v√†o b·ªô m√£ h√≥a embedding.
        
    - T·∫°o l·∫≠p ch·ªâ m·ª•c d·∫°ng ph·∫≥ng, c√≥ th·ªÉ s·ª≠ d·ª•ng Reranker ƒë·ªÉ ch·ªçn top-K chunk c√≥ li√™n quan nh·∫•t.
        

#### üß† **2.2. L·∫≠p ch·ªâ m·ª•c ph√¢n c·∫•p (Hierarchical Indexing) - Truy xu·∫•t hai giai ƒëo·∫°n (2-phase Retrieval):**

- **Giai ƒëo·∫°n 1: Truy xu·∫•t th√¥ (Coarse Retrieval):**
    
    - Embedding c√°c summary ho·∫∑c keyphrase c·ªßa chunk.
        
    - S·ª≠ d·ª•ng truy v·∫•n ƒë·ªÉ so s√°nh v√† ch·ªçn Top-K chunk li√™n quan.
        
- **Giai ƒëo·∫°n 2: Truy xu·∫•t chi ti·∫øt (Fine Retrieval):**
    
    - V·ªõi m·ªói chunk ƒë√£ ch·ªçn ·ªü giai ƒëo·∫°n th√¥:
        
        - Embedding l·∫°i c√°c c√¢u g·ªëc, facts, ho·∫∑c sub-chunks.
            
        - L·∫•y top-K‚Äô ƒë∆°n v·ªã b·ªô nh·ªõ chi ti·∫øt.
            

**K·∫øt qu·∫£:** C√°c th√¥ng tin n√†y ƒë∆∞·ª£c ƒë∆∞a v√†o LLM ƒë·ªÉ ƒë·ªçc v√† tr·∫£ l·ªùi (Reading stage).

### 1.4.3 **Embedding ƒëa kh√≥a cho vi·ªác l·∫≠p ch·ªâ m·ª•c (Multi-Key Embedding for Indexing):**

**Ph∆∞∆°ng ph√°p:**

- V·ªõi m·ªói chunk, t·∫°o v√† embedding song song c√°c kh√≥a:
    
    - `K1 = V + summary`
        
    - `K2 = V + fact`
        
    - `K3 = V + keyphrase`
        
- K·∫øt h·ª£p k·∫øt qu·∫£ truy h·ªìi t·ª´ c√°c lu·ªìng b·∫±ng c√°c ph∆∞∆°ng ph√°p nh∆∞ voting, weighted fusion, ho·∫∑c union-rerank.
    

**L√Ω do:**

- **Summary:** B·∫Øt ng·ªØ nghƒ©a chung.
    
- **Fact:** H·ªó tr·ª£ l·∫≠p lu·∫≠n logic.
    
- **Keyphrase:** Kh·ªõp t·ª´ kh√≥a trong truy v·∫•n c·ª• th·ªÉ.
    

  

## üîÅ T·ªïng pipeline c·∫£i ti·∫øn

  

```Plain
Session
  ‚Üì
Conversation-aware Chunking (LLMs Chunk + Raptor Chunk)
  ‚Üì
Chunk-level Value ‚Üí Extract (summary, fact, keyphrase)
  ‚Üì
Embed:
   - K1 = V + summary
   - K2 = V + fact
   - K3 = V + keyphrase
  ‚Üì
Phase 1: Coarse Retrieval t·ª´ summary/keyphrase
  ‚Üì
Phase 2: Fine Retrieval t·ª´ facts
  ‚Üì
Reading Strategy: CoN + JSON (Chain-of-Note)
  ‚Üì
Answer
```

  

---

---

D∆∞·ªõi ƒë√¢y l√† to√†n b·ªô **k·ªãch b·∫£n th·ª≠ nghi·ªám** (experimental settings) t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng √Ω t∆∞·ªüng c·∫£i ti·∫øn m√† Qu·ªëc ƒë·ªÅ xu·∫•t ‚Äî ƒë∆∞·ª£c t·ªï ch·ª©c theo d·∫°ng **ma tr·∫≠n th√≠ nghi·ªám** ƒë·ªÉ c√≥ th·ªÉ d·ªÖ d√†ng tri·ªÉn khai th·ª±c nghi·ªám, ƒë√°nh gi√° t·ª´ng th√†nh ph·∫ßn v√† k·∫øt h·ª£p c·ªßa pipeline.

  

---

  

## üéØ **M·ª§C TI√äU TH·ª¨ NGHI·ªÜM**

  

> Ki·ªÉm ch·ª©ng c√°c c·∫£i ti·∫øn v·ªÅ chunking, indexing, embedding, retrieval v√† reading strategy nh·∫±m c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa h·ªá th·ªëng long-term memory QA (v√≠ d·ª• tr√™n benchmark nh∆∞ LONGMEMEVAL).

  

---

  

## üß™ **K·ªäCH B·∫¢N TH·ª¨ NGHI·ªÜM CH√çNH**

  

### üîπ **I. Chunking Strategy**

  

|   |   |   |
|---|---|---|
|M√£|T√™n ph∆∞∆°ng ph√°p|M√¥ t·∫£|
|C1|No Chunking (baseline)|D√πng c·∫£ session ho·∫∑c round l√†m value tr·ª±c ti·∫øp|
|C2|LLM-based Chunking|Ph√¢n chia ƒëo·∫°n theo ch·ªß ƒë·ªÅ/ng·ªØ nghƒ©a b·∫±ng LLM|
|C3|RAPTOR Chunking|Chunking d·∫°ng c√¢y ph√¢n c·∫•p theo RAPTOR|
|C4|LLM + RAPTOR Hybrid|Chunk theo LLM ‚Üí d√πng RAPTOR ƒë·ªÉ t√≥m t·∫Øt t·ª´ng chunk|

  

---

  

### üîπ **II. Value Representation**

  

|   |   |   |
|---|---|---|
|M√£|D·∫°ng value ƒë·∫ßu v√†o|M√¥ t·∫£|
|V1|Full Session|Kh√¥ng chia nh·ªè, ƒë·ªÉ nguy√™n session|
|V2|Round-based|M·ªói round l√† m·ªôt value|
|V3|Chunked|Chunk theo chi·∫øn l∆∞·ª£c C2, C3, C4|
|V4|Summary|T√≥m t·∫Øt c·ªßa chunk ho·∫∑c session|
|V5|Fact|Fact tr√≠ch t·ª´ chunk/session|

  

---

  

### üîπ **III. Key Design (Indexing)**

  

|   |   |   |
|---|---|---|
|M√£|T√™n thi·∫øt k·∫ø key|M√¥ t·∫£|
|K1|K = V|D√πng raw value l√†m key|
|K2|K = fact|Key l√† facts ƒë√£ tr√≠ch|
|K3|K = summary|Key l√† summary|
|K4|K = V + fact|N·ªëi fact v√†o value ƒë·ªÉ t·∫°o key|
|K5|K = V + summary|N·ªëi summary v√†o value|
|K6|K = V + fact + summary + keyphrase|Multi-key (concat t·∫•t c·∫£)|
|K7|Multi-path index|T·∫°o nhi·ªÅu lo·∫°i key ri√™ng bi·ªát, embed ƒë·ªôc l·∫≠p|

  

---

  

### üîπ **IV. Retrieval Strategy**

  

|   |   |   |
|---|---|---|
|M√£|Ph∆∞∆°ng ph√°p truy h·ªìi|M√¥ t·∫£|
|R1|Flat Retrieval|Retrieval ƒë∆°n l·ªõp, cosine / FAISS|
|R2|Coarse ‚Üí Fine Retrieval (2-phase)|Truy xu·∫•t 2 pha: summary ‚Üí fact|
|R3|Flat + Reranker|Retrieval s∆° c·∫•p r·ªìi rerank b·∫±ng LLM|
|R4|Multi-path Fusion|Truy h·ªìi theo t·ª´ng key, r·ªìi h·ª£p k·∫øt qu·∫£ (voting / union)|

  

---

  

### üîπ **V. Reading Strategy**

  

|   |   |   |
|---|---|---|
|M√£|K·ªπ thu·∫≠t ƒë·ªçc k·∫øt qu·∫£|M√¥ t·∫£|
|RS1|Direct Answer|ƒê∆∞a chunk v√†o, y√™u c·∫ßu LLM tr·∫£ l·ªùi th·∫≥ng|
|RS2|Chain-of-Note (CoN)|Tr√≠ch info tr∆∞·ªõc r·ªìi reasoning sau|
|RS3|JSON + CoN|ƒê∆∞a input d·∫°ng JSON c√≥ c·∫•u tr√∫c, d√πng CoN|
|RS4|CoT + CoN|K·∫øt h·ª£p chain-of-thought reasoning v·ªõi CoN|

  

---

  

## ‚úÖ **K·∫æT H·ª¢P TH·ª¨ NGHI·ªÜM G·ª¢I √ù (FULL COMBO)**

  

|   |   |   |   |   |   |
|---|---|---|---|---|---|
|#|Chunking|Value|Key|Retrieval|Reading|
|1|C1|V1|K1|R1|RS1|
|2|C2|V3|K4|R2|RS3|
|3|C3|V3|K6|R2|RS2|
|4|C4|V3|K7|R4|RS3|
|5|C3|V5|K2|R3|RS1|
|6|C2|V3|K5|R1|RS2|
|7|C4|V3|K6|R3|RS4|

  

> üí° _B·∫°n c√≥ th·ªÉ ch·ªçn 3-5 combo ƒë·ªÉ th·ª≠ nghi·ªám A/B, ƒëo c√°c metric nh∆∞:_
> 
>   
> 
> - **QA Accuracy (top-K)**
>     
> - **Recall@K, NDCG@K**
>     
> - **Inference time / latency**
>     
> - **Token count (input to reader)**
>     

  

---

  

## üì¶ **D·ªØ li·ªáu s·ª≠ d·ª•ng**

  

- Dataset ch√≠nh: `LONGMEMEVAL-S` v√† `LONGMEMEVAL-M`
    
- C√≥ th·ªÉ m·ªü r·ªông th·ª≠ nghi·ªám v·ªõi `MemoryBank`, `PerLTQA` ƒë·ªÉ ki·ªÉm tra ƒë·ªô generalize.
    

  

---

  

N·∫øu b·∫°n c·∫ßn m√¨nh gi√∫p vi·∫øt script pipeline ch·∫°y th·ª≠ nghi·ªám ho·∫∑c d·ª±ng YAML config cho t·ª´ng scenario ƒë·ªÉ qu·∫£n l√Ω ch·∫°y b·∫±ng `Hydra`/`WandB` ho·∫∑c h·ªá th·ªëng ML pipeline kh√°c th√¨ c·ª© n√≥i nh√©. C≈©ng c√≥ th·ªÉ d·ª±ng l·∫°i b·∫£ng n√†y d∆∞·ªõi d·∫°ng Notion template ho·∫∑c Google Sheet ƒë·ªÉ d·ªÖ qu·∫£n l√Ω.

  

  

---

  

# Hi·ªÉu s√¢u h∆°n v·ªÅ Datase:

  

  

LongMemEval l√† m·ªôt b·ªô d·ªØ li·ªáu to√†n di·ªán, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng ghi nh·ªõ d√†i h·∫°n c·ªßa c√°c tr·ª£ l√Ω tr√≤ chuy·ªán. B·ªô d·ªØ li·ªáu n√†y bao g·ªìm 500 c√¢u h·ªèi ch·∫•t l∆∞·ª£ng cao, t·∫≠p trung v√†o nƒÉm kh·∫£ nƒÉng c·ªët l√µi:ÓàÜ

  

1. **Tr√≠ch xu·∫•t th√¥ng tin (Information Extraction):** Kh·∫£ nƒÉng nh·ªõ l·∫°i th√¥ng tin c·ª• th·ªÉ t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c d√†i, bao g·ªìm c·∫£ chi ti·∫øt do ng∆∞·ªùi d√πng ho·∫∑c tr·ª£ l√Ω cung c·∫•p.ÓàÜ
    

2. **L√Ω lu·∫≠n ƒëa phi√™n (Multi-Session Reasoning):** Kh·∫£ nƒÉng t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu phi√™n tr√≤ chuy·ªán ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi ph·ª©c t·∫°p y√™u c·∫ßu s·ª± t·ªïng h·ª£p v√† so s√°nh.ÓàÜ
    

3. **C·∫≠p nh·∫≠t ki·∫øn th·ª©c (Knowledge Updates):** Kh·∫£ nƒÉng nh·∫≠n bi·∫øt v√† c·∫≠p nh·∫≠t th√¥ng tin c√° nh√¢n c·ªßa ng∆∞·ªùi d√πng theo th·ªùi gian.ÓàÜ
    

4. **L√Ω lu·∫≠n th·ªùi gian (Temporal Reasoning):** Nh·∫≠n th·ª©c v·ªÅ c√°c kh√≠a c·∫°nh th·ªùi gian c·ªßa th√¥ng tin ng∆∞·ªùi d√πng, bao g·ªìm c·∫£ th·ªùi gian ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p r√µ r√†ng v√† si√™u d·ªØ li·ªáu th·ªùi gian trong c√°c t∆∞∆°ng t√°c.ÓàÜ
    

5. **T·ª´ ch·ªëi tr·∫£ l·ªùi (Abstention):** Kh·∫£ nƒÉng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn th√¥ng tin kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong l·ªãch s·ª≠ t∆∞∆°ng t√°c.ÓàÜ
    

  

L·∫•y c·∫£m h·ª©ng t·ª´ b√†i ki·ªÉm tra "t√¨m kim trong ƒë·ªëng c·ªè kh√¥", LongMemEval s·ª≠ d·ª•ng m·ªôt quy tr√¨nh ki·ªÉm so√°t thu·ªôc t√≠nh ƒë·ªÉ t·∫°o ra l·ªãch s·ª≠ tr√≤ chuy·ªán m·∫°ch l·∫°c, c√≥ th·ªÉ m·ªü r·ªông v√† ƒë∆∞·ª£c ƒë√°nh d·∫•u th·ªùi gian cho m·ªói c√¢u h·ªèi. H·ªá th·ªëng tr√≤ chuy·ªán c·∫ßn ph√¢n t√≠ch c√°c t∆∞∆°ng t√°c ƒë·ªông ƒë·ªÉ ghi nh·ªõ v√† tr·∫£ l·ªùi c√¢u h·ªèi sau khi t·∫•t c·∫£ c√°c phi√™n t∆∞∆°ng t√°c ƒë√£ di·ªÖn ra.ÓàÜ

  

**C·∫•u tr√∫c B·ªô D·ªØ Li·ªáu:**

  

B·ªô d·ªØ li·ªáu bao g·ªìm ba t·ªáp ch√≠nh:ÓàÜ

  

1. **longmemeval_s.json:** M·ªói l·ªãch s·ª≠ tr√≤ chuy·ªán ti√™u th·ª• kho·∫£ng 115.000 token (~40 phi√™n l·ªãch s·ª≠).ÓàÜ
    

2. **longmemeval_m.json:** M·ªói l·ªãch s·ª≠ tr√≤ chuy·ªán ch·ª©a kho·∫£ng 500 phi√™n.ÓàÜ
    

3. **longmemeval_oracle.json:** Ch·ªâ bao g·ªìm c√°c phi√™n ch·ª©a b·∫±ng ch·ª©ng c·∫ßn thi·∫øt.ÓàÜ
    

  

M·ªói t·ªáp ch·ª©a 500 tr∆∞·ªùng h·ª£p ƒë√°nh gi√°, m·ªói tr∆∞·ªùng h·ª£p bao g·ªìm c√°c tr∆∞·ªùng:ÓàÜ

  

- **question_id:** ID duy nh·∫•t cho m·ªói c√¢u h·ªèi.ÓàÜ
    

- **question_type:** Lo·∫°i c√¢u h·ªèi, nh∆∞ single-session-user, single-session-assistant, single-session-preference, temporal-reasoning, knowledge-update, v√† multi-session. N·∫øu question_id k·∫øt th√∫c b·∫±ng _abs, ƒë√≥ l√† c√¢u h·ªèi t·ª´ ch·ªëi tr·∫£ l·ªùi.ÓàÜ
    

- **question:** N·ªôi dung c√¢u h·ªèi.ÓàÜ
    

- **answer:** C√¢u tr·∫£ l·ªùi mong ƒë·ª£i t·ª´ m√¥ h√¨nh.ÓàÜ
    

- **question_date:** Ng√†y c·ªßa c√¢u h·ªèi.ÓàÜ
    

- **haystack_session_ids:** Danh s√°ch ID c·ªßa c√°c phi√™n l·ªãch s·ª≠ (s·∫Øp x·∫øp theo th·ªùi gian).ÓàÜ
    

- **haystack_dates:** Danh s√°ch c√°c m·ªëc th·ªùi gian c·ªßa c√°c phi√™n l·ªãch s·ª≠.ÓàÜ
    

- **haystack_sessions:** Danh s√°ch n·ªôi dung th·ª±c t·∫ø c·ªßa c√°c phi√™n tr√≤ chuy·ªán gi·ªØa ng∆∞·ªùi d√πng v√† tr·ª£ l√Ω. M·ªói phi√™n l√† m·ªôt danh s√°ch c√°c l∆∞·ª£t trao ƒë·ªïi, m·ªói l∆∞·ª£t c√≥ ƒë·ªãnh d·∫°ng {"role": user/assistant, "content": n·ªôi dung tin nh·∫Øn}. ƒê·ªëi v·ªõi c√°c l∆∞·ª£t ch·ª©a b·∫±ng ch·ª©ng c·∫ßn thi·∫øt, c√≥ th√™m tr∆∞·ªùng has_answer: true.ÓàÜ
    

- **answer_session_ids:** Danh s√°ch ID c·ªßa c√°c phi√™n ch·ª©a b·∫±ng ch·ª©ng, d√πng ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa vi·ªác nh·ªõ l·∫°i ·ªü c·∫•p ƒë·ªô phi√™n.ÓàÜ
    

  

**Thi·∫øt l·∫≠p M√¥i Tr∆∞·ªùng:**

  

ƒê·ªÉ s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu, b·∫°n c√≥ th·ªÉ t·∫£i xu·ªëng t·ª´ [Hugging Face](https://huggingface.co/datasets/xiaowu0162/longmemeval) v√† gi·∫£i n√©n v√†o th∆∞ m·ª•c `data/`. Khuy·∫øn ngh·ªã s·ª≠ d·ª•ng m√¥i tr∆∞·ªùng conda ƒë·ªÉ c√†i ƒë·∫∑t c√°c y√™u c·∫ßu c·∫ßn thi·∫øt:ÓàÜ

  

```Bash
conda create -n longmemeval python=3.9
conda activate longmemeval
pip install -r requirements-full.txt
```

  

ÓàÜ

  

**ƒê√°nh Gi√° H·ªá Th·ªëng:**

  

ƒê·ªÉ ki·ªÉm tra h·ªá th·ªëng c·ªßa b·∫°n tr√™n LongMemEval, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c t·∫≠p l·ªánh ƒë√°nh gi√° ƒë∆∞·ª£c cung c·∫•p. L∆∞u ƒë·∫ßu ra c·ªßa h·ªá th·ªëng v√†o t·ªáp JSONL v·ªõi m·ªói d√≤ng ch·ª©a hai tr∆∞·ªùng: `question_id` v√† `hypothesis`. Sau ƒë√≥, ch·∫°y t·∫≠p l·ªánh ƒë√°nh gi√°:ÓàÜ

  

```Bash
export OPENAI_API_KEY=YOUR_API_KEY
cd src/evaluation
python3 evaluate_qa.py gpt-4o your_hypothesis_file ../../data/longmemeval_oracle.json
```

  

ÓàÜ

  

T·∫≠p l·ªánh n√†y s·∫Ω l∆∞u nh·∫≠t k√Ω ƒë√°nh gi√° v√†o t·ªáp `[your_hypothesis_file].log`. B·∫°n c√≥ th·ªÉ t·ªïng h·ª£p c√°c ƒëi·ªÉm s·ªë t·ª´ nh·∫≠t k√Ω b·∫±ng l·ªánh:ÓàÜ

  

```Bash
python3 print_qa_metrics.py gpt-4o your_hypothesis_file.log ../../data/longmemeval_oracle.json
```

  

ÓàÜ

  

**T·∫°o L·ªãch S·ª≠ Tr√≤ Chuy·ªán T√πy Ch·ªânh:**

  

LongMemEval h·ªó tr·ª£ bi√™n so·∫°n l·ªãch s·ª≠ tr√≤ chuy·ªán v·ªõi ƒë·ªô d√†i t√πy √Ω cho m·ªói tr∆∞·ªùng h·ª£p c√¢u h·ªèi, cho ph√©p b·∫°n d·ªÖ d√†ng tƒÉng ƒë·ªô kh√≥. ƒê·ªÉ t·∫°o l·ªãch s·ª≠ t√πy ch·ªânh, b·∫°n c√≥ th·ªÉ l√†m theo ƒë·ªãnh d·∫°ng trong `2_questions` v√† `6_session_cache` ƒë·ªÉ t·∫°o c√¢u h·ªèi v√† c√°c phi√™n b·∫±ng ch·ª©ng, sau ƒë√≥ ch·∫°y t·∫≠p l·ªánh `sample_haystack_and_timestamp.py` v·ªõi c√°c tham s·ªë ph√π h·ª£p.ÓàÜ

  

**Ch·∫°y Th·ª≠ Nghi·ªám H·ªá Th·ªëng Ghi Nh·ªõ:**

  

Ch√∫ng t√¥i cung c·∫•p m√£ th·ª≠ nghi·ªám cho vi·ªác truy xu·∫•t b·ªô nh·ªõ v√† t·∫°o c√¢u tr·∫£ l·ªùi c√≥ h·ªó tr·ª£ truy xu·∫•t d∆∞·ªõi c√°c th∆∞ m·ª•c `src/retrieval

  

  

---

Long-TermMemoryMethods Toequipchatassistantswithlong-termmemorycapabilities, three major techniques are commonly explored. The first approach involves directly adapting LLMs to process extensive history information as long-context inputs (Beltagy et al., 2020; Kitaev et al., 2020; Fu et al., 2024; An et al., 2024). While this method avoids the need for complex architectures, it is inefficient and susceptible to the ‚Äúlost-in-the-middle‚Äù phenomenon, where the ability of LLMs to utilize contextual information weakens as the input length grows (Shi et al., 2023; Liu et al., 2024). A second line of research integrates differentiable memory modules into language models, proposing specialized architectural designs and training strategies to enhance memory capabilities (Weston et al., 2014; Wu et al., 2022; Zhong et al., 2022; Wang et al., 2023). Lastly, several studies approach long-term memory from the perspective of context compression, developing techniques 3 Published as a conference paper at ICLR 2025 to condense lengthy histories into compact representations, whether in the form of LLM internal representations (Mu et al., 2023; Chevalier et al., 2023), discrete tokens (Jiang et al., 2023; Xu et al., 2024), or retrievable text segments via retrieval-augmented generation (RAG, Shi et al. (2024); Wang et al. (2023); Sarthi et al. (2024); Chen et al. (2023a); Guti¬¥ errez et al. (2024)). Although LONGMEMEVAL can evaluate any memory system, we will take an online context compression perspective, where each history interaction session is sequentially processed, stored, and accessed on-demand through indexing and retrieval mechanisms (¬ß4). This formulation aligns with current literature (Zhong et al., 2024; Guti¬¥ errez et al., 2024) and commercial systems (OpenAI, 2024; Coze, 2024). Its plug-and-play nature also facilitates the integration into existing chat assistant systems

  

  

D∆∞·ªõi ƒë√¢y l√† b·∫£n d·ªãch ti·∫øng Vi·ªát ƒëo·∫°n vƒÉn b·∫°n cung c·∫•p:

  

---

  

### **C√°c ph∆∞∆°ng ph√°p tr√≠ nh·ªõ d√†i h·∫°n (Long-Term Memory Methods)**

  

ƒê·ªÉ trang b·ªã kh·∫£ nƒÉng ghi nh·ªõ d√†i h·∫°n cho c√°c tr·ª£ l√Ω h·ªôi tho·∫°i, hi·ªán c√≥ ba k·ªπ thu·∫≠t ch√≠nh th∆∞·ªùng ƒë∆∞·ª£c nghi√™n c·ª©u:

  

1. **Ph∆∞∆°ng ph√°p th·ª© nh·∫•t** l√† ƒëi·ªÅu ch·ªânh tr·ª±c ti·∫øp c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs) ƒë·ªÉ x·ª≠ l√Ω l∆∞·ª£ng l·ªõn th√¥ng tin l·ªãch s·ª≠ d∆∞·ªõi d·∫°ng ƒë·∫ßu v√†o d√†i (long-context input) _(Beltagy et al., 2020; Kitaev et al., 2020; Fu et al., 2024; An et al., 2024)_. Ph∆∞∆°ng ph√°p n√†y gi√∫p tr√°nh vi·ªác ph·∫£i thi·∫øt k·∫ø ki·∫øn tr√∫c ph·ª©c t·∫°p, tuy nhi√™n l·∫°i **k√©m hi·ªáu qu·∫£** v√† d·ªÖ g·∫∑p hi·ªán t∆∞·ª£ng **"m·∫•t th√¥ng tin ·ªü gi·ªØa" (lost-in-the-middle)** ‚Äì khi m√† kh·∫£ nƒÉng c·ªßa LLM trong vi·ªác t·∫≠n d·ª•ng th√¥ng tin ng·ªØ c·∫£nh suy gi·∫£m theo ƒë·ªô d√†i ƒë·∫ßu v√†o tƒÉng l√™n _(Shi et al., 2023; Liu et al., 2024)_.
    

2. **H∆∞·ªõng nghi√™n c·ª©u th·ª© hai** l√† t√≠ch h·ª£p c√°c **module b·ªô nh·ªõ ph√¢n bi·ªát ƒë∆∞·ª£c (differentiable memory modules)** v√†o trong m√¥ h√¨nh ng√¥n ng·ªØ. C√°c nghi√™n c·ª©u n√†y ƒë·ªÅ xu·∫•t c√°c thi·∫øt k·∫ø ki·∫øn tr√∫c chuy√™n bi·ªát v√† chi·∫øn l∆∞·ª£c hu·∫•n luy·ªán nh·∫±m tƒÉng c∆∞·ªùng kh·∫£ nƒÉng ghi nh·ªõ c·ªßa m√¥ h√¨nh _(Weston et al., 2014; Wu et al., 2022; Zhong et al., 2022; Wang et al., 2023)_.
    

3. **Cu·ªëi c√πng**, nhi·ªÅu nghi√™n c·ª©u ti·∫øp c·∫≠n tr√≠ nh·ªõ d√†i h·∫°n t·ª´ g√≥c ƒë·ªô **n√©n ng·ªØ c·∫£nh (context compression)**, ph√°t tri·ªÉn c√°c k·ªπ thu·∫≠t nh·∫±m **tinh g·ªçn l·ªãch s·ª≠ h·ªôi tho·∫°i d√†i** th√†nh c√°c bi·ªÉu di·ªÖn nh·ªè g·ªçn h∆°n ‚Äì c√≥ th·ªÉ d∆∞·ªõi d·∫°ng bi·ªÉu di·ªÖn n·ªôi t·∫°i trong LLM _(Mu et al., 2023; Chevalier et al., 2023)_, c√°c token r·ªùi r·∫°c _(Jiang et al., 2023; Xu et al., 2024)_, ho·∫∑c c√°c ƒëo·∫°n vƒÉn b·∫£n c√≥ th·ªÉ truy xu·∫•t ƒë∆∞·ª£c th√¥ng qua k·ªπ thu·∫≠t sinh c√≥ h·ªó tr·ª£ truy h·ªìi (Retrieval-Augmented Generation - RAG) _(Shi et al., 2024; Wang et al., 2023; Sarthi et al., 2024; Chen et al., 2023a; Guti√©rrez et al., 2024)_.
    

  

M·∫∑c d√π **LONGMEMEVAL** c√≥ th·ªÉ ƒë∆∞·ª£c d√πng ƒë·ªÉ ƒë√°nh gi√° b·∫•t k·ª≥ h·ªá th·ªëng tr√≠ nh·ªõ n√†o,

trong b√†i n√†y ch√∫ng t√¥i ch·ªçn c√°ch ti·∫øp c·∫≠n theo h∆∞·ªõng **n√©n ng·ªØ c·∫£nh tr·ª±c tuy·∫øn (online context compression)**,

n∆°i m√† m·ªói phi√™n t∆∞∆°ng t√°c trong l·ªãch s·ª≠ s·∫Ω ƒë∆∞·ª£c **x·ª≠ l√Ω tu·∫ßn t·ª±, l∆∞u tr·ªØ v√† truy xu·∫•t theo y√™u c·∫ßu** th√¥ng qua c√°c c∆° ch·∫ø ƒë√°nh ch·ªâ m·ª•c (indexing) v√† truy h·ªìi (retrieval) (¬ß4).

  

C√°ch ti·∫øp c·∫≠n n√†y ph√π h·ª£p v·ªõi c√°c c√¥ng tr√¨nh hi·ªán t·∫°i _(Zhong et al., 2024; Guti√©rrez et al., 2024)_

c≈©ng nh∆∞ c√°c h·ªá th·ªëng th∆∞∆°ng m·∫°i nh∆∞ **OpenAI (2024)** v√† **Coze (2024)**.

ƒê·∫∑c bi·ªát, nh·ªù v√†o t√≠nh **"plug-and-play"** (c·∫Øm v√†o l√† ch·∫°y), ph∆∞∆°ng ph√°p n√†y c√≥ th·ªÉ d·ªÖ d√†ng t√≠ch h·ª£p v√†o c√°c h·ªá th·ªëng tr·ª£ l√Ω h·ªôi tho·∫°i hi·ªán c√≥.

  

---

  

N·∫øu b·∫°n mu·ªën m√¨nh t√≥m l·∫°i th√†nh b·∫£ng so s√°nh 3 h∆∞·ªõng ti·∫øp c·∫≠n ho·∫∑c bi·ªÉu ƒë·ªì s∆° ƒë·ªì h√≥a th√¨ m√¨nh c√≥ th·ªÉ v·∫Ω li·ªÅn nh√©!

  

## 1.4 ƒê√≥ng g√≥p c·ªßa ƒë·ªì √°n

ƒê·ªì √°n n√†y c√≥ 2 ƒë√≥ng g√≥p ch√≠nh nh∆∞ sau:

1. ƒê·ªì √°n ƒë·ªÅ xu·∫•t gi·∫£i ph√°p k·∫øt h·ª£p c√°c k·ªπ thu·∫≠t ph√¢n ƒëo·∫°n kh√°c nhau nh·∫±m tƒÉng
    

hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng truy xu·∫•t th√¥ng tin.

2. Th·ª±c hi·ªán th·ª≠ nghi·ªám k·∫øt h·ª£p c√°c k·ªπ thu·∫≠t truy xu·∫•t nh·∫±m c·∫£i thi·ªán k·∫øt qu·∫£
    

ƒë·∫ßu ra.

## 1.5 B·ªë c·ª•c ƒë·ªì √°n

To√†n b·ªô b√°o c√°o ƒë·ªì √°n t·ªët nghi·ªáp ƒë∆∞·ª£c tri·ªÉn khai trong 5 ch∆∞∆°ng. C√°c ch∆∞∆°ng

c√≤n l·∫°i c·ªßa b√°o c√°o c√≥ n·ªôi dung nh∆∞ sau.

Ch∆∞∆°ng 2 ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c n·ªôi dung l√Ω thuy·∫øt nh·∫±m ph·ª•c v·ª• vi·ªác nghi√™n c·ª©u, x√¢y

d·ª±ng th·ª≠ nghi·ªám v√† ƒë√°nh gi√° gi·∫£i ph√°p ƒë·ªÅ xu·∫•t. Trong ch∆∞∆°ng n√†y, t√¥i s·∫Ω tr√¨nh b√†y

t·ªïng quan v·ªÅ m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn, c√°c ·ª©ng d·ª•ng, h·∫°n ch·∫ø v√† m·ªôt s·ªë d√≤ng m√¥

h√¨nh ng√¥n ng·ªØ l·ªõn ph·ªï bi·∫øn. K·ªπ thu·∫≠t RAG v·ªõi c√°c th√†nh ph·∫ßn v√† c√°c gi·∫£i ph√°p

hi·ªán c√≥ c≈©ng s·∫Ω ƒë∆∞·ª£c ph√¢n t√≠ch chi ti·∫øt ·ªü ch∆∞∆°ng n√†y.

Ch∆∞∆°ng 3 tr√¨nh b√†y chi ti·∫øt v·ªÅ gi·∫£i ph√°p ƒë·ªÅ xu·∫•t. Tr∆∞·ªõc h·∫øt, t√¥i m√¥ t·∫£ t·ªïng quan

v·ªÅ lu·ªìng x·ª≠ l√Ω, sau ƒë√≥ l√† ƒëi s√¢u v√†o t·ª´ng m√¥-ƒëun. Trong m√¥-ƒëun ph√¢n ƒëo·∫°n, t√¥i

tr√¨nh b√†y hai k·ªπ thu·∫≠t ph√¢n ƒëo·∫°n t√¥i l·∫•y l√†m √Ω t∆∞·ªüng ƒë√≥ l√† ph√¢n ƒëo·∫°n s·ª≠ d·ª•ng m√¥

h√¨nh ng√¥n ng·ªØ l·ªõn v√† RAPTOR. Sau ƒë√≥, t√¥i ƒë·ªÅ xu·∫•t vi·ªác k·∫øt h·ª£p hai k·ªπ thu·∫≠t n√†y

ƒë·ªÉ b·ªï tr·ª£ cho nhau. Trong m√¥-ƒëun truy xu·∫•t, t√¥i tr√¨nh b√†y vi·ªác k·∫øt h·ª£p hai k·ªπ thu·∫≠t

ƒë√≥ l√†: i) t√¨m ki·∫øm m·ª©c ng·ªØ nghƒ©a v√† ii) t√¨m ki·∫øm m·ª©c t·ª´ v·ª±ng nh·∫±m c·∫£i thi·ªán m·ª©c

ƒë·ªô ph√π h·ª£p c·ªßa c√°c t√†i li·ªáu t√¨m ki·∫øm ƒë∆∞·ª£c.

Ch∆∞∆°ng 4 tr√¨nh b√†y c·ª• th·ªÉ v·ªÅ c√°c k·ªãch b·∫£n th·ª≠ nghi·ªám, th√¥ng s·ªë c·∫•u h√¨nh th·ª≠

nghi·ªám, k·∫øt qu·∫£ th·ª±c nghi·ªám v√† c√°c ƒë√°nh gi√°, nh·∫≠n x√©t v·ªÅ c√°c ph∆∞∆°ng ph√°p th·ª≠

nghi·ªám. Trong ch∆∞∆°ng n√†y, t√¥i s·ª≠ d·ª•ng m·ªôt s·ªë ƒë·ªô ƒëo t·ª± ƒë·ªông th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng

cho h·ªèi ƒë√°p v√† ƒë√°nh gi√° b·∫±ng m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn. Nh·ªØng nh·∫≠n x√©t v√† ƒë√°nh gi√°

hi·ªáu nƒÉng c·ªßa ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t so v·ªõi c√°c ph∆∞∆°ng ph√°p tham chi·∫øu c≈©ng ƒë∆∞·ª£c

tr√¨nh b√†y t·∫°i ch∆∞∆°ng n√†y.

Ch∆∞∆°ng 5 l√† ch∆∞∆°ng cu·ªëi c√πng. Trong ch∆∞∆°ng n√†y, t√¥i n√™u ra k·∫øt lu·∫≠n v·ªÅ ph∆∞∆°ng

ph√°p ƒë·ªÅ xu·∫•t, nh·ªØng ∆∞u ƒëi·ªÉm c≈©ng nh∆∞ nh·ªØng h·∫°n ch·∫ø c√≤n t·ªìn t·∫°i c≈©ng nh∆∞ ƒë·ªÅ ra

c√°c h∆∞·ªõng ph√°t tri·ªÉn trong t∆∞∆°ng lai.

  

---

  

## **üìå 2. T·ªïng quan nghi√™n c·ª©u (Related Work)**

  

### **2.1. H·∫°n ch·∫ø c·ªßa LLMs v·ªÅ tr√≠ nh·ªõ**

  

- LLMs hi·ªán nay **ch·ªâ c√≥ tr√≠ nh·ªõ ng·∫Øn h·∫°n**, b·ªã gi·ªõi h·∫°n b·ªüi context window (128K tokens v·ªõi GPT-4-turbo, 1M tokens v·ªõi Claude 3). - 2M r·∫•t to
    
- C√°c m√¥ h√¨nh kh√¥ng th·ªÉ duy tr√¨ b·ªëi c·∫£nh h·ªôi tho·∫°i **qua nhi·ªÅu phi√™n l√†m vi·ªác**.
    

  

### **2.2. C√°c ph∆∞∆°ng ph√°p hi·ªán t·∫°i**

  

#### **(1) LLMs l∆∞u tr·ªØ ng·∫Øn h·∫°n

  

  

  

#### **(2) Retrieval-Augmented Generation (RAG)**

  

- **∆Øu ƒëi·ªÉm**: LLM c√≥ th·ªÉ truy xu·∫•t d·ªØ li·ªáu t·ª´ ngu·ªìn ngo√†i khi c·∫ßn.
    
- **Nh∆∞·ª£c ƒëi·ªÉm**: Kh√¥ng nh·ªõ th√¥ng tin theo th·ªùi gian, ch·ªâ ho·∫°t ƒë·ªông khi c√≥ truy v·∫•n t√¨m ki·∫øm.
    

  

#### **(3) C√°c nghi√™n c·ª©u tr∆∞·ªõc ƒë√¢y**

  

- OpenAI ƒëang ph√°t tri·ªÉn **t√°c nh√¢n c√≥ tr√≠ nh·ªõ** nh∆∞ng ch∆∞a c√¥ng b·ªë chi ti·∫øt.
    
- Meta AI th·ª≠ nghi·ªám chatbot c√≥ kh·∫£ nƒÉng **nh·ªõ s·ªü th√≠ch ng∆∞·ªùi d√πng** nh∆∞ng g·∫∑p th√°ch th·ª©c v·ªÅ quy·ªÅn ri√™ng t∆∞.
    

![[Pasted image 20250322054143.png]]

  

üìå **ƒêi·ªÉm kh√°c bi·ªát c·ªßa nghi√™n c·ª©u n√†y:**

‚úÖ ƒê·ªÅ xu·∫•t m√¥ h√¨nh **Memory-Augmented AI** t·ªëi ∆∞u h∆°n, c√≥ th·ªÉ **h·ªçc h·ªèi theo th·ªùi gian m√† kh√¥ng b·ªã qu√° t·∫£i d·ªØ li·ªáu**.

‚úÖ K·∫øt h·ª£p gi·ªØa **Memory-Augmented Learning & RAG** ƒë·ªÉ t·ªëi ∆∞u h√≥a b·ªô nh·ªõ.

  

---

  

## **üìå 3. Ph∆∞∆°ng ph√°p nghi√™n c·ª©u (Methodology)**

  

### **3.1. Ki·∫øn tr√∫c ƒë·ªÅ xu·∫•t**

  

M√¥ h√¨nh **Memory-Augmented AI Agent** g·ªìm c√°c th√†nh ph·∫ßn ch√≠nh:

1Ô∏è‚É£ **Short-Term Memory (STM)**: L∆∞u tr·ªØ th√¥ng tin trong ph·∫°m vi c·ª≠a s·ªï ng·ªØ c·∫£nh hi·ªán t·∫°i.

2Ô∏è‚É£ **Long-Term Memory (LTM)**: L∆∞u tr·ªØ th√¥ng tin quan tr·ªçng v√†o **Vector Database**.

3Ô∏è‚É£ **Memory Management Algorithm**: Quy·∫øt ƒë·ªãnh **n√™n nh·ªõ g√¨, qu√™n g√¨**. (l∆∞u t·∫•t th√¨ b·ªã ph√¨ng b·ªô nh·ªõ? )

-b·ªè: Tr√≠ nh·ªõ v·ªÅ s·ªü th√≠ch

- b·ªè: Tr√≠ nh·ªõ v·ªÅ c√°c s·ª± ki·ªán ƒë√£ qua
    
- Tr√≠ nh·ªõ v·ªÅ c√°c l·ªãch s·∫Øp t·ªõi
    

4Ô∏è‚É£ **Knowledge Update Mechanism**: C·∫≠p nh·∫≠t v√† qu√™n th√¥ng tin c≈© khi c·∫ßn.

- C·∫≠p nh·∫≠t d·ª±a tr√™n th·ªùi gian (User ng√†y x∆∞a th√≠ch ch∆°i ƒë√° b√≥ng.G·∫´y ch√¢n => Hi·ªán t·∫°i th√¨ kh√¥ng).
    

  

üìå **M√¥ h√¨nh s·ª≠ d·ª•ng c√°c c√¥ng ngh·ªá:**

  

- **LLM (GPT-4, Claude 3, Llama 2)**.
    
- **Vector Database (FAISS, Pinecone, Weaviate)** ƒë·ªÉ l∆∞u tr√≠ nh·ªõ d√†i h·∫°n.
    
- **LangChain / LlamaIndex** ƒë·ªÉ qu·∫£n l√Ω truy xu·∫•t th√¥ng tin.
    

  

---

  

## **üìå 4. Th·ª±c nghi·ªám & K·∫øt qu·∫£ (Experiments & Results)**

  

### **4.1. Thi·∫øt l·∫≠p th·ª≠ nghi·ªám**

  

**B√†i to√°n:** So s√°nh hi·ªáu su·∫•t gi·ªØa **Memory-Augmented AI Agent** v√† **LLM th√¥ng th∆∞·ªùng** trong h·ªôi tho·∫°i d√†i h·∫°n.

  

üîπ **D·ªØ li·ªáu th·ª≠ nghi·ªám:**

  

- **T·∫≠p h·ªôi tho·∫°i th·ª±c t·∫ø** (chƒÉm s√≥c kh√°ch h√†ng, tr·ª£ l√Ω ·∫£o).
    
- **T·∫≠p h·ªôi tho·∫°i t·ªïng h·ª£p** (h·ªôi tho·∫°i k√©o d√†i > 10,000 tokens).
    

## 4. Th·ª±c nghi·ªám v√† ƒë√°nh gi√°

  

### 4.1 Deep Memory Retrieval (DMR)

  

- **DMR** (gi·ªõi thi·ªáu trong MemGPT) c√≥ 500 cu·ªôc h·ªôi tho·∫°i nhi·ªÅu phi√™n (multi-session).
    
- Zep ƒë·∫°t **94.8%** ƒë·ªô ch√≠nh x√°c khi d√πng GPT-4-turbo (v√† 98.2% khi d√πng m·ªôt bi·∫øn th·ªÉ GPT-4o-mini), nh·ªânh h∆°n so v·ªõi MemGPT (93.4%).
    
- Tuy nhi√™n, b·ªô DMR ch·ªâ c√≥ h·ªôi tho·∫°i kh√° ng·∫Øn (kho·∫£ng 60 tin nh·∫Øn m·ªói cu·ªôc), ch∆∞a th·ª±c s·ª± ki·ªÉm tra kh·∫£ nƒÉng ‚Äúsi√™u d√†i h·∫°n‚Äù.
    

  

### 4.2 LongMemEval (LME)

  

- **LongMemEval** c√≥ c√°c ƒëo·∫°n h·ªôi tho·∫°i d√†i h∆°n nhi·ªÅu (trung b√¨nh 115.000 tokens), m√¥ ph·ªèng t√¨nh hu·ªëng doanh nghi·ªáp th·ª±c t·∫ø ph·ª©c t·∫°p.
    

  

C√°c h·ªá th·ªëng tr·ª£ l√Ω tr√≤ chuy·ªán ng√¥n ng·ªØ l·ªõn g·∫ßn ƒë√¢y (LLM) c√≥ c√°c th√†nh ph·∫ßn b·ªô nh·ªõ t√≠ch h·ª£p ƒë·ªÉ theo d√µi l·ªãch s·ª≠ tr√≤ chuy·ªán c√≥ s·ª± h·ªó tr·ª£ c·ªßa ng∆∞·ªùi d√πng, cho ph√©p c√°c ph·∫£n h·ªìi ch√≠nh x√°c v√† c√° nh√¢n h√≥a h∆°n. Tuy nhi√™n, kh·∫£ nƒÉng b·ªô nh·ªõ d√†i h·∫°n c·ªßa h·ªç trong c√°c t∆∞∆°ng t√°c b·ªÅn v·ªØng v·∫´n ch∆∞a ƒë∆∞·ª£c khai th√°c. B√†i vi·∫øt n√†y gi·ªõi thi·ªáu Longmemeval, m·ªôt ƒëi·ªÉm chu·∫©n to√†n di·ªán ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ƒë√°nh gi√° nƒÉm kh·∫£ nƒÉng b·ªô nh·ªõ d√†i h·∫°n c·ªët l√µi c·ªßa c√°c tr·ª£ l√Ω tr√≤ chuy·ªán: tr√≠ch xu·∫•t th√¥ng tin, l√Ω lu·∫≠n ƒëa phi√™n, l√Ω lu·∫≠n th·ªùi gian, c·∫≠p nh·∫≠t ki·∫øn th·ª©c v√† ki√™ng khem. V·ªõi 500 c√¢u h·ªèi ƒë∆∞·ª£c qu·∫£n l√Ω t·ªâ m·ªâ ƒë∆∞·ª£c nh√∫ng trong l·ªãch s·ª≠ tr√≤ chuy·ªán h·ªó tr·ª£ ng∆∞·ªùi d√πng c√≥ th·ªÉ m·ªü r·ªông, Longmemeval ƒë∆∞a ra m·ªôt th√°ch th·ª©c ƒë√°ng k·ªÉ ƒë·ªëi v·ªõi c√°c h·ªá th·ªëng b·ªô nh·ªõ d√†i h·∫°n hi·ªán c√≥, v·ªõi c√°c tr·ª£ l√Ω tr√≤ chuy·ªán th∆∞∆°ng m·∫°i v√† LLM b·ªëi c·∫£nh d√†i cho th·∫•y ƒë·ªô ch√≠nh x√°c gi·∫£m 30% khi ghi nh·ªõ th√¥ng tin qua c√°c t∆∞∆°ng t√°c ƒë∆∞·ª£c duy tr√¨. Sau ƒë√≥, ch√∫ng t√¥i tr√¨nh b√†y m·ªôt khung th·ªëng nh·∫•t ph√¢n chia thi·∫øt k·∫ø b·ªô nh·ªõ d√†i h·∫°n th√†nh b·ªën l·ª±a ch·ªçn thi·∫øt k·∫ø tr√™n c√°c giai ƒëo·∫°n l·∫≠p ch·ªâ m·ª•c, truy xu·∫•t v√† ƒë·ªçc. ƒê∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n nh·ªØng hi·ªÉu bi·∫øt th·ª≠ nghi·ªám quan tr·ªçng, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt s·ªë thi·∫øt k·∫ø b·ªô nh·ªõ bao g·ªìm ph√¢n t√°ch phi√™n ƒë·ªÉ t·ªëi ∆∞u h√≥a m·ª©c ƒë·ªô chi ti·∫øt gi√° tr·ªã, m·ªü r·ªông ch√≠nh ƒë∆∞·ª£c th·ª±c hi·ªán ƒë·ªÉ tƒÉng c∆∞·ªùng c·∫•u tr√∫c ch·ªâ s·ªë v√† m·ªü r·ªông truy v·∫•n th·ªùi gian ƒë·ªÉ tinh ch·ªânh ph·∫°m vi t√¨m ki·∫øm. K·∫øt qu·∫£ th·ª≠ nghi·ªám cho th·∫•y c√°c t·ªëi ∆∞u h√≥a n√†y c·∫£i thi·ªán ƒë√°ng k·ªÉ c·∫£ vi·ªác thu h·ªìi b·ªô nh·ªõ v√† tr·∫£ l·ªùi c√¢u h·ªèi h·∫° ngu·ªìn tr√™n longmemeval. Nh√¨n chung, nghi√™n c·ª©u c·ªßa ch√∫ng t√¥i cung c·∫•p c√°c ngu·ªìn l·ª±c v√† h∆∞·ªõng d·∫´n c√≥ gi√° tr·ªã ƒë·ªÉ th√∫c ƒë·∫©y kh·∫£ nƒÉng b·ªô nh·ªõ d√†i h·∫°n c·ªßa c√°c tr·ª£ l√Ω tr√≤ chuy·ªán d·ª±a tr√™n LLM, m·ªü ƒë∆∞·ªùng cho AI tr√≤ chuy·ªán c√° nh√¢n h√≥a v√† ƒë√°ng tin c·∫≠y h∆°n.

  

- Zep c·∫£i thi·ªán k·∫øt qu·∫£ so v·ªõi baseline (d√πng to√†n b·ªô h·ªôi tho·∫°i) ·ªü h·∫ßu h·∫øt c√°c lo·∫°i c√¢u h·ªèi, ƒë·∫∑c bi·ªát:
    
    - Lo·∫°i c√¢u ‚Äúmulti-session,‚Äù ‚Äúpreference,‚Äù ‚Äútemporal reasoning‚Äù tƒÉng ƒë√°ng k·ªÉ.
        
    - ƒê·ªô tr·ªÖ (latency) gi·∫£m ƒë·∫øn 90% so v·ªõi vi·ªác nh√©t to√†n b·ªô h·ªôi tho·∫°i v√†o prompt (v√¨ prompt c·ªßa Zep ng·∫Øn g·ªçn h∆°n).
        

üîπ **Ti√™u ch√≠ ƒë√°nh gi√°:**

  

|   |   |   |
|---|---|---|
|**Ti√™u ch√≠**|**Memory-Augmented AI**|**LLM th√¥ng th∆∞·ªùng**|
|**Kh·∫£ nƒÉng duy tr√¨ b·ªëi c·∫£nh**|‚úÖ T·ªët|‚ùå K√©m|
|**ƒê·ªô ch√≠nh x√°c ph·∫£n h·ªìi**|‚úÖ Cao h∆°n|‚ùå Gi·∫£m khi h·ªôi tho·∫°i d√†i|
|**T·ªëc ƒë·ªô ph·∫£n h·ªìi**|‚ùå Ch·∫≠m h∆°n|‚úÖ Nhanh h∆°n|
|**Kh·∫£ nƒÉng c√° nh√¢n h√≥a**|‚úÖ C√≥ th·ªÉ nh·ªõ s·ªü th√≠ch ng∆∞·ªùi d√πng|‚ùå Kh√¥ng nh·ªõ th√¥ng tin c≈©|

  

Chi ti·∫øt c√°c ti√™u ch√≠ ƒë√°nh gi√°:

  

- **Tr√≠ch xu·∫•t th√¥ng tin (Information Extraction)**: Kh·∫£ nƒÉng nh·ªõ l·∫°i th√¥ng tin c·ª• th·ªÉ t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c d√†i, bao g·ªìm c·∫£ chi ti·∫øt ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p b·ªüi ng∆∞·ªùi d√πng ho·∫∑c tr·ª£ l√Ω.[Di Wu](https://xiaowu0162.github.io/long-mem-eval/?utm_source=chatgpt.com)
    

- **Suy lu·∫≠n ƒëa phi√™n (Multi-Session Reasoning)**: Kh·∫£ nƒÉng t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu phi√™n l·ªãch s·ª≠ ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi ph·ª©c t·∫°p li√™n quan ƒë·∫øn vi·ªác t·ªïng h·ª£p v√† so s√°nh.
    

- **Suy lu·∫≠n th·ªùi gian (Temporal Reasoning)**: Nh·∫≠n th·ª©c v·ªÅ c√°c kh√≠a c·∫°nh th·ªùi gian c·ªßa th√¥ng tin ng∆∞·ªùi d√πng, bao g·ªìm c·∫£ c√°c ƒë·ªÅ c·∫≠p th·ªùi gian r√µ r√†ng v√† si√™u d·ªØ li·ªáu d·∫•u th·ªùi gian trong c√°c t∆∞∆°ng t√°c.
    

- **C·∫≠p nh·∫≠t ki·∫øn th·ª©c (Knowledge Updates)**: Kh·∫£ nƒÉng nh·∫≠n bi·∫øt c√°c thay ƒë·ªïi trong th√¥ng tin c√° nh√¢n c·ªßa ng∆∞·ªùi d√πng v√† c·∫≠p nh·∫≠t ki·∫øn th·ª©c v·ªÅ ng∆∞·ªùi d√πng m·ªôt c√°ch ƒë·ªông theo th·ªùi gian.
    

- **T·ª´ ch·ªëi tr·∫£ l·ªùi (Abstention)**: Kh·∫£ nƒÉng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn th√¥ng tin kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong l·ªãch s·ª≠ t∆∞∆°ng t√°c, t·ª©c l√† th√¥ng tin kh√¥ng ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn trong l·ªãch s·ª≠ t∆∞∆°ng t√°c.
    

### **4.2. K·∫øt qu·∫£ th·ª±c nghi·ªám**

  

üìå **Memory-Augmented AI c·∫£i thi·ªán 38% kh·∫£ nƒÉng duy tr√¨ b·ªëi c·∫£nh h·ªôi tho·∫°i so v·ªõi LLM th√¥ng th∆∞·ªùng.**

üìå **T·ªëc ƒë·ªô ph·∫£n h·ªìi ch·∫≠m h∆°n ~10% nh∆∞ng ƒë·ªô ch√≠nh x√°c tƒÉng 25%.**

  

---

  

## **üìå 5. K·∫øt lu·∫≠n & H∆∞·ªõng ph√°t tri·ªÉn (Conclusion & Future Work)**

  

### **5.1. K·∫øt lu·∫≠n**

  

- **Memory-Augmented AI Agents c√≥ th·ªÉ c·∫£i thi·ªán ƒë√°ng k·ªÉ kh·∫£ nƒÉng duy tr√¨ h·ªôi tho·∫°i d√†i h·∫°n.**
    
- **H·∫°n ch·∫ø c·ªßa m√¥ h√¨nh l√† t·ªëc ƒë·ªô ph·∫£n h·ªìi, nh∆∞ng c√≥ th·ªÉ t·ªëi ∆∞u h√≥a.**
    

  

### **5.2. H∆∞·ªõng ph√°t tri·ªÉn**

  

‚úÖ **T·ªëi ∆∞u thu·∫≠t to√°n qu·∫£n l√Ω b·ªô nh·ªõ** ƒë·ªÉ c·∫£i thi·ªán t·ªëc ƒë·ªô.

‚úÖ **K·∫øt h·ª£p v·ªõi RAG** ƒë·ªÉ AI c√≥ th·ªÉ truy xu·∫•t th√¥ng tin t·ª´ d·ªØ li·ªáu ngo√†i.

‚úÖ **M·ªü r·ªông th·ª≠ nghi·ªám tr√™n nhi·ªÅu lƒ©nh v·ª±c** nh∆∞ gi√°o d·ª•c, chƒÉm s√≥c s·ª©c kh·ªèe.

  

---

  

## **üìå 6. T√†i li·ªáu tham kh·∫£o (References)**

  

- [KARMA: Memory-Augmented AI Research](https://arxiv.org/abs/2409.14908)
    
- [AriGraph: Knowledge Memory for LLMs](https://arxiv.org/abs/2407.04363)
    
- [Meta AI‚Äôs Memory-Augmented Chatbot](https://www.theverge.com/2025/1/27/24352992/meta-ai-memory-personalization)
    

  

  

---

  

25/03/2025

  

Tuy·ªát v·ªùi. M√¨nh s·∫Ω vi·∫øt m·ªôt survey th·∫≠t chi ti·∫øt v·ªÅ c√°c h∆∞·ªõng nghi√™n c·ª©u Long-Term Memory trong h·ªá th·ªëng h·ªôi tho·∫°i, t·ª´ giai ƒëo·∫°n s∆° khai ƒë·∫øn c√°c k·ªπ thu·∫≠t hi·ªán ƒë·∫°i nh∆∞ memory modules, context compression, v√† c√°c h·ªá personalized memory. B√†i vi·∫øt s·∫Ω ph√¢n t√≠ch c√°c c·ªôt m·ªëc quan tr·ªçng, c√°c baseline n·ªïi b·∫≠t, v√† nh·ªØng benchmark m·ªõi nh·∫•t nh∆∞ LongMemEval, nh·∫±m gi√∫p b·∫°n c√≥ n·ªÅn t·∫£ng v·ªØng ch·∫Øc ƒë·ªÉ ph√°t tri·ªÉn th√†nh m·ªôt b√†i b√°o khoa h·ªçc.

  

M√¨nh s·∫Ω tr√¨nh b√†y l·∫°i theo d·∫°ng academic report chu·∫©n, bao g·ªìm c√°c m·ª•c ti√™u kh·∫£o s√°t, ph√¢n lo·∫°i c√°c ph∆∞∆°ng ph√°p, so s√°nh, v√† g·ª£i √Ω h∆∞·ªõng m·ªü r·ªông. M√¨nh s·∫Ω g·ª≠i l·∫°i b·∫£n ƒë·∫ßy ƒë·ªß sau khi ho√†n t·∫•t.

  

# Gi·ªõi thi·ªáu

  

C√°c h·ªá th·ªëng h·ªèi-ƒë√°p (QA) v√† ƒë·ªëi tho·∫°i s·ªõm th∆∞·ªùng **kh√¥ng c√≥ c∆° ch·∫ø b·ªô nh·ªõ d√†i h·∫°n**, x·ª≠ l√Ω m·ªói truy v·∫•n ƒë·ªôc l·∫≠p m√† kh√¥ng l∆∞u l·∫°i th√¥ng tin cu·ªôc h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥. Ch·∫≥ng h·∫°n, m√¥ h√¨nh ƒë·ªçc hi·ªÉu BiDAF (Bi-Directional Attention Flow) v√† c√°c bi·∫øn th·ªÉ c·∫£i ti·∫øn (BiDAF++) ƒë∆∞·ª£c d√πng cho SQuAD v√† c√°c b·ªô d·ªØ li·ªáu QA tr∆∞·ªõc nƒÉm 2019 ch·ªâ ch√∫ tr·ªçng vi·ªác t√¨m ƒë√°p √°n trong m·ªôt ƒëo·∫°n vƒÉn b·∫£n ng·∫Øn, kh√¥ng l∆∞u gi·ªØ ng·ªØ c·∫£nh h·ªôi tho·∫°i ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=4,representation%20generated%20when%20answering%20previous)). T∆∞∆°ng t·ª±, h·ªá th·ªëng DrQA c·ªßa Facebook (2017) th·ª±c hi·ªán QA m·ªü tr√™n Wikipedia b·∫±ng c√°ch truy xu·∫•t v√† ƒë·ªçc t√†i li·ªáu, nh∆∞ng m·ªói c√¢u h·ªèi ƒë·ªÅu ƒë∆∞·ª£c tr·∫£ l·ªùi t√°ch bi·ªát, kh√¥ng c√≥ k√Ω ·ª©c v·ªÅ c√°c c√¢u h·ªèi tr∆∞·ªõc ƒë√≥ ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=,JASIS%2C%2038%3A389%E2%80%93404%2C%201987)). Khi c√°c tr·ª£ l√Ω ·∫£o v√† chatbot tr·ªü n√™n ph·ªï bi·∫øn, h·∫°n ch·∫ø ‚Äú_tr√≠ nh·ªõ c√° v√†ng_‚Äù n√†y b·ªôc l·ªô r√µ: m√¥ h√¨nh d·ªÖ l·∫∑p l·∫°i c√¢u h·ªèi, qu√™n th√¥ng tin ng∆∞·ªùi d√πng cung c·∫•p tr∆∞·ªõc ƒë√≥, ho·∫∑c kh√¥ng th·ªÉ duy tr√¨ t√≠nh nh·∫•t qu√°n qua nhi·ªÅu l∆∞·ª£t t∆∞∆°ng t√°c. Do ƒë√≥, **tr√≠ nh·ªõ d√†i h·∫°n trong h·ªôi tho·∫°i** (long-term memory) ƒë√£ tr·ªü th√†nh m·ªôt h∆∞·ªõng nghi√™n c·ª©u quan tr·ªçng, h∆∞·ªõng ƒë·∫øn vi·ªác gi√∫p h·ªá th·ªëng **ghi nh·ªõ th√¥ng tin xuy√™n su·ªët c√°c phi√™n tr√≤ chuy·ªán** v√† c√° nh√¢n h√≥a ph·∫£n h·ªìi theo l·ªãch s·ª≠ t∆∞∆°ng t√°c.

  

**Memory-Augmented Conversational Systems** (h·ªá th·ªëng ƒë·ªëi tho·∫°i tƒÉng c∆∞·ªùng b·ªô nh·ªõ) l√† c√°c m√¥ h√¨nh ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ kh·∫Øc ph·ª•c h·∫°n ch·∫ø tr√™n b·∫±ng c√°ch t√≠ch h·ª£p m·ªôt th√†nh ph·∫ßn b·ªô nh·ªõ v√†o pipeline ƒë·ªëi tho·∫°i ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=integrated%20memory%20components%20to%20track,context%20LLMs)). ƒêi·ªÅu n√†y cho ph√©p chatbot _ghi nh·ªõ v√† s·ª≠ d·ª•ng l·∫°i_ c√°c th√¥ng tin tr∆∞·ªõc ƒë√≥ ‚Äì v√≠ d·ª• nh∆∞ s·ªü th√≠ch, ti·ªÉu s·ª≠ ng∆∞·ªùi d√πng, t√¨nh hu·ªëng ƒë√£ x·∫£y ra ‚Äì nh·∫±m t·∫°o ra ph·∫£n h·ªìi ch√≠nh x√°c h∆°n v√† c√≥ t√≠nh c√° nh√¢n h√≥a. B√†i survey n√†y s·∫Ω tr√¨nh b√†y chi ti·∫øt s·ª± ph√°t tri·ªÉn c·ªßa lƒ©nh v·ª±c n√†y: t·ª´ nh·ªØng m√¥ h√¨nh QA _ti·ªÅn 2019_ kh√¥ng c√≥ tr√≠ nh·ªõ d√†i h·∫°n, ƒë·∫øn c√°c h·ªá th·ªëng _hi·ªán ƒë·∫°i (2023-nay)_ c√≥ kh·∫£ nƒÉng ghi nh·ªõ ƒëa phi√™n, c·∫≠p nh·∫≠t ki·∫øn th·ª©c v√† suy lu·∫≠n th·ªùi gian. Ch√∫ng t√¥i ph√¢n t√≠ch ba h∆∞·ªõng ti·∫øp c·∫≠n ch√≠nh ƒë·ªÉ t√≠ch h·ª£p b·ªô nh·ªõ: (1) ƒë∆∞a to√†n b·ªô ng·ªØ c·∫£nh d√†i v√†o ƒë·∫ßu v√†o m√¥ h√¨nh (long-context input), (2) s·ª≠ d·ª•ng module b·ªô nh·ªõ ph√¢n bi·ªát c√≥ th·ªÉ hu·∫•n luy·ªán c√πng m√¥ h√¨nh (differentiable memory modules), v√† (3) n√©n ng·ªØ c·∫£nh v√† truy h·ªìi th√¥ng tin khi c·∫ßn (context compression & retrieval). B√™n c·∫°nh ƒë√≥, ch√∫ng t√¥i ƒëi·ªÉm qua c√°c m√¥ h√¨nh ti√™u bi·ªÉu ·ªü m·ªói giai ƒëo·∫°n nh∆∞ BiDAF++, DrQA, ORConvQA, MemoryBank, Theanine, LD-Agent‚Ä¶, so s√°nh m·ªôt s·ªë h·ªá th·ªëng n·ªÅn t·∫£ng (baseline) n·ªïi b·∫≠t nh∆∞ MemNN, **Keep Me Updated** v√† **LD-Agent**, c≈©ng nh∆∞ c√°c b·ªô d·ªØ li·ªáu v√† benchmark ƒë√°nh gi√° tr√≠ nh·ªõ ƒë·ªëi tho·∫°i (LongMemEval, LOCOMO, v.v.) c√πng c√°c ti√™u ch√≠ ƒë√°nh gi√° quan tr·ªçng (kh·∫£ nƒÉng nh·ªõ ‚Äì recall, ·∫£o gi√°c ‚Äì hallucination, c·∫≠p nh·∫≠t ki·∫øn th·ª©c ‚Äì knowledge update, suy lu·∫≠n th·ªùi gian ‚Äì temporal reasoning, _abstention_...). Cu·ªëi c√πng, ch√∫ng t√¥i th·∫£o lu·∫≠n nh·ªØng h∆∞·ªõng m·ªü r·ªông ƒë·∫ßy h·ª©a h·∫πn, ch·∫≥ng h·∫°n k·∫øt h·ª£p c∆° ch·∫ø **RAG** (Retrieval-Augmented Generation) v·ªõi c·∫≠p nh·∫≠t b·ªô nh·ªõ ƒë·ªông, truy h·ªìi th√≠ch ·ª©ng, hay s·ª≠ d·ª•ng m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) nh∆∞ m·ªôt module h·ªó tr·ª£ qu·∫£n l√Ω tr√≠ nh·ªõ.

  

# C√°c h∆∞·ªõng ti·∫øp c·∫≠n ch√≠nh ƒë·ªÉ t√≠ch h·ª£p tr√≠ nh·ªõ d√†i h·∫°n

  

C√≥ ba c√°ch ti·∫øp c·∫≠n ph·ªï bi·∫øn nh·∫±m trang b·ªã kh·∫£ nƒÉng nh·ªõ d√†i h·∫°n cho h·ªá th·ªëng h·ªôi tho·∫°i: (1) **M·ªü r·ªông ng·ªØ c·∫£nh ƒë·∫ßu v√†o (long-context input)** ‚Äì cung c·∫•p cho m√¥ h√¨nh m·ªôt chu·ªói h·ªôi tho·∫°i r·∫•t d√†i ƒë·ªÉ n√≥ t·ª± t√¨m th√¥ng tin c·∫ßn nh·ªõ; (2) **Module b·ªô nh·ªõ kh·∫£ vi (differentiable memory)** ‚Äì thi·∫øt k·∫ø m·ªôt ki·∫øn tr√∫c m·∫°ng n∆°-ron v·ªõi th√†nh ph·∫ßn b·ªô nh·ªõ ngo√†i c√≥ th·ªÉ ƒë·ªçc/ghi trong qu√° tr√¨nh hu·∫•n luy·ªán; (3) **N√©n v√† truy h·ªìi ng·ªØ c·∫£nh (context compression & retrieval)** ‚Äì t√≥m t·∫Øt ho·∫∑c l∆∞u tr·ªØ th√¥ng tin quan tr·ªçng t·ª´ h·ªôi tho·∫°i v√†o m·ªôt kho b·ªô nh·ªõ ngo√†i, v√† truy v·∫•n n√≥ khi c·∫ßn thi·∫øt cho ph·∫£n h·ªìi. D∆∞·ªõi ƒë√¢y, ch√∫ng t√¥i ph√¢n t√≠ch chi ti·∫øt t·ª´ng h∆∞·ªõng ti·∫øp c·∫≠n, c√πng c√°c v√≠ d·ª• m√¥ h√¨nh ti√™u bi·ªÉu.

  

## Ti·∫øp c·∫≠n 1: M·ªü r·ªông ng·ªØ c·∫£nh ƒë·∫ßu v√†o

  

C√°ch ƒë∆°n gi·∫£n nh·∫•t ƒë·ªÉ m√¥ h√¨nh ‚Äúnh·ªõ‚Äù l√† **cung c·∫•p to√†n b·ªô l·ªãch s·ª≠ h·ªôi tho·∫°i trong ph·∫ßn input** c·ªßa n√≥, nh·∫±m cho ph√©p m√¥ h√¨nh t·ª± truy xu·∫•t nh·ªØng chi ti·∫øt c·∫ßn thi·∫øt. Trong c√°c h·ªá QA/h·ªôi tho·∫°i truy·ªÅn th·ªëng, ƒëi·ªÅu n√†y th∆∞·ªùng t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác n·ªëi chu·ªói c√°c l∆∞·ª£t h·ªèi-ƒë√°p tr∆∞·ªõc v√†o c√¢u h·ªèi hi·ªán t·∫°i. V√≠ d·ª•, tr√™n b·ªô d·ªØ li·ªáu h·ªôi tho·∫°i ng·ªØ c·∫£nh CoQA/QuAC, m√¥ h√¨nh BiDAF++ ƒë√£ ƒë∆∞·ª£c c·∫£i ti·∫øn ƒë·ªÉ ch·∫•p nh·∫≠n th√™m 2 l∆∞·ª£t h·ªèi-ƒë√°p tr∆∞·ªõc ƒë√≥ l√†m ng·ªØ c·∫£nh, b√™n c·∫°nh ƒëo·∫°n vƒÉn c·∫ßn ƒë·ªçc ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=4,representation%20generated%20when%20answering%20previous)). Vi·ªác ƒë∆°n gi·∫£n n·ªëi th√™m l·ªãch s·ª≠ nh∆∞ v·∫≠y gi√∫p m√¥ h√¨nh tr·∫£ l·ªùi t·ªët h∆°n c√°c c√¢u h·ªèi ph·ª• thu·ªôc b·ªëi c·∫£nh (v√≠ d·ª• ƒë·∫°i t·ª´, tham chi·∫øu ƒë·∫øn th√¥ng tin nh·∫Øc ·ªü c√¢u h·ªèi tr∆∞·ªõc). T∆∞∆°ng t·ª±, trong ƒë·ªëi tho·∫°i m·ªü, m·ªôt s·ªë m√¥ h√¨nh d·ª±a tr√™n BERT/GPT ban ƒë·∫ßu c≈©ng th·ª±c hi·ªán b·∫±ng c√°ch **prepend** to√†n b·ªô n·ªôi dung cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥ v√†o prompt ƒë·∫ßu v√†o ·ªü m·ªói l∆∞·ª£t ƒë√°p.

  

C√πng v·ªõi s·ª± ph√°t tri·ªÉn c·ªßa c√°c Transformer c√≥ c·ª≠a s·ªï ng·ªØ c·∫£nh l·ªõn, h∆∞·ªõng ti·∫øp c·∫≠n n√†y ng√†y c√†ng t·ªè ra h·ªØu d·ª•ng h∆°n. C√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) hi·ªán nay nh∆∞ GPT-4 hay Claude c√≥ th·ªÉ ch·∫•p nh·∫≠n ng·ªØ c·∫£nh d√†i h√†ng ch·ª•c ngh√¨n token, cho ph√©p l∆∞u gi·ªØ nguy√™n v·∫πn n·ªôi dung nhi·ªÅu phi√™n tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥. Tuy nhi√™n, c√°ch l√†m n√†y **ƒë·ªëi m·∫∑t v·ªõi nh·ªØng h·∫°n ch·∫ø**: (i) Chi ph√≠ t√≠nh to√°n tƒÉng l√™n ƒë√°ng k·ªÉ khi ƒë·ªô d√†i input l·ªõn, g√¢y ch·∫≠m tr·ªÖ v√† t·ªën t√†i nguy√™n; (ii) M·∫∑c d√π input r·∫•t d√†i, m√¥ h√¨nh v·∫´n c√≥ th·ªÉ **‚Äúqu√™n‚Äù** c√°c chi ti·∫øt quan tr·ªçng ho·∫∑c **gi·∫£m ƒë·ªô ch√≠nh x√°c** khi ph·∫£i x·ª≠ l√Ω qu√° nhi·ªÅu th√¥ng tin kh√¥ng li√™n quan. Nghi√™n c·ª©u g·∫ßn ƒë√¢y cho th·∫•y ngay c·∫£ c√°c chat GPT c√≥ ng·ªØ c·∫£nh m·ªü r·ªông v·∫´n s·ª•t gi·∫£m ~30% ƒë·ªô ch√≠nh x√°c khi ph·∫£i ghi nh·ªõ th√¥ng tin tr·∫£i d√†i qua m·ªôt cu·ªôc tr√≤ chuy·ªán k√©o d√†i ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=meticulously%20curated%20questions%20embedded%20within,augmented%20key)). Nguy√™n nh√¢n l√† c∆° ch·∫ø t·ª± ch√∫ √Ω c√≥ khuynh h∆∞·ªõng t·∫≠p trung v√†o n·ªôi dung g·∫ßn th·ªùi ƒëi·ªÉm hi·ªán t·∫°i, c√≤n c√°c chi ti·∫øt t·ª´ r·∫•t l√¢u v·ªÅ tr∆∞·ªõc d√π n·∫±m trong input c≈©ng c√≥ th·ªÉ b·ªã lu m·ªù. Do ƒë√≥, m·ªü r·ªông ng·ªØ c·∫£nh ƒë·∫ßu v√†o _ch∆∞a ph·∫£i gi·∫£i ph√°p t·ªëi ∆∞u_ cho tr√≠ nh·ªõ d√†i h·∫°n, ƒë·∫∑c bi·ªát khi h·ªôi tho·∫°i k√©o d√†i h√†ng trƒÉm l∆∞·ª£t.

  

M·ªôt s·ªë c·∫£i ti·∫øn ƒë√£ ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t trong h∆∞·ªõng n√†y nh·∫±m gi√∫p m√¥ h√¨nh t·∫≠n d·ª•ng ng·ªØ c·∫£nh d√†i hi·ªáu qu·∫£ h∆°n. Ch·∫≥ng h·∫°n, **Transformer-XL** (Dai et al., 2019) gi·ªõi thi·ªáu c∆° ch·∫ø ghi nh·ªõ c√°c tr·∫°ng th√°i ·∫©n v√† t√°i s·ª≠ d·ª•ng ch√∫ng ·ªü c√°c ph√¢n ƒëo·∫°n sau, t·∫°o m·ªôt d·∫°ng _b·ªô nh·ªõ ng·∫Øn h·∫°n tr∆∞·ª£t_ h·ªó tr·ª£ k·∫øt n·ªëi ng·ªØ c·∫£nh d√†i. **Compressive Transformer** (Rae et al., 2019) ti·∫øn th√™m b∆∞·ªõc n·ªØa khi n√©n c√°c tr·∫°ng th√°i c≈© l·∫°i (v√≠ d·ª• l·∫•y m·∫´u ho·∫∑c trung b√¨nh) thay v√¨ b·ªè h·∫≥n, gi√∫p m√¥ h√¨nh c√≥ ‚Äúk√Ω ·ª©c t√≥m l∆∞·ª£c‚Äù v·ªÅ nh·ªØng ƒëo·∫°n r·∫•t xa. M·∫∑c d√π v·∫≠y, c√°c k·ªπ thu·∫≠t n√†y v·∫´n ho·∫°t ƒë·ªông trong khu√¥n kh·ªï tr·ªçng s·ªë m√¥ h√¨nh v√† ƒë·ªô d√†i ng·ªØ c·∫£nh c·ªë ƒë·ªãnh, ch·ª© ch∆∞a cung c·∫•p m·ªôt kho nh·ªõ linh ho·∫°t c√≥ th·ªÉ t√πy √Ω ƒë·ªçc ghi.

  

T√≥m l·∫°i, cung c·∫•p ng·ªØ c·∫£nh h·ªôi tho·∫°i d√†i v√†o tr·ª±c ti·∫øp m√¥ h√¨nh l√† c√°ch d·ªÖ d√†ng tri·ªÉn khai (kh√¥ng c·∫ßn thay ƒë·ªïi ki·∫øn tr√∫c), v√† c√≥ hi·ªáu qu·∫£ nh·∫•t ƒë·ªãnh trong c√°c t√¨nh hu·ªëng h·ªôi tho·∫°i ng·∫Øn ho·∫∑c trung b√¨nh. Nh∆∞ng v·ªõi c√°c t∆∞∆°ng t√°c l√¢u d√†i, ƒëa phi√™n, ph∆∞∆°ng ph√°p n√†y b·ªôc l·ªô h·∫°n ch·∫ø v·ªÅ c·∫£ hi·ªáu nƒÉng l·∫´n ƒë·ªô tin c·∫≠y. ƒêi·ªÅu ƒë√≥ d·∫´n t·ªõi nhu c·∫ßu v·ªÅ nh·ªØng ki·∫øn tr√∫c c√≥ **b·ªô nh·ªõ ngo√†i** r√µ r·ªát h∆°n, n·∫±m ngo√†i chu·ªói input ƒë∆°n thu·∫ßn ‚Äì v·ªën l√† n·ªôi dung c·ªßa hai h∆∞·ªõng ti·∫øp c·∫≠n sau.

  

## Ti·∫øp c·∫≠n 2: Module b·ªô nh·ªõ kh·∫£ vi (Differentiable Memory)

  

H∆∞·ªõng ti·∫øp c·∫≠n th·ª© hai t√≠ch h·ª£p tr√≠ nh·ªõ d√†i h·∫°n ngay trong **ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh** d∆∞·ªõi d·∫°ng m·ªôt module b·ªô nh·ªõ ƒë·∫∑c bi·ªát c√≥ th·ªÉ ƒë·ªçc/ghi th√¥ng tin. Kh√°c v·ªõi vi·ªác nh·ªìi nh√©t m·ªçi th·ª© v√†o input, ·ªü ƒë√¢y m√¥ h√¨nh c√≥ m·ªôt **b·ªô nh·ªõ r·ªùi** (external memory) ‚Äì v√≠ d·ª• m·ªôt ma tr·∫≠n ho·∫∑c d·∫£i √¥ nh·ªõ ‚Äì cho ph√©p l∆∞u tr·∫°ng th√°i cu·ªôc tho·∫°i v√† truy xu·∫•t l·∫°i khi c·∫ßn th√¥ng qua c∆° ch·∫ø attention ho·∫∑c ƒë·ªçc-ghi kh·∫£ vi (differentiable read/write). √ù t∆∞·ªüng n√†y ƒë∆∞·ª£c ti√™n phong b·ªüi Weston et al. (2014) v·ªõi m√¥ h√¨nh **Memory Networks**, k·∫øt h·ª£p gi·ªØa m·ªôt th√†nh ph·∫ßn suy lu·∫≠n (inference) v√† m·ªôt th√†nh ph·∫ßn b·ªô nh·ªõ d√†i h·∫°n ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=,chaining%20multiple%20supporting%20sentences%20to)). B·ªô nh·ªõ n√†y c√≥ th·ªÉ coi nh∆∞ m·ªôt **c∆° s·ªü tri th·ª©c ƒë·ªông**: t·∫°i m·ªói b∆∞·ªõc, m√¥ h√¨nh c√≥ th·ªÉ ghi c√°c th√¥ng tin m·ªõi v√†o c√°c √¥ nh·ªõ, v√† khi tr·∫£ l·ªùi th√¨ th·ª±c hi·ªán chu tr√¨nh ch√∫ √Ω l√™n b·ªô nh·ªõ ƒë·ªÉ _ch·ªçn l·ªçc c√°c ƒëo·∫°n li√™n quan_ ph·ª•c v·ª• suy lu·∫≠n. Tr√™n c√°c t√°c v·ª• QA ƒë∆°n gi·∫£n, Memory Network ƒë√£ ch·ª©ng t·ªè kh·∫£ nƒÉng **x√¢u chu·ªói l·∫≠p lu·∫≠n nhi·ªÅu b∆∞·ªõc** nh·ªù ƒë·ªçc t·ª´ nhi·ªÅu √¥ nh·ªõ (ch·∫≥ng h·∫°n tr·∫£ l·ªùi c√¢u h·ªèi c·∫ßn 2-3 c√¢u h·ªó tr·ª£) ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=these%20models%20in%20the%20context,understanding%20the%20intension%20of%20verbs)).

  

Ti·∫øp n·ªëi h∆∞·ªõng n√†y, nhi·ªÅu ki·∫øn tr√∫c b·ªô nh·ªõ kh·∫£ vi kh√°c ra ƒë·ªùi: **End-to-End Memory Network** (Sukhbaatar et al., 2015) t·ªëi ∆∞u h√≥a Memory Network b·∫±ng c∆° ch·∫ø attention ƒëa l∆∞·ª£t; **Dynamic Memory Network** (Kumar et al., 2016) √°p d·ª•ng th√†nh c√¥ng cho hi·ªÉu ng√¥n ng·ªØ v√† ph√¢n t√≠ch c·∫£m x√∫c; ƒë·∫∑c bi·ªát l√† m√¥ h√¨nh **Differentiable Neural Computer (DNC)** c·ªßa DeepMind, m·ªôt b·ªô nh·ªõ ngo√†i c√≥ m√¥ ƒëun ƒë·ªçc/ghi ƒë∆∞·ª£c ƒëi·ªÅu khi·ªÉn b·ªüi m·ªôt m·∫°ng LSTM ([Differentiable neural computer - Wikipedia](https://en.wikipedia.org/wiki/Differentiable_neural_computer#:~:text=In%20artificial%20intelligence%20%2C%20a,1)) ([Differentiable neural computer - Wikipedia](https://en.wikipedia.org/wiki/Differentiable_neural_computer#:~:text=DNC%20indirectly%20takes%20inspiration%20from,by%20finding%20a%20%2052)). DNC ƒë∆∞·ª£c v√≠ nh∆∞ _m√°y Turing th·∫ßn kinh_, c√≥ thanh ghi nh·ªõ v√† b·ªô ƒëi·ªÅu khi·ªÉn h·ªçc c√°ch ghi nh·ªõ chu·ªói d·ªØ li·ªáu v√† truy v·∫•n khi c·∫ßn. Graves et al. (2016) cho th·∫•y DNC c√≥ th·ªÉ h·ªçc c√°ch **l∆∞u tr·ªØ v√† truy h·ªìi th√¥ng tin d·∫°ng ƒë·ªì th·ªã tu·∫ßn t·ª±**, v√≠ d·ª• ghi l·∫°i m·ªôt tuy·∫øn ƒë∆∞·ªùng v√† sau ƒë√≥ xu·∫•t ra ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t, hay t·∫°o ra l·ªùi gi·∫£i cho b√†i to√°n d∆∞·ªùng nh∆∞ c·∫ßn kh·∫£ nƒÉng ‚Äúl·∫≠p tr√¨nh‚Äù ([Differentiable neural computer - Wikipedia](https://en.wikipedia.org/wiki/Differentiable_neural_computer#:~:text=So%20far%2C%20DNCs%20have%20been,video%20commentaries%20or%20semantic%20text)). Nh·ªØng m√¥ h√¨nh n√†y **gi√°n ti·∫øp ch·ª©ng minh** m·∫°ng n∆°-ron c√≥ kh·∫£ nƒÉng m√¥ ph·ªèng h√†nh vi nh·ªõ v√† suy lu·∫≠n phi tuy·∫øn t√≠nh n·∫øu ƒë∆∞·ª£c trang b·ªã b·ªô nh·ªõ ngo√†i ƒë·ªß m·∫°nh.

  

Trong b·ªëi c·∫£nh h·ªôi tho·∫°i, module b·ªô nh·ªõ kh·∫£ vi h·ª©a h·∫πn gi√∫p chatbot **nh·ªõ c√°c th√¥ng tin t·ª´ c√°c l∆∞·ª£t tr∆∞·ªõc** m√† kh√¥ng c·∫ßn mang to√†n b·ªô n·ªôi dung ƒë√≥ trong ng·ªØ c·∫£nh m·ªói l·∫ßn. Thay v√†o ƒë√≥, th√¥ng tin s·∫Ω ƒë∆∞·ª£c vi·∫øt v√†o b·ªô nh·ªõ (v√≠ d·ª• vector ·∫©n ƒë·∫°i di·ªán cho c√¢u tho·∫°i quan tr·ªçng) v√† sau ƒë√≥ ƒë·ªçc ra khi ph·∫£i ph·∫£n h·ªìi. M·ªôt v√≠ d·ª• ƒë∆°n gi·∫£n: m·ªôt **Memory Network** c√≥ th·ªÉ l∆∞u tr·ªØ c√°c ph√°t ng√¥n c·ªßa ng∆∞·ªùi d√πng d∆∞·ªõi d·∫°ng vector trong √¥ nh·ªõ, v√† m·ªói l·∫ßn tr·∫£ l·ªùi, m√¥ h√¨nh truy t√¨m vector n√†o c√≥ li√™n quan nh·∫•t ƒë·∫øn c√¢u h·ªèi hi·ªán t·∫°i ƒë·ªÉ s·ª≠ d·ª•ng ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=memory%20component%3B%20they%20learn%20how,chaining%20multiple%20supporting%20sentences%20to)). V·ªÅ nguy√™n t·∫Øc, ph∆∞∆°ng ph√°p n√†y c√≥ th·ªÉ m·ªü r·ªông tr√≠ nh·ªõ t√πy √Ω (ch·ªâ c·∫ßn tƒÉng s·ªë √¥ nh·ªõ) v√† m√¥ h√¨nh c√≥ th·ªÉ h·ªçc c√°ch ghi ƒë√® ho·∫∑c l√†m m·ªù d·∫ßn c√°c √¥ √≠t quan tr·ªçng ‚Äì t∆∞∆°ng t·ª± c∆° ch·∫ø qu√™n c√≥ ch·ªß ƒë√≠ch.

  

Tuy nhi√™n, **th√°ch th·ª©c l·ªõn** c·ªßa h∆∞·ªõng ti·∫øp c·∫≠n n√†y n·∫±m ·ªü vi·ªác _hu·∫•n luy·ªán_ v√† _quy m√¥_. Vi·ªác hu·∫•n luy·ªán end-to-end ƒë·ªÉ m√¥ h√¨nh v·ª´a l√†m t·ªët nhi·ªám v·ª• ƒë·ªëi tho·∫°i, v·ª´a t·ªëi ∆∞u c√°ch ƒë·ªçc/ghi b·ªô nh·ªõ kh√¥ng h·ªÅ d·ªÖ d√†ng, ƒë·∫∑c bi·ªát tr√™n d·ªØ li·ªáu h·ªôi tho·∫°i t·ª± nhi√™n ph·ª©c t·∫°p. K·∫øt qu·∫£ l√† c√°c ki·∫øn tr√∫c b·ªô nh·ªõ kh·∫£ vi t·ª´ng th√†nh c√¥ng tr√™n nhi·ªám v·ª• gi·∫£ l·∫≠p (nh∆∞ b√†i to√°n bAbI c·ªßa Facebook) l·∫°i √≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c h·ªá th·ªëng h·ªôi tho·∫°i m·ªü r·ªông th·ª±c t·∫ø. Thay v√†o ƒë√≥, c·ªông ƒë·ªìng chuy·ªÉn sang c√°c ph∆∞∆°ng ph√°p d√πng b·ªô nh·ªõ ngo√†i nh∆∞ng _kh√¥ng train chung v·ªõi m√¥ h√¨nh_, t·ª©c l√† h∆∞·ªõng (3) d∆∞·ªõi ƒë√¢y. G·∫ßn ƒë√¢y, m·ªôt s·ªë nghi√™n c·ª©u c·ªë g·∫Øng k·∫øt h·ª£p LLM v·ªõi module nh·ªõ kh·∫£ vi ‚Äì v√≠ d·ª• **PlugLM** (Cheng et al., 2022) ch√®n m·ªôt b·ªô nh·ªõ key-value c√≥ th·ªÉ c·∫≠p nh·∫≠t v√†o m√¥ h√¨nh pretrained ƒë·ªÉ t√°ch r·ªùi ph·∫ßn l∆∞u tr·ªØ ki·∫øn th·ª©c kh·ªèi tham s·ªë m√¥ h√¨nh ([Language model with Plug-in Knowldge Memory | OpenReview](https://openreview.net/forum?id=Plr5l7r0jY6#:~:text=of%20knowledge%20PLM%20needs%20to,also%20keep%20absorbing%20new%20knowledge)). D√π c√≥ k·∫øt qu·∫£ kh·∫£ quan trong c·∫≠p nh·∫≠t ki·∫øn th·ª©c m·ªõi m√† kh√¥ng t√°i hu·∫•n luy·ªán to√†n b·ªô m√¥ h√¨nh ([Language model with Plug-in Knowldge Memory | OpenReview](https://openreview.net/forum?id=Plr5l7r0jY6#:~:text=adaptation%20setting%2C%20PlugLM%20could%20be,task%20knowledge)), c√°ch l√†m n√†y v·∫´n hi·∫øm khi √°p d·ª•ng tr·ª±c ti·∫øp trong ƒë·ªëi tho·∫°i m·ªü. N√≥i t√≥m l·∫°i, module b·ªô nh·ªõ kh·∫£ vi l√† m·ªôt h∆∞·ªõng mang nhi·ªÅu ti·ªÅm nƒÉng v·ªÅ m·∫∑t l√Ω thuy·∫øt, nh∆∞ng ƒë·ªô ph·ª©c t·∫°p khi hu·∫•n luy·ªán v√† t√≠ch h·ª£p khi·∫øn n√≥ ch∆∞a ph·ªï bi·∫øn b·∫±ng c√°ch ti·∫øp c·∫≠n d·ª±a tr√™n truy h·ªìi th√¥ng tin.

  

## Ti·∫øp c·∫≠n 3: N√©n ng·ªØ c·∫£nh v√† truy h·ªìi th√¥ng tin

  

Hi·ªán nay, **ph·ªï bi·∫øn nh·∫•t** trong c√°c h·ªá th·ªëng ƒë·ªëi tho·∫°i c√≥ tr√≠ nh·ªõ d√†i h·∫°n l√† h∆∞·ªõng ti·∫øp c·∫≠n d·ª±a tr√™n **b·ªô nh·ªõ ngo√†i k·∫øt h·ª£p truy h·ªìi (retrieval)**. Thay v√¨ gi·ªØ to√†n b·ªô l·ªãch s·ª≠ trong input hay thi·∫øt k·∫ø m·ªôt module nh·ªõ ph·ª©c t·∫°p b√™n trong, ph∆∞∆°ng ph√°p n√†y t√°ch bi·ªát h·∫≥n m·ªôt **kho l∆∞u tr·ªØ th√¥ng tin h·ªôi tho·∫°i** (conversation memory repository) d∆∞·ªõi d·∫°ng vƒÉn b·∫£n ho·∫∑c vector, v√† s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n truy h·ªìi (th∆∞·ªùng qua embedding v√† so kh·ªõp ng·ªØ nghƒ©a) ƒë·ªÉ l·∫•y ra nh·ªØng m·∫©u th√¥ng tin c·∫ßn thi·∫øt cho m·ªói l∆∞·ª£t ƒë·ªëi tho·∫°i. C√°ch ti·∫øp c·∫≠n n√†y ch·ªãu ·∫£nh h∆∞·ªüng t·ª´ th√†nh c√¥ng c·ªßa m√¥ h√¨nh **open-domain QA** v√† **retrieval-augmented generation (RAG)**, n∆°i m√¥ h√¨nh language model ƒë∆∞·ª£c b·ªï tr·ª£ b·ªüi m·ªôt c∆° ch·∫ø t√¨m ki·∫øm tri th·ª©c b√™n ngo√†i. ƒêi·ªÉm kh√°c bi·ªát l√† ·ªü ƒë√¢y, kho l∆∞u tr·ªØ kh√¥ng ph·∫£i tri th·ª©c chung c·ªë ƒë·ªãnh (nh∆∞ Wikipedia) m√† ch√≠nh l√† _nh·ªØng g√¨ ƒë√£ di·ªÖn ra trong cu·ªôc h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥_.

  

Quy tr√¨nh chung th∆∞·ªùng g·ªìm c√°c b∆∞·ªõc: (i) **L∆∞u tr·ªØ**: m·ªói khi k·∫øt th√∫c m·ªôt phi√™n ho·∫∑c m·ªôt s·ªë l∆∞·ª£t tho·∫°i, h·ªá th·ªëng s·∫Ω tr√≠ch xu·∫•t c√°c th√¥ng tin c·ªët l√µi (v√≠ d·ª•: s·ª± ki·ªán v·ª´a x·∫£y ra, t√≠nh c√°ch ho·∫∑c s·ªü th√≠ch ng∆∞·ªùi d√πng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p, c√¢u h·ªèi ch∆∞a ƒë∆∞·ª£c tr·∫£ l·ªùi,...) v√† l∆∞u v√†o b·ªô nh·ªõ d√†i h·∫°n. Vi·ªác l∆∞u tr·ªØ n√†y c√≥ th·ªÉ ·ªü d·∫°ng vƒÉn b·∫£n th√¥ (nh∆∞ t·∫≠p c√°c c√¢u t√≥m t·∫Øt) ho·∫∑c vector embedding (nh∆∞ trung b√¨nh bi·ªÉu di·ªÖn c·ªßa c√¢u n√≥i). (ii) **Truy v·∫•n**: khi ƒë·ªëi tho·∫°i ti·∫øp t·ª•c, tr∆∞·ªõc khi t·∫°o c√¢u tr·∫£ l·ªùi, m√¥ h√¨nh s·∫Ω truy v·∫•n b·ªô nh·ªõ ƒë·ªÉ l·∫•y ra nh·ªØng m·∫©u th√¥ng tin li√™n quan ƒë·∫øn ng·ªØ c·∫£nh hi·ªán t·∫°i. Ch·∫≥ng h·∫°n, n·∫øu ng∆∞·ªùi d√πng h·ªèi l·∫°i ‚Äú_H√¥m tr∆∞·ªõc b·∫°n h·ª©a g√¨ v·ªõi t√¥i?_‚Äù, h·ªá th·ªëng s·∫Ω t√¨m trong b·ªô nh·ªõ m·ª•c n√†o ch·ª©a n·ªôi dung l·ªùi h·ª©a. (iii) **S·ª≠ d·ª•ng**: c√°c k·∫øt qu·∫£ truy h·ªìi ƒë∆∞·ª£c ƒë∆∞a v√†o m√¥ h√¨nh (nh∆∞ m·ªôt ƒëo·∫°n context th√™m v√†o prompt c·ªßa LLM) ƒë·ªÉ sinh ra ph·∫£n h·ªìi cu·ªëi c√πng. C∆° ch·∫ø n√†y t∆∞∆°ng t·ª± pipeline _retrieve-then-read_ ƒë√£ th√†nh c√¥ng trong QA m·ªü ([[2005.11364] Open-Retrieval Conversational Question Answering](https://arxiv.org/abs/2005.11364#:~:text=retrieval%20conversational%20question%20answering%20,the%20reranker%20component%20contributes%20to)), ch·ªâ kh√°c l√† ‚Äúcorpus‚Äù ·ªü ƒë√¢y ch√≠nh l√† l·ªãch s·ª≠ h·ªôi tho·∫°i qu√° kh·ª©.

  

**∆Øu ƒëi·ªÉm ch√≠nh** c·ªßa h∆∞·ªõng n√†y l√† kh·∫£ nƒÉng m·ªü r·ªông v√† ki·ªÉm so√°t: Ta c√≥ th·ªÉ duy tr√¨ m·ªôt b·ªô nh·ªõ r·∫•t l·ªõn (h√†ng ngh√¨n s·ª± ki·ªán) m√† kh√¥ng l√†m ‚Äúqu√° t·∫£i‚Äù m√¥ h√¨nh t·∫°i th·ªùi ƒëi·ªÉm sinh ƒë·∫ßu ra, b·ªüi v√¨ lu√¥n ch·ªâ m·ªôt ph·∫ßn nh·ªè (v√≠ d·ª• 5-10 ƒëo·∫°n) ƒë∆∞·ª£c truy h·ªìi l√†m ng·ªØ c·∫£nh m·ªói l∆∞·ª£t. ƒê·ªìng th·ªùi, ta c√≥ th·ªÉ **c·∫≠p nh·∫≠t** ho·∫∑c **ƒëi·ªÅu ch·ªânh** n·ªôi dung b·ªô nh·ªõ ƒë·ªôc l·∫≠p v·ªõi m√¥ h√¨nh (v√¨ n√≥ n·∫±m ngo√†i), gi√∫p d·ªÖ d√†ng th√™m th√¥ng tin m·ªõi, x√≥a th√¥ng tin l·ªói th·ªùi, hay s·ª≠a sai n·∫øu chatbot ghi nh·ªõ nh·∫ßm. Nh·ªØng h·ªá th·ªëng h·ªôi tho·∫°i d√†i h·∫°n m·∫°nh g·∫ßn ƒë√¢y h·∫ßu h·∫øt ƒë·ªÅu theo ki·∫øn tr√∫c n√†y, k·∫øt h·ª£p v·ªõi nhi·ªÅu k·ªπ thu·∫≠t tinh vi ƒë·ªÉ tƒÉng ch·∫•t l∆∞·ª£ng t√≥m t·∫Øt v√† truy h·ªìi.

  

M·ªôt v√≠ d·ª• ti√™u bi·ªÉu l√† **ORConvQA** (Open-Retrieval Conversational QA) c·ªßa Qu et al. (2020). Thay v√¨ gi·∫£ ƒë·ªãnh c√¢u tr·∫£ l·ªùi lu√¥n n·∫±m trong m·ªôt ƒëo·∫°n vƒÉn cho tr∆∞·ªõc nh∆∞ CoQA, ORConvQA cho ph√©p m√¥ h√¨nh **truy t√¨m b·∫±ng ch·ª©ng** t·ª´ m·ªôt t·∫≠p t√†i li·ªáu l·ªõn tr∆∞·ªõc khi tr·∫£ l·ªùi ([[2005.11364] Open-Retrieval Conversational Question Answering](https://arxiv.org/abs/2005.11364#:~:text=passage,We%20further%20show%20that%20our)). H·ªá th·ªëng c·ªßa h·ªç g·ªìm ba th√†nh ph·∫ßn Transformer: truy h·ªìi (retriever), t√°i x·∫øp h·∫°ng, v√† ƒë·ªçc hi·ªÉu, cho ph√©p t√¨m ki·∫øm th√¥ng tin qua nhi·ªÅu l∆∞·ª£t h·ªèi ƒë√°p. K·∫øt qu·∫£ ch·ªâ ra r·∫±ng vi·ªác t√≠ch h·ª£p _history modeling_ (m√¥ h√¨nh h√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i) v√†o c·∫£ truy h·ªìi l·∫´n ƒë·ªçc hi·ªÉu gi√∫p c·∫£i thi·ªán ƒë√°ng k·ªÉ ƒë·ªô ch√≠nh x√°c ([[2005.11364] Open-Retrieval Conversational Question Answering](https://arxiv.org/abs/2005.11364#:~:text=to,the%20reranker%20component%20contributes%20to)) ‚Äì minh ch·ª©ng cho l·ª£i √≠ch c·ªßa vi·ªác l∆∞u v√† s·ª≠ d·ª•ng ng·ªØ c·∫£nh t·ª´ c√°c l∆∞·ª£t tr∆∞·ªõc. ORConvQA l√† c·∫ßu n·ªëi t·ª´ QA thu·∫ßn t√∫y sang ƒë·ªëi tho·∫°i c√≥ tr√≠ nh·ªõ, cho th·∫•y **k·∫øt h·ª£p retrieval v·ªõi context h·ªôi tho·∫°i** l√† h∆∞·ªõng ƒëi h·ªØu √≠ch.

  

Trong ƒë·ªëi tho·∫°i m·ªü, d·ª± √°n **BlenderBot 2.0** c·ªßa Facebook (Roller et al., 2021) l·∫ßn ƒë·∫ßu ti√™n gi·ªõi thi·ªáu m·ªôt chatbot c√≥ kh·∫£ nƒÉng **‚Äúnh·ªõ‚Äù c√°c cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥**. C·ª• th·ªÉ, BlenderBot 2.0 l∆∞u l·∫°i _t√≥m t·∫Øt_ c·ªßa m·ªói phi√™n t∆∞∆°ng t√°c v·ªõi ng∆∞·ªùi d√πng trong m·ªôt c∆° s·ªü d·ªØ li·ªáu b·ªô nh·ªõ l√¢u d√†i. Khi g·∫∑p l·∫°i ng∆∞·ªùi d√πng ƒë√≥ ho·∫∑c trong phi√™n k·∫ø ti·∫øp, bot s·∫Ω truy v·∫•n c∆° s·ªü n√†y ƒë·ªÉ t√¨m c√°c th√¥ng tin li√™n quan (v√≠ d·ª•: t√™n ng∆∞·ªùi d√πng, s·ªü th√≠ch ƒë√£ ƒë·ªÅ c·∫≠p) v√† ƒëi·ªÅu ch·ªânh ph·∫£n h·ªìi cho ph√π h·ª£p. Song song, BlenderBot 2.0 c√≤n t√≠ch h·ª£p t√¨m ki·∫øm Internet, nh∆∞ng ƒëi·ªÉm m·∫•u ch·ªët l√† n√≥ ch·ª©ng minh ƒë∆∞·ª£c vi·ªác **ghi nh·ªõ v√† truy xu·∫•t d·ªØ ki·ªán t·ª´ c√°c phi√™n tr∆∞·ªõc** gi√∫p bot tr·ªü n√™n t·ª± nhi√™n v√† nh·∫•t qu√°n h∆°n h·∫≥n so v·ªõi phi√™n b·∫£n tr∆∞·ªõc ƒë√≥ (BlenderBot 1) v·ªën ch·ªâ nh·ªõ trong ph·∫°m vi phi√™n hi·ªán t·∫°i. ƒê√¢y l√† m·ªôt minh h·ªça s·ªõm cho hi·ªáu qu·∫£ c·ªßa memory augmentation trong ƒë·ªëi tho·∫°i.

  

ƒê·ªÉ qu·∫£n l√Ω b·ªô nh·ªõ hi·ªáu qu·∫£, c√°c nghi√™n c·ª©u g·∫ßn ƒë√¢y t·∫≠p trung v√†o **k·ªπ thu·∫≠t t√≥m t·∫Øt v√† c·∫≠p nh·∫≠t b·ªô nh·ªõ**. Thay v√¨ l∆∞u t·∫•t c·∫£ m·ªçi c√¢u, h·ªá th·ªëng s·∫Ω **t√≥m t·∫Øt ng·∫Øn g·ªçn** nh·ªØng th√¥ng tin quan tr·ªçng sau m·ªói phi√™n. Bae et al. (2022) ‚Äì trong h·ªá th·ªëng **‚ÄúKeep Me Updated!‚Äù** ‚Äì s·ª≠ d·ª•ng m·ªôt m√¥-ƒëun t√≥m t·∫Øt ƒë·ªÉ tr√≠ch xu·∫•t c√°c c√¢u **ti·ªÉu s·ª≠ ng∆∞·ªùi d√πng** sau m·ªói phi√™n tr√≤ chuy·ªán v√† l∆∞u ch√∫ng v√†o b·ªô nh·ªõ () (). Quan tr·ªçng h∆°n, h·ªç thi·∫øt k·∫ø c∆° ch·∫ø **qu·∫£n l√Ω b·ªô nh·ªõ ƒë·ªông**: m·ªói khi c√≥ th√¥ng tin m·ªõi, h·ªá th·ªëng so s√°nh v·ªõi c√°c c√¢u nh·ªõ c≈© v√† th·ª±c hi·ªán b·ªën thao t√°c c√≥ th·ªÉ ‚Äì _gi·ªØ nguy√™n (PASS), thay th·∫ø (REPLACE), th√™m m·ªõi (APPEND), ho·∫∑c x√≥a b·ªè (DELETE)_ ‚Äì nh·∫±m lo·∫°i b·ªè m√¢u thu·∫´n ho·∫∑c tr√πng l·∫∑p (). Ch·∫≥ng h·∫°n, n·∫øu b·ªô nh·ªõ c√≥ c√¢u ‚ÄúCh∆∞a x√©t nghi·ªám COVID‚Äù v√† phi√™n m·ªõi ph√°t hi·ªán ‚ÄúV·ª´a nh·∫≠n k·∫øt qu·∫£ d∆∞∆°ng t√≠nh COVID‚Äù, m√¥-ƒëun s·∫Ω _thay th·∫ø_ c√¢u c≈© b·∫±ng c√¢u m·ªõi trong b·ªô nh·ªõ (). Nh·ªù ƒë√≥, b·ªô nh·ªõ lu√¥n ƒë∆∞·ª£c duy tr√¨ _c·∫≠p nh·∫≠t_ v√† _nh·∫•t qu√°n_ v·ªõi t√¨nh tr·∫°ng hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng. Th√≠ nghi·ªám cho th·∫•y c√°ch ti·∫øp c·∫≠n n√†y gi√∫p chatbot duy tr√¨ ƒë∆∞·ª£c **t√≠nh ch√≠nh x√°c c·ªßa tr√≠ nh·ªõ** qua nhi·ªÅu phi√™n, c·∫£i thi·ªán t√≠nh g·∫Øn k·∫øt v√† t·ª± nhi√™n trong ƒë·ªëi tho·∫°i d√†i ().

  

M·ªôt h∆∞·ªõng kh√°c ƒë·ªÉ n√©n th√¥ng tin l√† s·ª≠ d·ª•ng LLM t·ª± ƒë·ªông t·∫°o **b·∫£n t√≥m t·∫Øt ƒë·ªá quy**. Wang et al. (2023) ƒë·ªÅ xu·∫•t ph∆∞∆°ng ph√°p _Recursively Summarizing_ v·ªõi GPT-4: chia h·ªôi tho·∫°i r·∫•t d√†i th√†nh c√°c ƒëo·∫°n nh·ªè, l·∫ßn l∆∞·ª£t d√πng LLM t√≥m t·∫Øt t·ª´ng ƒëo·∫°n, r·ªìi l·∫°i t√≥m t·∫Øt ti·∫øp c√°c b·∫£n t√≥m t·∫Øt ƒë·ªÉ t·∫°o n√™n m·ªôt _‚Äúsi√™u t√≥m t·∫Øt‚Äù_ cu·ªëi c√πng l√†m b·ªô nh·ªõ ([[2308.15022] Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022#:~:text=long%20conversation%2C%20these%20chatbots%20fail,consistent%20responses%20in%20a%20long)). M√¥ h√¨nh ƒë·ªëi tho·∫°i s·∫Ω tham kh·∫£o c√°c t√≥m t·∫Øt n√†y thay v√¨ to√†n b·ªô chi ti·∫øt cu·ªôc tr√≤ chuy·ªán. K·ªπ thu·∫≠t ƒë·ªá quy n√†y gi√∫p l∆∞u gi·ªØ ƒë∆∞·ª£c √Ω ch√≠nh c·ªßa nh·ªØng h·ªôi tho·∫°i h√†ng trƒÉm l∆∞·ª£t d∆∞·ªõi d·∫°ng v√†i ƒëo·∫°n vƒÉn s√∫c t√≠ch. Th√∫ v·ªã l√† nh√≥m t√°c gi·∫£ nh·∫≠n th·∫•y ph∆∞∆°ng ph√°p c·ªßa h·ªç c√≥ th·ªÉ **k·∫øt h·ª£p c·ªông h∆∞·ªüng** v·ªõi c·∫£ LLM c√≥ ng·ªØ c·∫£nh d√†i (8K-16K) l·∫´n m√¥ h√¨nh t√≠ch h·ª£p retrieval, gi√∫p n√¢ng cao hi·ªáu qu·∫£ tr√™n c√°c h·ªôi tho·∫°i c·ª±c d√†i ([[2308.15022] Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022#:~:text=consistent%20response%20with%20the%20help,scripts%20will%20be%20released%20later)). ƒêi·ªÅu n√†y g·ª£i √Ω h∆∞·ªõng t∆∞∆°ng lai: k·∫øt h·ª£p gi·ªØa t√≥m t·∫Øt v√† truy h·ªìi m·ªôt c√°ch th√≠ch ·ª©ng.

  

ƒê·ªëi v·ªõi pha **truy h·ªìi**, h·∫ßu h·∫øt c√°c h·ªá th·ªëng d√πng **embedding kh√¥ng gian**: l∆∞u c√°c memory d∆∞·ªõi d·∫°ng vector nh√∫ng v√† s·ª≠ d·ª•ng _kho·∫£ng c√°ch ng·ªØ nghƒ©a_ ƒë·ªÉ t√¨m ki·∫øm. ƒê·ªô ch√≠nh x√°c truy h·ªìi ph·ª• thu·ªôc nhi·ªÅu v√†o c√°ch bi·ªÉu di·ªÖn v√† t·ªï ch·ª©c b·ªô nh·ªõ. Pan et al. (2024) trong c√¥ng tr√¨nh **SeCom** nh·∫•n m·∫°nh t·∫ßm quan tr·ªçng c·ªßa **‚Äúƒë∆°n v·ªã b·ªô nh·ªõ‚Äù**: h·ªç so s√°nh l∆∞u tr·ªØ theo t·ª´ng l∆∞·ª£t tho·∫°i, theo t·ª´ng phi√™n, v√† theo ƒëo·∫°n t√≥m t·∫Øt, nh·∫≠n th·∫•y m·ªói c√°ch c√≥ ∆∞u nh∆∞·ª£c ƒëi·ªÉm ri√™ng ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=To%20deliver%20coherent%20and%20personalized,retrieval%20accuracy%20across%20different%20granularities)) ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=Building%20on%20these%20insights%2C%20we,as%20DialSeg711%2C%20TIAGE%2C%20and%20SuperDialSeg)). SeCom ƒë·ªÅ xu·∫•t m·ªôt chi·∫øn l∆∞·ª£c k·∫øt h·ª£p: d√πng m·ªôt m√¥ h√¨nh ph√¢n ƒëo·∫°n ch·ªß ƒë·ªÅ ƒë·ªÉ chia h·ªôi tho·∫°i th√†nh c√°c **ƒëo·∫°n s·ª± ki·ªán ng·∫Øn**, l∆∞u m·ªói ƒëo·∫°n nh∆∞ m·ªôt b·∫£n ghi b·ªô nh·ªõ, ƒë·ªìng th·ªùi √°p d·ª•ng k·ªπ thu·∫≠t **‚Äún√©n th√¥ng tin nhi·ªÖu‚Äù** ƒë·ªÉ l·ªçc b·ªõt ph·∫ßn kh√¥ng li√™n quan trong m·ªói ƒëo·∫°n tr∆∞·ªõc khi l∆∞u ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=Building%20on%20these%20insights%2C%20we,as%20DialSeg711%2C%20TIAGE%2C%20and%20SuperDialSeg)). K·∫øt qu·∫£, c√°ch l∆∞u tr·ªØ theo ƒëo·∫°n ch·ªß ƒë·ªÅ gi√∫p tƒÉng ch·∫•t l∆∞·ª£ng truy h·ªìi tr√™n c√°c benchmark h·ªôi tho·∫°i d√†i nh∆∞ LOCOMO, v√¨ n√≥ c√¢n b·∫±ng gi·ªØa chi ti·∫øt v√† t·ªïng qu√°t. B√™n c·∫°nh ƒë√≥, m·ªôt s·ªë c·∫£i ti·∫øn kh√°c g·ªìm **truy h·ªìi theo th·ªùi gian** (∆∞u ti√™n c√°c s·ª± ki·ªán g·∫ßn ƒë√¢y n·∫øu c√¢u h·ªèi ch·ª©a m·ªëc th·ªùi gian ‚Äì xem Wu et al. 2023) hay **m·ªü r·ªông truy v·∫•n b·∫±ng tri th·ª©c** (v√≠ d·ª• n·∫øu h·ªèi ‚Äúanh ·∫•y‚Äù th√¨ truy v·∫•n m·ªü r·ªông ‚Äúanh ·∫•y‚Äù = t√™n c·ª• th·ªÉ t·ª´ b·ªô nh·ªõ). Nh·ªØng t·ªëi ∆∞u n√†y ƒë√£ ƒë∆∞·ª£c t·ªïng k·∫øt trong nghi√™n c·ª©u LongMemEval, ƒë·ªÅ xu·∫•t khung ‚ÄúIndexing-Retrieval-Reading‚Äù cho thi·∫øt k·∫ø b·ªô nh·ªõ, trong ƒë√≥: **ƒë√°nh ch·ªâ m·ª•c** t·ªëi ∆∞u b·∫±ng c√°ch l∆∞u tr·ªØ theo phi√™n nh·ªè (session decomposition) v√† m·ªü r·ªông kh√≥a b·∫±ng d·ªØ ki·ªán, **truy h·ªìi** t·ªëi ∆∞u b·∫±ng c√°ch c√¢n nh·∫Øc ng·ªØ c·∫£nh th·ªùi gian, v√† **ƒë·ªçc** hi·ªáu qu·∫£ b·∫±ng c√°ch k·∫øt h·ª£p b·ªô nh·ªõ v√†o input LLM ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=showing%20a%2030,term)).

  

G·∫ßn ƒë√¢y, xu·∫•t hi·ªán nh·ªØng h·ªá th·ªëng tr√≠ nh·ªõ ti√™n ti·∫øn t·∫≠n d·ª•ng s·ª©c m·∫°nh LLM: v√≠ d·ª• **MemoryBank** (Zhong et al., 2023) v√† **THEANINE** (Ong et al., 2024). MemoryBank t√≠ch h·ª£p m·ªôt **c∆° ch·∫ø c·∫≠p nh·∫≠t b·ªô nh·ªõ l·∫•y c·∫£m h·ª©ng t·ª´ ƒë∆∞·ªùng cong l√£ng qu√™n c·ªßa Ebbinghaus** ‚Äì nghƒ©a l√† m√¥ ph·ªèng vi·ªác k√Ω ·ª©c phai nh·∫°t d·∫ßn theo th·ªùi gian n·∫øu kh√¥ng nh·∫Øc l·∫°i ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=personality%20over%20time%20by%20synthesizing,based%20chatbot%20named)). C·ª• th·ªÉ, MemoryBank cho ph√©p AI ‚Äúqu√™n‚Äù b·ªõt nh·ªØng k√Ω ·ª©c √≠t quan tr·ªçng ho·∫∑c l√¢u kh√¥ng d√πng, v√† **c·ªßng c·ªë** nh·ªØng k√Ω ·ª©c hay ƒë∆∞·ª£c truy xu·∫•t, nh·ªù ƒë√≥ b·ªô nh·ªõ ho·∫°t ƒë·ªông hi·ªáu qu·∫£ v√† gi·ªëng ng∆∞·ªùi h∆°n ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=personality%20over%20time%20by%20synthesizing,based%20chatbot%20named)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=Memory%20Updating)). H·ªç tri·ªÉn khai MemoryBank tr√™n m·ªôt chatbot b·∫°n ƒë·ªìng h√†nh (SiliconFriend), cho th·∫•y bot c√≥ th·ªÉ **ti·∫øp thu v√† th√≠ch nghi v·ªõi t√≠nh c√°ch ng∆∞·ªùi d√πng** qua th·ªùi gian, ƒë·ªìng th·ªùi nh·ªõ ƒë∆∞·ª£c c√°c s·ª± ki·ªán c·ªët l√µi trong qu√° kh·ª© (v√≠ d·ª• s·ªü th√≠ch, m·ª•c ti√™u ng∆∞·ªùi d√πng) nh·ªù c∆° ch·∫ø n√†y ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=psychological%20counseling%2C%20and%20secretarial%20assistance,the%20memory%2C%20thereby%20offering%20a)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=,memory%20works%20through%20repeated%20retrieval)). Trong khi ƒë√≥, THEANINE l·∫°i ch·ªçn c√°ch **kh√¥ng x√≥a b·ªè k√Ω ·ª©c c≈©**, thay v√†o ƒë√≥ qu·∫£n l√Ω m·ªôt **ƒë·ªì th·ªã k√Ω ·ª©c theo d√≤ng th·ªùi gian** n·ªëi c√°c s·ª± ki·ªán theo quan h·ªá nh√¢n qu·∫£ v√† th·ªùi gian ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=to%20improve%20retrieval%20quality%2C%20we,human%20efforts%20when%20assessing%20agent)). M·ªói khi c·∫ßn t·∫°o ph·∫£n h·ªìi, m√¥ h√¨nh s·∫Ω l·∫ßn theo _timeline_ c√°c s·ª± ki·ªán li√™n quan, t·∫°o n√™n m·ªôt ng·ªØ c·∫£nh di·ªÖn gi·∫£i v√¨ sao ng∆∞·ªùi d√πng c√≥ tr·∫°ng th√°i hi·ªán t·∫°i. C√°ch n√†y nh·∫•n m·∫°nh t·∫ßm quan tr·ªçng c·ªßa **ng·ªØ c·∫£nh ti·∫øn h√≥a**: v√≠ d·ª•, thay v√¨ ch·ªâ bi·∫øt ‚Äúng∆∞·ªùi d√πng th√≠ch du l·ªãch‚Äù, bot c√≤n bi·∫øt _l·ªãch s·ª≠_ tr∆∞·ªõc ƒë√¢y ng∆∞·ªùi d√πng ƒë√£ t·ª´ng _s·ª£ ƒëi m√°y bay r·ªìi sau ƒë√≥ m·ªõi th√≠ch du l·ªãch_ ‚Äì t·ª´ ƒë√≥ ph·∫£n h·ªìi tinh t·∫ø h∆°n. THEANINE cho th·∫•y vi·ªác **li√™n k·∫øt c√°c m·∫£nh memory** th√†nh chu·ªói c√≥ th·ªÉ gi√∫p m√¥ h√¨nh hi·ªÉu r√µ s·ª± thay ƒë·ªïi v√† nh·∫•t qu√°n trong t√≠nh c√°ch ng∆∞·ªùi d√πng theo th·ªùi gian, m√† kh√¥ng c·∫ßn x√≥a k√Ω ·ª©c c≈© (v·ªën c≈©ng mang th√¥ng tin h·ªØu √≠ch v·ªÅ thay ƒë·ªïi h√†nh vi) ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=constantly%20memorize%20perceived%20information%20and,Along)) ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=conversations,human%20efforts%20when%20assessing%20agent)).

  

Cu·ªëi c√πng, framework **LD-Agent** (Hao Li et al., 2024) ƒë·∫°i di·ªán cho xu h∆∞·ªõng t√≠ch h·ª£p _ƒëa th√†nh ph·∫ßn_: h·ªá th·ªëng n√†y chia t√°c v·ª• th√†nh **3 m√¥-ƒëun** ƒë·ªôc l·∫≠p ‚Äì (i) **nh·∫≠n th·ª©c s·ª± ki·ªán** (event perception) ƒë·ªÉ t√≥m t·∫Øt s·ª± ki·ªán ch√≠nh m·ªói phi√™n v√†o b·ªô nh·ªõ d√†i h·∫°n, (ii) **tr√≠ch xu·∫•t persona** ƒë·ªông cho c·∫£ ng∆∞·ªùi d√πng v√† chatbot, v√† (iii) **t·∫°o ph·∫£n h·ªìi** (response generation) c√≥ ƒëi·ªÅu ki·ªán tr√™n ng·ªØ c·∫£nh hi·ªán t·∫°i + b·ªô nh·ªõ s·ª± ki·ªán truy h·ªìi + persona ƒë√£ nh·∫≠n di·ªán ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=the%20Long,Agent%20are)). B·ªô nh·ªõ s·ª± ki·ªán c·ªßa LD-Agent bao g·ªìm hai ph·∫ßn: **b·ªô nh·ªõ d√†i h·∫°n** ch·ª©a l·ªãch s·ª≠ c√°c s·ª± ki·ªán t√≥m t·∫Øt qua nhi·ªÅu phi√™n (ƒë∆∞·ª£c l∆∞u v·ªõi d·∫•u th·ªùi gian v√† ph√¢n ƒëo·∫°n theo ch·ªß ƒë·ªÅ), v√† **b·ªô nh·ªõ ng·∫Øn h·∫°n** cho phi√™n hi·ªán t·∫°i (ƒë·∫£m b·∫£o th√¥ng tin m·ªõi nh·∫•t lu√¥n ƒë∆∞·ª£c ch√∫ tr·ªçng) ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=the%20Long,Agent%20are)) (). Khi ph·∫£n h·ªìi, h·ªá th·ªëng d√πng m·ªôt c∆° ch·∫ø truy h·ªìi theo ch·ªß ƒë·ªÅ ƒë·ªÉ l·∫•y ra c√°c s·ª± ki·ªán c≈© li√™n quan t·ª´ b·ªô nh·ªõ d√†i h·∫°n, k·∫øt h·ª£p v·ªõi n·ªôi dung ng·∫Øn h·∫°n, c√πng v·ªõi h·ªì s∆° persona ƒë√£ c·∫≠p nh·∫≠t, r·ªìi ƒë∆∞a v√†o m√¥-ƒëun sinh. C√°ch ti·∫øp c·∫≠n module h√≥a n√†y gi√∫p d·ªÖ d√†ng tinh ch·ªânh t·ª´ng ph·∫ßn (v√≠ d·ª• thay m√¥ h√¨nh t√≥m t·∫Øt s·ª± ki·ªán kh√°c t·ªët h∆°n, ho·∫∑c √°p d·ª•ng k·ªπ thu·∫≠t LoRA ƒë·ªÉ c·∫≠p nh·∫≠t persona linh ho·∫°t), ƒë·ªìng th·ªùi cho th·∫•y t·∫ßm quan tr·ªçng c·ªßa vi·ªác **qu·∫£n l√Ω ƒë·ªìng th·ªùi ki·∫øn th·ª©c s·ª± ki·ªán v√† th√¥ng tin c√° nh√¢n** cho ƒë·ªëi tho·∫°i d√†i h·∫°n. C√°c th√≠ nghi·ªám c·ªßa LD-Agent ch·ªâ ra r·∫±ng vi·ªác t√≠ch h·ª£p c·∫£ hai lo·∫°i b·ªô nh·ªõ (s·ª± ki·ªán + persona) gi√∫p chatbot ƒë·∫°t ƒë·ªô t·ª± nhi√™n v√† ch√≠nh x√°c cao h∆°n r√µ r·ªát tr√™n nhi·ªÅu benchmark kh√°c nhau ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=generation,various%20illustrative%20benchmarks%2C%20models%2C%20and)).

  

T·ªïng k·∫øt l·∫°i, c√°ch ti·∫øp c·∫≠n n√©n v√† truy h·ªìi ng·ªØ c·∫£nh hi·ªán l√† h∆∞·ªõng **∆∞u vi·ªát nh·∫•t** ƒë·ªÉ hi·ªán th·ª±c h√≥a tr√≠ nh·ªõ d√†i h·∫°n trong ƒë·ªëi tho·∫°i. N√≥ t·∫≠n d·ª•ng ƒë∆∞·ª£c s·ª©c m·∫°nh c·ªßa c√°c m√¥ h√¨nh pretrained (b·∫±ng c√°ch cung c·∫•p cho ch√∫ng ‚Äúcontext m·ªü r·ªông‚Äù khi c·∫ßn), ƒë·ªìng th·ªùi tr√°nh ƒë∆∞·ª£c c√°c h·∫°n ch·∫ø v·ªÅ ƒë·ªô d√†i v√† qu√™n th√¥ng tin do t·ª± m√¥ h√¨nh x·ª≠ l√Ω. C√°c nghi√™n c·ª©u ƒëang ti·∫øp t·ª•c c·∫£i ti·∫øn ·ªü c·∫£ kh√¢u t√≥m t·∫Øt (ƒë·ªÉ l∆∞u ƒë√∫ng v√† ƒë·ªß th√¥ng tin c·∫ßn nh·ªõ) l·∫´n kh√¢u truy h·ªìi (ƒë·ªÉ t√¨m ch√≠nh x√°c th√¥ng tin khi c·∫ßn ƒë·∫øn). Ph·∫ßn ti·∫øp theo, ch√∫ng t√¥i s·∫Ω so s√°nh m·ªôt s·ªë h·ªá th·ªëng ti√™u bi·ªÉu thu·ªôc h∆∞·ªõng n√†y v√† c√°c baseline li√™n quan, tr∆∞·ªõc khi ƒëi v√†o ƒë√°nh gi√° t·ªïng th·ªÉ tr√™n c√°c benchmark.

  

# So s√°nh c√°c h·ªá th·ªëng ti√™u bi·ªÉu c√≥ b·ªô nh·ªõ h·ªôi tho·∫°i

  

ƒê·ªÉ minh h·ªça c·ª• th·ªÉ s·ª± kh√°c bi·ªát gi·ªØa c√°c h∆∞·ªõng ti·∫øp c·∫≠n v√† hi·ªáu qu·∫£ c·ªßa tr√≠ nh·ªõ d√†i h·∫°n, b·∫£ng d∆∞·ªõi ƒë√¢y so s√°nh **m·ªôt s·ªë h·ªá th·ªëng ti√™u bi·ªÉu** t·ª´ tr∆∞·ªõc ƒë·∫øn nay:

  

- **MemNN (Memory Network, 2015)**: ƒê√¢y l√† baseline ki·ªÉu (2) ‚Äì m√¥ h√¨nh c√≥ b·ªô nh·ªõ kh·∫£ vi. MemNN l∆∞u tr·ªØ c√°c ph√°t ng√¥n tr∆∞·ªõc d∆∞·ªõi d·∫°ng vector trong b·ªô nh·ªõ v√† s·ª≠ d·ª•ng attention ƒë·ªÉ ch·ªçn ra vector li√™n quan nh·∫•t khi tr·∫£ l·ªùi ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=,chaining%20multiple%20supporting%20sentences%20to)). M√¥ h√¨nh n√†y ho·∫°t ƒë·ªông t·ªët tr√™n c√°c b√†i to√°n gi·∫£ l·∫≠p ng·∫Øn (nh∆∞ bAbI) nh∆∞ng ch∆∞a ƒë∆∞·ª£c ch·ª©ng minh hi·ªáu qu·∫£ tr√™n ƒë·ªëi tho·∫°i m·ªü ph·ª©c t·∫°p. **∆Øu ƒëi·ªÉm**: c√≥ kh·∫£ nƒÉng suy lu·∫≠n nhi·ªÅu b∆∞·ªõc nh·ªù ƒë·ªçc nhi·ªÅu √¥ nh·ªõ; **Nh∆∞·ª£c ƒëi·ªÉm**: kh√≥ hu·∫•n luy·ªán end-to-end, kh√¥ng t·ª± ƒë·ªông c·∫≠p nh·∫≠t khi th√¥ng tin thay ƒë·ªïi (c·∫ßn ghi ƒë√® th·ªß c√¥ng).
    

- **Baseline kh√¥ng nh·ªõ (No Memory)**: ƒê√¢y l√† h·ªá th·ªëng ki·ªÉu tr·∫£ l·ªùi ƒë·ªôc l·∫≠p t·ª´ng l∆∞·ª£t, v√≠ d·ª• DrQA ho·∫∑c c√°c model seq2seq kh√¥ng cung c·∫•p l·ªãch s·ª≠ v√†o input. H·ªá th·ªëng n√†y ho√†n to√†n _qu√™n_ m·ªçi th·ª© sau m·ªói l∆∞·ª£t, n√™n **kh√¥ng th·ªÉ** tr·∫£ l·ªùi c√°c c√¢u h·ªèi ph·ª• thu·ªôc ng·ªØ c·∫£nh tr∆∞·ªõc (vd: ‚ÄúAnh ·∫•y‚Äù l√† ai?) v√† d·ªÖ tr·∫£ l·ªùi l·∫∑p l·∫°i. K·∫øt qu·∫£ ƒë·ªëi tho·∫°i th∆∞·ªùng k√©m t·ª± nhi√™n v√† kh√¥ng duy tr√¨ ƒë∆∞·ª£c m·∫°ch th√¥ng tin.
    

- **Keep Me Updated (Bae et al., 2022)**: H·ªá th·ªëng n√†y thu·ªôc h∆∞·ªõng (3) ‚Äì d√πng b·ªô nh·ªõ ngo√†i vƒÉn b·∫£n v·ªõi c·∫≠p nh·∫≠t ƒë·ªông. N√≥ t√≥m t·∫Øt th√¥ng tin ng∆∞·ªùi d√πng sau m·ªói phi√™n v√† th·ª±c hi·ªán c√°c ph√©p c·∫≠p nh·∫≠t (th√™m/x√≥a/thay th·∫ø) ƒë·ªÉ b·ªô nh·ªõ lu√¥n nh·∫•t qu√°n (). **∆Øu ƒëi·ªÉm**: ƒë·∫£m b·∫£o th√¥ng tin m·ªõi nh·∫•t lu√¥n ƒë∆∞·ª£c ghi nh·ªõ, tr√°nh m√¢u thu·∫´n (nh·ªù chi·∫øn l∆∞·ª£c c·∫≠p nh·∫≠t) (); cho th·∫•y _c√†ng nhi·ªÅu phi√™n_ th√¨ bot c√†ng nh·ªõ t·ªët h∆°n v√† t∆∞∆°ng t√°c t·ª± nhi√™n h∆°n (). **H·∫°n ch·∫ø**: ch·ªâ l∆∞u th√¥ng tin d∆∞·ªõi d·∫°ng vƒÉn b·∫£n ng·∫Øn n√™n ƒë√¥i khi m·∫•t chi ti·∫øt, v√† ch∆∞a x·ª≠ l√Ω t·ªët tr∆∞·ªùng h·ª£p nhi·ªÅu th√¥ng tin kh√°c lo·∫°i (v√¨ t·∫•t c·∫£ l∆∞u chung m·ªôt n∆°i).
    

- **LD-Agent (Hao Li et al., 2024)**: ƒê·∫°i di·ªán ti√™n ti·∫øn cho h∆∞·ªõng (3) v·ªõi c·∫•u tr√∫c module h√≥a. LD-Agent c√≥ **b·ªô nh·ªõ hai t·∫ßng** (d√†i h·∫°n + ng·∫Øn h·∫°n) v√† th√™m **m√¥-ƒëun persona** ri√™ng ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=the%20Long,Agent%20are)). Nh·ªù ƒë√≥, n√≥ kh√¥ng ch·ªâ nh·ªõ s·ª± ki·ªán m√† c√≤n duy tr√¨ ƒë∆∞·ª£c t√≠nh c√°ch, th√¥ng tin nh√¢n kh·∫©u c·ªßa c·∫£ ng∆∞·ªùi d√πng v√† agent. **∆Øu ƒëi·ªÉm**: ki·∫øn tr√∫c linh ho·∫°t, truy h·ªìi theo ch·ªß ƒë·ªÅ gi√∫p t√¨m ƒë√∫ng s·ª± ki·ªán; persona ƒë·ªông gi√∫p ƒë·ªëi tho·∫°i nh·∫•t qu√°n vai; ƒë·∫°t k·∫øt qu·∫£ t·ªët tr√™n nhi·ªÅu t√°c v·ª• (h·ªèi ƒë√°p, tr√≤ chuy·ªán nhi·ªÅu ch·ªß ƒë·ªÅ) ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=generation,various%20illustrative%20benchmarks%2C%20models%2C%20and)). **Nh∆∞·ª£c ƒëi·ªÉm**: ph·ª©c t·∫°p, c·∫ßn d·ªØ li·ªáu hu·∫•n luy·ªán phong ph√∫ (v√≠ d·ª• d·ªØ li·ªáu g√°n nh√£n persona).
    

- **Theanine (NAACL 2025)**: M√¥ h√¨nh n√†y c≈©ng thu·ªôc (3) nh∆∞ng v·ªõi c√°ch qu·∫£n l√Ω memory ƒë·∫∑c bi·ªát (ƒë·ªì th·ªã timeline) ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=to%20improve%20retrieval%20quality%2C%20we,human%20efforts%20when%20assessing%20agent)). **∆Øu**: kh√¥ng x√≥a k√Ω ·ª©c c≈©, do ƒë√≥ s·ª≠ d·ª•ng ƒë∆∞·ª£c c·∫£ b·ªëi c·∫£nh l√¢u d√†i ƒë·ªÉ suy lu·∫≠n s·ª± thay ƒë·ªïi; d√πng LLM t·∫°o _memory timeline_ gi√∫p gi·∫£i th√≠ch ƒë∆∞·ª£c m·∫°ch s·ª± ki·ªán. Tuy nhi√™n, do kh√¥ng x√≥a n√™n **th√°ch th·ª©c** l√† ki·ªÉm so√°t k√≠ch th∆∞·ªõc b·ªô nh·ªõ v√† tr√°nh retrieval nh·∫ßm t·ª´ nh·ªØng k√Ω ·ª©c qu√° c≈© kh√¥ng c√≤n ƒë√∫ng.
    

- **MemoryBank (AAAI 2023)**: H·ªá th·ªëng (3) v·ªõi c∆° ch·∫ø qu√™n c√≥ ch·ªçn l·ªçc. **∆Øu**: gi·ªëng n√£o ng∆∞·ªùi h∆°n ‚Äì t·ª± ƒë·ªông l√†m m·ªù c√°c memory √≠t quan tr·ªçng, c·ªßng c·ªë memory quan tr·ªçng ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=personality%20over%20time%20by%20synthesizing,based%20chatbot%20named)). Ngo√†i ra, MemoryBank l∆∞u tr·ªØ ƒëa d·∫°ng: _log h·ªôi tho·∫°i chi ti·∫øt, b·∫£n t√≥m t·∫Øt s·ª± ki·ªán ƒë·ªãnh k·ª≥, v√† h·ªì s∆° ng∆∞·ªùi d√πng_ (user portrait) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=Memory%20Storage%3A%20The%20Warehouse%20of,Memories)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=level%20overviews%20of%20daily%20events,tailor%20its%20responses%20over%20time)), do ƒë√≥ cung c·∫•p ng·ªØ c·∫£nh r·∫•t phong ph√∫ cho m√¥ h√¨nh. K·∫øt qu·∫£ cho th·∫•y chatbot t√≠ch h·ª£p MemoryBank c√≥ th·ªÉ **th·ªÉ hi·ªán s·ª± th·∫•u hi·ªÉu v√† ghi nh·ªõ** v∆∞·ª£t tr·ªôi, nh∆∞ nh·ªõ s·ªü th√≠ch ng∆∞·ªùi d√πng qua nhi·ªÅu tu·∫ßn l·ªÖ ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=psychological%20counseling%2C%20and%20secretarial%20assistance,the%20memory%2C%20thereby%20offering%20a)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=,tailor%20its%20responses%20over%20time)). ƒêi·ªÉm c·∫ßn c·∫£i ti·∫øn l√† ƒë·∫£m b·∫£o c∆° ch·∫ø qu√™n kh√¥ng v√¥ t√¨nh lo·∫°i b·ªè th√¥ng tin c·∫ßn thi·∫øt n·∫øu th·ªùi gian k√©o d√†i (c√¢n b·∫±ng gi·ªØa qu√™n v√† nh·ªõ ƒë√∫ng).
    

  

Nh√¨n chung, **xu h∆∞·ªõng ph√°t tri·ªÉn** cho th·∫•y s·ª± chuy·ªÉn d·ªãch t·ª´ c√°c m√¥ h√¨nh kh√¥ng nh·ªõ ho·∫∑c nh·ªõ ng·∫Øn h·∫°n (BiDAF++, DrQA) sang c√°c h·ªá th·ªëng c√≥ b·ªô nh·ªõ ng√†y c√†ng th√¥ng minh h∆°n (Keep Me Updated, MemoryBank, Theanine, LD-Agent). B·∫£ng so s√°nh tr√™n nh·∫•n m·∫°nh vai tr√≤ c·ªßa c√°c th√†nh ph·∫ßn nh∆∞ **c·∫≠p nh·∫≠t b·ªô nh·ªõ** (update), **c·∫•u tr√∫c h√≥a th√¥ng tin** (theo s·ª± ki·ªán, theo persona), c≈©ng nh∆∞ nh·ªØng ph∆∞∆°ng ph√°p l·∫•y c·∫£m h·ª©ng t·ª´ t√¢m l√Ω h·ªçc (qu√™n c√≥ ch·ªçn l·ªçc) ƒë·ªÉ n√¢ng cao ch·∫•t l∆∞·ª£ng t∆∞∆°ng t√°c d√†i h·∫°n. Ph·∫ßn ti·∫øp theo, ch√∫ng t√¥i s·∫Ω gi·ªõi thi·ªáu c√°c **benchmark v√† ti√™u ch√≠ ƒë√°nh gi√°** ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t nh·∫±m ƒëo l∆∞·ªùng m·ªôt c√°ch h·ªá th·ªëng kh·∫£ nƒÉng ghi nh·ªõ d√†i h·∫°n c·ªßa c√°c m√¥ h√¨nh ƒë·ªëi tho·∫°i n√†y.

  

# Benchmark v√† ti√™u ch√≠ ƒë√°nh gi√° tr√≠ nh·ªõ trong h·ªôi tho·∫°i

  

ƒê·ªÉ ƒë√°nh gi√° kh√°ch quan kh·∫£ nƒÉng ghi nh·ªõ v√† s·ª≠ d·ª•ng th√¥ng tin d√†i h·∫°n, c√°c nh√† nghi√™n c·ª©u ƒë√£ x√¢y d·ª±ng m·ªôt s·ªë **benchmark chuy√™n bi·ªát** c≈©ng nh∆∞ s·ª≠ d·ª•ng c√°c b·ªô d·ªØ li·ªáu h·ªôi tho·∫°i c√≥ y·∫øu t·ªë nh·ªõ. D∆∞·ªõi ƒë√¢y l√† c√°c b·ªô d·ªØ li·ªáu v√† ti√™u ch√≠ n·ªïi b·∫≠t:

  

- **LongMemEval (Wu et al., 2024)** ‚Äì ƒê√¢y l√† m·ªôt b·ªô ƒë√°nh gi√° to√†n di·ªán ƒë·∫ßu ti√™n t·∫≠p trung v√†o **5 k·ªπ nƒÉng tr√≠ nh·ªõ l√µi** c·ªßa tr·ª£ l√Ω chat ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,on%20memorizing%20information%20across%20sustained)). NƒÉm k·ªπ nƒÉng ƒë√≥ bao g·ªìm: (1) **Nh·ªõ v√† tr√≠ch th√¥ng tin** (Information Extraction) ‚Äì ki·ªÉm tra xem m√¥ h√¨nh c√≥ nh·ªõ ch√≠nh x√°c c√°c chi ti·∫øt ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p tr∆∞·ªõc ƒë√≥ hay kh√¥ng; (2) **Suy lu·∫≠n ƒëa phi√™n** (Multi-session reasoning) ‚Äì ƒë√°nh gi√° kh·∫£ nƒÉng k·∫øt n·ªëi th√¥ng tin qua nhi·ªÅu phi√™n tr√≤ chuy·ªán r·ªùi (v√≠ d·ª•: ng∆∞·ªùi d√πng n√≥i A ·ªü tu·∫ßn tr∆∞·ªõc v√† B ·ªü tu·∫ßn n√†y, li·ªáu bot c√≥ k·∫øt h·ª£p A v√† B ƒë·ªÉ tr·∫£ l·ªùi?); (3) **Suy lu·∫≠n th·ªùi gian** (Temporal reasoning) ‚Äì ki·ªÉm tra hi·ªÉu bi·∫øt v·ªÅ tr√¨nh t·ª± th·ªùi gian, nguy√™n nh√¢n-k·∫øt qu·∫£ theo th·ªùi gian (v√≠ d·ª• s·ª± ki·ªán X x·∫£y ra sau Y th√¨ h·ªá qu·∫£ ra sao); (4) **C·∫≠p nh·∫≠t ki·∫øn th·ª©c** (Knowledge updates) ‚Äì ƒë√°nh gi√° vi·ªác bot c√≥ s·ª≠ d·ª•ng th√¥ng tin m·ªõi thay cho th√¥ng tin c≈© khi ch√∫ng m√¢u thu·∫´n (gi·ªëng b√†i to√°n c·∫≠p nh·∫≠t tr√≠ nh·ªõ COVID ·ªü tr√™n); (5) **Abstention (t·ª´ ch·ªëi)** ‚Äì xem m√¥ h√¨nh c√≥ bi·∫øt t·ª´ ch·ªëi tr·∫£ l·ªùi khi kh√¥ng ch·∫Øc do thi·∫øu tr√≠ nh·ªõ hay kh√¥ng (tr√°nh tr∆∞·ªùng h·ª£p ƒëo√°n b·ª´a/hallucinate) ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,term)). LongMemEval g·ªìm 500 c√¢u h·ªèi ƒë∆∞·ª£c g√†i c·∫©n th·∫≠n v√†o c√°c l·ªãch s·ª≠ h·ªôi tho·∫°i d√†i, m·ªói c√¢u h·ªèi t∆∞∆°ng ·ª©ng ki·ªÉm tra m·ªôt kh√≠a c·∫°nh tr√™n. K·∫øt qu·∫£ th·ª±c nghi·ªám cho th·∫•y c√°c chatbot hi·ªán t·∫°i (k·ªÉ c·∫£ m√¥ h√¨nh l·ªõn v·ªõi ng·ªØ c·∫£nh d√†i) **gi·∫£m hi·ªáu su·∫•t t·ªõi ~30%** khi ph·∫£i ghi nh·ªõ th√¥ng tin tr·∫£i d√†i, so v·ªõi c√°c c√¢u h·ªèi ng·∫Øn h·∫°n ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=meticulously%20curated%20questions%20embedded%20within,augmented%20key)). ƒêi·ªÅu n√†y kh·∫≥ng ƒë·ªãnh ƒë·ªô kh√≥ c·ªßa b√†i to√°n v√† s·ª± c·∫ßn thi·∫øt c·ªßa c√°c ph∆∞∆°ng ph√°p memory augmentation. LongMemEval hi·ªán ƒë∆∞·ª£c coi l√† th∆∞·ªõc ƒëo ti√™u chu·∫©n, khuy·∫øn kh√≠ch c√°c nghi√™n c·ª©u t∆∞∆°ng lai c·∫£i thi·ªán c·∫£ 5 k·ªπ nƒÉng k·ªÉ tr√™n ƒë·ªÉ ti·∫øn t·ªõi tr·ª£ l√Ω ƒë·ªëi tho·∫°i ƒë√°ng tin c·∫≠y h∆°n ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=interactions,term)).
    

- **LOCOMO (Maharana et al., 2024)** ‚Äì L√† vi·∫øt t·∫Øt c·ªßa _Long Conversation Model_, ƒë√¢y ƒë∆∞·ª£c b√°o c√°o l√† b·ªô d·ªØ li·ªáu h·ªôi tho·∫°i _d√†i nh·∫•t_ hi·ªán nay, v·ªõi trung b√¨nh **300 l∆∞·ª£t tho·∫°i (9k token)** m·ªói h·ªôi tho·∫°i ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=%28i%29%20LOCOMO%C2%A0%28Maharana%20et%C2%A0al,on%20the%20recently%20released%20official)). LOCOMO m√¥ ph·ªèng c√°c cu·ªôc tr√≤ chuy·ªán li√™n t·ª•c, nhi·ªÅu ch·ªß ƒë·ªÅ, ƒë√≤i h·ªèi m√¥ h√¨nh ph·∫£i duy tr√¨ t∆∞∆°ng t√°c m·∫°ch l·∫°c trong th·ªùi gian r·∫•t d√†i. ƒê·ªÉ ƒë√°nh gi√°, t√°c gi·∫£ d√πng GPT-4 sinh ra c√°c c√¢u h·ªèi ki·ªÉm tra v·ªÅ n·ªôi dung ƒë√£ n√≥i t·ª´ r·∫•t s·ªõm trong phi√™n, nh·∫±m xem bot c√≥ nh·ªõ hay kh√¥ng ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=%28i%29%20LOCOMO%C2%A0%28Maharana%20et%C2%A0al,on%20the%20recently%20released%20official)). Ngo√†i ra, LOCOMO c√≤n ƒëo l∆∞·ªùng m·ª©c ƒë·ªô tr√¥i ch·∫£y v√† nh·∫•t qu√°n qua th∆∞·ªõc ƒëo **GPT4Score** v√† c√°c ch·ªâ s·ªë ng√¥n ng·ªØ t·ª± nhi√™n (BLEU, ROUGE) cho ph·∫£n h·ªìi c·ªßa m√¥ h√¨nh ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=long,in%20performance%20improvements%20up%20to)) ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=Methods%20LOCOMO%20Long,44)). C√πng v·ªõi LOCOMO, m·ªôt s·ªë bi·∫øn th·ªÉ nh∆∞ **Long-MT-Bench+** c≈©ng ƒë∆∞·ª£c d√πng ‚Äì ƒë√¢y l√† m·ªü r·ªông c·ªßa b·ªô ƒë√°nh gi√° Multi-Turn Dialogue (MT-Bench) d√†nh ri√™ng cho h·ªôi tho·∫°i d√†i. C√°c k·∫øt qu·∫£ baseline tr√™n LOCOMO cho th·∫•y n·∫øu m√¥ h√¨nh ch·ªâ d√πng l·ªãch s·ª≠ r·∫•t ng·∫Øn (ho·∫∑c kh√¥ng l·ªãch s·ª≠) th√¨ ƒëi·ªÉm s·ªë tr·∫£ l·ªùi ƒë√∫ng r·∫•t th·∫•p (~25-50), trong khi d√πng full history n√¢ng l√™n ~54 ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=match%20at%20L389%20LOCOMO%20Zero,77%203%2C288)). Tuy nhi√™n, d√πng full history phi·∫øn di·ªán c≈©ng g√¢y m·ªèi model (13,000 token) v√† kh√¥ng nh·∫•t thi·∫øt t·ªëi ∆∞u. Do v·∫≠y LOCOMO ƒë∆∞·ª£c d√πng ƒë·ªÉ th·ª≠ nghi·ªám c√°c chi·∫øn l∆∞·ª£c nh·ªõ: th√≠ d·ª• SeCom tr√™n LOCOMO ƒë·∫°t **GPT4Score ~69**, cao h∆°n h·∫≥n so v·ªõi m√¥ h√¨nh kh√¥ng module nh·ªõ (~24) ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=LOCOMO%20Zero%20History%2024,77%203%2C288)) ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=Methods%20LOCOMO%20Long,44)). ƒêi·ªÅu n√†y x√°c nh·∫≠n l·ª£i √≠ch r√µ r·ªát c·ªßa memory ƒë·ªëi v·ªõi h·ªôi tho·∫°i si√™u d√†i.
    

- **C√°c b·ªô d·ªØ li·ªáu personalized v√† multi-session**: Tr∆∞·ªõc khi c√≥ c√°c benchmark tr√™n, m·ªôt s·ªë b·ªô d·ªØ li·ªáu h·ªôi tho·∫°i ƒë∆∞·ª£c t·∫°o ra nh·∫±m ki·ªÉm tra m·ªôt ph·∫ßn kh√≠a c·∫°nh c·ªßa tr√≠ nh·ªõ. **Persona-Chat (Zhang et al., 2018)** cung c·∫•p cho m·ªói nh√¢n v·∫≠t m·ªôt h·ªì s∆° s·ªü th√≠ch (5 c√¢u m√¥ t·∫£) v√† y√™u c·∫ßu m√¥ h√¨nh tr√≤ chuy·ªán gi·ªØ ƒë√∫ng persona n√†y. ƒê√¢y l√† ki·ªÉm tra kh·∫£ nƒÉng **nh·ªõ th√¥ng tin h·ªì s∆° tƒ©nh** ‚Äì g·∫ßn v·ªõi memory ng·∫Øn h·∫°n (v√¨ persona kh√¥ng ƒë·ªïi). **MuTual (Cui et al., 2020)** v√† **DSTC7,8** cung c·∫•p c√°c ƒëo·∫°n h·ªôi tho·∫°i y√™u c·∫ßu suy lu·∫≠n logic gi·ªØa c√°c l∆∞·ª£t ‚Äì gi√°n ti·∫øp ƒë√≤i h·ªèi nh·ªõ n·ªôi dung tr∆∞·ªõc. **QuAC, CoQA (2018)** nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p, ƒë√°nh gi√° kh·∫£ nƒÉng tr·∫£ l·ªùi d·ª±a v√†o nhi·ªÅu l∆∞·ª£t h·ªèi tr∆∞·ªõc (context co-reference). Tuy nhi√™n, c√°c dataset n√†y th∆∞·ªùng ch·ªâ k√©o d√†i t·ªëi ƒëa v√†i ch·ª•c l∆∞·ª£t trong m·ªôt phi√™n, v√† kh√¥ng ƒë√°nh gi√° xuy√™n phi√™n hay c·∫≠p nh·∫≠t. G·∫ßn ƒë√¢y, m·ªôt s·ªë dataset h∆∞·ªõng ƒë·∫øn **ƒëa phi√™n**: v√≠ d·ª• **MSC (Multi-Session Chat)** (Xu et al., 2022) n·ªëi 2-3 phi√™n PersonaChat l·∫°i ƒë·ªÉ xem bot c√≥ nh·ªõ th√¥ng tin gi·ªØa c√°c phi√™n; hay **CareCall-Mem** (Bae et al., 2022) ‚Äì d·ªØ li·ªáu ti·∫øng H√†n m√† nh√≥m Keep Me Updated x√¢y d·ª±ng ‚Äì g·ªìm 5 phi√™n tr√≤ chuy·ªán gi·ªØa bot v√† m·ªôt ng∆∞·ªùi d√πng h∆∞ c·∫•u v·ªõi c√°c th√¥ng tin c√° nh√¢n thay ƒë·ªïi theo th·ªùi gian (s·ª©c kh·ªèe, th√≥i quen) () (). C√°c dataset n√†y ph·ª•c v·ª• hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh trong b·ªëi c·∫£nh **th√¥ng tin ng∆∞·ªùi d√πng thay ƒë·ªïi**: v√≠ d·ª• phi√™n 1 n√≥i ‚Äúgh√©t v·∫≠n ƒë·ªông‚Äù, phi√™n 3 l·∫°i n√≥i ‚Äúƒëang h·ªçc b∆°i‚Äù th√¨ bot ph·∫£i hi·ªÉu s·ªü th√≠ch ƒë√£ thay ƒë·ªïi. Ti√™u ch√≠ ƒë√°nh gi√° g·ªìm ƒë·ªô t·ª± nhi√™n, t√≠nh g·∫Øn k·∫øt, v√† quan tr·ªçng l√† **ƒë·ªô ch√≠nh x√°c c·ªßa th√¥ng tin** m√† bot n√≥i ra so v·ªõi h·ªì s∆° th·ª±c t·∫ø (tr√°nh nh·∫ßm th√¥ng tin c≈©).
    

- **Ti√™u ch√≠ ƒë√°nh gi√°**: D·ª±a tr√™n c√°c benchmark tr√™n, ta c√≥ th·ªÉ li·ªát k√™ nh·ªØng ti√™u ch√≠ ch√≠nh ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng tr√≠ nh·ªõ d√†i h·∫°n c·ªßa h·ªá th·ªëng h·ªôi tho·∫°i:
    
    - _Ch√≠nh x√°c th√¥ng tin ƒë√£ nh·ªõ (Memory Recall)_: Ki·ªÉm tra t·ªâ l·ªá th√¥ng tin ƒë√∫ng ƒë∆∞·ª£c bot nh·∫Øc l·∫°i khi c·∫ßn. V√≠ d·ª•, user ƒë√£ n√≥i h·ªç sinh nƒÉm 1990, sau 10 l∆∞·ª£t bot ƒë·ªÅ c·∫≠p l·∫°i ƒë√∫ng nƒÉm sinh hay kh√¥ng. Ti√™u ch√≠ n√†y ƒëo b·∫±ng c√¢u h·ªèi tr·ª±c ti·∫øp (nh∆∞ LongMemEval) ho·∫∑c so kh·ªõp v·ªõi log qu√° kh·ª©.
        
    
    - _Ph·∫£n h·ªìi nh·∫•t qu√°n, kh√¥ng ·∫£o gi√°c (Consistency & No-hallucination)_: ƒê√°nh gi√° xem bot c√≥ m√¢u thu·∫´n v·ªõi ch√≠nh n√≥ ho·∫∑c v·ªõi th·ª±c t·∫ø ƒë√£ bi·∫øt kh√¥ng, v√† c√≥ b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng c√≥ trong b·ªô nh·ªõ kh√¥ng. N·∫øu bot _qu√™n_ m·ªôt chi ti·∫øt v√† t·ª± ch·∫ø ra, ƒë√≥ l√† ƒëi·ªÉm tr·ª´ l·ªõn. Th∆∞·ªõc ƒëo c√≥ th·ªÉ b·∫±ng ki·ªÉm tra logic (v√≠ d·ª• Persona-Chat y√™u c·∫ßu kh√¥ng n√≥i sai persona), ho·∫∑c nh·ªù ƒë√°nh gi√° c·ªßa m√¥ h√¨nh/human xem c√¢u tr·∫£ l·ªùi c√≥ cƒÉn c·ª© qu√° kh·ª© hay kh√¥ng.
        
    
    - _C·∫≠p nh·∫≠t ki·∫øn th·ª©c k·ªãp th·ªùi (Knowledge Update Accuracy)_: Khi ng∆∞·ªùi d√πng cung c·∫•p th√¥ng tin m·ªõi ho·∫∑c ƒë√≠nh ch√≠nh, bot c√≥ ph·∫£n √°nh ƒë√∫ng s·ª± thay ƒë·ªïi trong c√°c l∆∞·ª£t sau kh√¥ng. Ti√™u ch√≠ n√†y th∆∞·ªùng ƒë√°nh gi√° theo k·ªãch b·∫£n: v√≠ d·ª• nh∆∞ b√†i to√°n COVID test ·ªü tr√™n ‚Äì sau khi user b√°o d∆∞∆°ng t√≠nh, bot ph·∫£i qu√™n th√¥ng tin ‚Äúch∆∞a x√©t nghi·ªám‚Äù tr∆∞·ªõc ƒë√≥. C√≥ th·ªÉ ƒëo b·∫±ng truy v·∫•n sau update xem bot tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin n√†o.
        
    
    - _Suy lu·∫≠n theo d√≤ng th·ªùi gian (Temporal Reasoning)_: Bot c√≥ hi·ªÉu m·ªëi quan h·ªá th·ªùi gian gi·ªØa c√°c s·ª± ki·ªán trong tr√≠ nh·ªõ kh√¥ng. V√≠ d·ª•, user n√≥i ‚ÄúnƒÉm 2020 t√¥i t·ªët nghi·ªáp‚Äù, sau ƒë√≥ h·ªèi ‚Äú2 nƒÉm sau t√¥i l√†m g√¨‚Äù ‚Äì bot ph·∫£i bi·∫øt 2 nƒÉm sau 2020 l√† 2022 v√† t√¨m trong memory xem 2022 c√≥ s·ª± ki·ªán g√¨ (ho·∫∑c tr·∫£ l·ªùi ch∆∞a bi·∫øt n·∫øu kh√¥ng c√≥). Kh·∫£ nƒÉng n√†y th∆∞·ªùng ƒëo b·∫±ng c√°c c√¢u h·ªèi y√™u c·∫ßu k·∫øt h·ª£p m·ªëc th·ªùi gian (nh∆∞ trong LongMemEval) ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,on%20memorizing%20information%20across%20sustained)).
        
    
    - _Kh·∫£ nƒÉng t·ª´ ch·ªëi khi kh√¥ng nh·ªõ (Abstention)_: M·ªôt h·ªá th·ªëng t·ªët c·∫ßn bi·∫øt gi·ªõi h·∫°n tr√≠ nh·ªõ c·ªßa m√¨nh, t·ª©c l√† n·∫øu th√¥ng tin kh√¥ng c√≥ trong b·ªô nh·ªõ th√¨ n√™n xin l·ªói ho·∫∑c t·ª´ ch·ªëi h∆°n l√† b·ªãa. Ti√™u ch√≠ n√†y ƒë√°nh gi√° t·ª∑ l·ªá bot **kh√¥ng ƒëo√°n b·ª´a**. LongMemEval ƒë∆∞a ra c√°c t√¨nh hu·ªëng m√† c√¢u h·ªèi ngo√†i ph·∫°m vi nh·ªØng g√¨ ƒë√£ n√≥i, y√™u c·∫ßu bot ph·∫£i ph·∫£n h·ªìi ki·ªÉu ‚ÄúT√¥i kh√¥ng nh·ªõ r√µ‚Ä¶‚Äù thay v√¨ cung c·∫•p th√¥ng tin sai ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,term)).
        
    

  

Ngo√†i ra, c√°c ti√™u ch√≠ t·ªïng quan nh∆∞ **ƒë·ªô h√†i l√≤ng ng∆∞·ªùi d√πng, ƒë·ªô t·ª± nhi√™n c·ªßa h·ªôi tho·∫°i, ƒëi·ªÉm ƒë√°nh gi√° c·ªßa gi√°m kh·∫£o** c≈©ng r·∫•t quan tr·ªçng, nh∆∞ng ch√∫ng ch·ªãu ·∫£nh h∆∞·ªüng nhi·ªÅu y·∫øu t·ªë ngo√†i tr√≠ nh·ªõ (nh∆∞ k·ªπ nƒÉng ng√¥n ng·ªØ chung c·ªßa m√¥ h√¨nh). Do ƒë√≥, c√°c benchmark chuy√™n bi·ªát c·ªë g·∫Øng c√¥ l·∫≠p ·∫£nh h∆∞·ªüng c·ªßa tr√≠ nh·ªõ ƒë·ªÉ ƒë√°nh gi√° c√¥ng b·∫±ng gi·ªØa c√°c gi·∫£i ph√°p.

  

# H∆∞·ªõng m·ªü r·ªông v√† k·∫øt lu·∫≠n

  

**Tr√≠ nh·ªõ d√†i h·∫°n cho h·ªá th·ªëng ƒë·ªëi tho·∫°i** v·∫´n l√† m·ªôt b√†i to√°n m·ªü v·ªõi nhi·ªÅu h∆∞·ªõng nghi√™n c·ª©u ti·ªÅm nƒÉng. D·ª±a tr√™n c√°c xu h∆∞·ªõng hi·ªán t·∫°i, c√≥ th·ªÉ g·ª£i √Ω m·ªôt s·ªë h∆∞·ªõng ph√°t tri·ªÉn ch√≠nh sau:

  

- **K·∫øt h·ª£p ch·∫∑t ch·∫Ω gi·ªØa truy h·ªìi v√† c·∫≠p nh·∫≠t tri th·ª©c**: Hi·ªán nay, retrieval augmented generation (RAG) ƒë√£ ph·ªï bi·∫øn trong QA m·ªü, nh∆∞ng th∆∞·ªùng v·ªõi _knowledge base_ tƒ©nh. M·ªü r·ªông h∆°n, ta c√≥ th·ªÉ t√≠ch h·ª£p RAG v√†o ƒë·ªëi tho·∫°i sao cho **kho tri th·ª©c ƒë∆∞·ª£c c·∫≠p nh·∫≠t li√™n t·ª•c trong qu√° tr√¨nh tr√≤ chuy·ªán**. V√≠ d·ª•, khi ng∆∞·ªùi d√πng cung c·∫•p m·ªôt th√¥ng tin m·ªõi, h·ªá th·ªëng ngay l·∫≠p t·ª©c th√™m n√≥ v√†o _b·ªô nh·ªõ tri th·ª©c_ v√† c√°c l∆∞·ª£t sau truy h·ªìi c√≥ th·ªÉ l·∫•y ra. ƒêi·ªÅu n√†y ƒë√≤i h·ªèi gi·∫£i quy·∫øt b√†i to√°n ƒë·ªìng b·ªô gi·ªØa th√†nh ph·∫ßn ghi nh·ªõ v√† th√†nh ph·∫ßn t√¨m ki·∫øm. M·ªôt h∆∞·ªõng l√† ph√°t tri·ªÉn c√°c ph∆∞∆°ng ph√°p **index ƒë·ªông**: c·∫≠p nh·∫≠t ch·ªâ m·ª•c b·ªô nh·ªõ theo th·ªùi gian th·ª±c, ho·∫∑c s·ª≠ d·ª•ng m√¥ h√¨nh h·ªçc tƒÉng c∆∞·ªùng ƒë·ªÉ quy·∫øt ƒë·ªãnh khi n√†o c·∫ßn _re-index_.
    

- **Truy h·ªìi th√≠ch ·ª©ng v√† c√≥ h∆∞·ªõng d·∫´n**: Thay v√¨ lu√¥n truy h·ªìi m·ªôt c√°ch m√°y m√≥c top-k ƒëo·∫°n gi·ªëng nh∆∞ hi·ªán nay, m√¥ h√¨nh c√≥ th·ªÉ h·ªçc c√°ch **ƒë·∫∑t truy v·∫•n th√¥ng minh** ho·∫∑c **ch·ªçn l·ªçc** t√πy t√¨nh hu·ªëng. Ch·∫≥ng h·∫°n, n·∫øu c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng r·∫•t r√µ r√†ng (nh∆∞ h·ªèi t√™n ƒë√£ cho tr∆∞·ªõc ƒë√≥), m·ªôt truy v·∫•n th·∫≥ng s·∫Ω hi·ªáu qu·∫£; nh∆∞ng n·∫øu c√¢u h·ªèi m∆° h·ªì, m√¥ h√¨nh c√≥ th·ªÉ t·ª± sinh ra m·ªôt truy v·∫•n r√µ h∆°n d·ª±a tr√™n ng·ªØ c·∫£nh ‚Äì t∆∞∆°ng t·ª± k·ªπ thu·∫≠t _query rewriting_ ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=Query%20Rewriting)). Ngo√†i ra, m√¥ h√¨nh n√™n h·ªçc _khi n√†o_ th√¨ c·∫ßn truy h·ªìi: ƒë√¥i khi, c√¢u h·ªèi hi·ªán t·∫°i kh√¥ng li√™n quan g√¨ ƒë·∫øn qu√° kh·ª©, vi·ªác truy h·ªìi ch·ªâ th√™m nhi·ªÖu. C√≥ th·ªÉ d√πng m·ªôt module ph·ª• (nh∆∞ m·ªôt classifier) ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ truy h·ªìi memory kh√¥ng ·ªü m·ªói l∆∞·ª£t. M·ªôt √Ω t∆∞·ªüng kh√°c l√† cho ch√≠nh LLM **h∆∞·ªõng d·∫´n vi·ªác truy h·ªìi**: v√≠ d·ª• tr∆∞·ªõc khi tr·∫£ l·ªùi, m√¥ h√¨nh t·ª± suy lu·∫≠n "ƒê·ªÉ tr·∫£ l·ªùi, t√¥i c·∫ßn nh·ªõ X", sau ƒë√≥ d√πng suy lu·∫≠n n√†y l√†m ch√¨a kh√≥a t√¨m ki·∫øm b·ªô nh·ªõ. ƒê√¢y l√† m·ªôt d·∫°ng _chain-of-thought for retrieval_ ƒë·∫ßy h·ª©a h·∫πn.
    

- **S·ª≠ d·ª•ng m√¥ h√¨nh ng√¥n ng·ªØ ph·ª• tr·ª£ cho qu·∫£n l√Ω tr√≠ nh·ªõ**: Thay v√¨ c√°c rule c·ª©ng (nh∆∞ 4 thao t√°c c·ªßa Keep Me Updated), ta c√≥ th·ªÉ d√πng m·ªôt LLM nh·ªè ho·∫∑c c√°c prompt ƒë·∫∑c bi·ªát cho ch√≠nh LLM l·ªõn ƒë·ªÉ qu·∫£n l√Ω memory. V√≠ d·ª•, c√≥ th·ªÉ tri·ªÉn khai m·ªôt _‚ÄúMemory Manager Agent‚Äù_ ch·∫°y song song: agent n√†y d√πng LLM ƒë·ªÉ ƒë·ªãnh k·ª≥ ƒë·ªçc l·ªãch s·ª≠ v√† vi·∫øt t√≥m t·∫Øt, l∆∞u v√†o vector DB; khi c·∫ßn th√¨ h·ªó tr·ª£ truy v·∫•n vector DB v√† cung c·∫•p k·∫øt qu·∫£ cho LLM ch√≠nh. C√°ch ti·∫øp c·∫≠n ki·∫øn tr√∫c agent n√†y ƒë√£ ƒë∆∞·ª£c Park et al. (2023) th·ª≠ nghi·ªám trong **Generative Agents**, n∆°i nhi·ªÅu agent LLM t∆∞∆°ng t√°c v·ªõi nhau v√† c√≥ b·ªô nh·ªõ s·ª± ki·ªán ƒë∆∞·ª£c ghi l·∫°i v√† suy di·ªÖn b·∫±ng LLM. M·ªôt ·ª©ng d·ª•ng kh√°c l√† d√πng LLM ƒë·ªÉ **ƒë√°nh gi√° v√† ch·ªânh s·ª≠a** memory: v√≠ d·ª• d√πng GPT-4 ƒë·ªçc to√†n b·ªô memory log v√† ph√°t hi·ªán m√¢u thu·∫´n ho·∫∑c l·ªói ƒë·ªÉ s·ª≠a (m·ªôt d·∫°ng reviewer). Nh√¨n chung, t·∫≠n d·ª•ng kh·∫£ nƒÉng ng√¥n ng·ªØ ƒëa nƒÉng c·ªßa LLM cho vi·ªác qu·∫£n tr·ªã tr√≠ nh·ªõ c√≥ th·ªÉ ƒëem l·∫°i linh ho·∫°t h∆°n so v·ªõi c√°ch l√†m thu·∫ßn heuristic.
    

- **M·ªü r·ªông sang ƒëa m√¥ h√¨nh v√† tri th·ª©c th·∫ø gi·ªõi**: Tr√≠ nh·ªõ h·ªôi tho·∫°i kh√¥ng ch·ªâ g·ªìm l·ªùi tho·∫°i ‚Äì trong nhi·ªÅu ·ª©ng d·ª•ng, n√≥ c·∫ßn nh·ªõ c·∫£ c√°c **th√¥ng tin th·ªã gi√°c, c·∫£m bi·∫øn, hay tri th·ª©c ngo√†i**. H∆∞·ªõng m·ªü l√† t√≠ch h·ª£p **b·ªô nh·ªõ chung cho ƒëa m√¥ h√¨nh**: v√≠ d·ª• m·ªôt robot tr·ª£ l√Ω nh√† th√¥ng minh c·∫ßn nh·ªõ h√¥m qua camera th·∫•y g√¨, ai ƒë√£ gh√© thƒÉm, ƒë·ªì v·∫≠t ƒë·∫∑t ·ªü ƒë√¢u... c√πng v·ªõi h·ªôi tho·∫°i v·ªõi ch·ªß nh√†. ƒêi·ªÅu n√†y ƒë·∫∑t ra b√†i to√°n l∆∞u tr·ªØ v√† truy h·ªìi c√°c **ƒë·∫°i di·ªán ƒëa m√¥ h√¨nh** (h√¨nh ·∫£nh, √¢m thanh) b√™n c·∫°nh vƒÉn b·∫£n. T∆∞∆°ng t·ª±, k·∫øt h·ª£p **knowledge graph** ho·∫∑c c∆° s·ªü tri th·ª©c v√†o memory: v√≠ d·ª• khi ng∆∞·ªùi d√πng n√≥i s·ªü th√≠ch, bot c√≥ th·ªÉ l∆∞u v√†o m·ªôt _knowledge graph node_ v·ªÅ ng∆∞·ªùi d√πng, li√™n k·∫øt v·ªõi c√°c node ho·∫°t ƒë·ªông t∆∞∆°ng ·ª©ng. Vi·ªác k·∫øt h·ª£p c·∫•u tr√∫c tri th·ª©c c√≥ th·ªÉ gi√∫p bot suy lu·∫≠n logic v√† nh·∫•t qu√°n h∆°n (tr√°nh m√¢u thu·∫´n th·ª±c t·∫ø). M·ªôt h∆∞·ªõng l√† m·ªói khi memory update, ƒë·ªìng th·ªùi c·∫≠p nh·∫≠t knowledge graph, v√† d√πng graph embedding ƒë·ªÉ h·ªó tr·ª£ retrieval song song.
    

- **ƒê√°nh gi√° v√† gi·∫£m thi·ªÉu nhi·ªÖu do tr√≠ nh·ªõ sai**: Khi t√≠ch h·ª£p b·ªô nh·ªõ, m·ªôt nguy c∆° l√† _nh·ªõ sai ho·∫∑c nh·ªõ m∆° h·ªì_ c√≥ th·ªÉ d·∫´n ƒë·∫øn ph·∫£n h·ªìi sai (hallucination do memory). Do ƒë√≥, c·∫ßn c∆° ch·∫ø **ƒë√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa memory**. M·ªôt h∆∞·ªõng l√† k√®m theo m·ªói m·∫©u memory m·ªôt ƒë·ªô tin c·∫≠y (confidence score) v√† th·ªùi gian, ƒë·ªÉ m√¥ h√¨nh ∆∞u ti√™n d√πng th√¥ng tin m·ªõi v√† c√≥ ƒë·ªô tin c·∫≠y cao. N·∫øu memory qu√° c≈©, m√¥ h√¨nh c√≥ th·ªÉ c·∫£nh b√°o. M·ªôt h∆∞·ªõng kh√°c l√† hu·∫•n luy·ªán m√¥ h√¨nh **ph√°t hi·ªán m√¢u thu·∫´n** gi·ªØa memory v√† message hi·ªán t·∫°i: n·∫øu ph√°t hi·ªán user n√≥i ƒëi·ªÅu tr√°i ng∆∞·ª£c h·∫≥n v·ªõi memory c≈©, c√≥ th·ªÉ k√≠ch ho·∫°t m·ªôt _quy tr√¨nh x√°c minh_, h·ªèi l·∫°i ng∆∞·ªùi d√πng ƒë·ªÉ ch·∫Øc ch·∫Øn tr∆∞·ªõc khi c·∫≠p nh·∫≠t.
    

  

T√≥m l·∫°i, **h·ªá th·ªëng ƒë·ªëi tho·∫°i t√≠ch h·ª£p tr√≠ nh·ªõ d√†i h·∫°n** ƒëang d·∫ßn tr·ªü n√™n kh·∫£ thi nh·ªù c√°c ti·∫øn b·ªô trong c·∫£ m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn l·∫´n k·ªπ thu·∫≠t qu·∫£n l√Ω tri th·ª©c. T·ª´ nh·ªØng m√¥ h√¨nh QA ƒë∆°n l∆∞·ª£t ƒë∆°n gi·∫£n, ch√∫ng ta ƒë√£ ch·ª©ng ki·∫øn s·ª± ra ƒë·ªùi c·ªßa c√°c chatbot c√≥ kh·∫£ nƒÉng ghi nh·ªõ h√†ng trƒÉm l∆∞·ª£t tho·∫°i, c√° nh√¢n h√≥a theo ng∆∞·ªùi d√πng, v√† c·∫≠p nh·∫≠t hi·ªÉu bi·∫øt theo th·ªùi gian. D√π v·∫´n c√≤n nh·ªØng th√°ch th·ª©c v·ªÅ t·ªëi ∆∞u v√† ƒë·ªô tin c·∫≠y, h∆∞·ªõng nghi√™n c·ª©u n√†y h·ª©a h·∫πn ƒëem l·∫°i c√°c tr·ª£ l√Ω ·∫£o **nh·ªõ l√¢u, hi·ªÉu s√¢u v√† ph·∫£n h·ªìi t·ª± nhi√™n** h∆°n ‚Äì m·ªôt b∆∞·ªõc ti·∫øn l·ªõn t·ªõi **AI ƒë·ªëi tho·∫°i mang t√≠nh c√° nh√¢n v√† ƒë√°ng tin c·∫≠y** trong t∆∞∆°ng lai g·∫ßn. C√°c nghi√™n c·ª©u m·ªõi nh∆∞ LongMemEval ƒëang t·∫°o n·ªÅn t·∫£ng ƒë·ªÉ **ƒë√°nh gi√° c√≥ h·ªá th·ªëng** c√°c ti·∫øn b·ªô, c√≤n c√°c √Ω t∆∞·ªüng k·∫øt h·ª£p memory v√† LLM (MemoryBank, THEANINE, LD-Agent) ƒëang m·ªü ƒë∆∞·ªùng cho th·∫ø h·ªá m√¥ h√¨nh h·ªôi tho·∫°i th√¥ng minh k·∫ø ti·∫øp. Ch√∫ng ta c√≥ th·ªÉ k·ª≥ v·ªçng trong t∆∞∆°ng lai, s·ª± k·∫øt h·ª£p gi·ªØa **c·ª≠a s·ªï ng·ªØ c·∫£nh l·ªõn** v√† **b·ªô nh·ªõ ngo√†i linh ho·∫°t** s·∫Ω gi√∫p x√≥a nh√≤a ranh gi·ªõi v·ªÅ tr√≠ nh·ªõ trong ƒë·ªëi tho·∫°i, cho ph√©p c√°c h·ªá th·ªëng AI tr√≤ chuy·ªán m·ªôt c√°ch m·∫°ch l·∫°c v√† hi·ªÉu bi·∫øt qua _nhi·ªÅu th√°ng, nhi·ªÅu nƒÉm_ t∆∞∆°ng t√°c v·ªõi con ng∆∞·ªùi.

  

**T√†i li·ªáu tham kh·∫£o:**

  

1. Wu, D. _et al._ (2024). _LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory_. **ICLR 2025 (preprint)** ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,term)) ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=meticulously%20curated%20questions%20embedded%20within,augmented%20key)).
    

2. Qu, C. _et al._ (2020). _Open-Retrieval Conversational Question Answering_. **SIGIR 2020** ([[2005.11364] Open-Retrieval Conversational Question Answering](https://arxiv.org/abs/2005.11364#:~:text=retrieval%20conversational%20question%20answering%20,the%20reranker%20component%20contributes%20to)).
    

3. Bae, S. _et al._ (2022). _Keep Me Updated! Memory Management in Long-term Conversations_. **Findings of EMNLP 2022** () ().
    

4. Li, H. _et al._ (2025). _Hello Again! LLM-powered Personalized Agent for Long-term Dialogue (LD-Agent)_. **NAACL 2025 (to appear)** ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=the%20Long,Agent%20are)).
    

5. Zhong, W. _et al._ (2023). _MemoryBank: Enhancing Large Language Models with Long-Term Memory_. **arXiv:2305.10250** ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=personality%20over%20time%20by%20synthesizing,based%20chatbot%20named)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=Memory%20Updating)).
    

6. Ong, K.T. _et al._ (2025). _THEANINE: Timeline-based Memory Management for Lifelong Dialogue Agents_. **NAACL 2025 (to appear)** ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=to%20improve%20retrieval%20quality%2C%20we,human%20efforts%20when%20assessing%20agent)).
    

7. Weston, J. _et al._ (2015). _Memory Networks_. **ICLR 2015** ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=,chaining%20multiple%20supporting%20sentences%20to)).
    

8. Graves, A. _et al._ (2016). _Hybrid computing using a neural network with dynamic external memory (DNC)_. **Nature 538, 471‚Äì476 (2016)** ([Differentiable neural computer - Wikipedia](https://en.wikipedia.org/wiki/Differentiable_neural_computer#:~:text=DNC%20indirectly%20takes%20inspiration%20from,by%20finding%20a%20%2052)).
    

9. Seo, M. _et al._ (2017). _Bidirectional Attention Flow for Machine Comprehension (BiDAF)_. **ICLR 2017** ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=4,representation%20generated%20when%20answering%20previous)).
    

10. Chen, D. _et al._ (2017). _Reading Wikipedia to Answer Open-Domain Questions (DrQA)_. **ACL 2017** ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=,JASIS%2C%2038%3A389%E2%80%93404%2C%201987)).