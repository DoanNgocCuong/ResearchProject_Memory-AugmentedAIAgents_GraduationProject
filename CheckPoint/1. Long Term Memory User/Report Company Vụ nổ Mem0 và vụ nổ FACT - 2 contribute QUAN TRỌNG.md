
  

  

# giáº£i phÃ¡p:

- cÃ¡c ká»¹ thuáº­t cáº£i thiá»‡n kháº£ nÄƒng Chunking, Retrieval Há»™i thoáº¡i.
    

  

  

- Äá»‹nh nghÄ©a. Ká»¹ thuáº­t : summary, ... Äá»‹nh nghÄ©a phiÃªn, Ä‘á»‹nh nghÄ©a há»™i thoáº¡i, ... => summary, ... CÃ“ Ká»¸ THUáº¬T SUMMARY, ká»¹ thuáº­t Ä‘Ã³ nhÆ° nÃ o? sÃ¢u hÆ¡n Ä‘i? Summary há»™i thoáº¡i nÃ³ khÃ¡c summary vÄƒn báº£n.
    

  

1. MOTIVATION RÃ• RÃ€NG => Má»šI TRIá»‚N?
    
2. DATASET? CÃCH Há»Œ ÄÃNH GIÃ NHÆ¯ NÃ€O?
    
3. METHOD? Ká»¸ THUáº¬T ÄÃ“ LÃ€ GÃŒ? ? Ká»¸ THUáº¬T ÄÃ“ NHÆ¯ NÃ€O? => Äá»ŠNH NGHÄ¨A ÄÆ¯á»¢C CÃC KHÃI NIá»†M => Má»šI CÃ“ Ká»¸ THUáº¬T.
    

- CÃC Ká»¸ THUáº¬T SUMMARY NHÆ¯ NÃ€O?
    
- ...
    
- ...
    

4. EXTRACT ?
    

---

# **Äá» tÃ i**: **"Long-Term Memory Augmentation for Conversational Question Answering Systems"** _(TÄƒng cÆ°á»ng trÃ­ nhá»› dÃ i háº¡n cho cÃ¡c há»‡ thá»‘ng há»i Ä‘Ã¡p há»™i thoáº¡i)_

1. # Motivation
    

### ğŸ”¥ **Äá»™ng lá»±c chÃ­nh:**

1. **CÃ¡ nhÃ¢n hÃ³a sÃ¢u sáº¯c** Ghi nhá»› thÃ´ng tin, sá»Ÿ thÃ­ch vÃ  hÃ nh vi ngÆ°á»i dÃ¹ng giÃºp chatbot/robot pháº£n há»“i phÃ¹ há»£p, táº¡o cáº£m giÃ¡c thÃ¢n quen vÃ  tÄƒng sá»± hÃ i lÃ²ng. ğŸ‘‰ **VÃ­ dá»¥**: Má»™t ngÆ°á»i dÃ¹ng thÆ°á»ng tÃ¬m cÃ´ng thá»©c Äƒn chay. Chatbot ghi nhá»› Ä‘iá»u nÃ y vÃ  luÃ´n gá»£i Ã½ mÃ³n chay, thay vÃ¬ cÃ¡c cÃ´ng thá»©c ngáº«u nhiÃªn. ğŸ‘‰ _Náº¿u khÃ´ng cÃ³ Ä‘iá»u nÃ y_: Chatbot cÃ³ thá»ƒ liÃªn tá»¥c Ä‘á» xuáº¥t mÃ³n máº·n, gÃ¢y phiá»n toÃ¡i vÃ  lÃ m ngÆ°á»i dÃ¹ng máº¥t thiá»‡n cáº£m, tá»« Ä‘Ã³ giáº£m kháº£ nÄƒng quay láº¡i sá»­ dá»¥ng.
    
2. **Duy trÃ¬ vÃ  tÃ¡i sá»­ dá»¥ng ngá»¯ cáº£nh** TrÃ­ nhá»› dÃ i háº¡n cho phÃ©p hiá»ƒu Ä‘Æ°á»£c lá»‹ch sá»­ há»™i thoáº¡i qua nhiá»u phiÃªn lÃ m viá»‡c, giá»¯ máº¡ch logic vÃ  trÃ¡nh yÃªu cáº§u ngÆ°á»i dÃ¹ng pháº£i láº·p láº¡i thÃ´ng tin. ğŸ‘‰ **VÃ­ dá»¥**: Trong má»™t chuá»—i há»™i thoáº¡i vá» Ä‘Æ¡n hÃ ng, chatbot nhá»› ráº±ng ngÆ°á»i dÃ¹ng Ä‘ang khiáº¿u náº¡i vá» sáº£n pháº©m X tá»« phiÃªn trÆ°á»›c vÃ  tiáº¿p tá»¥c há»— trá»£ ngay á»Ÿ phiÃªn sau. ğŸ‘‰ _Náº¿u khÃ´ng cÃ³ Ä‘iá»u nÃ y_: NgÆ°á»i dÃ¹ng sáº½ pháº£i láº·p láº¡i toÃ n bá»™ thÃ´ng tin khi quay láº¡i, gÃ¢y bá»±c bá»™i vÃ  táº¡o cáº£m giÃ¡c chatbot thiáº¿u chuyÃªn nghiá»‡p.
    
3. **Há»— trá»£ tÃ¡c vá»¥ dÃ i háº¡n** CÃ¡c á»©ng dá»¥ng nhÆ° theo dÃµi há»c táº­p, chÄƒm sÃ³c sá»©c khá»e hoáº·c hÃ nh trÃ¬nh khÃ¡ch hÃ ng yÃªu cáº§u chatbot ghi nhá»› vÃ  cáº­p nháº­t tiáº¿n trÃ¬nh liÃªn tá»¥c. ğŸ‘‰ **VÃ­ dá»¥**: Má»™t chatbot há»c táº­p ghi nhá»› ráº±ng há»c sinh Ä‘Ã£ yáº¿u á»Ÿ pháº§n thÃ¬ quÃ¡ khá»© hoÃ n thÃ nh vÃ  tiáº¿p tá»¥c luyá»‡n táº­p Ä‘iá»ƒm nÃ y trong cÃ¡c buá»•i sau. ğŸ‘‰ _Náº¿u khÃ´ng cÃ³ Ä‘iá»u nÃ y_: Chatbot sáº½ láº·p láº¡i nhá»¯ng ná»™i dung há»c Ä‘Ã£ á»•n, bá» sÃ³t Ä‘iá»ƒm yáº¿u cá»§a há»c sinh, lÃ m giáº£m hiá»‡u quáº£ há»c táº­p vÃ  cáº£m giÃ¡c â€œÄ‘Æ°á»£c Ä‘á»“ng hÃ nhâ€ cá»§a ngÆ°á»i há»c.
    

---

## 1.1 Dáº«n chá»©ng

![](https://csg2ej4iz2hz.sg.larksuite.com/space/api/box/stream/download/asynccode/?code=MDE5ZDE4NDZlNzk0NjEyMzJhZmQyN2FhMWU0ZjNmMjhfR1h3UVpiaFBOY0twYVI3WE1CNFd1S0VqdGdDajF1T2lfVG9rZW46RjJoRGJiRGl0b1ZDd3R4aXk2cmxqRW1yZ05iXzE3NDQ5MDA2MDk6MTc0NDkwNDIwOV9WNA)

Memory of Personalization

---

  

  

2. # Datasets vÃ  cÃ¡c metrics
    

![](https://csg2ej4iz2hz.sg.larksuite.com/space/api/box/stream/download/asynccode/?code=MWViMzFmOWJmYTEzYjA0ZDkzOTRiYWQwMWJhNTZmYTZfaHZmWExyd1RWVmpMOUE4VmsyZDNzV3ltUE1ubVJqMGtfVG9rZW46WGlNamJQd2p2b05MbHR4dk1sYWwzS0NKZ3hlXzE3NDQ5MDA2MDk6MTc0NDkwNDIwOV9WNA)

  

LongMemEval lÃ  má»™t bá»™ dá»¯ liá»‡u toÃ n diá»‡n, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng ghi nhá»› dÃ i háº¡n cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n. Bá»™ dá»¯ liá»‡u nÃ y bao gá»“m 500 cÃ¢u há»i cháº¥t lÆ°á»£ng cao, táº­p trung vÃ o nÄƒm kháº£ nÄƒng cá»‘t lÃµi:îˆ†

  

1. **TrÃ­ch xuáº¥t thÃ´ng tin (Information Extraction):** Kháº£ nÄƒng nhá»› láº¡i thÃ´ng tin cá»¥ thá»ƒ tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c dÃ i, bao gá»“m cáº£ chi tiáº¿t do ngÆ°á»i dÃ¹ng hoáº·c trá»£ lÃ½ cung cáº¥p.îˆ†
    
2. **LÃ½ luáº­n Ä‘a phiÃªn (Multi-Session Reasoning):** Kháº£ nÄƒng tá»•ng há»£p thÃ´ng tin tá»« nhiá»u phiÃªn trÃ² chuyá»‡n Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i phá»©c táº¡p yÃªu cáº§u sá»± tá»•ng há»£p vÃ  so sÃ¡nh.îˆ†
    
3. **Cáº­p nháº­t kiáº¿n thá»©c (Knowledge Updates):** Kháº£ nÄƒng nháº­n biáº¿t vÃ  cáº­p nháº­t thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng theo thá»i gian.îˆ†
    
4. **LÃ½ luáº­n thá»i gian (Temporal Reasoning):** Nháº­n thá»©c vá» cÃ¡c khÃ­a cáº¡nh thá»i gian cá»§a thÃ´ng tin ngÆ°á»i dÃ¹ng, bao gá»“m cáº£ thá»i gian Ä‘Æ°á»£c Ä‘á» cáº­p rÃµ rÃ ng vÃ  siÃªu dá»¯ liá»‡u thá»i gian trong cÃ¡c tÆ°Æ¡ng tÃ¡c.îˆ†
    
5. **Tá»« chá»‘i tráº£ lá»i (Abstention):** Kháº£ nÄƒng tá»« chá»‘i tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n thÃ´ng tin khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.îˆ†
    

  

  

---

3. # 1 cÃ¡ch lÃ m Ä‘Æ¡n giáº£n nháº¥t thá»§y tá»• cá»§a cÃ¡c bÃ i Memory:
    

https://arxiv.org/pdf/2410.10813

  

![](https://csg2ej4iz2hz.sg.larksuite.com/space/api/box/stream/download/asynccode/?code=NWYzYTE1NmJmMzYwMjcxN2FjMTA5NjBjZWI0ZDc1ZjdfMDVQSVNPU2VacnppOHZ6ZGZ6UDNwaEUzU0I1c0ZXU0RfVG9rZW46TlBhcGJmajNnb0Y0QXR4QmZxQWxySmc5Z0JlXzE3NDQ5MDA2MDk6MTc0NDkwNDIwOV9WNA)

  

![](https://csg2ej4iz2hz.sg.larksuite.com/space/api/box/stream/download/asynccode/?code=ODllYTExNDRiZWJkZGIwYjA4OTE5N2NjMGM2NzY3MGVfR0pqRVNaUXAyQU9UR0ZtNTNUWmdDTThRNGtsQlFwNzNfVG9rZW46TXl3QWJSNVM5b3NsemR4d08wdWw4ME1MZ3FiXzE3NDQ5MDA2MDk6MTc0NDkwNDIwOV9WNA)

  

- Llam3. 1- 8b
    
- Dataset cÅ©: 2023
    

```c++
Session
  â†“
Conversation-aware Chunking (LLMs Chunk + Raptor Chunk)
  â†“
Chunk-level Value â†’ Extract (summary, fact, keyphrase)
  â†“
Embed:
   - K1 = V + summary
   - K2 = V + fact
   - K3 = V + keyphrase
  â†“
Phase 1: Coarse Retrieval tá»« summary/keyphrase
  â†“
Phase 2: Fine Retrieval tá»« facts
  â†“
Reading Strategy: CoN + JSON (Chain-of-Note)
  â†“
Answer
```

```c++
2. Vá» cÃ¡ch lÃ m Ä‘Æ¡n giáº£n nháº¥t Ä‘Æ°á»£c xá»­ lÃ½ ráº¥t giá»‘ng RAG 
- Xá»­ lÃ½ trÆ°á»›c: 1 dialog thÃ¬ gá»“m nhiá»u session, 1 session thÃ¬ gá»“m nhiá»u round. (1 round lÃ  1 lÆ°á»£t mÃ  User trÃ² chuyá»‡n vá»›i AI). 
Äem 1 round/1 session => Chunking ra => Äi embedding. 
- Khi realtime: Khi user cÃ³ cÃ¢u há»i Ä‘áº¿n thÃ¬ sáº½ embedding cÃ¢u há»i + Query nÃ³ trong táº­p Embedding DB Ä‘á»ƒ tÃ¬m ra TOP K embedding gáº§n nháº¥t. 
=> Láº¥y lÃ m context. CÃ¢u há»i + Context => sinh ra cÃ¢u tráº£ lá»i. 
```

  

---

4. # CÃ¡c giáº£i phÃ¡p hiá»‡n Ä‘áº¡i vá»›i Graph:
    

LangGraph

**LangGraph** lÃ  má»™t **framework** Ä‘Æ°á»£c giá»›i thiá»‡u trong khÃ³a há»c "Long-Term Agentic Memory with LangGraph" do Harrison Chase, Co-Founder vÃ  CEO cá»§a LangChain, giáº£ng dáº¡y. KhÃ³a há»c nÃ y hÆ°á»›ng dáº«n cÃ¡ch xÃ¢y dá»±ng má»™t **agent** vá»›i kháº£ nÄƒng **ghi nhá»› dÃ i háº¡n**, cá»¥ thá»ƒ lÃ  trong viá»‡c quáº£n lÃ½ email cÃ¡ nhÃ¢n.

  

**Äiá»ƒm má»›i mÃ  LangGraph Ä‘á» cáº­p Ä‘áº¿n**:

  

1. **TÃ­ch há»£p ba loáº¡i memory trong agent**:
    
    2. **Semantic Memory**: LÆ°u trá»¯ cÃ¡c **facts** vá» ngÆ°á»i dÃ¹ng, nhÆ° sá»Ÿ thÃ­ch, thÃ³i quen, Ä‘á»ƒ sá»­ dá»¥ng trong cÃ¡c tÆ°Æ¡ng tÃ¡c sau nÃ y.
        
    3. **Episodic Memory**: Ghi nhá»› cÃ¡c **tÃ¬nh huá»‘ng cá»¥ thá»ƒ** Ä‘Ã£ xáº£y ra trong quÃ¡ khá»©, giÃºp agent hiá»ƒu ngá»¯ cáº£nh vÃ  cáº£i thiá»‡n pháº£n há»“i.
        
    4. **Procedural Memory**: LÆ°u trá»¯ cÃ¡c **hÆ°á»›ng dáº«n vÃ  quy trÃ¬nh** mÃ  agent cáº§n tuÃ¢n theo, giÃºp tá»‘i Æ°u hÃ³a hÃ nh vi dá»±a trÃªn pháº£n há»“i. ??
        

  

---

```c++
[Input há»™i thoáº¡i hiá»‡n táº¡i]

       â†“  
[Updater] â€” cáº­p nháº­t memory

       â†“  
[Retriever] â€” truy xuáº¥t memory

       â†“  
[LLM Core] â€” sinh pháº£n há»“i + cÃ¡ nhÃ¢n hÃ³a

       â†“  
[Output pháº£n há»“i]
```

```c++
Session
  â†“
Conversation-aware Chunking (LLMs Chunk + Raptor Chunk)
  â†“
Chunk-level Value â†’ Extract (summary, fact, keyphrase)
  â†“
Embed:
   - K1 = V + summary
   - K2 = V + fact
   - K3 = V + keyphrase
  â†“
Phase 1: Coarse Retrieval tá»« summary/keyphrase
  â†“
Phase 2: Fine Retrieval tá»« facts
  â†“
Reading Strategy: CoN + JSON (Chain-of-Note)
  â†“
Answer
```

  

## 2.2 Mem0 - Graph Memory

  

  

## 2.3 Kiáº¿n trÃºc Zep - Graph -2-2025

  

Zep sá»­ dá»¥ng **Graphiti** â€“ má»™t cÃ´ng cá»¥ xÃ¢y dá»±ng Ä‘á»“ thá»‹ tri thá»©c cÃ³ tÃ­nh thá»i gian (temporal), chia dá»¯ liá»‡u thÃ nh ba lá»›p:

  

1. **Episode Subgraph** (cÃ¡c â€œepisodeâ€):
    
    2. Má»—i â€œepisodeâ€ lÃ  má»™t Ä‘Æ¡n vá»‹ dá»¯ liá»‡u thÃ´, vÃ­ dá»¥: má»™t tin nháº¯n, má»™t Ä‘oáº¡n text, hay má»™t JSON.
        
    3. ThÃ´ng tin á»Ÿ Ä‘Ã¢y mang tÃ­nh â€œepisodicâ€ â€“ giá»‘ng khÃ¡i niá»‡m â€œbá»™ nhá»› sá»± kiá»‡nâ€ (episodic memory) trong tÃ¢m lÃ½ há»c.
        
2. **Semantic Entity Subgraph** (cÃ¡c thá»±c thá»ƒ vÃ  quan há»‡):
    
      
    
    2. Tá»« nhá»¯ng â€œepisodeâ€ (tin nháº¯n), há»‡ thá»‘ng trÃ­ch xuáº¥t **thá»±c thá»ƒ** (entity) vÃ  **má»‘i quan há»‡** (fact/edge) giá»¯a cÃ¡c thá»±c thá»ƒ Ä‘Ã³ (vÃ­ dá»¥: â€œA lÃ m viá»‡c táº¡i cÃ´ng ty B tá»« nÄƒm 2020 Ä‘áº¿n 2022â€).
        
    3. ThÃªm bÆ°á»›c â€œentity resolutionâ€ Ä‘á»ƒ gá»™p nhá»¯ng thá»±c thá»ƒ trÃ¹ng nhau (nhÆ°ng cÃ³ thá»ƒ Ä‘Æ°á»£c nháº¯c vá»›i tÃªn hÆ¡i khÃ¡c) thÃ nh má»™t node duy nháº¥t.
        
    4. ThÃ´ng tin thá»i gian (thá»i Ä‘iá»ƒm cÃ³ hiá»‡u lá»±c, lÃºc báº¯t Ä‘áº§u/káº¿t thÃºc) Ä‘Æ°á»£c lÆ°u cÃ¹ng cÃ¡c quan há»‡ Ä‘á»ƒ theo dÃµi thay Ä‘á»•i cá»§a sá»± tháº­t (â€œfactâ€) theo thá»i gian.
        
3. **Community Subgraph** (cÃ¡c cá»¥m cá»™ng Ä‘á»“ng):
    
    2. Há»‡ thá»‘ng gom nhÃ³m cÃ¡c thá»±c thá»ƒ, má»‘i quan há»‡ láº¡i thÃ nh â€œcá»™ng Ä‘á»“ngâ€ (community) â€“ vá» cÆ¡ báº£n lÃ  cÃ¡c cá»¥m cÃ¡c node liÃªn quan cháº·t cháº½.
        
    3. Má»—i cá»™ng Ä‘á»“ng cÃ³ má»™t â€œsummaryâ€ (tÃ³m táº¯t) mÃ´ táº£ khÃ¡i quÃ¡t cÃ¡c thá»±c thá»ƒ vÃ  thÃ´ng tin bÃªn trong.
        

  

Vá»›i cáº¥u trÃºc nÃ y, Zep duy trÃ¬ Ä‘Æ°á»£c lá»‹ch sá»­ thay Ä‘á»•i cá»§a thÃ´ng tin, vá»«a cÃ³ thá»ƒ linh hoáº¡t tráº£ lá»i cÃ¡c cÃ¢u há»i mang tÃ­nh thá»i gian (vÃ­ dá»¥: â€œA Ä‘Ã£ lÃ m viá»‡c á»Ÿ Ä‘Ã¢u vÃ o nÄƒm ngoÃ¡i?â€) vá»«a cÃ³ thá»ƒ tÃ³m táº¯t/khÃ¡i quÃ¡t báº±ng cÃ¡c summary.

  

---

  

## 3. CÆ¡ cháº¿ lÆ°u trá»¯ vÃ  truy xuáº¥t (Memory Retrieval)

  

Zep Ã¡p dá»¥ng chiáº¿n lÆ°á»£c tÃ¬m kiáº¿m (search) nhiá»u bÆ°á»›c:

  

1. **Táº¡o index (xÃ¢y dá»±ng embeddings, BM25, v.v.)**:
    
    2. Má»—i â€œfactâ€ vÃ  má»—i â€œthá»±c thá»ƒâ€ Ä‘á»u cÃ³ embedding (vector) Ä‘á»ƒ phá»¥c vá»¥ tÃ¬m kiáº¿m theo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine.
        
    3. NgoÃ i ra, tÃªn hoáº·c mÃ´ táº£ (summary) cá»§a thá»±c thá»ƒ, fact cÃ³ thá»ƒ dÃ¹ng cho tÃ¬m kiáº¿m full-text (BM25).
        
    4. CÃ³ thá»ƒ sá»­ dá»¥ng tÃ¬m kiáº¿m theo khoáº£ng cÃ¡ch trÃªn Ä‘á»“ thá»‹ (breadth-first search) Ä‘á»ƒ truy xuáº¥t thÃ´ng tin gáº§n cÃ¡c nÃºt quan trá»ng.
        
2. **Káº¿t há»£p, xáº¿p háº¡ng láº¡i (reranker)**:
    
    5. Sau khi láº¥y Ä‘Æ°á»£c danh sÃ¡ch káº¿t quáº£ theo nhiá»u cÃ¡ch (cosine, BM25, BFS), Zep Ã¡p dá»¥ng cÃ¡c thuáº­t toÃ¡n nhÆ° RRF (Reciprocal Rank Fusion), MMR (Maximal Marginal Relevance), hoáº·c mÃ´ hÃ¬nh cross-encoder Ä‘á»ƒ xáº¿p háº¡ng láº¡i, Æ°u tiÃªn cÃ¡c káº¿t quáº£ quan trá»ng nháº¥t.
        
3. **Káº¿t há»£p dá»¯ liá»‡u thÃ nh â€œcontext stringâ€**:
    
    2. Tá»« top-N â€œedgesâ€ (fact) vÃ  â€œnodesâ€ (entity) Ä‘Æ°á»£c xáº¿p háº¡ng cao, Zep xÃ¢y dá»±ng má»™t Ä‘oáº¡n vÄƒn báº£n (context) cÃ³ cáº¥u trÃºc, Ä‘á»ƒ Ä‘Æ°a vÃ o prompt cho LLM. Äoáº¡n nÃ y thÆ°á»ng bao gá»“m:
        
        - CÃ¡c fact kÃ¨m thá»i gian hiá»‡u lá»±c
            
        - ThÃ´ng tin tÃ³m táº¯t vá» thá»±c thá»ƒ
            
        - TÃ³m lÆ°á»£c cá»™ng Ä‘á»“ng náº¿u cáº§n
            

  

CÃ¡ch lÃ m nÃ y giÃºp â€œná»‘i dÃ iâ€ bá»™ nhá»› cá»§a LLM mÃ  khÃ´ng cáº§n táº£i toÃ n bá»™ lá»‹ch sá»­ vÃ o ngá»¯ cáº£nh. HÆ¡n ná»¯a, do Zep cÃ³ kháº£ nÄƒng ghi nháº­n vÃ  vÃ´ hiá»‡u hÃ³a (invalidate) cÃ¡c fact cÅ© khi cÃ³ mÃ¢u thuáº«n má»›i, nÃ³ cho phÃ©p cáº­p nháº­t thÃ´ng tin Ä‘á»™ng khÃ¡ hiá»‡u quáº£.