

## Cách thức tạo KG

- [HippoRAG/src/hipporag/utils/misc_utils.py at main · OSU-NLP-Group/HippoRAG](https://github.com/OSU-NLP-Group/HippoRAG/blob/main/src/hipporag/utils/misc_utils.py)
**Analyzing**

Pulling in details to give the best answer

Thought for 16 seconds

Trong HippoRAG (và HippoRAG 2), việc dựng Knowledge Graph (KG) diễn ra trong module lưu trữ embeddings, cụ thể là file `src/hipporag/embedding_store.py`. Quá trình này tóm tắt như sau:

1. **Thu thập triples**
    
    - Từ kết quả OpenIE (`TripleRawOutput`) của từng passage, hàm `flatten_facts` gom toàn bộ triples duy nhất lại thành một danh sách các tuple (subject, relation, object). .
        
2. **Xác định nodes**
    
    - Dùng hàm `extract_entity_nodes` để lấy ra toàn bộ các entity (subject và object) xuất hiện trong các triple, rồi duyệt thành tập `phrase nodes`. .
        
    - Ngoài ra, mỗi passage còn được coi là một `passage node` để giữ context.
        
3. **Xây dựng đồ thị (KG)**
    
    - Tạo igraph Graph từ list triples bằng `Graph.TupleList(graph_triples, directed=False)`.
        
    - Thêm thêm các cạnh “synonym edge” nối những phrase nodes có embedding cosine-similarity cao, và các cạnh “context edge” nối phrase node với passage node chứa nó.
        
4. **Lưu lại KG**
    
    - Đồ thị này được giữ trong `EmbeddingStore.graph` để dùng cho bước Personalized PageRank (PPR) khi truy vấn.
        

Tóm lại, phần “tạo KG” nằm ngay trong `embedding_store.py`, trong phương thức khởi tạo/indexing, với ba bước: chuyển OpenIE→flatten triples, extract entities thành nodes, rồi gọi igraph để vẽ cạnh từ triples và thêm các cạnh phụ trợ (synonym & context). Bạn có thể mở file này để xem chi tiết gọi `igraph.Graph.TupleList` và phần thêm cạnh.


# 2. Cách thức PPR hoạt động

**Designing a simple mini dataset**

I think the goal here is to create a simple example for PPR that shows how passages and triples work, while keeping it down to earth. I could go with something like: "What county is Erik Hort's birthplace in?" or "What sport does Serena Williams' spouse play?" Hm, simpler would be something like "What country is the capital of Rome in?" but that's just one hop. I want multi-hop - maybe "Which city hosts the university where Albert Einstein studied?"

Trying to simplify that!

Thought for 10 seconds

## Ví dụ “tí hon” – từ Erik Hort → Rockland County

**Câu hỏi người dùng:**

> _“Quận (county) của nơi sinh Erik Hort là quận nào?”_

Nước nào là nơi Cường sinh ra? 

- Cường sinh ra ở TB 
- TB là 1 phần của VN 

=> Top k Node Phrase => node Cường - sinh ra ở TB  (seed = similarty) và đoạn 1 (seed = s * w) (w=0.05) 
- Qua mỗi bước lan truyền - lan truyền mực. => 50% lượng mực sẽ ở lại node cũ và 50% lượng mực còn lại sẽ ở các node khác liền kề   -> lan truyền seed được đến passage 2. 
- Cuối cùng nó sẽ SO SÁNH các passage để mà tìm ra top k passage liên quan nhất (có seed lớn nhất). đưa vào context 
### ❶ Khung dữ liệu siêu nhỏ

| ID     | Đoạn văn (passage)                                   | Triple OpenIE trích được                   |
| ------ | ---------------------------------------------------- | ------------------------------------------ |
| **P1** | “**Erik Hort** was **born in** **Montebello**.”      | (Erik Hort, **born_in**, Montebello)       |
| **P2** | “**Montebello** is **part of** **Rockland County**.” | (Montebello, **part_of**, Rockland County) |

Từ hai triple này, HippoRAG lập **đồ thị**:

```
Erik_Hort ──born_in──► Montebello ──part_of──► Rockland_County
   ▲                              ▲
   │ contains                     │ contains
   │                              │ 
  P1                              P2      (hai node passage)
```

### ❷ Hạt giống (seed) khi truy vấn

- **Phrase-seed:** “Erik Hort” (từ triple lọc bởi LLM).
    
- **Passage-seed:** P1 (đoạn văn khớp truy vấn về “Erik Hort”) – điểm cosine với câu hỏi cao.
    

### ❸ Chạy Personalized PageRank (α = 0.5)

|Vòng lan|Xuất phát|Flow xác suất (mũi tên)|Ai “được bơm” thêm?|
|---|---|---|---|
|Bước 0|Seed only|–|Erik Hort, P1|
|Bước 1|Erik Hort|born_in → Montebello ; reset về seed|**Montebello**|
|Bước 2|Montebello|part_of → Rockland County ; contains → P2|**Rockland County**, **P2**|
|Bước 3+|Tiếp tục|reset + lan qua lại|P2 nhận thêm xác suất|

_Sau ~30 vòng lặp (đồ thị cực nhỏ nên hội tụ rất nhanh)._

### ❹ Tính điểm passage

Score(Pk)=∑v∈Pkπv\text{Score}(P_k)=\sum_{v\in P_k} \pi_v

- P1 vẫn có điểm khá cao (seed).
    
- **P2 vọt lên top-1** nhờ Montebello → P2 cạnh _contains_ + dòng chảy từ seed.
    

### ❺ Kết quả

|Thứ hạng sau PPR|Đoạn văn|Vì sao lên hạng?|
|---|---|---|
|**1**|**P2**: “…Montebello is part of Rockland County.”|Nhận xác suất “đa bước” (Erik Hort → Montebello → P2)|
|2|P1|Seed gốc, nhưng ít liên quan trực tiếp tới “county”|

LLM-reader đọc P2, trích ngay: **“Rockland County.”**

---

### _Tại sao cần PPR?_

Nếu chỉ dùng embedding:

_Query_ “county of Erik Hort’s birthplace” gần nghĩa với “Erik Hort”, **không** giống P2 ⇒ P2 khó lọt top-k.

Nhờ PPR, passage P2 **được “kéo” lên** dù _không_ giống trực tiếp truy vấn, vì đồ thị đã chứng minh liên kết đa bước giữa Erik Hort và Rockland County. Đây chính là _multi-hop reasoning trong một lượt_.


Thought for a couple of seconds

### Thử tưởng tượng thế này nhé – chỉ 5 nút, vẽ ra giấy là thấy ngay

|ID|Loại nút|Nhãn|Giải thích|
|---|---|---|---|
|A|Phrase|“Erik Hort”|người được hỏi|
|B|Phrase|“Montebello”|nơi sinh của Erik|
|C|Phrase|“Rockland County”|quận chứa Montebello|
|P1|Passage|“P1”|đoạn văn nói _Erik Hort sinh ở Montebello_|
|P2|Passage|“P2”|đoạn văn nói _Montebello thuộc Rockland County_|

**Cạnh (mũi tên)**

- A → B  (born_in)
    
- B → C  (part_of)
    
- A,B → P1 (contains)
    
- B,C → P2 (contains)
    

---

## 1. Giai đoạn “chấm mực” (khởi tạo seed)

- Câu hỏi: **“Quận nào chứa nơi sinh của Erik Hort?”**
    
- Ta “chấm” 50 % mực lên **A** (Erik Hort) vì nó khớp câu hỏi.
    
- 50 % còn lại ta rải đều lên các **passage cosine cao** (ở ví dụ nhỏ này P1 ≈ 10 %).
    

> Dòng mực này chính là **vector reset s** trong PPR.

---

## 2. “Mực loang” (1 vòng lan truyền)

- **Giả sử** mỗi nút chia mực của mình cho **tất cả cạnh đi ra**.
    
- **A (50 %)** → đổ hết sang **B** (1 cạnh) ⇒ B nhận 50 %.
    
- **P1 (10 %)** không có cạnh đi ra → mực ở lại.
    

Kết thúc vòng 1:

```
A: 0  B: 0.50  C: 0  P1: 0.10  P2: 0
```

---

## 3. “Quay về chấm” (teleport)

Quy định **α = 0.5**

- Nghĩa là **mỗi vòng**, ta **lấy 50 %** lượng mực của mọi nút **lau sạch** rồi **đổ lại đúng vị trí seed ban đầu**.
    
- 50 % còn lại giữ nguyên (tiếp tục loang ở vòng sau).
    

Áp dụng:

```
 Giữ lại 0.5 × (A=0,B=0.50,C=0,P1=0.10,P2=0)
 => A:0   B:0.25 C:0  P1:0.05 P2:0

 Đổ 0.5 tổng mực (0.5) trở lại seed:
    A +0.25     P1 +0.05   (vì seed A=0.5, P1=0.1 ban đầu)
Kết thúc vòng 1:  A:0.25 B:0.25 C:0 P1:0.10 P2:0
```

---

## 4. Vòng 2 (loang tiếp)

1. **Loang**
    
    - A (0.25) → B (+0.25)
        
    - B (0.25) → C (+0.25) **và** P2 (+0.25) (2 cạnh)
        
    - P1 (0.10) giữ
        
2. **Teleport 50 %**
    
    - Giữ lại: A0 B0.25→0.125 …
        
    - Chấm lại seed (0.25 A + 0.05 P1)
        

Kết quả gần đúng sau vòng 2:

```
A ≈ 0.25   B ≈ 0.13   C ≈ 0.13   P1 ≈ 0.10   P2 ≈ 0.13
```

---

## 5. Cứ thế 4-5 vòng nho nhỏ mực sẽ **ổn định**

- **Passage P2** (nói Rockland County) **dần nhận nhiều mực** vì nó hứng từ **B** và **C** qua cạnh _contains_.
    
- **Rank cuối**: P2 ≥ P1 dù câu hỏi **không chứa chữ “Rockland County”**.
    

> **Đó chính là “multi-hop reasoning một lượt”**:  
> A (Erik) → B (Montebello) → C (County) → P2 (đoạn văn đích).

---

## Tóm tắt 1 câu

> **PPR = cho mực loang trên đồ thị nhưng cứ nửa thì hút về seed, nhờ vậy mực len lỏi qua 2-3 cạnh rồi chảy tới passage xa, giúp RAG lấy luôn tài liệu multi-hop mà không cần gọi LLM nhiều lần.**

(Hiểu chưa? Nếu còn vướng đoạn nào cứ hỏi tiếp nhé!)


Thought for 14 seconds

### 1. Khởi tạo “seed” và chia trọng số ban đầu

|Loại node|Cách chọn làm seed|Trọng số/reset-probability ban đầu _s_|
|---|---|---|
|**Phrase nodes** (các cụm danh từ/đối tượng trong triple đã được lọc)|– Lấy tối đa _k = 5_ phrase có điểm COSINE cao nhất so với truy vấn.– Điểm của **mỗi phrase** = **trung bình** điểm xếp hạng của tất cả triple mà nó xuất hiện. citeturn6file13|Dùng thẳng điểm xếp hạng đó làm phần tử tương ứng trong vector _s_.|
|**Passage nodes** (toàn bộ đoạn văn gốc)|Không lọc – **tất cả** passage đều trở thành seed → giúp PPR “khám phá” xa. citeturn6file13|Điểm COSINE giữa passage & truy vấn **× hệ số w**. Hệ số này (mặc định **w = 0.05**) đảm bảo phrase-nodes vẫn chiếm ưu thế, còn passage-nodes chỉ nhận “mực loãng”. Thử nghiệm điều chỉnh _w_ được liệt kê ở Bảng 5. citeturn6file8|

> 💡 Hình ảnh “chấm mực”: **đổ nhiều mực (50 ± %) vào phrase-node quan trọng, rắc ít mực (≈ 5 %) loang trên các passage** chính là phép ẩn dụ cho bảng trên.

---

### 2. Lan truyền & cập nhật trọng số sau mỗi bước

HippoRAG 2 chạy **Personalized PageRank** trên đồ thị kết hợp (phrase + passage):

nt+1  =  (1−d)  s  +  d  A⊤nt\mathbf{n}_{t+1}\;=\;(1-d)\;\mathbf{s}\;+\;d\;A^{\top}\mathbf{n}_{t}

- **A**: ma trận chuyển tiếp chuẩn hoá (từ phrase → phrase, phrase → passage, passage → phrase).
    
- **d**: hệ số “tiếp tục bước” (_damping_, mặc định 0.5).
    
- s\mathbf{s}: vector reset vừa mô tả ở mục 1.
    

Ở mỗi vòng lặp:

1. **99 × %** xác suất bị “đẩy” theo các cạnh A (lan truyền qua quan hệ “synonym”, “contains”, “triple-edge” …).
    
2. **1 - d** phần xác suất “bốc hơi” trở về seed theo đúng tỷ lệ **s** → giữ cho phrase-node trọng yếu luôn được “tiếp máu”, đồng thời passage-node vẫn có chút “mực” duy trì.
    

Thuật toán lặp tới hội tụ (≤ 50 vòng), thu được vector phân bố cuối n∗\mathbf{n}^{*}.

---

### 3. Tính điểm cuối & truy hồi

- **Điểm node** = phần tử tương ứng trong n∗\mathbf{n}^{*}.
    
- **Điểm passage** = tổng n∗\mathbf{n}^{*} của **node passage** đó **+** của mọi phrase nằm bên trong (edge _contains_).
    
- Sắp xếp điểm passage → lấy top-k (k = 5) nạp vào reader LLM.
    

Nhờ cách **gieo “mực” phân tầng** (phrase chiếm phần lớn, passage rải mỏng) + **lan truyền PPR hai lớp**, HippoRAG 2:

- Giữ được **chỉ dẫn mạnh** của khái niệm trong truy vấn.
    
- Vẫn “ngửi” được các passage xa qua những đường liên kết multi-hop.
    

> _“Reset probabilities are assigned based on ranking scores for phrase nodes, while passage nodes receive scores proportional to their embedding similarity, **adjusted by a weight factor (§ 6.2)** to balance the influence between phrase nodes and passage nodes.”_ citeturn6file13

Điều này lý giải vì sao ngay cả với truy vấn đòi hỏi nhiều bước suy luận, HippoRAG 2 vẫn đạt **Recall@5 = 74.7 % trên MuSiQue và 90.4 % trên 2Wiki** – cao hơn hẳn dense retriever tốt nhất. citeturn6file9


```
theo tôi hiểu

- Ban đầu lấy top k phrase, (tiến hành LLMs lọc phrase nhiễu). Sau đó khởi tạo seed ban đầu cho top K đều bằng chính điểm similary của node

- Và lấy top k Passage, điểm seed ban đầu = Điểm COSINE giữa passage & truy vấn × hệ số w. Hệ số này (mặc định w = 0.05) đảm bảo phrase-nodes vẫn chiếm ưu thế, còn passage-nodes chỉ nhận “mực loãng”

===

Ở mỗi step. Lan truyền được tính theo tỷ lệ như nào ?
```


**Explaining Personalized PageRank**

To describe propagation, we need a Personalized PageRank equation: n_{t+1} = (1-d)_s + d_P^T n_t. Here, 'd' is the damping factor (usually 0.5). It governs how the probability splits between continuing on edges or teleporting. The transition matrix depends on edge types, such as phrase→phrase or passage→phrase. Normalization is key, and using bi-directional undirected edges is standard. In HippoRAG, specificity affects node weights. At each step, mass distributes based on edge weights, with a 50% chance to continue walking or teleport based on s.

Thought for 8 seconds

### Cách “dòng mực” lan truyền ở **mỗi bước** (Personalized PageRank ­– PPR)

Giả sử

- VV là tập node (phrase + passage)
    
- A∈R∣V∣×∣V∣A\in\mathbb R^{|V|\times |V|} là ma trận kề đã **chuẩn-hoá theo hàng**
    
    - Mỗi cạnh (u → v) có trọng số 1/deg⁡+(u)1/\deg^+(u) (nếu không gán trọng số khác).
        
- s∈R∣V∣\mathbf s\in\mathbb R^{|V|} là **vector seed** bạn vừa mô tả  
    (∑jsj=1\sum_j s_j = 1; phrase-node chiếm đa số, passage-node × w = 0.05).
    
- dd là **damping factor** – xác suất “tiếp tục bước” (bài báo đặt d=0.5d=0.5 sau tuning). citeturn6file13
    

---

#### Công thức lan truyền

  nt+1  =  (1−d) s  +  d A ⁣⊤nt  \boxed{\;\mathbf n_{t+1}\;=\;(1-d)\,\mathbf{s}\;+\;d\,A^{\!\top}\mathbf n_t\;}

- **Nhánh 1 – “teleport”**
    
    - 1−d=0.51-d = 0.5 ≈ 50 % khối “mực” **quay thẳng về seed** theo đúng tỉ lệ s\mathbf s
        
    - → bảo đảm các phrase quan trọng luôn được “tiếp máu”.
        
- **Nhánh 2 – “walk”**
    
    - d=0.5d = 0.5 ≈ 50 % còn lại **chảy dọc cạnh đồ thị**
        
        - phrase ⟷ phrase (synonym-edge, triple-edge)
            
        - phrase → passage (edge _contains_)
            
        - passage → phrase (ngược _contains_)
            
    - Trên mỗi node uu khối lượng phân bố đều cho các đỉnh kề vv:
        
        Auv=1deg⁡+(u) A_{uv}=\frac{1}{\deg^+(u)}
- Lặp cho tới khi ∥nt+1−nt∥1<10−6\|\mathbf n_{t+1}-\mathbf n_t\|_1<10^{-6} **hoặc** 50 vòng (kinh nghiệm cho KB cỡ MuSiQue chỉ ~20 vòng là hội tụ).
    

---

#### Hậu quả sau mỗi vòng

- **Phrase-node “đậm”**  
    – vì được “rót lại” 1−d1-d = 0.5 mỗi bước nên luôn giữ sắc tố cao → đảm bảo **định hướng**.
    
- **Passage-node “lợt”**  
    – ban đầu chỉ có w⋅cosinew\cdot\text{cosine} (≈ 5 %) nhưng nếu kết cấu đồ thị dẫn mực liên tục (phrase → passage) thì sẽ **tích mực dần** và leo hạng.
    
- Ở vòng cuối thu được n\*\mathbf n^\*.
    
    - **Điểm passage** = giá trị trên node-passage **+** tổng các phrase-con (_contains_).
        
    - Lấy top K (=5) cho reader.
        

---

> _“Reset probabilities are assigned based on ranking scores for phrase nodes, while passage nodes receive scores proportional to their embedding similarity, **adjusted by a weight factor (§ 6.2)** … We set the damping factor to **0.5** after tuning.”_ citeturn6file13

Như vậy, “dòng mực” ở mỗi bước luôn phân chia **50 % quay về seed, 50 % lan truyền dọc cạnh**, khiến các passage liên quan (dù xa 2-3-hop) dần “thấm mực” và nổi lên trong top-k.