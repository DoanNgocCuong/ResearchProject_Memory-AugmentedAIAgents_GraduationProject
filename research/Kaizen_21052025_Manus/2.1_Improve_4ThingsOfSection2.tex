\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Tổng quan}
Luận văn này khám phá việc ứng dụng và mở rộng HippoRAG 2, một framework Retrieval-Augmented Generation (RAG) tiên tiến lấy cảm hứng từ cơ chế bộ nhớ dài hạn của con người. Được phát triển bởi các nhà nghiên cứu tại OSU-NLP Group, HippoRAG 2 được thiết kế để giúp các Mô hình Ngôn ngữ Lớn (LLM) liên tục tích hợp kiến thức từ các tài liệu bên ngoài. Framework này giải quyết những hạn chế chính trong các hệ thống RAG truyền thống bằng cách kết hợp các thành phần lấy cảm hứng từ thần kinh học, mô phỏng quá trình ghi nhớ của con người. Trong công trình này, tôi nghiên cứu và mở rộng HippoRAG 2, đặc biệt tập trung vào việc áp dụng nó cho dữ liệu tiếng Việt và các tác vụ lập luận đa bước.

HippoRAG 2 hoạt động trong hai giai đoạn chính: giai đoạn Offline Indexing để xây dựng bộ nhớ và giai đoạn Online Retrieval \& QA để truy xuất thông tin và tạo phản hồi. Giai đoạn Offline Indexing xây dựng hệ thống bộ nhớ dài hạn cho framework bằng cách tạo ra một Đồ thị Tri thức (Knowledge Graph - KG) từ các tài liệu văn bản, chuyển đổi văn bản thô thành một biểu diễn có cấu trúc, nắm bắt cả thông tin khái niệm và chi tiết ngữ cảnh. Giai đoạn Online Retrieval \& QA sử dụng KG đã xây dựng này để truy xuất thông tin dựa trên truy vấn của người dùng và tạo ra các phản hồi thông minh, sử dụng các thuật toán phức tạp để điều hướng cấu trúc tri thức và trích xuất thông tin liên quan.

Luồng công việc tổng thể được minh họa trong Hình 1, thể hiện hai giai đoạn chính này. HippoRAG 2 được xây dựng dựa trên framework HippoRAG ban đầu nhưng giới thiệu một số cải tiến quan trọng nhằm tăng cường sự phù hợp với cơ chế bộ nhớ của con người. Những cải tiến này bao gồm việc tích hợp liền mạch thông tin khái niệm và ngữ cảnh, truy xuất nhận thức ngữ cảnh tốt hơn, và bộ nhớ nhận diện để cải thiện việc lựa chọn nút hạt giống. Nghiên cứu của tôi xem xét chi tiết các cơ chế này và khám phá hiệu quả của chúng khi áp dụng cho dữ liệu tiếng Việt.

\section{Offline Indexing – Giai đoạn Xây dựng Bộ nhớ}
Trong giai đoạn Offline Indexing, nhiệm vụ chính là xây dựng hệ thống bộ nhớ dài hạn bằng cách tạo ra một Đồ thị Tri thức (KG) từ các tài liệu văn bản. Các module trong giai đoạn này làm việc cùng nhau để trích xuất, xử lý và tổ chức thông tin một cách có cấu trúc nhằm hỗ trợ các bước truy xuất trong tương lai. Giai đoạn này rất quan trọng vì nó quyết định chất lượng và hiệu quả của các hoạt động truy xuất tiếp theo.

\subsection{Module 1: Phân đoạn Tài liệu}
Phân đoạn tài liệu là một bước quan trọng nhằm chia nhỏ tài liệu gốc thành các đoạn ngắn hơn, mỗi đoạn mang một ý nghĩa logic riêng biệt. Quá trình này đảm bảo rằng thông tin được tổ chức thành các đơn vị dễ quản lý, có thể được xử lý và truy xuất hiệu quả. HippoRAG 2 sử dụng các Mô hình Ngôn ngữ Lớn (LLM), như Qwen-1.5B-Instruct, để nâng cao khả năng phân đoạn vượt trội so với các phương pháp dựa trên quy tắc truyền thống.

Quá trình phân đoạn bắt đầu bằng việc tách văn bản thành các câu riêng biệt trong khi vẫn giữ nguyên ý nghĩa của chúng. Không giống như việc chia câu đơn giản, hệ thống sử dụng LLM để đánh giá ngữ cảnh và đưa ra quyết định sáng suốt về việc hợp nhất hay tách câu dựa trên cấu trúc logic của chúng. Phương pháp tiếp cận nhận thức ngữ cảnh này tạo ra các đoạn văn mạch lạc, ngắn gọn và dễ quản lý, được tối ưu hóa cho việc trích xuất triple tiếp theo.

Quá trình phân đoạn này đặc biệt quan trọng vì nó ảnh hưởng trực tiếp đến chất lượng của việc trích xuất triple và xây dựng đồ thị tri thức sau này. Bằng cách tạo ra các đoạn văn có tính liên kết logic, hệ thống đảm bảo rằng các triple được trích xuất duy trì được ngữ cảnh và mối quan hệ ngữ nghĩa phù hợp. Việc phân đoạn kém có thể dẫn đến thông tin bị phân mảnh hoặc mất các mối quan hệ ngữ cảnh, làm suy yếu hiệu quả của toàn bộ framework.

\subsubsection{Tinh chỉnh Mô hình Retriever và Reranking}
Trong phần mở rộng HippoRAG 2 của tôi, tôi đặc biệt tập trung vào việc tinh chỉnh các quy trình truy xuất và xếp hạng lại cho dữ liệu tiếng Việt. Hiệu quả của cả hai thành phần được tăng cường đáng kể thông qua việc tinh chỉnh trên dữ liệu cụ thể của miền. Cách tiếp cận dựa trên dữ liệu này cho phép hệ thống thích ứng với các miền kiến thức và mẫu truy vấn cụ thể, dẫn đến việc truy xuất thông tin chính xác và phù hợp hơn.

Đối với thành phần retriever, phương pháp tinh chỉnh của tôi bao gồm một quy trình nhiều giai đoạn sử dụng dữ liệu huấn luyện được tuyển chọn cẩn thận. Ban đầu, một bộ dữ liệu gồm các cặp truy vấn-đoạn văn được xây dựng, trong đó mỗi truy vấn được liên kết với các đoạn văn liên quan (ví dụ dương tính) và các đoạn văn không liên quan (ví dụ âm tính). Mô hình retriever, thường dựa trên kiến trúc bộ mã hóa kép (dual-encoder), sau đó được tinh chỉnh bằng cách sử dụng các mục tiêu học tương phản như mất mát InfoNCE. Việc huấn luyện này khuyến khích mô hình tạo ra các embedding đặt các truy vấn và đoạn văn có liên quan về mặt ngữ nghĩa gần nhau hơn trong không gian vector trong khi đẩy nội dung không liên quan ra xa hơn.

Dữ liệu huấn luyện để tinh chỉnh retriever thường được tăng cường thông qua các kỹ thuật như khai thác ví dụ âm tính khó (hard negative mining), trong đó các ví dụ âm tính thách thức, bề ngoài tương tự như ví dụ dương tính nhưng khác biệt về mặt ngữ nghĩa, được cố ý đưa vào. Điều này buộc mô hình phải học các phân biệt tinh tế hơn giữa nội dung liên quan và không liên quan. Ngoài ra, các kỹ thuật tạo truy vấn có thể được sử dụng để mở rộng dữ liệu huấn luyện, trong đó LLM tạo ra các truy vấn tiềm năng cho các đoạn văn để tạo thêm các cặp huấn luyện.

Đối với các mô hình reranking, quy trình tinh chỉnh mà tôi triển khai thậm chí còn phức tạp hơn. Các mô hình này thường sử dụng kiến trúc bộ mã hóa chéo (cross-encoder) cho phép tương tác sâu hơn giữa các biểu diễn truy vấn và đoạn văn. Dữ liệu huấn luyện bao gồm các cặp truy vấn-đoạn văn với nhãn mức độ liên quan được phân loại, thường theo thang điểm từ 0 (hoàn toàn không liên quan) đến 3 hoặc 4 (hoàn toàn liên quan). Mô hình reranker được tinh chỉnh bằng cách sử dụng các mục tiêu học theo điểm (pointwise), theo cặp (pairwise) hoặc theo danh sách (listwise).

Trong học theo điểm, mô hình được huấn luyện để dự đoán điểm liên quan tuyệt đối của mỗi cặp truy vấn-đoạn văn. Học theo cặp tập trung vào việc sắp xếp chính xác các cặp đoạn văn cho một truy vấn nhất định, trong khi học theo danh sách tối ưu hóa toàn bộ thứ hạng của các đoạn văn cho mỗi truy vấn. Các kỹ thuật như chưng cất kiến thức (knowledge distillation) cũng có thể được sử dụng, trong đó một mô hình lớn hơn, mạnh hơn (giáo viên) hướng dẫn việc huấn luyện một mô hình nhỏ hơn, hiệu quả hơn (học sinh).

Thích ứng miền là một khía cạnh quan trọng trong phương pháp tinh chỉnh của tôi cho cả retriever và reranker. Đối với các ứng dụng chuyên biệt với dữ liệu tiếng Việt, các mô hình được tiền huấn luyện trên các kho ngữ liệu chung được tinh chỉnh thêm trên dữ liệu cụ thể của miền. Quá trình này thường bao gồm học theo chương trình (curriculum learning), trong đó việc huấn luyện tiến triển từ các ví dụ chung đến các ví dụ ngày càng cụ thể theo miền, cho phép mô hình chuyển giao kiến thức chung trong khi thích ứng với các sắc thái của miền.

Quá trình tinh chỉnh kết hợp các kỹ thuật điều chuẩn khác nhau để ngăn chặn việc quá khớp (overfitting), bao gồm dropout, suy giảm trọng số (weight decay) và dừng sớm dựa trên hiệu suất xác thực. Các chiến lược tăng cường dữ liệu như cải cách truy vấn, diễn giải đoạn văn và tạo dữ liệu tổng hợp giúp cải thiện độ mạnh mẽ và khả năng khái quát hóa của mô hình.

Việc theo dõi hiệu suất trong quá trình tinh chỉnh theo dõi các chỉ số như Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG) và Recall@k. Tối ưu hóa siêu tham số thông qua các kỹ thuật như tối ưu hóa Bayes hoặc tìm kiếm lưới giúp xác định tốc độ học, kích thước lô và thời gian huấn luyện tối ưu cho từng ứng dụng cụ thể.

Thông qua phương pháp tinh chỉnh toàn diện này, việc triển khai các thành phần retriever và reranker của HippoRAG 2 trong nghiên cứu của tôi đạt được hiệu suất cao hơn đáng kể trong các tác vụ cụ thể của tiếng Việt so với các mô hình mục đích chung, đóng góp đáng kể vào hiệu quả tổng thể của framework.

\subsection{Module 2: OpenIE by LLM (Trích xuất Triple)}
Module OpenIE by LLM trong HippoRAG 2 sử dụng các mô hình mạnh mẽ như Llama-3.3-70B để trích xuất triple từ mỗi đoạn văn. Module này đại diện cho một bước quan trọng trong việc chuyển đổi văn bản phi cấu trúc thành kiến thức có cấu trúc có thể được tích hợp vào đồ thị tri thức. Quá trình trích xuất tập trung vào việc xác định các triple chủ thể-quan hệ-đối tượng nắm bắt các sự kiện và mối quan hệ thiết yếu trong văn bản.

Triple tuân theo định dạng (subject, relation, object), cung cấp một cách tự nhiên để biểu diễn thông tin thực tế. Ví dụ: Từ câu "Elon Musk đã sáng lập SpaceX," hệ thống trích xuất triple ("Elon Musk", "đã sáng lập", "SpaceX"). Mỗi triple được trích xuất tạo ra hai Phrase Node (subject và object) và một Relation Edge (có hướng từ subject đến object) trong đồ thị tri thức. Biểu diễn có cấu trúc này cho phép lưu trữ và truy xuất thông tin hiệu quả.

Một đổi mới quan trọng trong HippoRAG 2 là việc sử dụng phương pháp "schema-less open KG". Không giống như các đồ thị tri thức truyền thống bị giới hạn bởi các ontology được định nghĩa trước, phương pháp này cho phép trích xuất bất kỳ loại quan hệ nào mà không bị giới hạn bởi một schema cố định. Tính linh hoạt này cho phép hệ thống nắm bắt một phạm vi thông tin và mối quan hệ rộng hơn, làm cho nó dễ thích ứng hơn với các miền kiến thức đa dạng.

Phương pháp này mang lại những lợi thế đáng kể so với các hệ thống trích xuất thông tin truyền thống. Không giống như các phương pháp trích xuất dựa trên quy tắc hoặc thống kê, LLM có thể hiểu ngữ cảnh và trích xuất các mối quan hệ phức tạp mà các phương pháp cứng nhắc hơn có thể bỏ sót. Hệ thống không bị giới hạn bởi các tập hợp quan hệ được định nghĩa trước, cho phép biểu diễn thông tin đa dạng nắm bắt tốt hơn các sắc thái của ngôn ngữ tự nhiên. Hơn nữa, phương pháp không có schema giúp KG linh hoạt và dễ dàng mở rộng với kiến thức mới mà không cần thiết kế lại cấu trúc cơ bản.

\subsection{Module 3: Synonym Detection by Embedding}
Sau khi trích xuất triple, module Synonym Detection sử dụng các kỹ thuật embedding (Word2Vec, GloVe, BERT) để phát hiện các từ và cụm từ đồng nghĩa. Module này giải quyết một thách thức phổ biến trong truy xuất thông tin: sự đa dạng trong cách diễn đạt cùng một khái niệm trong ngôn ngữ tự nhiên. Bằng cách xác định và kết nối các thuật ngữ đồng nghĩa, hệ thống có thể bắc cầu qua những biến thể ngôn ngữ này và cải thiện hiệu suất truy xuất.

Quá trình bắt đầu bằng việc tính toán độ tương đồng cosine giữa các embedding của các Phrase Node khác nhau trong đồ thị tri thức. Khi độ tương đồng giữa hai node vượt quá ngưỡng được định nghĩa trước, một Synonym Edge được tạo ra để kết nối chúng. Ví dụ, hệ thống có thể kết nối "NYC" với "New York City" thông qua Synonym Edge, nhận ra rằng các thuật ngữ này đề cập đến cùng một thực thể mặc dù hình thức bề mặt của chúng khác nhau.

Module này mang lại một số lợi ích chính cho toàn bộ framework. Nó giúp hệ thống nhận diện các cách diễn đạt khác nhau của cùng một khái niệm, nâng cao khả năng truy vấn khi người dùng sử dụng từ đồng nghĩa hoặc biến thể của khái niệm. Bằng cách tạo kết nối giữa thông tin tương tự nhau trong các tài liệu khác nhau, nó cho phép truy xuất thông tin toàn diện hơn mà không bị giới hạn bởi các lựa chọn thuật ngữ cụ thể.

Không giống như các phương pháp dựa vào từ điển đồng nghĩa cố định, phương pháp dựa trên embedding này có thể phát hiện các mối quan hệ đồng nghĩa dựa trên sự tương đồng ngữ nghĩa thực tế trong không gian vector. Điều này cho phép nó xác định các mối quan hệ có thể không được nắm bắt trong các tài nguyên từ vựng được định nghĩa trước. Kết quả là một hệ thống linh hoạt và mạnh mẽ hơn, có thể kết nối thông tin giữa các tài liệu sử dụng thuật ngữ khác nhau nhưng truyền tải cùng một ý nghĩa.

\subsection{Module 4: Dense-Sparse Integration}
Module Dense-Sparse Integration kết hợp hai loại node trong đồ thị tri thức: Phrase node (mã hóa thưa thớt - sparse coding) và Passage node (mã hóa dày đặc - dense coding). Sự tích hợp này đại diện cho một đổi mới cơ bản trong HippoRAG 2 nhằm giải quyết sự đánh đổi cố hữu giữa độ chính xác khái niệm và sự phong phú về ngữ cảnh trong biểu diễn kiến thức.

Phrase node lưu trữ các khái niệm cô đọng, cụ thể là các chủ thể và đối tượng được trích xuất từ triple. Các node này biểu diễn thông tin ở định dạng thưa thớt, hiệu quả, tạo điều kiện cho việc suy luận và truy xuất nhanh chóng. Mặt khác, Passage node lưu trữ toàn bộ đoạn văn gốc, bảo toàn ngữ cảnh đầy đủ và thông tin chi tiết. Hai loại node này được liên kết thông qua các Context edge có nhãn "contains," có hướng từ Passage Node đến Phrase Node, cho biết một đoạn văn cụ thể chứa các khái niệm cụ thể.

Thiết kế của module này dựa trên lý thuyết mã hóa dày đặc và thưa thớt trong nhận thức của con người. Mã hóa thưa thớt, được đại diện bởi Phrase Node, hiệu quả về mặt lưu trữ và tạo điều kiện cho việc suy luận bằng cách tập trung vào các khái niệm thiết yếu. Mã hóa dày đặc, được đại diện bởi Passage Node, bảo toàn ngữ cảnh đầy đủ và thông tin chi tiết phong phú có thể bị mất trong các biểu diễn cô đọng hơn. Bằng cách kết hợp cả hai phương pháp, hệ thống đạt được sự cân bằng giữa hiệu quả xử lý và độ chính xác của thông tin.

Sự tích hợp này giải quyết những hạn chế chính của framework HippoRAG ban đầu. Nó khắc phục phương pháp tập trung vào thực thể vốn bỏ qua nhiều tín hiệu ngữ cảnh, cung cấp một biểu diễn kiến thức toàn diện hơn. Sự kết hợp này cải thiện đáng kể khả năng truy xuất thông tin so với các phương pháp chỉ dựa vào vector embedding. Có lẽ quan trọng nhất, nó cho phép hệ thống thực hiện suy luận nhanh thông qua các kết nối có cấu trúc giữa các Phrase Node trong khi vẫn có thể truy xuất thông tin chi tiết chính xác từ các Passage Node liên quan khi cần thiết.

Sự tích hợp dense-sparse đại diện cho một cải tiến cơ bản trong HippoRAG 2, giải quyết sự đánh đổi giữa khái niệm và ngữ cảnh tồn tại trong nhiều hệ thống biểu diễn kiến thức. Bằng cách kết hợp cả mã hóa thưa thớt (để biểu diễn khái niệm hiệu quả) và mã hóa dày đặc (để cung cấp thông tin ngữ cảnh phong phú), hệ thống đạt được sự cân bằng gần hơn với cách tổ chức bộ nhớ của con người, nơi cả khái niệm trừu tượng và ký ức tình tiết chi tiết cùng tồn tại và bổ sung cho nhau.

\section{Online Retrieval \& QA – Giai đoạn Truy hồi và Phản hồi trong Pipeline Đơn giản hóa}
Trong luận văn này, tôi đề xuất và thử nghiệm một pipeline RAG đơn giản hóa, lấy cảm hứng từ HippoRAG 2 nhưng lược bỏ các thành phần phức tạp như gán trọng số seed node và tìm kiếm đồ thị PPR. Mục tiêu là xây dựng một hệ thống hiệu quả, dễ triển khai hơn mà vẫn tận dụng được lợi thế của việc kết hợp cả thông tin từ passages (văn bản) và triples (kiến thức cấu trúc). Giai đoạn Online Retrieval \& QA trong pipeline này tập trung vào việc truy xuất, lọc và kết hợp hai nguồn thông tin này để cung cấp ngữ cảnh chất lượng cho LLM tạo ra câu trả lời.

Pipeline Online bao gồm các bước chính sau:

\subsection{Bước 1: Truy xuất Kép (Passages và Triples)}
Tương tự như HippoRAG 2, bước đầu tiên là thực hiện truy xuất song song để lấy cả các đoạn văn bản (passages) và các bộ ba (triples) liên quan đến truy vấn của người dùng từ kho kiến thức đã xây dựng ở giai đoạn Offline.
\begin{itemize}
    \item \textbf{Truy xuất Passage:} Sử dụng một mô hình retriever (ví dụ: BM25, DPR, hoặc mô hình đã được tinh chỉnh) để tìm và xếp hạng các đoạn văn bản (`Ranked Passages`) dựa trên độ tương đồng với truy vấn.
    \item \textbf{Truy xuất Triple:} Sử dụng một phương pháp phù hợp để tìm và xếp hạng các triple (`Ranked Triples`) liên quan đến truy vấn. Phương pháp "Query to Triple" dựa trên embedding (như NV-Embed-v2) là một lựa chọn hiệu quả được đề xuất trong HippoRAG 2.
\end{itemize}
\textbf{Cải tiến tiềm năng (Truy xuất Triple Lai - Hybrid Triple Retrieval):} Để nâng cao chất lượng của `Ranked Triples`, có thể áp dụng phương pháp truy xuất lai. Tuyến tính hóa tất cả triples thành câu, sau đó sử dụng cả BM25 (sparse) và mô hình embedding (dense) để truy xuất. Kết hợp kết quả (ví dụ: dùng RRF) để có danh sách `Ranked Triples` đầy đủ hơn, bao gồm cả các triple khớp từ khóa và ngữ nghĩa.

\subsection{Bước 2: Lọc Triple}
Danh sách `Ranked Triples` ban đầu có thể chứa nhiễu hoặc thông tin không thực sự liên quan. Do đó, cần có một bước lọc để chọn ra những facts cốt lõi, đáng tin cậy nhất (`Filtered Triples`).
\begin{itemize}
    \item \textbf{Cơ chế:} Có thể sử dụng một LLM nhỏ (như trong HippoRAG 2) để đánh giá mức độ liên quan của từng triple trong `Ranked Triples` với truy vấn gốc. Hoặc, có thể áp dụng các phương pháp lọc đơn giản hơn dựa trên ngưỡng điểm số tương đồng từ bước truy xuất triple.
    \item \textbf{Mục tiêu:} Thu được một tập hợp nhỏ các triple chất lượng cao, đại diện cho những kiến thức cấu trúc quan trọng nhất liên quan đến truy vấn.
\end{itemize}

\subsection{Bước 3: Chuẩn bị và Làm giàu Ngữ cảnh cho LLM}
Đây là bước then chốt để kết hợp thông tin từ passages và triples, đồng thời áp dụng các cải tiến nhằm tối ưu hóa ngữ cảnh đầu vào cho LLM.
\begin{itemize}
    \item \textbf{Ngữ cảnh cơ bản:} Lấy Top-K passages từ `Ranked Passages` và toàn bộ `Filtered Triples`.
    \item \textbf{Cải tiến 1 (Xếp hạng lại Passage dựa trên Triple - Triple-based Passage Reranking):} (Tùy chọn nhưng khuyến nghị) Sử dụng `Filtered Triples` để xếp hạng lại Top-N passages (với N > K) từ `Ranked Passages`. Tăng điểm cho những passage chứa các thực thể/mối quan hệ từ `Filtered Triples`. Sau đó chọn Top-K passages từ danh sách đã rerank. Điều này giúp ưu tiên các passage không chỉ liên quan đến truy vấn mà còn được xác thực bởi các facts đáng tin cậy từ KG.
    \item \textbf{Cải tiến 2 (Mở rộng Đồ thị Cục bộ - Lightweight Graph Expansion):} (Tùy chọn) Để làm giàu thêm ngữ cảnh, lấy các thực thể từ `Filtered Triples` và thực hiện tìm kiếm 1-hop trên KG. Thu thập các triple và/hoặc passage "hàng xóm" này. Bổ sung thông tin 1-hop này vào ngữ cảnh cuối cùng. Việc này giúp khôi phục một phần khả năng suy luận liên kết mà không cần PPR phức tạp.
    \item \textbf{Cải tiến 3 (Đưa Triple đã lọc vào Ngữ cảnh LLM - Inject Filtered Triples):} Khi xây dựng prompt cho LLM, bên cạnh các passage (đã rerank hoặc chưa, và có thể đã được mở rộng), cần đưa trực tiếp các `Filtered Triples` (đã được tuyến tính hóa thành câu) vào prompt. Điều này đảm bảo LLM nhận được các facts cốt lõi một cách rõ ràng.
\end{itemize}
Kết quả của bước này là một tập hợp ngữ cảnh phong phú và chất lượng cao, bao gồm các passage liên quan (đã được ưu tiên dựa trên facts) và các facts cấu trúc cốt lõi (filtered triples), có thể được bổ sung thêm thông tin 1-hop từ KG.

\subsection{Bước 4: Tạo câu trả lời (QA Reading)}
Bước cuối cùng là cung cấp ngữ cảnh đã chuẩn bị ở Bước 3 cho một LLM mạnh mẽ (ví dụ: GPT-4, Llama 3) để tổng hợp thông tin và tạo ra câu trả lời cuối cùng cho truy vấn của người dùng.
\begin{itemize}
    \item \textbf{Xây dựng Prompt:} Prompt bao gồm truy vấn gốc, các passage đã chọn lọc/làm giàu, và các filtered triples đã tuyến tính hóa.
    \item \textbf{Sinh câu trả lời:} LLM sử dụng ngữ cảnh được cung cấp để tạo ra một câu trả lời mạch lạc, chính xác và đầy đủ.
\end{itemize}
Bằng cách kết hợp thông tin từ cả văn bản và kiến thức cấu trúc, cùng với các cải tiến có mục tiêu trong việc lựa chọn và làm giàu ngữ cảnh, pipeline đơn giản hóa này hướng tới việc cải thiện chất lượng câu trả lời so với RAG truyền thống chỉ dựa trên passage, trong khi vẫn duy trì được sự đơn giản và hiệu quả về mặt tính toán so với các phương pháp phức tạp như HippoRAG 2 đầy đủ.

\end{document}

