# Phân tích chi tiết các Module trong Pipeline HippoRAG 2

Phần này đi sâu vào phân tích từng module cụ thể trong cả hai pha của pipeline HippoRAG 2, bao gồm chức năng, phương pháp triển khai được mô tả trong bài báo, và các hướng cải tiến tiềm năng dựa trên nghiên cứu hiện có.

## Pha 1: Lập chỉ mục Ngoại tuyến (Offline Indexing)

### Module 1.1: Trích xuất Triples bằng OpenIE (Sử dụng LLM)

**Chức năng:** Module này chịu trách nhiệm phân tích nội dung của các đoạn văn bản (passages) đầu vào và trích xuất thông tin có cấu trúc dưới dạng các bộ ba (subject, relation, object). Các bộ ba này tạo thành nền tảng cho việc xây dựng Knowledge Graph (KG).

**Phương pháp trong HippoRAG 2:** Bài báo đề cập rõ ràng việc sử dụng một Mô hình Ngôn ngữ Lớn (LLM), cụ thể là Llama-3.3-70B-Instruct, để thực hiện Trích xuất Thông tin Mở (OpenIE). Điều này đánh dấu một sự chuyển dịch so với các phương pháp OpenIE truyền thống (thường dựa trên các quy tắc ngôn ngữ hoặc các mô hình học máy nhỏ hơn). Việc sử dụng LLM cho OpenIE hứa hẹn khả năng hiểu ngữ cảnh sâu hơn và trích xuất các mối quan hệ phức tạp hơn, ít phụ thuộc vào các mẫu câu cố định.

**Nghiên cứu & Cải tiến tiềm năng:**
*   **Các phương pháp OpenIE truyền thống:** Các hệ thống như Stanford OpenIE, OpenIE-4, ReVerb, OLLIE thường dựa trên phân tích cú pháp phụ thuộc (dependency parsing) và các mẫu (patterns) được định nghĩa trước hoặc học được. Ưu điểm là tốc độ và tính giải thích được, nhưng nhược điểm là có thể bỏ lỡ các mối quan hệ được diễn đạt theo cách mới lạ hoặc phức tạp, và thường gặp khó khăn với ngôn ngữ không chính thức hoặc các cấu trúc câu đa dạng.
*   **OpenIE dựa trên LLM:** Xu hướng gần đây là tận dụng khả năng hiểu ngôn ngữ tự nhiên mạnh mẽ của LLM. Các phương pháp có thể bao gồm:
    *   **Prompting trực tiếp:** Đưa passage vào prompt và yêu cầu LLM trực tiếp xuất ra các triples. Kỹ thuật này đơn giản nhưng hiệu quả có thể phụ thuộc nhiều vào chất lượng prompt và khả năng "zero-shot" hoặc "few-shot" của LLM.
    *   **Fine-tuning LLM:** Tinh chỉnh một LLM trên một tập dữ liệu OpenIE lớn để chuyên biệt hóa mô hình cho nhiệm vụ này. Điều này có thể cải thiện đáng kể hiệu suất nhưng đòi hỏi dữ liệu huấn luyện và tài nguyên tính toán.
    *   **Sử dụng các mô hình LLM chuyên biệt cho IE:** Một số nghiên cứu phát triển các kiến trúc hoặc phương pháp huấn luyện LLM đặc thù cho các nhiệm vụ Information Extraction (IE), bao gồm OpenIE.
*   **Cải tiến tiềm năng:**
    *   **Kiểm soát chất lượng Triple:** LLM có thể tạo ra các triple không chính xác hoặc không liên quan (hallucination). Cần có cơ chế để đánh giá độ tin cậy của triple (ví dụ: dựa vào điểm xác suất từ LLM, đối chiếu với các nguồn khác, hoặc sử dụng một mô hình đánh giá riêng).
    *   **Chuẩn hóa Thực thể và Quan hệ (Normalization/Canonicalization):** Các triple được trích xuất có thể chứa các biểu diễn khác nhau cho cùng một thực thể hoặc quan hệ (ví dụ: "USA", "United States", "Mỹ"; "is located in", "located at"). Cần có bước chuẩn hóa để hợp nhất các biểu diễn này, thường liên quan đến Entity Linking và Relation Linking đến một ontology hoặc KG nền tảng (nếu có).
    *   **Xử lý Mơ hồ và Đồng tham chiếu (Ambiguity & Coreference Resolution):** LLM cần xử lý đúng các đại từ hoặc các tham chiếu mơ hồ để trích xuất triple chính xác. Tích hợp các kỹ thuật giải quyết đồng tham chiếu tiên tiến có thể cải thiện chất lượng.
    *   **Hiệu quả tính toán:** Sử dụng LLM lớn như Llama-70B cho OpenIE trên toàn bộ kho văn bản có thể rất tốn kém. Nghiên cứu các kỹ thuật chưng cất kiến thức (knowledge distillation) để tạo ra mô hình OpenIE nhỏ hơn từ LLM lớn, hoặc sử dụng các LLM nhỏ hơn nhưng được fine-tune hiệu quả, là hướng đi quan trọng.
    *   **Tích hợp với Ontology:** Kết hợp OpenIE với một ontology định trước có thể giúp cấu trúc hóa các triple tốt hơn và đảm bảo tính nhất quán.


### Module 1.2: Phát hiện Từ đồng nghĩa (Synonym Detection - Sử dụng Embedding)

**Chức năng:** Module này có nhiệm vụ xác định các nút cụm từ (phrase nodes) trong KG đại diện cho cùng một khái niệm hoặc thực thể, mặc dù chúng có thể có các biểu diễn bề mặt khác nhau. Việc liên kết các từ đồng nghĩa này giúp tạo ra một KG kết nối tốt hơn, cho phép lan truyền thông tin giữa các cách diễn đạt khác nhau của cùng một ý tưởng.

**Phương pháp trong HippoRAG 2:** Hệ thống sử dụng mô hình encoder embedding (cụ thể là NV-Embed-v2) để tính toán vector biểu diễn cho từng nút cụm từ. Sau đó, nó so sánh độ tương đồng cosine giữa các cặp vector embedding. Nếu độ tương đồng vượt qua một ngưỡng được xác định trước (ví dụ: 0.95 hoặc một giá trị được tinh chỉnh), hai nút cụm từ đó được coi là đồng nghĩa và một "cạnh đồng nghĩa" (synonym edge) được thêm vào giữa chúng trong KG. Cách tiếp cận này dựa trên giả định rằng các từ/cụm từ đồng nghĩa sẽ có biểu diễn vector gần nhau trong không gian embedding.

**Nghiên cứu & Cải tiến tiềm năng:**
*   **Ngưỡng cố định vs. Ngưỡng thích ứng:** Sử dụng một ngưỡng tương đồng cố định có thể không tối ưu. Một số cặp từ đồng nghĩa có thể có độ tương đồng thấp hơn ngưỡng, trong khi một số cặp không đồng nghĩa lại có thể vượt ngưỡng (đặc biệt là với các từ đa nghĩa hoặc các khái niệm gần nhau nhưng không hoàn toàn giống nhau). Có thể nghiên cứu các phương pháp xác định ngưỡng thích ứng dựa trên phân bố độ tương đồng hoặc đặc điểm của các nút cụm từ.
*   **Kết hợp nhiều nguồn bằng chứng:** Chỉ dựa vào độ tương đồng embedding có thể chưa đủ. Có thể kết hợp thêm các nguồn thông tin khác để xác định từ đồng nghĩa, ví dụ:
    *   **Từ điển đồng nghĩa (Thesauri):** Sử dụng các nguồn tài nguyên từ vựng có cấu trúc như WordNet hoặc các từ điển đồng nghĩa chuyên ngành.
    *   **Phân tích ngữ cảnh:** Xem xét ngữ cảnh mà các cụm từ xuất hiện trong các passages gốc. Nếu hai cụm từ thường xuyên xuất hiện trong các ngữ cảnh tương tự, khả năng chúng đồng nghĩa sẽ cao hơn.
    *   **Thông tin từ KG:** Phân tích cấu trúc liên kết của các nút cụm từ trong KG. Nếu hai nút có các nút láng giềng (quan hệ, thực thể liên quan) tương tự nhau, chúng có thể đồng nghĩa.
    *   **Sử dụng LLM:** Có thể dùng LLM để đánh giá xem hai cụm từ có phải là đồng nghĩa trong ngữ cảnh cụ thể hay không, thay vì chỉ dựa vào embedding.
*   **Xử lý Đa nghĩa (Polysemy):** Một từ hoặc cụm từ có thể có nhiều nghĩa. Chỉ dựa vào embedding có thể dẫn đến việc liên kết sai các nghĩa khác nhau. Cần các kỹ thuật nhận dạng nghĩa của từ (Word Sense Disambiguation - WSD) để đảm bảo rằng chỉ các nút cụm từ cùng nghĩa mới được liên kết.
*   **Chất lượng Embedding:** Hiệu quả của phương pháp này phụ thuộc rất nhiều vào chất lượng của mô hình embedding. Sử dụng các mô hình embedding được huấn luyện tốt, có khả năng nắm bắt sắc thái ngữ nghĩa tinh tế là rất quan trọng. Các mô hình embedding mới hơn hoặc được tinh chỉnh cho tên miền cụ thể có thể mang lại kết quả tốt hơn.
*   **Khả năng mở rộng:** Tính toán độ tương đồng cho tất cả các cặp nút cụm từ trong một KG lớn có thể rất tốn kém (O(n^2)). Cần các kỹ thuật lập chỉ mục và tìm kiếm lân cận gần đúng (Approximate Nearest Neighbor - ANN) như Faiss, ScaNN để tăng tốc quá trình này.

### Module 1.3: Tích hợp Passages vào KG (Dense-Sparse Integration)

**Chức năng:** Module này nhằm làm phong phú thêm KG bằng cách tích hợp trực tiếp thông tin ngữ cảnh từ các đoạn văn bản gốc (passages), thay vì chỉ dựa vào các khái niệm trừu tượng hóa trong các triple. Điều này giúp KG nắm bắt được cả "cái gì" (khái niệm, triple) và "ở đâu/như thế nào" (ngữ cảnh, passage).

**Phương pháp trong HippoRAG 2:** Đây là một cải tiến cốt lõi so với HippoRAG gốc. Mỗi passage trong kho văn bản được biểu diễn như một "nút đoạn văn" (passage node) riêng biệt trong KG. Sau đó, mỗi nút đoạn văn này được kết nối với tất cả các "nút cụm từ" (phrase nodes) đã được trích xuất từ chính passage đó thông qua các "cạnh ngữ cảnh" (context edge) với nhãn là "contains". Mô hình embedding cũng được sử dụng để tạo vector biểu diễn cho các nút đoạn văn này, tương tự như cách nó được thực hiện cho các nút cụm từ.

**Nghiên cứu & Cải tiến tiềm năng:**
*   **Biểu diễn Nút Đoạn văn:**
    *   **Embedding:** Sử dụng embedding của toàn bộ passage (như trong HippoRAG 2) là một cách phổ biến. Tuy nhiên, các mô hình embedding có giới hạn về độ dài đầu vào, và việc nén toàn bộ passage vào một vector duy nhất có thể làm mất thông tin. Các kỹ thuật như chia nhỏ passage hoặc sử dụng các kiến trúc embedding phân cấp (hierarchical embeddings) có thể được xem xét.
    *   **Biểu diễn lai (Hybrid Representation):** Kết hợp embedding với các đặc trưng khác của passage (ví dụ: các từ khóa chính, các thực thể nổi bật, chủ đề của passage) để tạo ra một biểu diễn phong phú hơn.
*   **Loại Cạnh Ngữ cảnh:** Cạnh "contains" đơn giản chỉ cho biết một passage chứa một cụm từ. Có thể xem xét các loại cạnh ngữ cảnh chi tiết hơn, ví dụ: "mentions_as_subject", "mentions_as_object", "defines", "explains", để thể hiện vai trò của cụm từ trong passage rõ ràng hơn. Điều này đòi hỏi khả năng phân tích sâu hơn từ module OpenIE.
*   **Trọng số Cạnh Ngữ cảnh:** Có thể gán trọng số cho các cạnh ngữ cảnh dựa trên mức độ quan trọng của cụm từ trong passage (ví dụ: dựa trên tần suất xuất hiện, vị trí trong câu, hoặc điểm TF-IDF) thay vì coi tất cả các cạnh là như nhau.
*   **Kết nối giữa các Nút Đoạn văn:** HippoRAG 2 chủ yếu kết nối passage với phrase. Có thể xem xét việc thêm các cạnh trực tiếp giữa các nút đoạn văn nếu chúng có sự liên quan về ngữ nghĩa (ví dụ: độ tương đồng embedding cao, cùng chủ đề, trích dẫn lẫn nhau). Điều này có thể tạo ra một lớp cấu trúc bổ sung trong KG.
*   **Cập nhật KG:** Khi có passages mới được thêm vào hoặc passages cũ được cập nhật, việc cập nhật các nút đoạn văn, nút cụm từ liên quan và các cạnh kết nối cần được thực hiện một cách hiệu quả mà không cần xây dựng lại toàn bộ KG.
*   **Tích hợp với Tóm tắt:** Thay vì chỉ kết nối với các cụm từ, có thể kết nối nút đoạn văn với một bản tóm tắt ngắn gọn của chính nó (có thể được tạo bởi LLM). Điều này có thể hữu ích cho các bước truy xuất sau này.

## Pha 2: Truy xuất Trực tuyến & Hỏi Đáp (Online Retrieval & QA)

### Module 2.1: Truy xuất Kép (Passages và Triples - Query to Triple)

**Chức năng:** Bước đầu tiên trong pha online là tìm kiếm các thông tin có khả năng liên quan nhất đến truy vấn của người dùng từ hai nguồn chính: kho văn bản (passages) và kho tri thức cấu trúc (triples).

**Phương pháp trong HippoRAG 2:**
*   **Truy xuất Passages:** Sử dụng mô hình embedding (NV-Embed-v2) để tính vector cho truy vấn và so sánh nó với các vector đã được tính toán trước của các nút đoạn văn (passage nodes) trong KG. Các passages được xếp hạng dựa trên độ tương đồng embedding (ví dụ: cosine similarity).
*   **Truy xuất Triples (Query to Triple):** Đây là một cải tiến so với HippoRAG gốc (vốn dùng NER to Node). Thay vì chỉ trích xuất thực thể từ truy vấn và khớp với nút cụm từ, HippoRAG 2 sử dụng embedding của *toàn bộ* truy vấn để so khớp với embedding của các triples (có thể là embedding của triple được tuyến tính hóa thành câu, hoặc một phương pháp embedding triple chuyên biệt). Các triples được xếp hạng dựa trên độ tương đồng embedding với truy vấn.

**Nghiên cứu & Cải tiến tiềm năng:**
*   **Truy xuất Passage:**
    *   **Truy xuất Lai (Hybrid Retrieval):** Kết hợp truy xuất dày đặc (dense, dựa trên embedding) với truy xuất thưa (sparse, dựa trên từ khóa như BM25) thường mang lại kết quả tốt hơn so với chỉ dùng một phương pháp. Các kỹ thuật như Reciprocal Rank Fusion (RRF) có thể được dùng để tổng hợp kết quả.
    *   **Mô hình Cross-Encoder Reranking:** Sau khi có danh sách ứng viên từ retriever (bi-encoder hoặc hybrid), có thể sử dụng một mô hình cross-encoder mạnh hơn (nhưng chậm hơn) để đánh giá lại độ liên quan giữa truy vấn và từng passage ứng viên, giúp cải thiện độ chính xác của top-k kết quả.
    *   **Truy xuất Đa vector (Multi-Vector Retrieval):** Biểu diễn passage bằng nhiều vector thay vì một vector duy nhất (ví dụ: ColBERT) có thể nắm bắt các khía cạnh ngữ nghĩa khác nhau và cải thiện khả năng khớp với các truy vấn đa dạng.
*   **Truy xuất Triple (Query to Triple):**
    *   **Embedding Triple:** Cách biểu diễn và tính embedding cho triple là rất quan trọng. Các phương pháp bao gồm: tuyến tính hóa triple thành câu rồi dùng embedding văn bản, hoặc sử dụng các mô hình embedding đồ thị tri thức chuyên biệt (ví dụ: TransE, RotatE, ComplEx) để học biểu diễn cho thực thể và quan hệ.
    *   **Truy xuất Lai cho Triple:** Tương tự như truy xuất passage, kết hợp phương pháp dựa trên embedding (dense) với phương pháp dựa trên khớp mẫu hoặc từ khóa (sparse) cho triples có thể hiệu quả.
    *   **Xem xét Cấu trúc Truy vấn:** Phân tích cấu trúc của truy vấn (ví dụ: xác định các thực thể, loại câu hỏi) có thể giúp định hướng quá trình truy xuất triple hiệu quả hơn.
    *   **Tích hợp Ontology/Schema:** Nếu KG có schema hoặc được liên kết với ontology, thông tin này có thể được sử dụng để cải thiện việc truy xuất triple (ví dụ: ưu tiên các triple phù hợp với loại thực thể hoặc quan hệ được mong đợi từ truy vấn).
*   **Tối ưu hóa Đồng thời:** Nghiên cứu cách tối ưu hóa cả hai quá trình truy xuất passage và triple một cách đồng thời, thay vì coi chúng là hai luồng độc lập, có thể dẫn đến kết quả tổng thể tốt hơn.

### Module 2.2: Bộ nhớ Nhận dạng (Recognition Memory - Lọc Triples bằng LLM)

**Chức năng:** Sau khi có danh sách các triple được xếp hạng (`Ranked Triples`) từ bước truy xuất, module này đóng vai trò như một bộ lọc để loại bỏ các triple không chính xác, không liên quan hoặc nhiễu, chỉ giữ lại những thông tin cấu trúc đáng tin cậy nhất (`Filtered Triples`) để sử dụng trong các bước tiếp theo (chọn nút mầm, tạo ngữ cảnh).

**Phương pháp trong HippoRAG 2:** Hệ thống sử dụng một LLM (Llama-3.3-70B-Instruct) làm bộ lọc. Các triple có thứ hạng cao nhất (top-k) từ `Ranked Triples` được đưa vào prompt cùng với truy vấn gốc. LLM được yêu cầu đánh giá và chỉ giữ lại những triple thực sự liên quan và hữu ích cho việc trả lời truy vấn. Bài báo đề cập việc sử dụng DSPy MIPROv2 để tối ưu hóa prompt cho nhiệm vụ lọc này.

**Nghiên cứu & Cải tiến tiềm năng:**
*   **Tiêu chí Lọc:** LLM có thể được hướng dẫn để lọc dựa trên nhiều tiêu chí:
    *   **Độ liên quan (Relevance):** Triple có trực tiếp trả lời hoặc cung cấp thông tin nền tảng cho truy vấn không?
    *   **Tính chính xác (Correctness):** Triple có đúng sự thật không (dựa trên kiến thức nội tại của LLM hoặc đối chiếu với passages)?
    *   **Tính mới lạ (Novelty):** Triple có cung cấp thông tin mới so với các triple khác đã được chọn không?
    *   **Độ tin cậy nguồn (Source Reliability):** Nếu có thông tin về nguồn gốc của triple, có thể sử dụng để đánh giá độ tin cậy.
*   **Phương pháp Lọc:**
    *   **Đánh giá Điểm (Scoring):** Yêu cầu LLM gán một điểm liên quan/tin cậy cho mỗi triple thay vì chỉ quyết định giữ/loại bỏ. Điều này cho phép lọc linh hoạt hơn dựa trên ngưỡng điểm.
    *   **Lọc theo Lô (Batch Filtering):** Đưa nhiều triple vào cùng một prompt để LLM xem xét chúng trong ngữ cảnh của nhau, có thể giúp loại bỏ thông tin trùng lặp.
    *   **Lọc Tương tác (Interactive Filtering):** Có thể thiết kế quy trình lọc nhiều bước, nơi LLM đưa ra lý do tại sao một triple bị loại bỏ, cho phép tinh chỉnh hoặc xem xét lại.
*   **Hiệu quả và Chi phí:** Sử dụng LLM lớn để lọc triple cho mỗi truy vấn có thể tốn kém và làm tăng độ trễ. Các hướng cải tiến bao gồm:
    *   **Sử dụng LLM nhỏ hơn:** Huấn luyện hoặc fine-tune một mô hình nhỏ hơn (ví dụ, thông qua chưng cất kiến thức từ LLM lớn) để thực hiện nhiệm vụ lọc.
    *   **Mô hình Phân loại:** Huấn luyện một mô hình phân loại nhị phân (classifier) đơn giản hơn để dự đoán xem một triple có liên quan đến truy vấn hay không, thay vì dùng LLM sinh.
    *   **Caching:** Lưu trữ kết quả lọc cho các cặp (truy vấn, triple) thường gặp.
*   **Tích hợp Phản hồi:** Sử dụng phản hồi từ người dùng hoặc từ kết quả cuối cùng của hệ thống QA để cải thiện mô hình lọc theo thời gian.

### Module 2.3: Chọn và Gán trọng số cho Nút Mầm (Seed Node Selection & Weighting)

**Chức năng:** Module này xác định các nút trong KG sẽ đóng vai trò là điểm khởi đầu (nút mầm - seed nodes) cho thuật toán lan truyền xác suất Personalized PageRank (PPR). Việc chọn đúng nút mầm và gán trọng số (xác suất đặt lại - reset probability) phù hợp là rất quan trọng để hướng PPR tập trung vào các phần liên quan nhất của KG đối với truy vấn.

**Phương pháp trong HippoRAG 2:**
*   **Chọn Nút Mầm:**
    *   **Từ Triples:** Các nút cụm từ (phrase nodes) có trong danh sách `Filtered Triples` (kết quả từ module Recognition Memory) được chọn làm nút mầm. Nếu không có triple nào được lọc, hệ thống chuyển sang sử dụng passages.
    *   **Từ Passages:** *Tất cả* các nút đoạn văn (passage nodes) cũng được chọn làm nút mầm. Việc này nhằm đảm bảo sự tham gia của thông tin ngữ cảnh và tăng cường khả năng khám phá các liên kết đa bước.
*   **Gán Trọng số (Xác suất Đặt lại):**
    *   **Nút Cụm từ:** Trọng số được gán dựa trên điểm xếp hạng trung bình của chúng từ bước truy xuất triple (Query to Triple). Các nút cụm từ xuất hiện trong các triple có điểm cao hơn sẽ nhận trọng số lớn hơn.
    *   **Nút Đoạn văn:** Trọng số được gán dựa trên độ tương đồng embedding của chúng với truy vấn (từ bước truy xuất passages). Tuy nhiên, tổng trọng số của các nút đoạn văn được nhân với một "hệ số cân bằng" (weight factor, mặc định là 0.05 trong bài báo) trước khi chuẩn hóa. Hệ số này dùng để điều chỉnh tầm quan trọng tương đối giữa thông tin từ triples (phrase nodes) và thông tin từ passages (passage nodes).
    *   **Chuẩn hóa:** Tổng tất cả các xác suất đặt lại của các nút mầm được chọn phải bằng 1.

**Nghiên cứu & Cải tiến tiềm năng:**
*   **Chiến lược Chọn Nút Mầm:**
    *   **Số lượng Nút Mầm:** Thay vì chọn tất cả phrase nodes từ filtered triples hoặc tất cả passage nodes, có thể giới hạn số lượng nút mầm (ví dụ: top-N phrase nodes, top-M passage nodes) để giảm độ phức tạp tính toán của PPR.
    *   **Đa dạng hóa Nút Mầm:** Cân nhắc việc chọn một tập hợp nút mầm đa dạng, bao phủ các khía cạnh khác nhau của truy vấn, thay vì chỉ tập trung vào các nút có điểm cao nhất.
    *   **Sử dụng LLM để Chọn Nút Mầm:** Có thể yêu cầu LLM phân tích truy vấn và KG cục bộ để đề xuất các nút mầm tiềm năng nhất.
*   **Chiến lược Gán Trọng số:**
    *   **Học Trọng số:** Thay vì dựa vào các quy tắc heuristic (điểm ranking, độ tương đồng), có thể học (learn) cách gán trọng số tối ưu cho các nút mầm, ví dụ, sử dụng reinforcement learning hoặc tối ưu hóa dựa trên hiệu suất cuối cùng của hệ thống QA.
    *   **Trọng số Động:** Điều chỉnh trọng số dựa trên đặc điểm của truy vấn (ví dụ: truy vấn cần nhiều thông tin ngữ cảnh hơn hay thông tin fact cụ thể hơn?) hoặc cấu trúc cục bộ của KG xung quanh các nút mầm.
    *   **Xem xét Hệ số Cân bằng (Weight Factor):** Giá trị 0.05 được chọn dựa trên validation set trong bài báo. Giá trị tối ưu có thể khác nhau tùy thuộc vào bộ dữ liệu, loại truy vấn, và chất lượng của KG. Cần có cơ chế để tinh chỉnh hoặc tự động điều chỉnh hệ số này.
*   **Kết hợp với các Tín hiệu Khác:** Ngoài điểm ranking và độ tương đồng, có thể xem xét các tín hiệu khác khi gán trọng số, ví dụ: độ trung tâm (centrality) của nút trong KG, độ tin cậy của triple/passage gốc.

### Module 2.4: Tìm kiếm Đồ thị bằng PPR (Personalized PageRank Graph Search)

**Chức năng:** Sau khi đã xác định các nút mầm và trọng số của chúng, module này thực hiện thuật toán Personalized PageRank (PPR) trên KG. PPR là một biến thể của PageRank, cho phép tính toán mức độ liên quan của tất cả các nút trong đồ thị đối với một tập hợp các nút khởi đầu (nút mầm) có trọng số. Kết quả là một phân bố xác suất trên các nút, phản ánh mức độ "gần gũi" hoặc "liên quan" của chúng đối với các nút mầm, và qua đó là đối với truy vấn gốc.

**Phương pháp trong HippoRAG 2:** Bài báo sử dụng thuật toán PPR tiêu chuẩn. Quá trình lặp (iterative process) bắt đầu từ các nút mầm đã được chọn và gán xác suất đặt lại. Trong mỗi bước lặp, xác suất được lan truyền từ các nút hiện tại đến các nút láng giềng thông qua các cạnh của KG. Đồng thời, ở mỗi bước, có một xác suất (thường gọi là teleport probability hoặc damping factor, alpha) để "quay trở lại" các nút mầm ban đầu theo phân bố xác suất đặt lại đã định. Quá trình này tiếp tục cho đến khi phân bố xác suất hội tụ. Điểm số PPR cuối cùng của mỗi nút phản ánh mức độ liên quan của nó.

**Nghiên cứu & Cải tiến tiềm năng:**
*   **Hiệu quả Tính toán PPR:** Tính toán PPR chính xác trên các đồ thị lớn có thể rất tốn kém về mặt thời gian và bộ nhớ. Các kỹ thuật tối ưu hóa bao gồm:
    *   **PPR Gần đúng (Approximate PPR):** Các thuật toán như Forward Push, Monte Carlo Random Walks cho phép ước tính điểm PPR nhanh hơn với một sai số chấp nhận được.
    *   **Tính toán PPR Offline:** Nếu có thể đoán trước các nút mầm tiềm năng, có thể tính toán trước một phần kết quả PPR offline.
    *   **PPR trên Đồ thị con (Subgraph PPR):** Chỉ thực hiện PPR trên một đồ thị con có liên quan đến truy vấn thay vì toàn bộ KG.
*   **Biến thể PPR:**
    *   **Topic-Sensitive PPR:** Điều chỉnh quá trình lan truyền dựa trên chủ đề hoặc loại quan hệ của các cạnh.
    *   **PPR có Trọng số Cạnh (Weighted Edges PPR):** Xem xét trọng số của các cạnh (ví dụ: độ tin cậy của triple, độ tương đồng ngữ nghĩa) trong quá trình lan truyền xác suất.
    *   **PPR dựa trên Đường đi (Path-based PPR):** Xem xét các đường đi cụ thể trong đồ thị thay vì chỉ lan truyền cục bộ.
*   **Xử lý Cấu trúc KG Động:** Nếu KG thay đổi thường xuyên (ví dụ: thêm triple/passage mới), cần các thuật toán PPR có khả năng cập nhật (incremental PPR) hiệu quả mà không cần tính toán lại từ đầu.
*   **Tích hợp Thông tin Khác vào PPR:** Có thể tích hợp các tín hiệu khác (ví dụ: embedding của nút, đặc trưng văn bản) vào quá trình lan truyền PPR thay vì chỉ dựa vào cấu trúc đồ thị.
*   **Diễn giải Kết quả PPR:** Hiểu tại sao một nút nhận được điểm PPR cao có thể hữu ích. Các kỹ thuật giải thích đường đi (path explanation) trong PPR có thể cung cấp insight.

### Module 2.5: Tạo Câu trả lời (QA Generation - Sử dụng LLM)

**Chức năng:** Đây là module cuối cùng trong pipeline, chịu trách nhiệm tổng hợp thông tin từ các nguồn đã được truy xuất và xử lý (cụ thể là các passages được xếp hạng cao nhất sau bước PPR) để tạo ra câu trả lời cuối cùng cho truy vấn của người dùng.

**Phương pháp trong HippoRAG 2:** Hệ thống lấy các đoạn văn bản (passages) có điểm PPR cao nhất (top-k). Các passages này, cùng với truy vấn gốc, được định dạng thành một prompt và đưa vào một LLM (được gọi là QA reader, ví dụ: Llama-3.3-70B-Instruct hoặc GPT-4o-mini trong các thí nghiệm). LLM sau đó được yêu cầu đọc ngữ cảnh được cung cấp và sinh ra câu trả lời cho truy vấn.

**Nghiên cứu & Cải tiến tiềm năng:**
*   **Xây dựng Prompt (Prompt Engineering):**
    *   **Định dạng Ngữ cảnh:** Cách trình bày các passages trong prompt (ví dụ: thứ tự, có đánh dấu nguồn, có tóm tắt ngắn gọn kèm theo) có thể ảnh hưởng đến khả năng hiểu và tổng hợp của LLM.
    *   **Tích hợp Triples vào Prompt:** Ngoài passages, có thể đưa trực tiếp các `Filtered Triples` (đã được tuyến tính hóa) vào prompt để cung cấp thêm facts cấu trúc rõ ràng cho LLM (như một trong các đề xuất cải tiến đã thảo luận trước đó).
    *   **Hướng dẫn Rõ ràng (Instruction Following):** Cung cấp hướng dẫn chi tiết cho LLM về cách sử dụng ngữ cảnh, cách xử lý thông tin mâu thuẫn (nếu có), và định dạng câu trả lời mong muốn.
*   **Lựa chọn và Xếp hạng lại Ngữ cảnh:**
    *   **Số lượng Passages (k):** Xác định số lượng passages tối ưu để đưa vào ngữ cảnh. Quá ít có thể thiếu thông tin, quá nhiều có thể gây nhiễu hoặc vượt quá giới hạn ngữ cảnh của LLM.
    *   **Xếp hạng lại dựa trên Truy vấn:** Có thể thực hiện một bước xếp hạng lại cuối cùng các passages dựa trên mức độ liên quan trực tiếp đến từng phần của truy vấn trước khi đưa vào LLM.
    *   **Nén Ngữ cảnh (Context Compression):** Sử dụng các kỹ thuật để tóm tắt hoặc trích xuất thông tin quan trọng nhất từ các passages dài trước khi đưa vào LLM.
*   **Khả năng Suy luận của LLM:**
    *   **Xử lý Thông tin Mâu thuẫn:** LLM cần có khả năng nhận biết và xử lý (ví dụ: chỉ ra sự mâu thuẫn, ưu tiên nguồn đáng tin cậy hơn) khi ngữ cảnh chứa thông tin trái ngược nhau.
    *   **Suy luận Đa bước:** Khuyến khích LLM thực hiện suy luận từng bước (Chain-of-Thought, Tree-of-Thought) dựa trên ngữ cảnh để trả lời các câu hỏi phức tạp.
    *   **Trích dẫn Nguồn (Citation):** Yêu cầu LLM trích dẫn nguồn (passage cụ thể) cho các phần thông tin trong câu trả lời để tăng tính minh bạch và kiểm chứng được.
*   **Đánh giá và Tinh chỉnh:**
    *   **Đánh giá Tự động:** Sử dụng các metric như ROUGE, BLEU, F1 (như trong bài báo), hoặc các phương pháp đánh giá dựa trên LLM (LLM-as-a-judge) để đo lường chất lượng câu trả lời.
    *   **Fine-tuning QA Reader:** Tinh chỉnh LLM đọc QA trên các cặp (truy vấn, ngữ cảnh, câu trả lời) cụ thể của miền ứng dụng để cải thiện hiệu suất.
*   **Xử lý Trường hợp Không có Câu trả lời:** LLM cần có khả năng nhận biết khi ngữ cảnh được cung cấp không đủ thông tin để trả lời truy vấn và thông báo điều đó thay vì "đoán mò" hoặc "ảo giác".
