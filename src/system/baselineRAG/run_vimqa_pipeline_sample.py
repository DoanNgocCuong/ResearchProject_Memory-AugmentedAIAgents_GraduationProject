"""
Run RAG Pipeline cho VIMQA_dev collection
ƒê·ªçc questions t·ª´ Excel, ch·∫°y qua RAG pipeline, v√† ghi k·∫øt qu·∫£ ra file m·ªõi
"""

import pandas as pd
import os
import sys
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import json

# Setup paths
current_file = Path(__file__)
sys.path.append(str(current_file.parent))

# Create outputs directory if it doesn't exist
outputs_dir = current_file.parent / "outputs"
outputs_dir.mkdir(exist_ok=True)

# Import RAG Pipeline
from pipelineRAG import RAGPipeline

class VIMQARunner:
    """
    Class ƒë·ªÉ ch·∫°y RAG Pipeline tr√™n VIMQA dataset
    """
    
    def __init__(
        self,
        collection_name: str = "VIMQA_dev",
        retriever_type: str = "vector",  # vector ho·∫∑c hybrid
        k: int = 5,
        generator_model: str = "gpt-4o-mini",
        temperature: float = 0.1
    ):
        """
        Kh·ªüi t·∫°o VIMQA Runner
        
        Args:
            collection_name: T√™n collection trong Qdrant
            retriever_type: Lo·∫°i retriever (vector, bm25, hybrid)
            k: S·ªë documents retrieve
            generator_model: Model OpenAI
            temperature: Temperature cho generation
        """
        self.collection_name = collection_name
        self.retriever_type = retriever_type
        self.k = k
        self.generator_model = generator_model
        self.temperature = temperature
        
        # Initialize RAG Pipeline
        print(f"üöÄ Initializing VIMQA Pipeline...")
        print(f"   Collection: {collection_name}")
        print(f"   Retriever: {retriever_type}")
        print(f"   K documents: {k}")
        print(f"   Model: {generator_model}")
        
        self.rag_pipeline = RAGPipeline(
            retriever_type=retriever_type,
            vector_store_type="qdrant",
            documents=None,  # S·ª≠ d·ª•ng existing collection
            collection_name=collection_name,
            k=k,
            generator_model=generator_model,
            temperature=temperature
        )
        
        print(f"‚úÖ Pipeline initialized successfully!")
    
    def load_questions_from_excel(self, file_path: str, question_column: str = "question") -> pd.DataFrame:
        """
        Load questions t·ª´ file Excel
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n file Excel
            question_column: T√™n c·ªôt ch·ª©a c√¢u h·ªèi
            
        Returns:
            DataFrame ch·ª©a questions
        """
        try:
            print(f"üìñ Loading questions from: {file_path}")
            
            # ƒê·ªçc file Excel
            if file_path.endswith('.xlsx') or file_path.endswith('.xls'):
                df = pd.read_excel(file_path)
            elif file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
            else:
                raise ValueError("Unsupported file format. Use .xlsx, .xls, or .csv")
            
            print(f"‚úÖ Loaded {len(df)} rows")
            print(f"üìã Columns: {list(df.columns)}")
            
            # Ki·ªÉm tra column question c√≥ t·ªìn t·∫°i kh√¥ng
            if question_column not in df.columns:
                print(f"‚ö†Ô∏è  Column '{question_column}' not found. Available columns: {list(df.columns)}")
                # T·ª± ƒë·ªông detect question column
                possible_cols = [col for col in df.columns if 'question' in col.lower() or 'c√¢u h·ªèi' in col.lower() or 'query' in col.lower()]
                if possible_cols:
                    question_column = possible_cols[0]
                    print(f"‚úÖ Auto-detected question column: '{question_column}'")
                else:
                    raise ValueError(f"Cannot find question column. Available: {list(df.columns)}")
            
            # Filter out empty questions
            original_len = len(df)
            df = df[df[question_column].notna() & (df[question_column] != "")]
            print(f"üìù Valid questions: {len(df)}/{original_len}")
            
            return df
            
        except Exception as e:
            print(f"‚ùå Error loading Excel file: {e}")
            raise
    
    def process_questions(
        self, 
        df: pd.DataFrame, 
        question_column: str = "question",
        batch_size: int = 10,
        save_intermediate: bool = True
    ) -> pd.DataFrame:
        """
        X·ª≠ l√Ω t·∫•t c·∫£ questions qua RAG pipeline
        
        Args:
            df: DataFrame ch·ª©a questions
            question_column: T√™n c·ªôt question
            batch_size: S·ªë questions x·ª≠ l√Ω c√πng l√∫c
            save_intermediate: C√≥ save k·∫øt qu·∫£ trung gian kh√¥ng
            
        Returns:
            DataFrame v·ªõi k·∫øt qu·∫£
        """
        results = []
        total_questions = len(df)
        
        print(f"\nü§ñ Processing {total_questions} questions...")
        print(f"üì¶ Batch size: {batch_size}")
        
        for i in range(0, total_questions, batch_size):
            batch_end = min(i + batch_size, total_questions)
            batch_df = df.iloc[i:batch_end].copy()
            
            print(f"\nüì¶ Processing batch {i//batch_size + 1}/{(total_questions-1)//batch_size + 1}")
            print(f"   Questions {i+1}-{batch_end}/{total_questions}")
            
            batch_results = []
            
            for idx, row in batch_df.iterrows():
                question = row[question_column]
                
                try:
                    print(f"   üîç Q{idx+1}: {question[:50]}...")
                    
                    # Query through RAG pipeline
                    result = self.rag_pipeline.query(
                        question=question,
                        return_sources=True,
                        return_documents=False
                    )
                    
                    # Prepare result row
                    result_row = row.copy()
                    result_row['ai_answer'] = result.get('answer', 'Error: No answer generated')
                    result_row['sources'] = str(result.get('sources', []))
                    result_row['num_documents_used'] = result.get('num_documents_used', 0)
                    result_row['has_error'] = 'error' in result
                    result_row['error_message'] = result.get('error', '')
                    
                    batch_results.append(result_row)
                    print(f"   ‚úÖ Completed")
                    
                except Exception as e:
                    print(f"   ‚ùå Error: {e}")
                    result_row = row.copy()
                    result_row['ai_answer'] = f"Error: {str(e)}"
                    result_row['sources'] = '[]'
                    result_row['num_documents_used'] = 0
                    result_row['has_error'] = True
                    result_row['error_message'] = str(e)
                    batch_results.append(result_row)
            
            results.extend(batch_results)
            
            # Save intermediate results
            if save_intermediate and len(results) > 0:
                temp_df = pd.DataFrame(results)
                temp_file = outputs_dir / f"temp_results_batch_{i//batch_size + 1}.xlsx"
                temp_df.to_excel(temp_file, index=False)
                print(f"üíæ Saved intermediate results to: {temp_file}")
        
        return pd.DataFrame(results)
    
    def save_results(
        self, 
        results_df: pd.DataFrame, 
        output_file: str = None,
        include_metadata: bool = True
    ) -> str:
        """
        L∆∞u k·∫øt qu·∫£ ra file Excel
        
        Args:
            results_df: DataFrame ch·ª©a k·∫øt qu·∫£
            output_file: T√™n file output
            include_metadata: C√≥ bao g·ªìm metadata kh√¥ng
            
        Returns:
            ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"vimqa_results_{timestamp}.xlsx"
        
        # Ensure output file is in outputs directory
        output_path = outputs_dir / output_file
        
        try:
            print(f"\nüíæ Saving results to: {output_path}")
            
            if include_metadata:
                # Add metadata sheet
                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                    # Main results
                    results_df.to_excel(writer, sheet_name='Results', index=False)
                    
                    # Metadata
                    metadata = {
                        'run_timestamp': [datetime.now().strftime("%Y-%m-%d %H:%M:%S")],
                        'collection_name': [self.collection_name],
                        'retriever_type': [self.retriever_type],
                        'k_documents': [self.k],
                        'generator_model': [self.generator_model],
                        'temperature': [self.temperature],
                        'total_questions': [len(results_df)],
                        'successful_answers': [len(results_df[~results_df['has_error']])],
                        'failed_answers': [len(results_df[results_df['has_error']])]
                    }
                    metadata_df = pd.DataFrame(metadata)
                    metadata_df.to_excel(writer, sheet_name='Metadata', index=False)
                    
                    print(f"‚úÖ Saved with metadata")
            else:
                results_df.to_excel(output_path, index=False)
                print(f"‚úÖ Saved results only")
            
            return str(output_path)
            
        except Exception as e:
            print(f"‚ùå Error saving results: {e}")
            raise
    
    def run_full_pipeline(
        self,
        input_file: str,
        output_file: str = None,
        question_column: str = "question",
        batch_size: int = 10
    ) -> str:
        """
        Ch·∫°y full pipeline: load ‚Üí process ‚Üí save
        
        Args:
            input_file: File Excel input
            output_file: File Excel output
            question_column: C·ªôt ch·ª©a questions
            batch_size: Batch size
            
        Returns:
            ƒê∆∞·ªùng d·∫´n file k·∫øt qu·∫£
        """
        print(f"\nüöÄ Starting VIMQA Full Pipeline")
        print(f"{'='*50}")
        
        # Step 1: Load questions
        df = self.load_questions_from_excel(input_file, question_column)
        
        # Step 2: Process questions
        results_df = self.process_questions(
            df, 
            question_column=question_column,
            batch_size=batch_size
        )
        
        # Step 3: Save results
        output_path = self.save_results(results_df, output_file)
        
        # Step 4: Summary
        total = len(results_df)
        successful = len(results_df[~results_df['has_error']])
        failed = len(results_df[results_df['has_error']])
        
        print(f"\nüéØ Pipeline Summary:")
        print(f"   Total questions: {total}")
        print(f"   Successful: {successful} ({successful/total*100:.1f}%)")
        print(f"   Failed: {failed} ({failed/total*100:.1f}%)")
        print(f"   Output file: {output_path}")
        
        return output_path

def main():
    """
    Main function ƒë·ªÉ ch·∫°y VIMQA pipeline
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="Run VIMQA RAG Pipeline")
    parser.add_argument("--input", "-i", required=True, help="Input Excel file path")
    parser.add_argument("--output", "-o", help="Output Excel file path")
    parser.add_argument("--question_column", "-q", default="question", help="Question column name")
    parser.add_argument("--collection", "-c", default="VIMQA_dev", help="Qdrant collection name")
    parser.add_argument("--retriever", "-r", default="vector", choices=["vector", "bm25", "hybrid"], help="Retriever type")
    parser.add_argument("--k", type=int, default=5, help="Number of documents to retrieve")
    parser.add_argument("--batch_size", "-b", type=int, default=10, help="Batch size for processing")
    parser.add_argument("--model", "-m", default="gpt-4o-mini", help="OpenAI model")
    parser.add_argument("--temperature", "-t", type=float, default=0.1, help="Generation temperature")
    
    args = parser.parse_args()
    
    try:
        # Initialize runner
        runner = VIMQARunner(
            collection_name=args.collection,
            retriever_type=args.retriever,
            k=args.k,
            generator_model=args.model,
            temperature=args.temperature
        )
        
        # Run pipeline
        output_file = runner.run_full_pipeline(
            input_file=args.input,
            output_file=args.output,
            question_column=args.question_column,
            batch_size=args.batch_size
        )
        
        print(f"\nüéâ Pipeline completed successfully!")
        print(f"üìÑ Results saved to: {output_file}")
        
    except Exception as e:
        print(f"\nüí• Pipeline failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # Example usage when run directly
    if len(sys.argv) == 1:
        print("üöÄ VIMQA RAG Pipeline Runner")
        print("="*40)
        print("\nExample usage:")
        print("python run_vimqa_pipeline.py --input questions.xlsx --output results.xlsx")
        print("\nArguments:")
        print("  --input, -i      : Input Excel file with questions")
        print("  --output, -o     : Output Excel file (optional)")
        print("  --question_column: Column name containing questions (default: 'question')")
        print("  --collection, -c : Qdrant collection name (default: 'VIMQA_dev')")
        print("  --retriever, -r  : Retriever type (vector/bm25/hybrid, default: vector)")
        print("  --k              : Number of documents to retrieve (default: 5)")
        print("  --batch_size, -b : Batch size (default: 10)")
        print("  --model, -m      : OpenAI model (default: gpt-4o-mini)")
        print("  --temperature, -t: Generation temperature (default: 0.1)")
        
        # Quick test v·ªõi sample data
        print("\nüß™ Running quick test...")
        
        # T·∫°o sample data
        sample_data = {
            'question': [
                'RAG l√† g√¨?',
                'Vector search ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?',
                '∆Øu ƒëi·ªÉm c·ªßa Python trong AI l√† g√¨?'
            ],
            'id': [1, 2, 3]
        }
        sample_df = pd.DataFrame(sample_data)
        sample_file = outputs_dir / "sample_questions.xlsx"
        sample_df.to_excel(sample_file, index=False)
        print(f"üìù Created sample file: {sample_file}")
        
        try:
            runner = VIMQARunner(
                collection_name="VIMQA_dev",
                retriever_type="vector",
                k=3
            )
            
            output_file = runner.run_full_pipeline(
                input_file=str(sample_file),
                batch_size=3
            )
            
            print(f"‚úÖ Test completed! Check: {output_file}")
            
        except Exception as e:
            print(f"‚ùå Test failed: {e}")
            
    else:
        main()
