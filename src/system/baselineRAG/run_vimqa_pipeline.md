"""
RAG Pipeline - Káº¿t há»£p retrieval vÃ  generation Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i
"""

from typing import List, Dict, Any, Optional
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv
import os
import sys
from pathlib import Path

# Setup paths
current_file = Path(__file__)
sys.path.append(str(current_file.parent))

# Import cÃ¡c module Ä‘Ã£ cÃ³
from _04_retrieval.retriever import DocumentRetriever
from _05_generation.generator import AnswerGenerator

# Load environment variables
load_dotenv()

class RAGPipeline:
    """
    Pipeline RAG hoÃ n chá»‰nh: Input -> Retrieval -> Generation -> Output
    """
    
    def __init__(
        self,
        # Retriever parameters
        retriever_type: str = "vector",
        vector_store_type: str = "qdrant",
        documents: Optional[List[Document]] = None,
        embeddings_model = None,
        hybrid_weights: List[float] = [0.5, 0.5],
        k: int = 5,
        
        # Vector store parameters
        qdrant_url: Optional[str] = None,
        qdrant_api_key: Optional[str] = None,
        collection_name: str = "VIMQA_dev",
        
        # Generator parameters
        generator_model: str = "gpt-4o-mini",
        temperature: float = 0,
        max_tokens: int = 4096,
        system_prompt: Optional[str] = None
    ):
        """
        Khá»Ÿi táº¡o RAG Pipeline
        
        Args:
            retriever_type: Loáº¡i retriever ("vector", "bm25", "hybrid", "compression")
            vector_store_type: Loáº¡i vector store ("qdrant")
            documents: List documents cho BM25 vÃ  hybrid
            embeddings_model: Model embedding
            hybrid_weights: Trá»ng sá»‘ cho hybrid search [vector_weight, bm25_weight]
            k: Sá»‘ lÆ°á»£ng documents tráº£ vá»
            qdrant_url: URL Qdrant server
            qdrant_api_key: API key Qdrant
            collection_name: TÃªn collection
            generator_model: Model OpenAI
            temperature: Äá»™ sÃ¡ng táº¡o cá»§a model
            max_tokens: Sá»‘ token tá»‘i Ä‘a
            system_prompt: System prompt tÃ¹y chá»‰nh
        """
        
        # Khá»Ÿi táº¡o embeddings model náº¿u chÆ°a cÃ³
        if embeddings_model is None:
            self.embeddings_model = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
            )
        else:
            self.embeddings_model = embeddings_model
        
        # Store documents for later use
        self.documents = documents or []
        
        # Khá»Ÿi táº¡o Document Retriever
        self.retriever = DocumentRetriever(
            retriever_type=retriever_type,
            vector_store_type=vector_store_type,
            documents=self.documents,
            embeddings_model=self.embeddings_model,
            hybrid_weights=hybrid_weights,
            k=k,
            qdrant_url=qdrant_url or os.getenv("QDRANT_URL"),
            qdrant_api_key=qdrant_api_key or os.getenv("QDRANT_API_KEY"),
            collection_name=collection_name
        )
        
        # Fix: Ensure system prompt is not None
        if system_prompt is None:
            system_prompt = """Báº¡n lÃ  má»™t trá»£ lÃ½ AI há»¯u Ã­ch, tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p. 
Náº¿u báº¡n khÃ´ng biáº¿t cÃ¢u tráº£ lá»i, hÃ£y nÃ³i báº¡n khÃ´ng biáº¿t. 
Chá»‰ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘á»ƒ tráº£ lá»i. 
Giá»¯ cÃ¢u tráº£ lá»i rÃµ rÃ ng vÃ  Ä‘Æ¡n giáº£n."""
        
        # Khá»Ÿi táº¡o Answer Generator
        self.generator = AnswerGenerator(
            model_name=generator_model,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system_prompt
        )
        
        # Fix: Add documents to vector store if using vector retriever
        if self.documents and retriever_type in ["vector", "hybrid"]:
            self._ensure_documents_indexed()
        
        print(f"RAG Pipeline initialized with:")
        print(f"  - Retriever: {retriever_type}")
        print(f"  - Vector Store: {vector_store_type}")
        print(f"  - Generator: {generator_model}")
        print(f"  - K documents: {k}")
        print(f"  - Documents loaded: {len(self.documents)}")
    
    def _ensure_documents_indexed(self):
        """
        Äáº£m báº£o documents Ä‘Ã£ Ä‘Æ°á»£c index vÃ o vector store
        """
        try:
            if hasattr(self.retriever, 'vector_store') and self.retriever.vector_store:
                # Try to get collection info
                try:
                    collection_info = self.retriever.vector_store.get_collection_info()
                    if collection_info and collection_info.get('vectors_count', 0) == 0:
                        print(f"ğŸ“ Indexing {len(self.documents)} documents to vector store...")
                        self.retriever.vector_store.add_documents(self.documents)
                        print(f"âœ… Documents indexed successfully")
                    else:
                        print(f"ğŸ“š Vector store already contains documents")
                except:
                    # If can't get collection info, try to add documents anyway
                    print(f"ğŸ“ Attempting to index {len(self.documents)} documents...")
                    if self.documents:
                        self.retriever.vector_store.add_documents(self.documents)
                        print(f"âœ… Documents added to vector store")
        except Exception as e:
            print(f"âš ï¸  Warning: Could not verify/index documents: {e}")
            print(f"    This might be okay if documents are already indexed")
    
    def query(
        self, 
        question: str,
        return_sources: bool = True,
        return_documents: bool = False
    ) -> Dict[str, Any]:
        """
        Xá»­ lÃ½ cÃ¢u há»i qua pipeline RAG hoÃ n chá»‰nh
        
        Args:
            question: CÃ¢u há»i cá»§a user
            return_sources: CÃ³ tráº£ vá» sources khÃ´ng
            return_documents: CÃ³ tráº£ vá» raw documents khÃ´ng
            
        Returns:
            Dictionary chá»©a answer vÃ  thÃ´ng tin bá»• sung
        """
        
        print(f"\nğŸ” Retrieving documents for: '{question}'")
        
        # BÆ°á»›c 1: Retrieve documents
        try:
            documents = self.retriever.retrieve_documents(question)
            print(f"âœ… Found {len(documents)} relevant documents")
            
            # Debug: Print document contents if found
            if len(documents) > 0:
                print("ğŸ“„ Retrieved documents preview:")
                for i, doc in enumerate(documents[:2]):  # Show first 2 docs
                    content_preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                    print(f"   {i+1}. {content_preview}")
            else:
                print("âš ï¸  No documents found, trying fallback...")
                # Fallback: use original documents if retrieval fails
                if self.documents:
                    documents = self.documents[:self.retriever.k]
                    print(f"ğŸ“š Using fallback documents: {len(documents)}")
                    
        except Exception as e:
            print(f"âŒ Retrieval failed: {e}")
            # Fallback to original documents
            if self.documents:
                documents = self.documents[:self.retriever.k]
                print(f"ğŸ“š Using fallback documents: {len(documents)}")
            else:
                return {
                    "question": question,
                    "answer": "Xin lá»—i, khÃ´ng thá»ƒ tÃ¬m kiáº¿m tÃ i liá»‡u liÃªn quan.",
                    "error": str(e),
                    "num_documents_used": 0
                }
        
        # BÆ°á»›c 2: Generate answer
        print(f"ğŸ¤– Generating answer...")
        try:
            if return_sources:
                result = self.generator.generate_answer_with_sources(question, documents)
                answer = result["answer"]
                sources = result["sources"]
            else:
                answer = self.generator.generate_answer(question, documents)
                sources = []
                
            print(f"âœ… Answer generated successfully")
            
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            print(f"    Error details: {type(e).__name__}: {str(e)}")
            return {
                "question": question,
                "answer": "Xin lá»—i, khÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i.",
                "error": str(e),
                "num_documents_used": len(documents) if 'documents' in locals() else 0
            }
        
        # Prepare result
        result = {
            "question": question,
            "answer": answer,
            "num_documents_used": len(documents)
        }
        
        if return_sources:
            result["sources"] = sources
            
        if return_documents:
            result["documents"] = [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in documents
            ]
        
        return result
    
    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:
        """
        Xá»­ lÃ½ nhiá»u cÃ¢u há»i cÃ¹ng lÃºc
        
        Args:
            questions: List cÃ¡c cÃ¢u há»i
            
        Returns:
            List cÃ¡c káº¿t quáº£
        """
        results = []
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ“ Processing question {i}/{len(questions)}")
            result = self.query(question)
            results.append(result)
            
        return results
    
    def add_documents(self, new_documents: List[Document]):
        """
        ThÃªm documents má»›i vÃ o há»‡ thá»‘ng
        
        Args:
            new_documents: List documents má»›i
        """
        try:
            # Add to local storage
            self.documents.extend(new_documents)
            
            if hasattr(self.retriever, 'vector_store') and self.retriever.vector_store:
                # ThÃªm vÃ o vector store
                self.retriever.vector_store.add_documents(new_documents)
                print(f"âœ… Added {len(new_documents)} documents to vector store")
                
            # Cáº­p nháº­t documents cho BM25 náº¿u cáº§n
            if hasattr(self.retriever, 'documents') and self.retriever.documents is not None:
                self.retriever.documents.extend(new_documents)
                # Reinitialize BM25 retriever if needed
                if hasattr(self.retriever.retriever, 'bm25_retriever'):
                    self.retriever._initialize_retriever()
                print(f"âœ… Updated BM25 index with {len(new_documents)} new documents")
                
        except Exception as e:
            print(f"âŒ Failed to add documents: {e}")
    
    def get_system_info(self) -> Dict[str, Any]:
        """
        Láº¥y thÃ´ng tin vá» há»‡ thá»‘ng RAG
        """
        return {
            "retriever_type": self.retriever.retriever_type,
            "vector_store_type": self.retriever.vector_store_type,
            "generator_model": self.generator.model_name,
            "k_documents": self.retriever.k,
            "temperature": self.generator.temperature,
            "collection_name": getattr(self.retriever.vector_store, 'collection_name', 'N/A'),
            "total_documents": len(self.documents)
        }

def create_rag_pipeline(
    documents: Optional[List[Document]] = None,
    retriever_type: str = "bm25",  # Changed default to BM25 for reliability
    generator_model: str = "gpt-4o-mini"
) -> RAGPipeline:
    """
    Factory function Ä‘á»ƒ táº¡o RAG pipeline Ä‘Æ¡n giáº£n
    
    Args:
        documents: Documents Ä‘á»ƒ index
        retriever_type: Loáº¡i retriever
        generator_model: Model generator
        
    Returns:
        RAGPipeline instance
    """
    return RAGPipeline(
        retriever_type=retriever_type,
        documents=documents,
        generator_model=generator_model
    )

if __name__ == "__main__":
    """
    Demo vÃ  test RAG Pipeline
    """
    from langchain_core.documents import Document
    
    # Táº¡o sample documents
    sample_docs = [
        Document(
            page_content="RAG (Retrieval-Augmented Generation) lÃ  má»™t ká»¹ thuáº­t káº¿t há»£p viá»‡c tÃ¬m kiáº¿m thÃ´ng tin liÃªn quan vá»›i viá»‡c táº¡o sinh ngÃ´n ngá»¯. RAG giÃºp cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n.",
            metadata={"source": "rag_intro.txt", "topic": "AI"}
        ),
        Document(
            page_content="Vector search sá»­ dá»¥ng embeddings Ä‘á»ƒ tÃ¬m kiáº¿m tÃ i liá»‡u tÆ°Æ¡ng tá»± vá» máº·t ngá»¯ nghÄ©a. NÃ³ hiá»‡u quáº£ hÆ¡n keyword search trong nhiá»u trÆ°á»ng há»£p.",
            metadata={"source": "vector_search.txt", "topic": "Search"}
        ),
        Document(
            page_content="OpenAI GPT-4 lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n cÃ³ kháº£ nÄƒng hiá»ƒu vÃ  táº¡o sinh vÄƒn báº£n má»™t cÃ¡ch tá»± nhiÃªn. NÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng trong nhiá»u á»©ng dá»¥ng AI.",
            metadata={"source": "gpt4_info.txt", "topic": "LLM"}
        ),
        Document(
            page_content="Python lÃ  ngÃ´n ngá»¯ láº­p trÃ¬nh phá»• biáº¿n trong AI vÃ  machine learning. CÃ¡c thÆ° viá»‡n nhÆ° LangChain giÃºp xÃ¢y dá»±ng á»©ng dá»¥ng AI dá»… dÃ ng hÆ¡n.",
            metadata={"source": "python_ai.txt", "topic": "Programming"}
        )
    ]
    
    print("ğŸš€ Testing RAG Pipeline...")
    
    # Test 1: BM25 retriever (more reliable for testing)
    print("\n" + "="*50)
    print("TEST 1: BM25 Retriever + GPT-4o-mini")
    print("="*50)
    try:
        rag = RAGPipeline(
            retriever_type="bm25",
            documents=sample_docs,
            k=3
        )
        
        # Test single query
        result = rag.query("RAG lÃ  gÃ¬?")
        print(f"\nğŸ“‹ Question: {result['question']}")
        print(f"ğŸ’¡ Answer: {result['answer']}")
        print(f"ğŸ“š Sources: {result.get('sources', [])}")
        print(f"ğŸ“„ Documents used: {result['num_documents_used']}")
        
    except Exception as e:
        print(f"âŒ BM25 test failed: {e}")
        import traceback
        traceback.print_exc()
    
    # Test 2: Vector retriever
    print("\n" + "="*50)
    print("TEST 2: Vector Retriever")
    print("="*50)
    try:
        rag_vector = RAGPipeline(
            retriever_type="vector",
            documents=sample_docs,
            k=2,
            collection_name="VIMQA_dev"  # Use unique collection name
        )
        
        result = rag_vector.query("Vector search hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?")
        print(f"\nğŸ“‹ Question: {result['question']}")
        print(f"ğŸ’¡ Answer: {result['answer']}")
        print(f"ğŸ“š Sources: {result.get('sources', [])}")
        
    except Exception as e:
        print(f"âŒ Vector test failed: {e}")
        import traceback
        traceback.print_exc()
    
    # Test 3: Batch queries (only if BM25 test passed)
    print("\n" + "="*50)
    print("TEST 3: Batch Queries")
    print("="*50)
    try:
        if 'rag' in locals():
            questions = [
                "Python cÃ³ Æ°u Ä‘iá»ƒm gÃ¬ trong AI?",
                "GPT-4 lÃ  gÃ¬?"
            ]
            
            results = rag.batch_query(questions)
            for i, result in enumerate(results, 1):
                print(f"\n{i}. {result['question']}")
                if 'error' not in result:
                    print(f"   Answer: {result['answer'][:100]}...")
                else:
                    print(f"   Error: {result['error']}")
                    
    except Exception as e:
        print(f"âŒ Batch test failed: {e}")
    
    # Test 4: System info
    print("\n" + "="*50)
    print("TEST 4: System Information")
    print("="*50)
    try:
        if 'rag' in locals():
            info = rag.get_system_info()
            print("ğŸ”§ System Configuration:")
            for key, value in info.items():
                print(f"   {key}: {value}")
                
    except Exception as e:
        print(f"âŒ System info test failed: {e}")
    
    # Test 5: Simple functional test
    print("\n" + "="*50)
    print("TEST 5: Simple Functional Test")
    print("="*50)
    try:
        simple_rag = create_rag_pipeline(documents=sample_docs, retriever_type="bm25")
        result = simple_rag.query("Python cÃ³ gÃ¬ hay?", return_documents=True)
        
        print(f"\nğŸ“‹ Question: {result['question']}")
        if 'error' not in result:
            print(f"ğŸ’¡ Answer: {result['answer']}")
            print(f"ğŸ“„ Documents used: {result['num_documents_used']}")
            
            if result.get('documents'):
                print("\nğŸ“‘ Retrieved documents:")
                for i, doc in enumerate(result['documents'], 1):
                    print(f"   {i}. {doc['content'][:50]}...")
        else:
            print(f"âŒ Error: {result['error']}")
        
    except Exception as e:
        print(f"âŒ Simple test failed: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ… RAG Pipeline testing completed!")
