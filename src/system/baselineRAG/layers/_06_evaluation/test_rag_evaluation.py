"""
File test cho há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ RAG.
Cháº¡y file nÃ y Ä‘á»ƒ test toÃ n bá»™ há»‡ thá»‘ng vá»›i dá»¯ liá»‡u máº«u.
"""

import os
import sys
from typing import List
from langchain_core.documents import Document

# Import cÃ¡c module Ä‘Ã¡nh giÃ¡
from evaluator_retrieval import RetrievalEvaluator
from evaluator_generation import GenerationEvaluator
from evaluation import RAGEvaluator, TestCase

def create_sample_documents() -> List[Document]:
    """Táº¡o tÃ i liá»‡u máº«u Ä‘á»ƒ test."""
    return [
        Document(
            page_content="""
            RAG (Retrieval-Augmented Generation) lÃ  má»™t phÆ°Æ¡ng phÃ¡p trong AI káº¿t há»£p hai ká»¹ thuáº­t:
            1. Retrieval: TÃ¬m kiáº¿m thÃ´ng tin liÃªn quan tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u
            2. Generation: Sinh ra cÃ¢u tráº£ lá»i dá»±a trÃªn thÃ´ng tin Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c
            
            RAG Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ¡c chatbot vÃ  há»‡ thá»‘ng Q&A Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c
            vÃ  cung cáº¥p thÃ´ng tin cáº­p nháº­t mÃ  khÃ´ng cáº§n train láº¡i model.
            """,
            metadata={"source": "ai_handbook", "page": 1, "topic": "RAG_basics"}
        ),
        Document(
            page_content="""
            Quy trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a RAG:
            1. NgÆ°á»i dÃ¹ng Ä‘áº·t cÃ¢u há»i
            2. Há»‡ thá»‘ng tÃ¬m kiáº¿m tÃ i liá»‡u liÃªn quan trong vector database
            3. Káº¿t há»£p cÃ¢u há»i vÃ  tÃ i liá»‡u tÃ¬m Ä‘Æ°á»£c
            4. ÄÆ°a vÃ o Language Model Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i
            5. Tráº£ vá» cÃ¢u tráº£ lá»i cho ngÆ°á»i dÃ¹ng
            
            Æ¯u Ä‘iá»ƒm: Cáº­p nháº­t thÃ´ng tin real-time, giáº£m hallucination
            NhÆ°á»£c Ä‘iá»ƒm: Phá»¥ thuá»™c vÃ o cháº¥t lÆ°á»£ng retrieval
            """,
            metadata={"source": "ai_handbook", "page": 2, "topic": "RAG_process"}
        ),
        Document(
            page_content="""
            CÃ¡c á»©ng dá»¥ng phá»• biáº¿n cá»§a RAG:
            - Customer support chatbots
            - Internal knowledge management systems
            - Research assistants
            - Code documentation systems
            - Legal document analysis
            
            RAG Ä‘áº·c biá»‡t hiá»‡u quáº£ cho cÃ¡c domain cáº§n thÃ´ng tin cáº­p nháº­t thÆ°á»ng xuyÃªn
            nhÆ° tin tá»©c, chÃ­nh sÃ¡ch, hÆ°á»›ng dáº«n ká»¹ thuáº­t.
            """,
            metadata={"source": "ai_applications", "page": 5, "topic": "RAG_applications"}
        ),
        Document(
            page_content="""
            Thá»i tiáº¿t hÃ´m nay á»Ÿ HÃ  Ná»™i khÃ¡ Ä‘áº¹p vá»›i náº¯ng vÃ ng vÃ  nhiá»‡t Ä‘á»™ khoáº£ng 25Â°C.
            Dá»± bÃ¡o tuáº§n tá»›i sáº½ cÃ³ mÆ°a ráº£i rÃ¡c vÃ o chiá»u tá»‘i.
            NgÆ°á»i dÃ¢n nÃªn chuáº©n bá»‹ Ã¡o mÆ°a khi ra Ä‘Æ°á»ng.
            """,
            metadata={"source": "weather_report", "date": "2024-01-15"}
        )
    ]

def create_test_cases() -> List[TestCase]:
    """Táº¡o cÃ¡c test case Ä‘á»ƒ Ä‘Ã¡nh giÃ¡."""
    documents = create_sample_documents()
    
    test_cases = [
        TestCase(
            question="RAG lÃ  gÃ¬?",
            answer="RAG (Retrieval-Augmented Generation) lÃ  phÆ°Æ¡ng phÃ¡p AI káº¿t há»£p tÃ¬m kiáº¿m thÃ´ng tin vÃ  sinh text Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n.",
            documents=documents,
            reference_answer="RAG lÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p retrieval vÃ  generation Ä‘á»ƒ táº¡o ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c tÃ¬m kiáº¿m.",
            category="basic_concept",
            difficulty="easy"
        ),
        TestCase(
            question="RAG hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?",
            answer="RAG hoáº¡t Ä‘á»™ng qua 5 bÆ°á»›c: nháº­n cÃ¢u há»i, tÃ¬m kiáº¿m tÃ i liá»‡u, káº¿t há»£p thÃ´ng tin, sinh cÃ¢u tráº£ lá»i qua LM, vÃ  tráº£ káº¿t quáº£ cho user.",
            documents=documents,
            reference_answer="RAG hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch tÃ¬m kiáº¿m tÃ i liá»‡u liÃªn quan, sau Ä‘Ã³ káº¿t há»£p vá»›i cÃ¢u há»i Ä‘á»ƒ Ä‘Æ°a vÃ o language model sinh ra cÃ¢u tráº£ lá»i.",
            category="process",
            difficulty="medium"
        ),
        TestCase(
            question="Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a RAG?",
            answer="Æ¯u Ä‘iá»ƒm: cáº­p nháº­t real-time, giáº£m hallucination. NhÆ°á»£c Ä‘iá»ƒm: phá»¥ thuá»™c cháº¥t lÆ°á»£ng retrieval.",
            documents=documents,
            reference_answer="RAG cÃ³ Æ°u Ä‘iá»ƒm lÃ  cung cáº¥p thÃ´ng tin cáº­p nháº­t vÃ  giáº£m thiá»ƒu hallucination, nhÆ°ng nhÆ°á»£c Ä‘iá»ƒm lÃ  phá»¥ thuá»™c vÃ o cháº¥t lÆ°á»£ng cá»§a khÃ¢u retrieval.",
            category="analysis",
            difficulty="medium"
        ),
        TestCase(
            question="RAG Ä‘Æ°á»£c á»©ng dá»¥ng trong lÄ©nh vá»±c nÃ o?",
            answer="RAG Ä‘Æ°á»£c á»©ng dá»¥ng trong chatbot há»— trá»£ khÃ¡ch hÃ ng, há»‡ thá»‘ng quáº£n lÃ½ kiáº¿n thá»©c, trá»£ lÃ½ nghiÃªn cá»©u, vÃ  phÃ¢n tÃ­ch tÃ i liá»‡u phÃ¡p lÃ½.",
            documents=documents,
            reference_answer="RAG Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i trong customer support, knowledge management, research assistance vÃ  legal document analysis.",
            category="applications",
            difficulty="easy"
        ),
        TestCase(
            question="Thá»i tiáº¿t hÃ´m nay nhÆ° tháº¿ nÃ o?",
            answer="HÃ´m nay thá»i tiáº¿t Ä‘áº¹p vá»›i náº¯ng vÃ ng vÃ  nhiá»‡t Ä‘á»™ 25Â°C. Tuáº§n tá»›i cÃ³ mÆ°a ráº£i rÃ¡c.",
            documents=documents,
            reference_answer="Thá»i tiáº¿t hÃ´m nay á»Ÿ HÃ  Ná»™i khÃ¡ Ä‘áº¹p vá»›i náº¯ng vÃ ng vÃ  nhiá»‡t Ä‘á»™ khoáº£ng 25Â°C.",
            category="irrelevant",
            difficulty="easy"
        )
    ]
    
    return test_cases

def test_retrieval_evaluator():
    """Test RetrievalEvaluator."""
    print("\n" + "="*60)
    print("ğŸ” TESTING RETRIEVAL EVALUATOR")
    print("="*60)
    
    try:
        evaluator = RetrievalEvaluator()
        documents = create_sample_documents()
        
        # Test 1: CÃ¢u há»i liÃªn quan
        print("\nğŸ“ Test 1: CÃ¢u há»i vá» RAG (liÃªn quan)")
        result = evaluator.evaluate_retrieval("RAG lÃ  gÃ¬?", documents)
        
        print(f"âœ… Äiá»ƒm relevance: {result['relevance_score']}/100")
        print(f"ğŸ“Š Sá»‘ tÃ i liá»‡u: {result['retrieved_docs_count']}")
        print(f"ğŸš« TÃ i liá»‡u khÃ´ng liÃªn quan: {result['irrelevant_docs']}")
        print(f"â“ ThÃ´ng tin thiáº¿u: {len(result['missing_topics'])} Ä‘iá»ƒm")
        
        # Test 2: CÃ¢u há»i khÃ´ng liÃªn quan  
        print("\nğŸ“ Test 2: CÃ¢u há»i vá» thá»i tiáº¿t (khÃ´ng liÃªn quan)")
        result2 = evaluator.evaluate_retrieval("Thá»i tiáº¿t hÃ´m nay nhÆ° tháº¿ nÃ o?", documents)
        
        print(f"âœ… Äiá»ƒm relevance: {result2['relevance_score']}/100")
        print(f"ğŸš« TÃ i liá»‡u khÃ´ng liÃªn quan: {result2['irrelevant_docs']}")
        
        # Test 3: ÄÃ¡nh giÃ¡ tá»«ng tÃ i liá»‡u
        print("\nğŸ“ Test 3: ÄÃ¡nh giÃ¡ tá»«ng tÃ i liá»‡u riÃªng láº»")
        doc_results = evaluator.evaluate_document_relevance("RAG lÃ  gÃ¬?", documents[:2])
        for i, doc_result in enumerate(doc_results):
            print(f"   TÃ i liá»‡u {i}: {doc_result['relevance_score']}/100 Ä‘iá»ƒm")
        
        print("\nâœ… RetrievalEvaluator test completed!")
        
    except Exception as e:
        print(f"âŒ RetrievalEvaluator test failed: {e}")

def test_generation_evaluator():
    """Test GenerationEvaluator."""
    print("\n" + "="*60)
    print("âš¡ TESTING GENERATION EVALUATOR")
    print("="*60)
    
    try:
        evaluator = GenerationEvaluator()
        documents = create_sample_documents()
        
        # Test with reference answer
        print("\nğŸ“ Test 1: ÄÃ¡nh giÃ¡ vá»›i reference answer")
        question = "RAG lÃ  gÃ¬?"
        answer = "RAG lÃ  phÆ°Æ¡ng phÃ¡p AI káº¿t há»£p tÃ¬m kiáº¿m vÃ  sinh text."
        reference = "RAG lÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p retrieval vÃ  generation."
        
        result = evaluator.evaluate_answer(question, answer, documents, reference)
        
        print(f"âœ… LLM Score: {result['llm_score']}/100")
        print(f"ğŸ“Š BLEU-1: {result.get('bleu_1', 'N/A'):.3f}")
        print(f"ğŸ“Š BLEU-2: {result.get('bleu_2', 'N/A'):.3f}")
        print(f"ğŸ“Š BLEU-3: {result.get('bleu_3', 'N/A'):.3f}")
        print(f"ğŸ“Š BLEU-4: {result.get('bleu_4', 'N/A'):.3f}")
        print(f"ğŸ“Š Rouge-L: {result.get('rouge_l', 'N/A'):.3f}")
        print(f"ğŸ“Š F1: {result.get('f1', 'N/A'):.3f}")
        print(f"â“ Missing info: {len(result['missing_information'])} Ä‘iá»ƒm")
        print(f"ğŸ”¥ Hallucinations: {len(result['hallucinations'])} Ä‘iá»ƒm")
        
        # Test without reference answer
        print("\nğŸ“ Test 2: ÄÃ¡nh giÃ¡ khÃ´ng cÃ³ reference answer")
        result2 = evaluator.evaluate_answer(question, answer, documents)
        
        print(f"âœ… LLM Score: {result2['llm_score']}/100")
        print(f"ğŸ“Š Context-based BLEU-1: {result2.get('bleu_1', 'N/A'):.3f}")
        print(f"ğŸ“Š Context-based Rouge-L: {result2.get('rouge_l', 'N/A'):.3f}")
        
        print("\nâœ… GenerationEvaluator test completed!")
        
    except Exception as e:
        print(f"âŒ GenerationEvaluator test failed: {e}")

def test_full_rag_evaluator():
    """Test RAGEvaluator (Ä‘Ã¡nh giÃ¡ tá»•ng há»£p)."""
    print("\n" + "="*60)
    print("ğŸš€ TESTING FULL RAG EVALUATOR")
    print("="*60)
    
    try:
        evaluator = RAGEvaluator(output_dir="test_results")
        documents = create_sample_documents()
        
        # Test single evaluation
        print("\nğŸ“ Test 1: ÄÃ¡nh giÃ¡ Ä‘Æ¡n láº»")
        question = "RAG lÃ  gÃ¬?"
        answer = "RAG lÃ  phÆ°Æ¡ng phÃ¡p AI káº¿t há»£p tÃ¬m kiáº¿m thÃ´ng tin vÃ  sinh text."
        reference = "RAG lÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p retrieval vÃ  generation."
        
        result = evaluator.evaluate_full_pipeline(
            question=question,
            answer=answer,
            documents=documents,
            reference_answer=reference,
            category="basic_concept"
        )
        
        print(f"âœ… Äiá»ƒm tá»•ng thá»ƒ: {result['scores']['overall_score']}/100")
        print(f"ğŸ” Äiá»ƒm retrieval: {result['scores']['retrieval_score']}/100")
        print(f"âš¡ Äiá»ƒm generation: {result['scores']['generation_llm_score']}/100")
        
        metrics = result.get('metrics', {})
        if metrics:
            print(f"ğŸ“Š BLEU-1: {metrics.get('bleu_1', 'N/A'):.3f}")
            print(f"ğŸ“Š Rouge-L: {metrics.get('rouge_l', 'N/A'):.3f}")
            print(f"ğŸ“Š F1: {metrics.get('f1', 'N/A'):.3f}")
        
        print(f"ğŸ’¡ TÃ³m táº¯t: {result['analysis']['summary']['overall_assessment']}")
        
        print("\nâœ… Single evaluation test completed!")
        
    except Exception as e:
        print(f"âŒ RAGEvaluator single test failed: {e}")

def test_batch_evaluation():
    """Test batch evaluation."""
    print("\n" + "="*60)
    print("ğŸ“Š TESTING BATCH EVALUATION")
    print("="*60)
    
    try:
        evaluator = RAGEvaluator(output_dir="test_results")
        test_cases = create_test_cases()
        
        print(f"ğŸ¯ Chuáº©n bá»‹ Ä‘Ã¡nh giÃ¡ {len(test_cases)} test cases...")
        
        # Run batch evaluation
        batch_result = evaluator.batch_evaluate(
            test_cases=test_cases,
            save_results=True,
            output_prefix="test_batch",
            create_visualizations=False  # Set False Ä‘á»ƒ trÃ¡nh lá»—i náº¿u thiáº¿u matplotlib
        )
        
        print("\nğŸ“ˆ Káº¾T QUáº¢ BATCH EVALUATION:")
        print(f"âœ… Tá»•ng sá»‘ test cases: {batch_result['metadata']['total_test_cases']}")
        print(f"âœ… ÄÃ¡nh giÃ¡ thÃ nh cÃ´ng: {batch_result['metadata']['successful_evaluations']}")
        print(f"âŒ ÄÃ¡nh giÃ¡ tháº¥t báº¡i: {batch_result['metadata']['failed_evaluations']}")
        
        avg_scores = batch_result['statistics']['average_scores']
        print(f"\nğŸ“Š ÄIá»‚M TRUNG BÃŒNH:")
        print(f"   Tá»•ng thá»ƒ: {avg_scores['overall']:.1f}/100")
        print(f"   Retrieval: {avg_scores['retrieval']:.1f}/100")
        print(f"   Generation: {avg_scores['generation']:.1f}/100")
        print(f"   BLEU-1: {avg_scores.get('bleu_1', 0):.3f}")
        print(f"   Rouge-L: {avg_scores.get('rouge_l', 0):.3f}")
        print(f"   F1: {avg_scores.get('f1', 0):.3f}")
        
        # Category analysis
        if batch_result.get('category_analysis'):
            print(f"\nğŸ·ï¸ PHÃ‚N TÃCH THEO CATEGORY:")
            for category, stats in batch_result['category_analysis'].items():
                print(f"   {category}: {stats['average_score']:.1f}/100 ({stats['count']} cases)")
        
        # Performance insights
        insights = batch_result.get('performance_insights', {})
        if insights:
            print(f"\nğŸ’¡ INSIGHTS:")
            print(f"   ÄÃ¡nh giÃ¡ tá»•ng thá»ƒ: {insights.get('overall_performance', 'N/A')}")
            if insights.get('best_category'):
                print(f"   Category tá»‘t nháº¥t: {insights['best_category']}")
            if insights.get('worst_category'):
                print(f"   Category cáº§n cáº£i thiá»‡n: {insights['worst_category']}")
            
            if insights.get('recommendations'):
                print(f"\nğŸ“‹ KHUYáº¾N NGHá»Š:")
                for rec in insights['recommendations']:
                    print(f"   â€¢ {rec}")
        
        print("\nâœ… Batch evaluation test completed!")
        print(f"ğŸ“ Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c: test_results/")
        
    except Exception as e:
        print(f"âŒ Batch evaluation test failed: {e}")

def test_metrics_calculation():
    """Test tÃ­nh toÃ¡n cÃ¡c metrics riÃªng láº»."""
    print("\n" + "="*60)
    print("ğŸ”¢ TESTING METRICS CALCULATION")
    print("="*60)
    
    try:
        evaluator = GenerationEvaluator()
        
        # Test BLEU calculation
        print("\nğŸ“ Test BLEU calculation:")
        gen_text = "RAG lÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p tÃ¬m kiáº¿m vÃ  sinh text"
        ref_text = "RAG lÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p retrieval vÃ  generation"
        
        gen_tokens = evaluator._tokenize(gen_text)
        ref_tokens = evaluator._tokenize(ref_text)
        
        print(f"Generated tokens: {gen_tokens}")
        print(f"Reference tokens: {ref_tokens}")
        
        for n in range(1, 5):
            bleu = evaluator._calculate_bleu(gen_tokens, ref_tokens, n)
            print(f"BLEU-{n}: {bleu:.3f}")
        
        # Test Rouge-L calculation
        rouge_l = evaluator._calculate_rouge_l(gen_tokens, ref_tokens)
        print(f"Rouge-L: {rouge_l:.3f}")
        
        # Test F1 calculation
        f1 = evaluator._calculate_f1(gen_tokens, ref_tokens)
        print(f"F1: {f1:.3f}")
        
        print("\nâœ… Metrics calculation test completed!")
        
    except Exception as e:
        print(f"âŒ Metrics calculation test failed: {e}")

def main():
    """Cháº¡y táº¥t cáº£ cÃ¡c test."""
    print("ğŸ¯ Báº®T Äáº¦U KIá»‚M TRA Há»† THá»NG ÄÃNH GIÃ RAG")
    print("="*80)
    
    # Táº¡o thÆ° má»¥c káº¿t quáº£ náº¿u chÆ°a cÃ³
    os.makedirs("test_results", exist_ok=True)
    
    # Cháº¡y tá»«ng test
    test_retrieval_evaluator()
    test_generation_evaluator()
    test_metrics_calculation()
    test_full_rag_evaluator()
    test_batch_evaluation()
    
    print("\n" + "="*80)
    print("ğŸ‰ HOÃ€N THÃ€NH Táº¤T Cáº¢ KIá»‚M TRA!")
    print("="*80)
    print("ğŸ“ Káº¿t quáº£ test Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c: test_results/")
    print("ğŸ’¡ Báº¡n cÃ³ thá»ƒ xem cÃ¡c file JSON, CSV Ä‘á»ƒ phÃ¢n tÃ­ch chi tiáº¿t.")

if __name__ == "__main__":
    # Kiá»ƒm tra dependencies
    required_modules = ['langchain_core', 'langchain_openai', 'openai', 'dotenv']
    missing_modules = []
    
    for module in required_modules:
        try:
            __import__(module)
        except ImportError:
            missing_modules.append(module)
    
    if missing_modules:
        print("âŒ Thiáº¿u cÃ¡c module sau:")
        for module in missing_modules:
            print(f"   - {module}")
        print("\nğŸ’¡ CÃ i Ä‘áº·t báº±ng lá»‡nh: pip install langchain langchain-openai python-dotenv")
        sys.exit(1)
    
    # Kiá»ƒm tra API key
    if not os.getenv('OPENAI_API_KEY'):
        print("âš ï¸ Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y OPENAI_API_KEY trong biáº¿n mÃ´i trÆ°á»ng.")
        print("ğŸ’¡ Táº¡o file .env vÃ  thÃªm: OPENAI_API_KEY=your_api_key_here")
        
        # CÃ³ thá»ƒ váº«n cháº¡y test nhÆ°ng sáº½ fail á»Ÿ pháº§n gá»i API
        choice = input("Báº¡n cÃ³ muá»‘n tiáº¿p tá»¥c test (cÃ³ thá»ƒ bá»‹ lá»—i API)? (y/n): ")
        if choice.lower() != 'y':
            sys.exit(1)
    
    main()
