1. MOTIVATION R√ï R√ÄNG => M·ªöI TRI·ªÇN?  
2. DATASET? C√ÅCH H·ªå ƒê√ÅNH GI√Å NH∆Ø N√ÄO?
3. METHOD? K·ª∏ THU·∫¨T ƒê√ì L√Ä G√å? ? K·ª∏ THU·∫¨T ƒê√ì NH∆Ø N√ÄO? => ƒê·ªäNH NGHƒ®A ƒê∆Ø·ª¢C C√ÅC KH√ÅI NI·ªÜM => M·ªöI C√ì K·ª∏ THU·∫¨T.  

---
Prompt 1: T√¥i mu·ªën b·∫°n ph·∫£i hi·ªÉu th·∫≠t s√¢u. m·ªói ph·∫ßn ƒë·∫∑t c√°c c√¢u h·ªèi ph·∫£n bi·ªán s·∫Øc x·∫£o ƒë·ªÉ t·ª± tr·∫£ l·ªùi. Gi·ªëng nh∆∞ 1 ng∆∞·ªùi ph√°, 1 ng∆∞·ªùi tr·∫£ l·ªùi, l·∫•y d·∫´n ch·ª©ng
Prompt 2: Gi·∫£i t√≠ch t·ª´ng kh√°i ni·ªám v√† li√™n h·ªá n√≥ v·ªõi th·ª© th·∫≠t d·ªÖ nh·ªõ ƒë∆°n gi·∫£n cho h·ªçc sinh c·∫•p 2

üß† PH·∫¶N 1: V·∫•n ƒë·ªÅ c·ªßa RAG hi·ªán t·∫°i & S·ª± xu·∫•t hi·ªán c·ªßa HippoRAG
üîç Gi·∫£i th√≠ch l·∫°i:
RAG hi·ªán t·∫°i ho·∫°t ƒë·ªông theo c∆° ch·∫ø:
- T√°ch vƒÉn b·∫£n th√†nh c√°c chunk nh·ªè
- D√πng embedding ‚Üí l∆∞u v√†o VectorDB
- Khi c√≥ query ‚Üí t√¨m vector t∆∞∆°ng ƒë·ªìng ‚Üí l·∫•y context ‚Üí ƒë∆∞a v√†o LLM
Nh∆∞ng v·ªõi c√°c t√°c v·ª• multi-hop reasoning, nhi·ªÅu th√¥ng tin ·ªü r·∫£i r√°c c√°c ƒëo·∫°n kh√°c nhau th√¨ RAG hi·ªán t·∫°i kh√¥ng ƒë·ªß hi·ªáu qu·∫£.
Ph√° 1.1 T·∫°i sao n√≥i v·ªõi c√°c t√°c v·ª• multi-hop reasoning, nhi·ªÅu th√¥ng tin ·ªü r·∫£i r√°c c√°c ƒëo·∫°n kh√°c nhau th√¨ RAG hi·ªán t·∫°i kh√¥ng ƒë·ªß hi·ªáu qu·∫£? 
Gi·∫£ s·ª≠ RAG truy·ªÅn th·ªëng retrieve ƒë∆∞·ª£c c·∫£ hai ƒëo·∫°n r·ªùi r·∫°c, th√¨ n√≥ v·∫´n n√©m h·∫øt v√†o context window c·ªßa LLM m√†? L√∫c ƒë√≥ LLM n·ªëi l·∫°i logic l√† xong ch·ª©?

Ph√° 1: RAG truy·ªÅn th·ªëng, trong l√∫c query ra c√°c ƒëo·∫°n chunking th√¨ n√≥ ch·∫≥ng nh·∫ª kh√¥ng c√≥ t√≠nh m√≥c n·ªëi?
‚ÄúGi·∫£ s·ª≠ RAG truy·ªÅn th·ªëng retrieve ƒë∆∞·ª£c c·∫£ hai ƒëo·∫°n r·ªùi r·∫°c, th√¨ n√≥ v·∫´n n√©m h·∫øt v√†o context window c·ªßa LLM m√†? L√∫c ƒë√≥ LLM n·ªëi l·∫°i logic l√† xong ch·ª©, c·∫ßn g√¨ ph·∫£i HippoRAG r∆∞·ªùm r√†?‚Äù 
üéØ 1. Kh√¥ng c√≥ g√¨ ƒë·∫£m b·∫£o RAG s·∫Ω retrieve ƒë√∫ng 2 ƒëo·∫°n
V·∫•n ƒë·ªÅ l·ªõn nh·∫•t c·ªßa RAG:
- Retrieval = vector similarity ‚Üí ch·ªâ ch·ªçn Top-K
- N·∫øu th√¥ng tin n·∫±m ·ªü 2 ƒëo·∫°n kh√°c nhau, v√† m·ªói c√°i ƒë·ªÅu ‚Äúh∆°i li√™n quan‚Äù, n√≥ c√≥ th·ªÉ b·ªã ƒë·∫©y ra kh·ªèi Top-K
V√≠ d·ª• c·ª• th·ªÉ:
Top-K = 3
 B·∫°n c·∫ßn ƒëo·∫°n A v√† ƒëo·∫°n B
 Nh∆∞ng c√≥ ƒëo·∫°n C, D r·∫•t gi·ªëng c√¢u h·ªèi v·ªÅ m·∫∑t ng·ªØ nghƒ©a ‚Üí l√™n Top
 A ho·∫∑c B b·ªã lo·∫°i v√¨ vector kh√¥ng ƒë·ªß g·∫ßn
üí• ‚Üí L√∫c n√†y LLM kh√¥ng c√≥ nguy√™n li·ªáu ƒë·ªÉ suy lu·∫≠n, d√π n√≥ gi·ªèi c·ª° n√†o. 

TƒÉng k. 
D·ªØ li·ªáu n·∫±m r·∫£i r√°c t·∫°i nhi·ªÅu n∆°i. 

üß† 2. LLM suy lu·∫≠n = t·ªën compute + d·ªÖ l·ªói logic
C√¢u h·ªèi:
"Which Stanford researcher who is not Alice but studies Alzheimer's worked at the same place as Thomas?"
Dataset:
902. Alice is a Stanford professor.
903. Thomas researches Alzheimer‚Äôs at UCSF.
904. Mike works at UCSF.
905. Sarah studies Alzheimer‚Äôs.
906. Sarah and Thomas are labmates at UCSF.

‚ö†Ô∏è V·∫•n ƒë·ªÅ c·ªßa RAG khi retrieve Top-K
Gi·∫£ s·ª≠ retrieve ƒë∆∞·ª£c 3 ƒëo·∫°n:
- (2) Thomas ‚Üí Alzheimer + UCSF
- (4) Sarah ‚Üí Alzheimer
- (5) Sarah ‚Üí labmate of Thomas ‚Üí UCSF
LLM ph·∫£i l√†m:
1267. G·∫Øn Sarah v·ªõi UCSF (t·ª´ labmate)
1268. Tr·ª´ Alice kh·ªèi k·∫øt qu·∫£
1269. Ch·ªçn ng∆∞·ªùi v·ª´a Stanford researcher, v·ª´a Alzheimer, v·ª´a ·ªü UCSF, nh∆∞ng kh√¥ng ph·∫£i Alice

---
‚ú≥Ô∏è ƒê√¢y l√† 3-hop reasoning. Kh√¥ng ph·∫£i v√¨ LLM kh√¥ng th·ªÉ, m√† v√¨:
- N√≥ ph·∫£i gi·ªØ ·ªïn ƒë·ªãnh attention gi·ªØa 4-5 ƒëo·∫°n vƒÉn
- N√≥ kh√¥ng bi·∫øt ch·∫Øc m·ªëi li√™n k·∫øt l√† quan tr·ªçng hay ch·ªâ l√† ‚Äúnhi·ªÖu‚Äù
- Kh√¥ng c√≥ logic structure r√µ r√†ng ƒë·ªÉ l·∫ßn t·ª´ A ‚Üí B ‚Üí C
‚û°Ô∏è Khi n√†y, retrieval d·∫°ng vector (d√π chunk t·ªët) kh√¥ng cung c·∫•p structure reasoning.
 Ch·ªâ n√©m ƒëo·∫°n v√†o context ‚Üí LLM ph·∫£i lo c·∫£ ƒë·ªçc, t√¨m logic, t·ªïng h·ª£p, tr√°nh sai l·ªách.

C√¢u h·ªèi ti·∫øp ƒë·∫∑t ra l√†: ‚ùó‚ÄúB√¢y gi·ªù c√≥ model 4o, Claude Opus, GPT-4-Turbo‚Ä¶ n√≥ c√≥ th·ªÉ context window 128k tokens, b·∫°n n√©m s·∫°ch v√¥ l√† n√≥ l√†m reasoning c·ª±c t·ªët. Th·∫ø th√¨ l√†m retrieval ƒë·ªÉ l√†m g√¨ n·ªØa? HippoRAG c√≥ c√≤n gi√° tr·ªã kh√¥ng?‚Äù
Tr·∫£ l·ªùi: ƒê√∫ng! Nh∆∞ng ch·ªâ khi b·∫°n s·ªëng trong th·∫ø gi·ªõi l√Ω t∆∞·ªüng.

‚ú≥Ô∏è PH·∫¶N I: Th·ª´a nh·∫≠n s·ª± th·∫≠t
‚úÖ ƒê√∫ng, model 4o, Claude Opus, GPT-4 Turbo hi·ªán nay ƒë√£ c√≥ kh·∫£ nƒÉng context-wide reasoning r·∫•t m·∫°nh.
üìå B·∫°n c√≥ th·ªÉ ƒë∆∞a v√†o:
- M·ªôt dataset 80k tokens g·ªìm nhi·ªÅu t√†i li·ªáu
- M·ªôt c√¢u h·ªèi c·∫ßn n·ªëi 3‚Äì4 th√¥ng tin l·∫°i
- V√† model s·∫Ω tr·∫£ l·ªùi ƒë√∫ng ‚Äì th·∫≠m ch√≠ c√≤n gi·∫£i th√≠ch t·∫°i sao.

---
‚ú≥Ô∏è PH·∫¶N II: Nh∆∞ng ƒë√≥ kh√¥ng ph·∫£i l√† c√°ch l√†m AI hi·ªáu qu·∫£ ho·∫∑c quy m√¥
‚ùó L√Ω do 1: Context-wide LLM reasoning = t·ªën compute kh·ªßng khi·∫øp
N√©m v√†o 128k tokens = gi√° infer cao, latency cao
- Claude Opus ho·∫∑c GPT-4 Turbo 128k input ‚Üí th·ªùi gian tr·∫£ l·ªùi l√¢u h∆°n, chi ph√≠ t√≠nh theo token c·ª±c l·ªõn (v√†i $ m·ªói l∆∞·ª£t n·∫øu full)
- B·∫°n kh√¥ng th·ªÉ scale 1 tri·ªáu c√¢u h·ªèi ki·ªÉu ƒë√≥

---
‚ùó L√Ω do 2: LLM = black box reasoning, kh√¥ng th·ªÉ debug, kh√¥ng th·ªÉ ki·ªÉm so√°t
- LLM kh√¥ng cho b·∫°n bi·∫øt reasoning path
- B·∫°n kh√¥ng bi·∫øt model n·ªëi A ‚Üí B ‚Üí C nh∆∞ th·∫ø n√†o
- N·∫øu n√≥ tr·∫£ sai? B·∫°n kh√¥ng bi·∫øt s·ª≠a t·ª´ ƒë√¢u
‚û°Ô∏è Retrieval reasoning = transparent, modular, debuggable
‚Üí HippoRAG = ta l·∫ßn ƒë∆∞·ª£c ƒë∆∞·ªùng ƒëi qua graph
 ‚Üí C√≥ th·ªÉ ghi log reasoning path
 ‚Üí C√≥ th·ªÉ gi·∫£i th√≠ch cho ng∆∞·ªùi d√πng ho·∫∑c ki·ªÉm tra b·∫±ng rule engine

---
‚ùó L√Ω do 3: Kh√¥ng ph·∫£i l√∫c n√†o c≈©ng c√≥ quy·ªÅn ‚Äún√©m h·∫øt v√†o‚Äù
- N·∫øu user h·ªèi li√™n t·ª•c, m·ªói c√¢u ƒë·ªÅu c·∫ßn to√†n b·ªô t√†i li·ªáu ‚Üí c·ª±c k·ª≥ l√£ng ph√≠
- V√≠ d·ª•: app legal, ho·∫∑c chatbot h·ªçc thu·∫≠t, ho·∫∑c QA y khoa
B·∫°n c·∫ßn selective retrieval ‚Üí RAG gi·∫£i quy·∫øt
 B·∫°n c·∫ßn reasoning t·ª´ c√°c concept, entity ‚Üí kh√¥ng c√≥ s·∫µn text chunk ‚Üí HippoRAG gi·∫£i quy·∫øt

üß† 3. HippoRAG cho ph√©p retrieval ch√≠nh l√† reasoning
ƒê√¢y l√† ƒëi·ªÉm ‚Äúƒë·∫≠p n√°t ph·∫£n bi·ªán‚Äù:
üß† HippoRAG kh√¥ng ch·ªù ƒë·∫øn khi LLM l√†m reasoning, m√† n√≥ ƒë√£ reasoning ngay t·ª´ b∆∞·ªõc truy h·ªìi.
- V·ªõi Personalized PageRank ‚Üí n√≥ t√¨m ra node trung gian (John Smith) tr∆∞·ªõc khi v√†o LLM
- Tr·∫£ v·ªÅ ƒëo·∫°n ƒë√£ ƒë·ªß logic ‚Üí LLM ch·ªâ c·∫ßn n√≥i l·∫°i
üí° ƒêi·ªÅu n√†y bi·∫øn retrieval th√†nh m·ªôt ‚Äúm√¥-ƒëun l·∫≠p lu·∫≠n‚Äù (reasoning module), kh√¥ng ch·ªâ l√† ‚Äúscanner ƒëo·∫°n vƒÉn‚Äù.

Ph√° 1.2 ƒê·∫∑t v·∫•n ƒë·ªÅ? 
Tuy·ªát v·ªùi, Qu·ªëc ƒëang d√πng k·ªπ thu·∫≠t ph·∫£n bi·ªán c·ª±c k·ª≥ hay ƒë·ªÉ ‚Äúƒë·ªçc s√¢u‚Äù ‚Äî ki·ªÉu Socratic method trong nghi√™n c·ª©u. M√¨nh s·∫Ω ƒë√≥ng c·∫£ ng∆∞·ªùi h·ªèi s·∫Øc s·∫£o v√† ng∆∞·ªùi tr·∫£ l·ªùi ch·∫∑t ch·∫Ω, b√°m s√°t ph·∫ßn ‚Äú1. Introduction‚Äù c·ªßa b√†i b√°o HippoRAG 2.

---
üß© PH√ÇN T√çCH PH·∫¶N ƒê·∫∂T V·∫§N ƒê·ªÄ (INTRODUCTION - HippoRAG 2)

---
‚öîÔ∏è Ph·∫£n bi·ªán 1:
H·ªèi: T·∫°i sao c·ª© ph·∫£i b·∫Øt AI h·ªçc nh∆∞ con ng∆∞·ªùi? Vi·ªác m√¥ ph·ªèng ‚Äúlong-term memory‚Äù c·ªßa ng∆∞·ªùi c√≥ th·∫≠t s·ª± c·∫ßn thi·∫øt cho LLM kh√¥ng?
Tr·∫£ l·ªùi: V√¨ m·ª•c ti√™u cu·ªëi c√πng c·ªßa LLM l√† tr·ªü th√†nh tr·ª£ l√Ω th√¥ng minh v√† hi·ªáu qu·∫£ nh∆∞ con ng∆∞·ªùi. Trong th·∫ø gi·ªõi th·ª±c, con ng∆∞·ªùi li√™n t·ª•c h·ªçc, t√≠ch h·ª£p th√¥ng tin m·ªõi m√† kh√¥ng qu√™n c√°i c≈© ‚Äì ƒë√¢y l√† ƒëi·ªÅu LLM hi·ªán t·∫°i r·∫•t y·∫øu.
B·∫±ng ch·ª©ng:
- LLM g·∫∑p v·∫•n ƒë·ªÅ catastrophic forgetting n·∫øu fine-tune.
- RAG tuy ti·∫øp c·∫≠n t·ªët h∆°n nh∆∞ng v·∫´n kh√¥ng c√≥ kh·∫£ nƒÉng k·∫øt n·ªëi m·∫°ch l·∫°c, nh∆∞ ng∆∞·ªùi nh·ªõ chuy·ªán ƒë√£ x·∫£y ra t·ª´ l√¢u v√† li√™n k·∫øt n√≥ v·ªõi b·ªëi c·∫£nh hi·ªán t·∫°i.
V√¨ th·∫ø, mu·ªën LLM tr·ªü th√†nh tr·ª£ l√Ω l√¢u d√†i, c·∫≠p nh·∫≠t li√™n t·ª•c, n√≥ c·∫ßn m·ªôt h·ªá th·ªëng b·ªô nh·ªõ d√†i h·∫°n linh ho·∫°t v√† li√™n k·∫øt ‚Äì gi·ªëng nh∆∞ c·ªßa ng∆∞·ªùi.

---
‚öîÔ∏è Ph·∫£n bi·ªán 2:
H·ªèi: RAG ƒëang ch·∫°y t·ªët m√†. T·∫°i sao l·∫°i n√≥i RAG truy·ªÅn th·ªëng kh√¥ng ƒë·ªß? C√≥ b·∫±ng ch·ª©ng g√¨?
Tr·∫£ l·ªùi: RAG truy·ªÅn th·ªëng ch·ªß y·∫øu d·ª±a v√†o vector retrieval ƒë·ªôc l·∫≠p, m·ªói chunk ƒë∆∞·ª£c x·∫øp h·∫°ng d·ª±a v√†o ƒë·ªô t∆∞∆°ng t·ª± embedding v·ªõi query ‚Äì kh√¥ng c√≥ c∆° ch·∫ø ƒë·ªÉ hi·ªÉu m·ªëi quan h·ªá gi·ªØa c√°c chunk, kh√¥ng hi·ªÉu m·∫°ch truy·ªán, kh√¥ng l√†m multi-hop reasoning.
B·∫±ng ch·ª©ng c·ª• th·ªÉ trong b√†i:
‚Äú...inability to capture two vital aspects: sense-making (hi·ªÉu ng·ªØ c·∫£nh ph·ª©c t·∫°p) v√† associativity (k·∫øt n·ªëi ƒëa b∆∞·ªõc gi·ªØa th√¥ng tin r·ªùi r·∫°c)‚Äù ‚Äì (trang 1)
V√¨ v·∫≠y, tuy RAG t·ªët trong vi·ªác ‚Äút√¨m th√¥ng tin g·∫ßn nh·∫•t‚Äù, nh∆∞ng:
- Kh√¥ng li√™n k·∫øt c√°c ƒëo·∫°n ki·∫øn th·ª©c l·∫°i v·ªõi nhau.
- Kh√¥ng bi·∫øt ‚Äúƒëi v√≤ng‚Äù ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi y√™u c·∫ßu reasoning.
=> ƒê√≥ l√† l√Ω do c·∫ßn m·ªôt m√¥ h√¨nh v∆∞·ª£t RAG truy·ªÅn th·ªëng.

---
‚öîÔ∏è Ph·∫£n bi·ªán 3:
H·ªèi: Th·∫ø c√≤n c√°c m√¥ h√¨nh m·ªõi nh∆∞ GraphRAG, RAPTOR? H·ªç ƒë√£ c·ªë kh·∫Øc ph·ª•c r·ªìi m√†?
Tr·∫£ l·ªùi: ƒê√∫ng, nh∆∞ng theo t√°c gi·∫£:
- Nh·ªØng m√¥ h√¨nh ƒë√≥ t·∫≠p trung qu√° h·∫πp v√†o m·ªôt kh√≠a c·∫°nh (sense-making ho·∫∑c associativity) n√™n m·∫•t c√¢n b·∫±ng.
- Khi l√†m t·ªët ph·∫ßn n√†y th√¨ l·∫°i suy gi·∫£m hi·ªáu nƒÉng ·ªü ph·∫ßn kh√°c. V√≠ d·ª•:
  - GraphRAG t·ªët cho discourse ‚Üí k√©m factual QA.
  - RAPTOR t√≥m t·∫Øt t·ªët ‚Üí nh∆∞ng g√¢y nhi·ªÖu khi QA nhi·ªÅu b∆∞·ªõc.
  - HippoRAG 1 t·ªët v·ªõi multi-hop ‚Üí nh∆∞ng thi·∫øu contextualization.
B√†i b√°o vi·∫øt:
‚Äú...each method experiences the largest performance decay in tasks outside its own setup.‚Äù ‚Üí t·ª©c l√† m√¥ h√¨nh t·ªët ch·ªó n√†y th√¨ g√£y ch·ªó kia.

---
‚öîÔ∏è Ph·∫£n bi·ªán 4:
H·ªèi: V·∫≠y m·ª•c ti√™u ch√≠nh c·ªßa HippoRAG l√† g√¨? Kh√°c bi·ªát c·ªët l√µi so v·ªõi RAG v√† c√°c m√¥ h√¨nh m·ªü r·ªông?
Tr·∫£ l·ªùi:
 HippoRAG nh·∫Øm t·ªõi m·ªôt b·ªô nh·ªõ d√†i h·∫°n t·ªïng qu√°t, c√≥ th·ªÉ ho·∫°t ƒë·ªông t·ªët ·ªü:
- Factual QA (ghi nh·ªõ s·ª± ki·ªán c∆° b·∫£n),
- Multi-hop QA (li√™n k·∫øt ki·∫øn th·ª©c),
- Discourse understanding (di·ªÖn gi·∫£i m·∫°ch truy·ªán d√†i).
C·ªët l√µi:
- M√¥ ph·ªèng b·ªô nh·ªõ sinh h·ªçc con ng∆∞·ªùi:
  - Neocortex ‚Üí LLM
  - Parahippocampal ‚Üí encoder g·ª£i nh·ªõ ng·ªØ nghƒ©a
  - Hippocampus ‚Üí KG m·ªü k·∫øt n·ªëi tri th·ª©c
- D√πng Personalized PageRank (PPR) ƒë·ªÉ t√¨m ki·∫øm c√≥ li√™n k·∫øt ‚Äì kh√¥ng ch·ªâ ƒëi·ªÉm, m√† l√† ƒë∆∞·ªùng ƒëi ki·∫øn th·ª©c.
- Th√™m LLM v√†o retrieval ƒë·ªÉ l·ªçc nhi·ªÖu ‚Üí kh√¥ng ch·ªâ t√¨m, m√† c√≤n nh·∫≠n di·ªán ƒë√∫ng ƒëo·∫°n c·∫ßn nh·ªõ.

---
‚öîÔ∏è Ph·∫£n bi·ªán 5:
H·ªèi: V·∫≠y HippoRAG c√≥ b·∫±ng ch·ª©ng n√†o m·∫°nh ƒë·ªÉ ch·ª©ng minh l√† c·∫ßn thi·∫øt kh√¥ng?
Tr·∫£ l·ªùi:
- HippoRAG 2 ƒë·∫°t ƒëi·ªÉm cao nh·∫•t ·ªü c·∫£ 3 lo·∫°i nhi·ªám v·ª• (Factual, Associative, Sense-making) ‚Äì ƒëi·ªÅu m√† kh√¥ng m√¥ h√¨nh n√†o tr∆∞·ªõc ƒë√≥ l√†m ƒë∆∞·ª£c.
- V√≠ d·ª•:
  - +7 ƒëi·ªÉm F1 ·ªü Associativity tasks so v·ªõi embedding t·ªët nh·∫•t (NV-Embed-v2).
  - Kh√¥ng b·ªã suy gi·∫£m ·ªü Factual QA nh∆∞ GraphRAG, RAPTOR.
=> ƒê√¢y l√† b·∫±ng ch·ª©ng cho th·∫•y HippoRAG c√≥ th·ªÉ c√¢n b·∫±ng gi·ªØa s·ª± ch√≠nh x√°c, kh·∫£ nƒÉng di·ªÖn gi·∫£i v√† k·∫øt n·ªëi tri th·ª©c, ƒëi·ªÅu RAG truy·ªÅn th·ªëng kh√¥ng l√†m ƒë∆∞·ª£c.

---
Ch√†o Qu·ªëc,
M√¨nh s·∫Ω gi·∫£i th√≠ch chi ti·∫øt v·ªÅ ba lo·∫°i nhi·ªám v·ª•: Factual, Associative, Sense-making, v√† m·ªëi li√™n h·ªá gi·ªØa c√°c nhi·ªám v·ª• n√†y v·ªõi b·ªô d·ªØ li·ªáu LongMemEval.

---
1. Ba lo·∫°i nhi·ªám v·ª•: Factual, Associative, Sense-making
a. Factual Tasks (Nhi·ªám v·ª• th·ª±c t·∫ø)
- ƒê·ªãnh nghƒ©a: Y√™u c·∫ßu h·ªá th·ªëng truy xu·∫•t v√† cung c·∫•p th√¥ng tin ch√≠nh x√°c v·ªÅ m·ªôt s·ª± ki·ªán ho·∫∑c th·ª±c t·∫ø c·ª• th·ªÉ.ÓàÜ
- V√≠ d·ª•: "Ai l√† t·ªïng th·ªëng ƒë·∫ßu ti√™n c·ªßa Hoa K·ª≥?"ÓàÜ
- M·ª•c ti√™u: Ki·ªÉm tra kh·∫£ nƒÉng ghi nh·ªõ v√† truy xu·∫•t th√¥ng tin ch√≠nh x√°c t·ª´ b·ªô nh·ªõ d√†i h·∫°n.ÓàÜ
b. Associative Tasks (Nhi·ªám v·ª• li√™n k·∫øt)
- ƒê·ªãnh nghƒ©a: Y√™u c·∫ßu h·ªá th·ªëng k·∫øt n·ªëi nhi·ªÅu m·∫£nh th√¥ng tin r·ªùi r·∫°c ƒë·ªÉ suy lu·∫≠n ho·∫∑c tr·∫£ l·ªùi c√¢u h·ªèi.ÓàÜ
- V√≠ d·ª•: "Ng∆∞·ªùi ph√°t minh ra ƒëi·ªán tho·∫°i ƒë√£ s·ªëng ·ªü ƒë√¢u?"ÓàÜ
  - Ph√¢n t√≠ch: C·∫ßn bi·∫øt Alexander Graham Bell l√† ng∆∞·ªùi ph√°t minh ra ƒëi·ªán tho·∫°i v√† √¥ng s·ªëng ·ªü Scotland v√† Canada.ÓàÜ
- M·ª•c ti√™u: ƒê√°nh gi√° kh·∫£ nƒÉng li√™n k·∫øt v√† suy lu·∫≠n t·ª´ nhi·ªÅu ngu·ªìn th√¥ng tin kh√°c nhau.ÓàÜ
c. Sense-making Tasks (Nhi·ªám v·ª• hi·ªÉu ng·ªØ c·∫£nh)
- ƒê·ªãnh nghƒ©a: Y√™u c·∫ßu h·ªá th·ªëng hi·ªÉu v√† di·ªÖn gi·∫£i c√°c t√¨nh hu·ªëng ph·ª©c t·∫°p ho·∫∑c c√¢u chuy·ªán d√†i.ÓàÜ
- V√≠ d·ª•: "T√≥m t·∫Øt n·ªôi dung ch√≠nh c·ªßa cu·ªôc h·ªçp h√¥m qua."ÓàÜ
- M·ª•c ti√™u: Ki·ªÉm tra kh·∫£ nƒÉng t·ªïng h·ª£p, hi·ªÉu ng·ªØ c·∫£nh v√† tr√¨nh b√†y l·∫°i th√¥ng tin m·ªôt c√°ch m·∫°ch l·∫°c.ÓàÜ

---
2. M·ªëi li√™n h·ªá gi·ªØa QA v√† LongMemEval
LongMemEval l√† m·ªôt b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng ghi nh·ªõ d√†i h·∫°n c·ªßa c√°c tr·ª£ l√Ω tr√≤ chuy·ªán. B·ªô d·ªØ li·ªáu n√†y t·∫≠p trung v√†o nƒÉm kh·∫£ nƒÉng c·ªët l√µi:
1. Tr√≠ch xu·∫•t th√¥ng tin (Information Extraction): Kh·∫£ nƒÉng nh·ªõ l·∫°i th√¥ng tin c·ª• th·ªÉ t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c d√†i.ÓàÜ
2. L√Ω lu·∫≠n ƒëa phi√™n (Multi-Session Reasoning): Kh·∫£ nƒÉng t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu phi√™n tr√≤ chuy·ªán.ÓàÜ
3. C·∫≠p nh·∫≠t ki·∫øn th·ª©c (Knowledge Updates): Kh·∫£ nƒÉng nh·∫≠n bi·∫øt v√† c·∫≠p nh·∫≠t th√¥ng tin c√° nh√¢n c·ªßa ng∆∞·ªùi d√πng theo th·ªùi gian.ÓàÜ
4. L√Ω lu·∫≠n th·ªùi gian (Temporal Reasoning): Nh·∫≠n th·ª©c v·ªÅ c√°c kh√≠a c·∫°nh th·ªùi gian c·ªßa th√¥ng tin ng∆∞·ªùi d√πng.ÓàÜ
5. T·ª´ ch·ªëi tr·∫£ l·ªùi (Abstention): Kh·∫£ nƒÉng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn th√¥ng tin kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong l·ªãch s·ª≠ t∆∞∆°ng t√°c.ÓàÜ
M·ªëi li√™n h·ªá:
- Factual Tasks t∆∞∆°ng ·ª©ng v·ªõi Tr√≠ch xu·∫•t th√¥ng tin, n∆°i h·ªá th·ªëng c·∫ßn truy xu·∫•t th√¥ng tin ch√≠nh x√°c t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c.ÓàÜ
- Associative Tasks li√™n quan ƒë·∫øn L√Ω lu·∫≠n ƒëa phi√™n, y√™u c·∫ßu h·ªá th·ªëng k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu phi√™n ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c.ÓàÜ
- Sense-making Tasks g·∫Øn li·ªÅn v·ªõi L√Ω lu·∫≠n th·ªùi gian v√† C·∫≠p nh·∫≠t ki·∫øn th·ª©c, ƒë√≤i h·ªèi h·ªá th·ªëng hi·ªÉu ng·ªØ c·∫£nh t·ªïng th·ªÉ v√† c·∫≠p nh·∫≠t th√¥ng tin m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng.ÓàÜ

---

D∆∞·ªõi ƒë√¢y l√† c√°ch ph√¢n bi·ªát r√µ r√†ng gi·ªØa Factual, Associative, Sense-making, b·∫±ng c√°ch ƒëi t·ª´ ƒë·ªãnh nghƒ©a ‚ûù v√≠ d·ª• c·ª• th·ªÉ ‚ûù nƒÉng l·ª±c c·∫ßn ki·ªÉm tra:

---
üß† 1. Factual Memory ‚Äì Ghi nh·ªõ s·ª± th·∫≠t c·ª• th·ªÉ
üßæ ƒê·ªãnh nghƒ©a:
Kh·∫£ nƒÉng truy xu·∫•t th√¥ng tin ƒë∆°n l·∫ª, ƒë√£ ƒë∆∞·ª£c n√™u r√µ r√†ng trong ng·ªØ li·ªáu ƒë·∫ßu v√†o.
‚ùìV√≠ d·ª• c√¢u h·ªèi:
"Ai l√† th·ªß t∆∞·ªõng ƒê·ª©c nƒÉm 2023?"
- ƒê√¢y l√† c√¢u h·ªèi ƒë√≤i h·ªèi 1 ƒëo·∫°n vƒÉn b·∫£n ch·ª©a th√¥ng tin ch√≠nh x√°c, kh√¥ng c·∫ßn suy lu·∫≠n hay li√™n k·∫øt.
üéØ NƒÉng l·ª±c ƒë∆∞·ª£c ki·ªÉm tra:
- Kh·∫£ nƒÉng l∆∞u tr·ªØ & truy xu·∫•t ch√≠nh x√°c th√¥ng tin ƒë√£ ƒë·ªçc.
- T∆∞∆°ng ƒë∆∞∆°ng v·ªõi nh·ªõ t√™n, ng√†y th√°ng, con s·ªë, ƒë·ªãnh nghƒ©a.

---
üîó 2. Associativity ‚Äì K·∫øt n·ªëi th√¥ng tin r·ªùi r·∫°c
üßæ ƒê·ªãnh nghƒ©a:
Kh·∫£ nƒÉng k·∫øt n·ªëi nhi·ªÅu m·∫£nh ki·∫øn th·ª©c t·ª´ c√°c ph·∫ßn kh√°c nhau trong d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
‚ùìV√≠ d·ª• c√¢u h·ªèi:
"Ng∆∞·ªùi s√°ng l·∫≠p Microsoft c√≥ qu√™ g·ªëc ·ªü ƒë√¢u?"
- C·∫ßn n·ªëi hai m·∫£nh th√¥ng tin:
  1. "Bill Gates l√† ng∆∞·ªùi s√°ng l·∫≠p Microsoft"
  2. "Bill Gates sinh ra ·ªü Seattle"
‚ü∂ C·∫ßn √≠t nh·∫•t 2 ƒëo·∫°n d·ªØ li·ªáu, kh√¥ng c√≥ ƒëo·∫°n n√†o ch·ª©a ƒë·∫ßy ƒë·ªß c√¢u tr·∫£ l·ªùi.
üéØ NƒÉng l·ª±c ƒë∆∞·ª£c ki·ªÉm tra:
- Reasoning nhi·ªÅu b∆∞·ªõc
- Multi-hop retrieval
- Nh∆∞ c√°ch n√£o ng∆∞·ªùi li√™n t∆∞·ªüng v√† x√¢u chu·ªói ki·∫øn th·ª©c l·∫°i v·ªõi nhau.

---
üìñ 3. Sense-making ‚Äì Hi·ªÉu v√† t·ªïng h·ª£p ng·ªØ c·∫£nh ph·ª©c t·∫°p
üßæ ƒê·ªãnh nghƒ©a:
Kh·∫£ nƒÉng hi·ªÉu m·ªôt t√¨nh hu·ªëng d√†i, nhi·ªÅu l·ªõp, nhi·ªÅu ·∫©n √Ω; t√°i c·∫•u tr√∫c v√† r√∫t ra √Ω nghƒ©a to√†n c·ª•c.
‚ùìV√≠ d·ª• c√¢u h·ªèi:
"T·∫°i sao nh√¢n v·∫≠t ch√≠nh l·∫°i quy·∫øt ƒë·ªãnh b·ªè tr·ªën ·ªü cu·ªëi truy·ªán?"
- Tr·∫£ l·ªùi ƒë∆∞·ª£c ƒë√≤i h·ªèi ph·∫£i ƒë·ªçc c·∫£ m·∫°ch truy·ªán, hi·ªÉu ƒë∆∞·ª£c:
  - T√¢m l√Ω nh√¢n v·∫≠t
  - Di·ªÖn bi·∫øn s·ª± ki·ªán
  - C√°c y·∫øu t·ªë x√£ h·ªôi, ng·ªØ c·∫£nh
üéØ NƒÉng l·ª±c ƒë∆∞·ª£c ki·ªÉm tra:
- Discourse understanding
- Cohesion & summarization
- Kh·∫£ nƒÉng c·ªßa LLM gi·ªëng nh∆∞ hi·ªÉu v√† ph√¢n t√≠ch vƒÉn h·ªçc ‚Äì ƒëi·ªÅu m√† vector retrieval ƒë∆°n thu·∫ßn kh√¥ng l√†m ƒë∆∞·ª£c.

---
üìä T√≥m t·∫Øt so s√°nh
This content is only supported in a Lark Docs

---

üß© PH·∫¶N 1: T·∫†I SAO C√ÅC M√î H√åNH RAG HI·ªÜN T·∫†I B·ªä H·∫†N CH·∫æ?

---
‚öîÔ∏è Ph·∫£n bi·ªán 1:
H·ªèi: Vector-based RAG hi·ªán t·∫°i ƒë√£ r·∫•t m·∫°nh v·ªõi embedding hi·ªán ƒë·∫°i. V·∫≠y t·∫°i sao v·∫´n b·ªã ƒë√°nh gi√° l√† "thi·∫øu t√≠nh ng∆∞·ªùi"?
Tr·∫£ l·ªùi: V√¨ vector-based RAG ch·ªâ l√†m ƒë∆∞·ª£c retrieval ƒë·ªôc l·∫≠p: m·ªói ƒëo·∫°n vƒÉn (passage) ƒë∆∞·ª£c nh√∫ng v√† truy xu·∫•t ƒë·ªôc l·∫≠p. N√≥ kh√¥ng c√≥ kh·∫£ nƒÉng:
- Nh·∫≠n bi·∫øt m·ªëi quan h·ªá gi·ªØa c√°c ƒëo·∫°n (multi-hop)
- Kh√¥ng hi·ªÉu ƒë∆∞·ª£c m·∫°ch th√¥ng tin d√†i h·∫°n
- Kh√¥ng h·ªó tr·ª£ truy v·∫•n theo d√≤ng suy nghƒ© (reasoning chain)
Gi·ªëng nh∆∞ b·∫°n nh·ªõ ƒë∆∞·ª£c 1000 m·∫£nh th√¥ng tin, nh∆∞ng kh√¥ng bi·∫øt m·∫£nh n√†o n·ªëi v·ªõi m·∫£nh n√†o ‚Äî b·∫°n kh√¥ng th·ªÉ k·ªÉ l·∫°i m·ªôt c√¢u chuy·ªán, c≈©ng kh√¥ng th·ªÉ suy lu·∫≠n.
üìå H·∫°n ch·∫ø n√†y khi·∫øn vector-based RAG t·ªët trong factual QA, nh∆∞ng fail trong associative v√† sense-making.

---
‚öîÔ∏è Ph·∫£n bi·ªán 2:
H·ªèi: Th·∫ø c√≤n c√°c RAG m·ªõi nh∆∞ RAPTOR, GraphRAG, LightRAG? H·ªç t·∫°o graph, t·∫°o t√≥m t·∫Øt r·ªìi m√†?
Tr·∫£ l·ªùi: ƒê√∫ng l√† h·ªç ƒë√£ th·ª≠:
- RAPTOR: T·∫°o c√°c summary t√≥m g·ªçn, r·ªìi retrieve t·ª´ c√°c ƒëo·∫°n t√≥m t·∫Øt ƒë√≥.
- GraphRAG: T·∫°o c√°c knowledge graph t·ª´ entity, li√™n k·∫øt qua c·ªông ƒë·ªìng (community detection).
- LightRAG: T·∫°o 2 l·ªõp retrieval ‚Äì coarse v√† fine, t√≠ch h·ª£p vector v√† graph.
Nh∆∞ng t·∫°i sao v·∫´n ch∆∞a ƒë·ªß?
- Ch√∫ng thi·∫øu c√¢n b·∫±ng: c·∫£i thi·ªán ch·ªó n√†y, l·∫°i gi·∫£m hi·ªáu qu·∫£ ·ªü ch·ªó kh√°c.
- V√≠ d·ª•:
  - RAPTOR t·ªët khi t√≥m t·∫Øt context ‚Üí nh∆∞ng t·∫°o nhi·ªÖu cho QA v√¨ LLM summary c√≥ th·ªÉ b·ªè s√≥t chi ti·∫øt.
  - GraphRAG d√πng graph m·∫°nh ‚Üí nh∆∞ng kh√¥ng t∆∞∆°ng t√°c t·ªët v·ªõi truy v·∫•n, v√¨ query kh√¥ng li√™n k·∫øt tr·ª±c ti·∫øp v·ªõi edge trong KG.
  - LightRAG th√¨ retrieval t·ªët nh∆∞ng kh√¥ng reasoning ƒë∆∞·ª£c.
üìâ Trong c√°c benchmark t·ªïng h·ª£p (b·∫£ng 2 b√†i b√°o), t·∫•t c·∫£ c√°c RAG c√≥ c·∫•u tr√∫c ƒë·ªÅu t·ª•t ƒëi·ªÉm r√µ r·ªát ·ªü m·ªôt trong 3 lo·∫°i nhi·ªám v·ª•.

---
‚öîÔ∏è Ph·∫£n bi·ªán 3:
H·ªèi: V·∫≠y v·∫•n ƒë·ªÅ s√¢u h∆°n n·∫±m ·ªü ƒë√¢u? V√¨ sao c√°c m√¥ h√¨nh c≈© kh√¥ng ƒë·∫°t ƒë∆∞·ª£c b·ªô nh·ªõ d√†i h·∫°n gi·ªëng con ng∆∞·ªùi?
Tr·∫£ l·ªùi: V√¨ ch√∫ng thi·∫øu 3 th·ª© m√† n√£o ng∆∞·ªùi l√†m r·∫•t t·ªët:
1. Li√™n k·∫øt tri th·ª©c d·∫°ng ƒë·ªì th·ªã ‚Äì n√£o ng∆∞·ªùi l∆∞u th√¥ng tin theo m·∫°ng li√™n k·∫øt.
2. Nh·∫≠n di·ªán ng·ªØ c·∫£nh (recognition memory) ‚Äì bi·∫øt th√¥ng tin n√†o li√™n quan trong l√∫c truy xu·∫•t.
3. K·∫øt h·ª£p gi·ªØa kh√°i ni·ªám (concept) v√† ng·ªØ c·∫£nh (context) ‚Äì ch·ª© kh√¥ng l∆∞u r·ªùi r·∫°c.
C√°c m√¥ h√¨nh tr∆∞·ªõc:
- T·∫≠p trung v√†o embedding ho·∫∑c graph, nh∆∞ng kh√¥ng k·∫øt n·ªëi ch√∫ng l·∫°i.
- Kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c c√°c query d√†i, ph·ª©c t·∫°p, ho·∫∑c c·∫ßn suy lu·∫≠n nhi·ªÅu b∆∞·ªõc.
- Kh√¥ng c√≥ c∆° ch·∫ø ki·ªÉm tra ‚Äúli·ªáu ƒëo·∫°n n√†y c√≥ th·ª±c s·ª± ƒë√∫ng ng·ªØ c·∫£nh truy v·∫•n kh√¥ng?‚Äù (g·ªçi l√† recognition filter trong HippoRAG).

---
üß¨ PH·∫¶N 2: HIPPORAG GI·∫¢I QUY·∫æT TH·∫æ N√ÄO?

---
‚öîÔ∏è Ph·∫£n bi·ªán 4:
H·ªèi: Nghe nh∆∞ ai c≈©ng c√≥ knowledge graph r·ªìi, HippoRAG c√≥ g√¨ kh√°c?
Tr·∫£ l·ªùi: HippoRAG kh√¥ng ch·ªâ c√≥ KG, m√† l√† m·ªôt h·ªá th·ªëng m√¥ ph·ªèng tr√≠ nh·ªõ ng∆∞·ªùi:
- Neocortex ‚Üí LLM: d√πng ƒë·ªÉ suy lu·∫≠n, ƒë·ªçc hi·ªÉu, quy·∫øt ƒë·ªãnh.
- Hippocampus ‚Üí KG m·ªü: m√¥ h√¨nh h√≥a k·∫øt n·ªëi tri th·ª©c.
- Parahippocampal region ‚Üí embedding encoder: d√πng ƒë·ªÉ so kh·ªõp v√† g·ª£i nh·ªõ.
=> HippoRAG d√πng PageRank c√° nh√¢n h√≥a tr√™n KG ƒë·ªÉ t√¨m ra nh·ªØng ƒëo·∫°n li√™n k·∫øt t·ªët nh·∫•t, sau ƒë√≥ k·∫øt h·ª£p l·∫°i ƒë·ªÉ feed v√†o LLM ƒë·ªçc hi·ªÉu.

---
‚öîÔ∏è Ph·∫£n bi·ªán 5:
H·ªèi: V·∫≠y so v·ªõi c√°c m√¥ h√¨nh tr∆∞·ªõc, HippoRAG c√≥ g√¨ v∆∞·ª£t tr·ªôi?
Tr·∫£ l·ªùi:
 üëâ HippoRAG 1 ƒë√£ h∆°n c√°c m√¥ h√¨nh c≈© nh·ªù:
- D√πng Personalized PageRank tr√™n KG ƒë·ªÉ l√†m multi-hop retrieval.
- T√°ch ri√™ng index (offline) v√† truy xu·∫•t (online) ‚Üí ti·ªán l·ª£i cho continual learning.
Nh∆∞ng n√≥ v·∫´n ch∆∞a ƒë·ªß, v√¨:
- D·ª±a qu√° nhi·ªÅu v√†o entity (NER), n√™n b·ªè l·ª° ng·ªØ c·∫£nh.
- Kh√¥ng l·ªçc triple n√†o li√™n quan th·∫≠t s·ª± ƒë·∫øn query.
üëâ HippoRAG 2 ƒë√£ kh·∫Øc ph·ª•c th√™m:
- Query-to-Triple linking thay v√¨ NER ‚Üí k·∫øt n·ªëi t·ªët h∆°n.
- Recognition memory b·∫±ng LLM ‚Üí l·ªçc triple "r√°c".
- Dense-sparse integration ‚Üí k·∫øt h·ª£p phrase node (kh√°i ni·ªám) v√† passage node (ng·ªØ c·∫£nh).
K·∫øt qu·∫£:
+7 ƒëi·ªÉm F1 ·ªü associative tasks, kh√¥ng t·ª•t performance ·ªü factual hay discourse.

---
üîç PH·∫¶N 3: T·ªîNG K·∫æT ‚Äì HIPPORAG C√ì G√å KH√ÅC V√Ä HAY?
This content is only supported in a Lark Docs


---
[Image]
[Image]
R·∫•t t·ªët, Qu·ªëc ƒëang ƒëi ƒë√∫ng h∆∞·ªõng c·ªßa ‚Äúnghi√™n c·ª©u nghi√™m t√∫c‚Äù ‚Äì kh√¥ng ch·ªâ ghi nh·∫≠n k·∫øt qu·∫£, m√† ph·∫£i gi·∫£i th√≠ch s√¢u s·∫Øc v√¨ sao l·∫°i ra k·∫øt qu·∫£ ƒë√≥, b·∫±ng logic, k·ªπ thu·∫≠t v√† ph·∫£n bi·ªán.
M√¨nh s·∫Ω ti·∫øp t·ª•c ph√¢n t√≠ch theo phong c√°ch:
üé≠ M·ªôt ng∆∞·ªùi ph√° (h·ªèi xo√°y) ‚Äì M·ªôt ng∆∞·ªùi tr·∫£ l·ªùi (tr·∫£ ƒë√≤n)
 cho t·ª´ng m√¥ h√¨nh, d·ª±a tr√™n pipeline n·ªôi t·∫°i v√† d·ªØ li·ªáu th·ª±c nghi·ªám t·ª´ Table 2 (QA) + Table 3 (Retrieval).

---
üß† 1. RAG G·ªêC
Retrieval b·∫±ng embedding vector, kh√¥ng c·∫•u tr√∫c.
 QA ƒë∆°n thu·∫ßn d·ª±a v√†o ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√¢u h·ªèi v√† ƒëo·∫°n vƒÉn.

---
‚ùìPh√°: T·∫°i sao RAG g·ªëc l·∫°i t·ªët ·ªü factual QA (NQ, PopQA)?
‚úÖ Tr·∫£ l·ªùi: V√¨:
- C√¢u h·ªèi d·∫°ng factual th∆∞·ªùng ch·ªâ c·∫ßn 1 ƒëo·∫°n ng·∫Øn ch·ª©a ƒë√∫ng entity ‚Üí d·ªÖ t√¨m b·∫±ng embedding similarity.
- C√°c embedding hi·ªán ƒë·∫°i (NV-Embed-v2, GritLM) r·∫•t m·∫°nh ·ªü semantic retrieval.
üìå V√≠ d·ª•:
C√¢u h·ªèi: ‚ÄúWhen was Albert Einstein born?‚Äù ‚Üí ch·ªâ c·∫ßn ƒëo·∫°n c√≥ "Albert Einstein was born in 1879..." ‚Üí l√† ƒë·ªß ‚Üí vector retrieval l√†m t·ªët.

---
‚ùìPh√°: T·∫°i sao l·∫°i r·∫•t y·∫øu ·ªü multi-hop v√† sense-making?
‚úÖ Tr·∫£ l·ªùi: V√¨:
- RAG g·ªëc kh√¥ng li√™n k·∫øt th√¥ng tin gi·ªØa nhi·ªÅu ƒëo·∫°n ‚Üí kh√¥ng th·ªÉ reasoning nhi·ªÅu b∆∞·ªõc.
- Kh√¥ng c√≥ c∆° ch·∫ø hi·ªÉu to√†n truy·ªán, ng·ªØ c·∫£nh r·ªông ‚Üí th·∫•t b·∫°i ·ªü NarrativeQA.
üìå V√≠ d·ª•:
C√¢u h·ªèi: ‚ÄúWhat county is Erik Hort‚Äôs birthplace a part of?‚Äù ‚Üí c·∫ßn bi·∫øt: Erik Hort ‚Üí Montebello ‚Üí thu·ªôc h·∫°t g√¨?
 ‚Üí Vector retrieval ch·ªâ t√¨m ƒë∆∞·ª£c ƒëo·∫°n ƒë·∫ßu ti√™n, kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c sang ƒë·ªãa l√Ω.

---
üß† 2. RAPTOR
RAG v·ªõi t√≥m t·∫Øt (summarization) t·ª´ng c·ª•m t√†i li·ªáu. D·ª±a v√†o Gaussian clustering ƒë·ªÉ nh√≥m r·ªìi summary.

---
‚ùìPh√°: T√≥m t·∫Øt th√¨ ph·∫£i gi√∫p t√¨m nhanh h∆°n ch·ª©? Sao l·∫°i t·ª•t ƒëi·ªÉm ·ªü factual QA?
‚úÖ Tr·∫£ l·ªùi: V√¨:
- LLM t√≥m t·∫Øt th∆∞·ªùng b·ªè qua c√°c chi ti·∫øt nh·ªè nh∆∞ s·ªë li·ªáu, ƒë·ªãa ƒëi·ªÉm, t√™n ng∆∞·ªùi ‚Üí trong khi ƒë√≥ l·∫°i l√† m·∫•u ch·ªët c·ªßa factual QA.
- Sau t√≥m t·∫Øt, ƒëo·∫°n g·ªëc kh√¥ng c√≤n ‚Üí truy h·ªìi m·∫•t ch√≠nh x√°c.
üìå T√°c gi·∫£ n√≥i r√µ:
"LLM summarization mechanism... introduces noise into the retrieval corpora."

---
‚ùìPh√°: V·∫≠y t·∫°i sao v·∫´n t·ªët ·ªü sense-making?
‚úÖ Tr·∫£ l·ªùi:
- V√¨ trong discourse d√†i (nh∆∞ NarrativeQA), ƒë·ªçc t·ª´ng ƒëo·∫°n kh√¥ng ƒë·ªß hi·ªÉu m·∫°ch truy·ªán.
- Vi·ªác gom l·∫°i r·ªìi t√≥m t·∫Øt gi√∫p LLM th·∫•y ƒë∆∞·ª£c t·ªïng th·ªÉ ‚Üí hi·ªÉu c√¢u chuy·ªán nh∆∞ ng∆∞·ªùi ƒë·ªçc m·ªôt l·∫ßn.

---
üß† 3. GraphRAG
T·∫°o graph c·ªông ƒë·ªìng (community graph), gi√∫p hi·ªÉu nh√≥m th·ª±c th·ªÉ ho·∫∑c ƒëo·∫°n vƒÉn li√™n quan.

---
‚ùìPh√°: Sao t·∫°o ƒë∆∞·ª£c graph m√† v·∫´n ‚Äút·ª•t factual‚Äù?
‚úÖ Tr·∫£ l·ªùi:
- V√¨ GraphRAG kh√¥ng gi·ªØ ƒëo·∫°n vƒÉn g·ªëc, ch·ªâ d√πng node graph ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ LLM.
- N√™n chi ti·∫øt c·ª• th·ªÉ d·ªÖ b·ªã m·∫•t khi QA y√™u c·∫ßu entity ho·∫∑c s·ªë li·ªáu ch√≠nh x√°c.

---
‚ùìPh√°: V·∫≠y n√≥ gi√∫p ƒë∆∞·ª£c g√¨?
‚úÖ Tr·∫£ l·ªùi:
- D√πng graph structure ƒë·ªÉ kh√¥i ph·ª•c c√°c m·ªëi li√™n h·ªá gi·ªØa th·ª±c th·ªÉ ‚Äì s·ª± ki·ªán ‚Äì ƒë·ªãa ƒëi·ªÉm ‚Üí h·ªó tr·ª£ multi-hop v√† sense-making.
- Vi·ªác ph√°t hi·ªán c·ªông ƒë·ªìng gi√∫p nh√≥m th√¥ng tin t·ª´ c√°c ngu·ªìn kh√°c nhau ‚Üí tƒÉng associativity.
üìå K·∫øt qu·∫£: NarrativeQA ƒë·∫°t 23.0 (ch·ªâ sau HippoRAG 2)
 ‚Üí nghƒ©a l√† n√≥ hi·ªÉu truy·ªán t·ªët, nh∆∞ng kh√¥ng nh·ªõ chi ti·∫øt.

---
üß† 4. HippoRAG 1
D·ª±a v√†o OpenIE ƒë·ªÉ extract triple ‚Üí d√πng Personalized PageRank (PPR) ƒë·ªÉ reasoning.

---
‚ùìPh√°: V·∫≠y t·∫°i sao HippoRAG 1 r·∫•t t·ªët ·ªü associative QA?
‚úÖ Tr·∫£ l·ªùi:
- PPR + KG cho ph√©p ƒëi qua nhi·ªÅu hop trong graph ‚Üí gi·ªëng nh∆∞ nh·ªõ theo ‚Äúƒë∆∞·ªùng li√™n t∆∞·ªüng‚Äù.
- D·∫°ng QA c·∫ßn li√™n k·∫øt 2-3 ƒëo·∫°n l√† ƒëi·ªÉm m·∫°nh c·ªßa PPR.
üìå 2Wiki ƒë·∫°t 71.8, cao nh·∫•t trong b·∫£ng ‚Üí ch·ª©ng minh multi-hop c·ª±c t·ªët.

---
‚ùìPh√°: Nh∆∞ng v√¨ sao l·∫°i ch∆∞a t·ªët ·ªü sense-making?
‚úÖ Tr·∫£ l·ªùi:
- Do HippoRAG 1 ch·ªâ d·ª±a tr√™n entity-centric extraction (NER) ‚Üí thi·∫øu th√¥ng tin ng·ªØ c·∫£nh (context).
- Query kh√¥ng ƒë∆∞·ª£c embed to√†n b·ªô ƒë·ªÉ match triple, ch·ªâ match theo entity ‚Üí contextualization r·∫•t y·∫øu.
üìå T√°c gi·∫£ g·ªçi ƒë√¢y l√† ‚Äúentity-centric bias‚Äù.

---
üß† 5. HippoRAG 2
B·∫£n n√¢ng c·∫•p to√†n di·ªán:
- KG g·ªìm c·∫£ phrase node & passage node (dense-sparse integration)
- Query-to-triple matching (kh√¥ng NER)
- Triple filtering b·∫±ng LLM
- PPR search d√πng c·∫£ passage & phrase node

---
‚ùìPh√°: T·∫°i sao m√¥ h√¨nh n√†y ‚Äút·ªët ƒë·ªÅu c·∫£ 3‚Äù ‚Äî c√≥ g√¨ ƒë·∫∑c bi·ªát?
‚úÖ Tr·∫£ l·ªùi:
1. Factual QA:
- Gi·ªØ nguy√™n ƒëo·∫°n vƒÉn g·ªëc + phrase-level KG ‚Üí kh√¥ng m·∫•t chi ti·∫øt.
- D√πng embedding m·∫°nh + triple filter ‚Üí gi·ªØ th√¥ng tin chu·∫©n.
1. Associative QA:
- PPR graph search s√¢u, seed node t·ª´ c·∫£ phrase & passage ‚Üí c·ª±c k·ª≥ ph√π h·ª£p v·ªõi multi-hop reasoning.
- Filter gi√∫p lo·∫°i nhi·ªÖu ‚Üí tƒÉng ƒë·ªô ch√≠nh x√°c.
2. Sense-making:
- Dense-sparse integration gi√∫p gi·ªØ ƒë∆∞·ª£c c·∫£ kh√°i ni·ªám l·∫´n ng·ªØ c·∫£nh.
- Query kh√¥ng c√≤n b·ªã chia nh·ªè nh∆∞ NER ‚Üí c·∫£ c√¢u ƒë∆∞·ª£c d√πng ƒë·ªÉ hi·ªÉu context ‚Üí c·∫£i thi·ªán m·∫°nh NarrativeQA.
üìå B·∫±ng ch·ª©ng:
- HippoRAG 2 ƒë·∫°t cao nh·∫•t ·ªü F1 NarrativeQA = 25.9
- Recall@5 = 96.3 ·ªü HotpotQA ‚Üí qu√° v∆∞·ª£t tr·ªôi!

---
üìå T·ªïng K·∫øt Gi·∫£i Th√≠ch (c√≥ th·ªÉ ƒë∆∞a v√†o b√°o c√°o/lu·∫≠n vƒÉn):
This content is only supported in a Lark Docs

---
Gi·ªù c·∫ßn hi·ªÉu c√°c kh√°i ni·ªám chi ti·∫øt c·ªßa Hippo 1 v√† Hippo2 -- Gi·∫£i t√≠ch t·ª´ng kh√°i ni·ªám v√† li√™n h·ªá n√≥ v·ªõi th·ª© th·∫≠t d·ªÖ nh·ªõ ƒë∆°n gi·∫£n cho h·ªçc sinh c·∫•p 2

R·∫•t hay Qu·ªëc ∆°i, gi·ªù m√¨nh s·∫Ω gi·∫£i t√≠ch c√°c kh√°i ni·ªám c·ªët l√µi trong HippoRAG 1 v√† HippoRAG 2, li√™n h·ªá v·ªõi v√≠ d·ª• ƒë·ªùi th·∫≠t th·∫≠t d·ªÖ hi·ªÉu, ki·ªÉu gi·∫£i th√≠ch cho h·ªçc sinh c·∫•p 2.

---
üß† T·ªïng th·ªÉ: HippoRAG l√† g√¨?
HippoRAG l√† m√¥ h√¨nh tr√≠ nh·ªõ d√†i h·∫°n cho AI, l·∫•y c·∫£m h·ª©ng t·ª´ c√°ch con ng∆∞·ªùi ghi nh·ªõ v√† suy lu·∫≠n.
 N√≥ chia l√†m 3 b·ªô ph·∫≠n gi·ªëng nh∆∞ b·ªô n√£o ng∆∞·ªùi:
- Neocortex ‚Üí ch√≠nh l√† LLM (n√£o suy nghƒ©)
- Hippocampus ‚Üí n∆°i l∆∞u gi·ªØ k√Ω ·ª©c (graph)
- Parahippocampal Region ‚Üí k·∫øt n·ªëi √Ω t∆∞·ªüng (t√¨m ƒëi·ªÉm gi·ªëng nhau)

---
ü¶õ HippoRAG 1 ‚Äì GI·∫¢I T√çCH C√ÅC KH√ÅI NI·ªÜM CH√çNH

---
1. OpenIE ‚Üí Tr√≠ch xu·∫•t triple (subject ‚Äì predicate ‚Äì object)
üß© ƒê√¢y l√† b∆∞·ªõc l·∫•y th√¥ng tin t·ª´ vƒÉn b·∫£n v√† bi·∫øn n√≥ th√†nh m·ªôt ‚Äúki·∫øn th·ª©c nh·ªè c√≥ d·∫°ng: A li√™n quan ƒë·∫øn B b·∫±ng c√°ch C‚Äù.
üîç V√≠ d·ª• vƒÉn b·∫£n:
‚ÄúAlbert Einstein was born in Ulm in 1879.‚Äù
‚Üí s·∫Ω th√†nh triple:
- (Albert Einstein, born in, Ulm)
- (Albert Einstein, born in year, 1879)
ü™Ñ Li√™n h·ªá cho h·ªçc sinh:
 Gi·ªëng nh∆∞ khi b·∫°n ƒë·ªçc s√°ch v√† t·ª± ghi v·ªü ‚Äút√≥m t·∫Øt √Ω ch√≠nh‚Äù ki·ªÉu:
 ‚Üí ‚ÄúEinstein ‚Üí sinh ·ªü Ulm‚Äù, ‚ÄúEinstein ‚Üí sinh nƒÉm 1879‚Äù.

---
2. Graph of Knowledge Triples (Bi·ªÉu ƒë·ªì ki·∫øn th·ª©c)
üß† C√°c triple tr√™n ƒë∆∞·ª£c gh√©p th√†nh 1 b·∫£n ƒë·ªì ki·∫øn th·ª©c, gi·ªëng nh∆∞ b·∫£n ƒë·ªì t∆∞ duy (mindmap).
üìå M·ªói v√≤ng tr√≤n l√† 1 t·ª´ kh√≥a, m·ªói m≈©i t√™n l√† m·ªëi li√™n h·ªá.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 B·∫°n v·∫Ω s∆° ƒë·ªì mindmap m√¥n Sinh h·ªçc:
- ‚ÄúL√° c√¢y ‚Üí c√≥ ch·ª©c nƒÉng ‚Üí quang h·ª£p‚Äù
- ‚ÄúQuang h·ª£p ‚Üí t·∫°o ra ‚Üí oxy‚Äù
 ‚Üí L√† b·∫°n ƒëang v·∫Ω KG ƒë√≥!

---
3. Personalized PageRank (PPR)
üîÑ ƒê√¢y l√† c√°ch m√¥ h√¨nh t√¨m ƒë∆∞·ªùng ƒëi qua nhi·ªÅu b∆∞·ªõc trong b·∫£n ƒë·ªì ki·∫øn th·ª©c ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 Gi·ªëng nh∆∞ ch∆°i tr√≤ ‚Äúƒëi t√¨m kho b√°u‚Äù trong m√™ cung. B·∫°n b·∫Øt ƒë·∫ßu t·ª´ t·ª´ kh√≥a trong c√¢u h·ªèi, ƒëi theo c√°c m·ªëi li√™n h·ªá ƒë·ªÉ ƒë·∫øn c√¢u tr·∫£ l·ªùi.
C√¢u h·ªèi: ‚ÄúL√° c√¢y gi√∫p g√¨ cho m√¥i tr∆∞·ªùng?‚Äù
 ‚Üí L√° c√¢y ‚Üí quang h·ª£p ‚Üí t·∫°o oxy ‚Üí gi√∫p m√¥i tr∆∞·ªùng ‚Üí th·ªü

---
4. Entity-Centric Query Linking (Li√™n k·∫øt d·ª±a v√†o t√™n ri√™ng)
HippoRAG 1 d√πng c√°ch ph√°t hi·ªán th·ª±c th·ªÉ (entity) trong c√¢u h·ªèi nh∆∞ t√™n ng∆∞·ªùi, n∆°i ch·ªën, ƒë·ªÉ n·ªëi v√†o graph.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 C√¢u h·ªèi: ‚ÄúBill Gates h·ªçc tr∆∞·ªùng n√†o?‚Äù
 ‚Üí Ph√°t hi·ªán ‚ÄúBill Gates‚Äù l√† th·ª±c th·ªÉ ‚Üí t√¨m trong b·∫£n ƒë·ªì tr√≠ nh·ªõ.
ƒêi·ªÉm y·∫øu:
- Kh√¥ng hi·ªÉu c√¢u h·ªèi r·ªông (v√≠ d·ª•: ‚ÄúNg∆∞·ªùi s√°ng l·∫≠p Microsoft h·ªçc ·ªü ƒë√¢u?‚Äù)
 ‚Üí n·∫øu kh√¥ng c√≥ t·ª´ ‚ÄúBill Gates‚Äù, th√¨ kh√¥ng t√¨m ƒë∆∞·ª£c ‚Üí m·∫•t context

---
üß† HippoRAG 2 ‚Äì C·∫¢I TI·∫æN QUAN TR·ªåNG + V√ç D·ª§ ƒê·ªúI TH∆Ø·ªúNG

---
1. Dense-Sparse Integration: Gh√©p ki·∫øn th·ª©c v√† ng·ªØ c·∫£nh
HippoRAG 2 kh√¥ng ch·ªâ l∆∞u "ki·∫øn th·ª©c ng·∫Øn g·ªçn" m√† c√≤n l∆∞u c·∫£ ƒëo·∫°n vƒÉn ban ƒë·∫ßu (ng·ªØ c·∫£nh).
üìå Graph c√≥ 2 lo·∫°i node:
- Phrase Node (sparse): gi·ªëng ti√™u ƒë·ªÅ/t·ª´ kh√≥a
- Passage Node (dense): nguy√™n ƒëo·∫°n vƒÉn g·ªëc
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 Khi h·ªçc l·ªãch s·ª≠, b·∫°n v·ª´a g·∫°ch ƒë·∫ßu d√≤ng ‚ÄúTr·∫≠n ƒêi·ªán Bi√™n Ph·ªß th·∫Øng nƒÉm 1954‚Äù, v·ª´a gi·ªØ l·∫°i nguy√™n ƒëo·∫°n m√¥ t·∫£ tr·∫≠n ƒë√°nh ‚Üí v·∫≠y b·∫°n v·ª´a nh·ªõ "√Ω ch√≠nh" v·ª´a c√≥ ‚Äúng·ªØ c·∫£nh‚Äù.

---
2. Query-to-Triple Matching
Kh√¥ng ch·ªâ t√¨m entity n·ªØa, m√† d√πng to√†n c√¢u h·ªèi ƒë·ªÉ match v·ªõi c√°c triple.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 C√¢u h·ªèi: ‚ÄúNg∆∞·ªùi s√°ng l·∫≠p Microsoft h·ªçc ·ªü ƒë√¢u?‚Äù
 ‚Üí L√∫c n√†y, d√π kh√¥ng c√≥ t·ª´ ‚ÄúBill Gates‚Äù, m√¥ h√¨nh v·∫´n hi·ªÉu ƒë∆∞·ª£c v√† t√¨m ƒë∆∞·ª£c triple ph√π h·ª£p.

---
3. Recognition Memory ‚Äì l·ªçc triple b·∫±ng LLM
Sau khi t√¨m ra v√†i m·ªëi li√™n h·ªá ph√π h·ª£p, HippoRAG 2 nh·ªù LLM ki·ªÉm tra l·∫°i xem c√°i n√†o th·ª±c s·ª± li√™n quan.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 B·∫°n t√¨m 5 ƒëo·∫°n c√≥ v·∫ª ƒë√∫ng ƒë·ªÉ l√†m b√†i t·∫≠p. Sau ƒë√≥ b·∫°n ƒë·ªçc k·ªπ t·ª´ng ƒëo·∫°n, lo·∫°i b·ªè c√°i kh√¥ng li√™n quan ‚Üí gi·ªØ l·∫°i 2 ƒëo·∫°n t·ªët nh·∫•t ‚Üí ƒë√≥ l√† ‚Äúrecognition memory‚Äù.

---
4. Contextualized PPR Search
Khi ƒë√£ c√≥ c·∫£ t·ª´ kh√≥a v√† ƒëo·∫°n vƒÉn, m√¥ h√¨nh d√πng PageRank ƒë·ªÉ t√¨m con ƒë∆∞·ªùng t·ªët nh·∫•t qua b·∫£n ƒë·ªì tr√≠ nh·ªõ c√≥ c·∫£ ki·∫øn th·ª©c v√† ng·ªØ c·∫£nh.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 B·∫°n n·ªëi ‚Äút·ª´ kh√≥a‚Äù v·ªõi c√°c ƒëo·∫°n gi·∫£i th√≠ch trong s√°ch, r·ªìi ch·ªçn ra ƒëo·∫°n n√†o gi√∫p b·∫°n hi·ªÉu to√†n c·∫£nh nh·∫•t ƒë·ªÉ l√†m b√†i.

---
üéì T·ªïng k·∫øt: HippoRAG 1 vs 2 ‚Äì D·ªÖ hi·ªÉu cho h·ªçc sinh c·∫•p 2
This content is only supported in a Lark Docs

---
Tuy·ªát v·ªùi Qu·ªëc ∆°i, gi·ªù m√¨nh s·∫Ω gi·∫£i t√≠ch s√¢u t·ª´ng kh√°i ni·ªám trong s∆° ƒë·ªì pipeline c·ªßa HippoRAG 2, nh∆∞ Qu·ªëc y√™u c·∫ßu:
- Hi·ªÉu r√µ c∆° ch·∫ø
- Ph·∫£n bi·ªán ki·ªÉu ph√° ‚Äì ƒë√°p t·ª´ng b∆∞·ªõc
- Li√™n h·ªá v√≠ d·ª• khi c·∫ßn
S∆° ƒë·ªì ƒë∆∞·ª£c chia l√†m 2 ph·∫ßn:

---
üéØ PH·∫¶N 1: OFFLINE INDEXING
M·ª•c ti√™u: x√¢y d·ª±ng m·ªôt b·ªô nh·ªõ ki·∫øn th·ª©c gi√†u li√™n k·∫øt t·ª´ vƒÉn b·∫£n ƒë·∫ßu v√†o.

---
üîµ B∆∞·ªõc 1: OpenIE by LLM ‚Üí Tr√≠ch xu·∫•t Triple
T√°ch ƒëo·∫°n vƒÉn th√†nh c√°c ƒë∆°n v·ªã ki·∫øn th·ª©c d·∫°ng (subject, predicate, object)

---
‚ùìPh√°: T·∫°i sao d√πng OpenIE b·∫±ng LLM? Tr√≠ch xu·∫•t truy·ªÅn th·ªëng c√≥ v·∫•n ƒë·ªÅ g√¨?
‚úÖ ƒê√°p:
- OpenIE truy·ªÅn th·ªëng (Stanford, ClausIE...) ch·ªâ l√†m ƒë∆∞·ª£c tr√™n c√¢u ƒë∆°n, thi·∫øu kh·∫£ nƒÉng x·ª≠ l√Ω ng√¥n ng·ªØ ph·ª©c t·∫°p.
- LLM c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c m·ªánh ƒë·ªÅ d√†i, ng·ªØ nghƒ©a m·ªü r·ªông, ph·ª©c t·∫°p h∆°n ‚Üí t·∫°o triple ‚Äúgi√†u nghƒ©a‚Äù h∆°n.
üìå V√≠ d·ª•
 VƒÉn b·∫£n: "Marie Curie won two Nobel Prizes in different fields."
 ‚Üí Triple truy·ªÅn th·ªëng: ("Marie Curie", "won", "two Nobel Prizes")
 ‚Üí LLM c√≥ th·ªÉ t·∫°o th√™m triple: ("Marie Curie", "won Nobel in", "Physics") + ("Marie Curie", "won Nobel in", "Chemistry")

---
üîµ B∆∞·ªõc 2: Synonym Detection ‚Üí Ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a
N·ªëi c√°c node g·∫ßn nghƒ©a trong KG b·∫±ng Synonym Edge (ƒë∆∞·ªùng ƒë·ª©t n√©t trong s∆° ƒë·ªì)

---
‚ùìPh√°: Ph√°t hi·ªán synonym ki·ªÉu g√¨? C√≥ d√πng LLM kh√¥ng?
‚úÖ ƒê√°p: Kh√¥ng! D√πng cosine similarity trong embedding space
- C√°c phrase (t·ª´/c·ª•m t·ª´) ƒë∆∞·ª£c nh√∫ng (embedding)
- N·∫øu cosine similarity > ng∆∞·ª°ng ‚Üí ƒë∆∞·ª£c n·ªëi b·∫±ng "synonym edge"
üìå T√°c gi·∫£ kh√¥ng d√πng LLM v√¨:
- LLM ch·∫≠m h∆°n r·∫•t nhi·ªÅu
- Embedding-based synonym c√≥ th·ªÉ c·∫≠p nh·∫≠t li√™n t·ª•c, ƒë∆°n gi·∫£n h∆°n

---
‚ùìPh√°: C√≥ r·ªßi ro g√¨ kh√¥ng khi d√πng embedding ƒë·ªÉ n·ªëi synonym?
‚úÖ ƒê√°p:
- C√≥! C√≥ th·ªÉ n·ªëi nh·∫ßm c√°c t·ª´ g·∫ßn nghƒ©a nh∆∞ng kh√°c ng·ªØ c·∫£nh ("doctor" v√† "professor" ch·∫≥ng h·∫°n).
- Nh∆∞ng v√¨ ƒë√¢y l√† l·ªõp b·ªï tr·ª£ (synonym edge), kh√¥ng ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp n·∫øu downstream filter l√†m t·ªët.

---
üîµ B∆∞·ªõc 3: Dense‚ÄìSparse Integration
T√≠ch h·ª£p c·∫£ node "phrase" v√† node "passage" trong KG ‚Üí hybrid graph

---
‚ùìPh√°: T·∫°i sao c·∫ßn v·ª´a sparse (phrase) v·ª´a dense (passage)? Kh√¥ng gi·ªØ m·ªôt lo·∫°i cho g·ªçn?
‚úÖ ƒê√°p:
- Sparse (phrase): D·ªÖ truy h·ªìi, bi·ªÉu di·ªÖn kh√°i ni·ªám ‚Üí t·ªët cho reasoning
- Dense (passage): Gi·ªØ th√¥ng tin chi ti·∫øt ‚Üí t·ªët cho factual QA
 ‚Üí Gh√©p c·∫£ hai t·∫°o ra m·ªôt KG v·ª´a kh√°i qu√°t v·ª´a gi√†u ng·ªØ c·∫£nh.
ü™Ñ Li√™n h·ªá d·ªÖ hi·ªÉu: Gi·ªëng h·ªçc Sinh h·ªçc b·∫±ng s∆° ƒë·ªì + s√°ch gi√°o khoa:
- S∆° ƒë·ªì (phrase node) gi√∫p nh·ªõ nhanh
- SGK (passage node) gi√∫p hi·ªÉu s√¢u ‚Üí c·∫ßn c·∫£ hai ƒë·ªÉ thi ƒëi·ªÉm cao

---
‚úÖ T·ªïng k·∫øt PH·∫¶N 1: Graph c√≥ c√°c lo·∫°i node v√† edge sau:
This content is only supported in a Lark Docs

---
üîç PH·∫¶N 2: ONLINE RETRIEVAL & QA
M·ª•c ti√™u: Khi c√≥ query ‚Üí t√¨m ƒë∆∞·ªùng trong graph ‚Üí tr·∫£ l·ªùi.

---
üü¢ B∆∞·ªõc 1: Retrieving Passages and Triples

---
‚ùìPh√°: Sao ph·∫£i l·∫•y c·∫£ ƒëo·∫°n vƒÉn l·∫´n triple? Kh√¥ng ch·ªçn 1 c√°i cho nhanh?
‚úÖ ƒê√°p:
- Triple ƒë·ªÉ l√†m reasoning, k·∫øt n·ªëi m·∫°ch ki·∫øn th·ª©c
- Passage ƒë·ªÉ c√≥ th√¥ng tin chi ti·∫øt, l√†m QA cu·ªëi c√πng
‚Üí Hai l·ªõp retrieval b·ªï tr·ª£ nhau, kh√¥ng th·ªÉ thi·∫øu c√°i n√†o.

---
üü¢ B∆∞·ªõc 2: Triple Filtering = Recognition Memory
Sau khi l·∫•y triple v·ªÅ, ta l·ªçc xem c√°i n√†o th·ª±c s·ª± li√™n quan ƒë·∫øn query

---
‚ùìPh√°: T·∫°i sao c·∫ßn b∆∞·ªõc n√†y? Triple ƒë√£ ch·ªçn r·ªìi m√†?
‚úÖ ƒê√°p: V√¨:
- Retrieval ban ƒë·∫ßu b·∫±ng embedding ‚Üí c√≥ th·ªÉ nhi·ªÖu
- Recognition l√† b∆∞·ªõc gi·ªëng b·ªô nh·ªõ con ng∆∞·ªùi:
 ‚Üí ‚Äúm√¨nh t·ª´ng th·∫•y c√°i n√†y r·ªìi‚Ä¶ nh∆∞ng c√≥ ƒë√∫ng c√°i m√¨nh c·∫ßn kh√¥ng?‚Äù
üìå D√πng LLM ƒë·ªÉ so s√°nh l·∫°i t·ª´ng triple v·ªõi query ‚Üí gi·ªØ l·∫°i nh·ªØng c√°i t·ªët nh·∫•t
ü™Ñ Li√™n h·ªá h·ªçc sinh: Gi·ªëng ƒë·ªçc l·∫°i 5 ƒëo·∫°n b·∫°n highlight khi h·ªçc ‚Üí lo·∫°i 3 c√°i kh√¥ng li√™n quan, gi·ªØ l·∫°i 2 ƒëo·∫°n th·∫≠t s·ª± ƒë√∫ng ƒë·ªÅ.

---
üî¥ B∆∞·ªõc 3: Assigning Seed Node Weights
G√°n ƒëi·ªÉm cho c√°c node ‚Äúg·ªëc‚Äù trong KG ‚Üí chu·∫©n b·ªã cho b∆∞·ªõc lan truy·ªÅn PageRank

---
‚ùìPh√°: Seed Node l√† g√¨? T·∫°i sao ph·∫£i g√°n tr·ªçng s·ªë?
‚úÖ ƒê√°p:
- Seed Node l√† node trong graph c√≥ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn query
- Tr·ªçng s·ªë cao th√¨ node ƒë√≥ lan truy·ªÅn ·∫£nh h∆∞·ªüng m·∫°nh h∆°n trong PPR
üìå D√πng c·∫£:
- Passage t·ª´ query retrieval
- Phrase t·ª´ triple l·ªçc
 ‚Üí t·∫•t c·∫£ ƒë·ªÅu tr·ªü th√†nh seed node

---
üîÑ B∆∞·ªõc 4: PPR Graph Search
Personalized PageRank t√¨m ƒë∆∞·ªùng trong KG t·ª´ c√°c seed node

---
‚ùìPh√°: Kh√°c g√¨ so v·ªõi search b√¨nh th∆∞·ªùng?
‚úÖ ƒê√°p:
- PageRank lan t·ªèa ·∫£nh h∆∞·ªüng ‚Üí kh√¥ng t√¨m ƒë∆∞·ªùng ng·∫Øn nh·∫•t, m√† t√¨m ƒëo·∫°n c√≥ li√™n quan nh·∫•t trong to√†n b·ªô c·∫•u tr√∫c
- Personalized ‚Üí ∆∞u ti√™n c√°c v√πng g·∫ßn v·ªõi seed node
ü™Ñ Li√™n h·ªá h·ªçc sinh: Gi·ªëng h·ªçc theo s∆° ƒë·ªì t∆∞ duy, b·∫°n ∆∞u ti√™n ƒë·ªçc ph·∫ßn g·∫ßn ch·ªß ƒë·ªÅ ch√≠nh ‚Üí kh√¥ng l·∫≠t s√°ch lung tung

---
‚úÖ B∆∞·ªõc 5: QA Reading with Selected Passages
LLM ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë√£ ch·ªçn ‚Üí sinh ra c√¢u tr·∫£ l·ªùi.

---
‚ùìPh√°: N·∫øu ch·ªâ d√πng ƒëo·∫°n vƒÉn m√† kh√¥ng graph th√¨ sao?
‚úÖ ƒê√°p:
 ‚Üí Gi·ªëng RAG truy·ªÅn th·ªëng ‚Üí b·ªã l·ªách, thi·∫øu reasoning
 ‚Üí Nh·ªù b∆∞·ªõc graph search, m√¥ h√¨nh nh·∫≠n ƒë∆∞·ª£c ƒëo·∫°n vƒÉn ‚Äúgi√†u li√™n k·∫øt nh·∫•t‚Äù ‚Üí tr·∫£ l·ªùi m·∫°ch l·∫°c h∆°n, ch√≠nh x√°c h∆°n.

---
üîö T·ªïng K·∫øt: Pipeline HippoRAG 2 ‚Äì Hi·ªÉu s√¢u + v√≠ d·ª• h√≥a
This content is only supported in a Lark Docs

---
D∆∞·ªõi ƒë√¢y l√† ph·∫ßn gi·∫£i th√≠ch s√¢u t·ª´ng b∆∞·ªõc trong pipeline c·ªßa HippoRAG 2, k√®m theo c√°c c√¢u h·ªèi ‚Äúph√°‚Äù ‚Äì ‚Äúƒë√°p‚Äù ƒë·ªÉ ƒë√†o s√¢u hi·ªÉu bi·∫øt, gi·ªëng nh∆∞ ƒë·ªëi tho·∫°i gi·ªØa ng∆∞·ªùi ƒë·∫∑t c√¢u h·ªèi s·∫Øc s·∫£o v√† ng∆∞·ªùi tr·∫£ l·ªùi ch·∫∑t ch·∫Ω.

---
1. OpenIE by LLM ‚Äì Tr√≠ch xu·∫•t Triple t·ª´ vƒÉn b·∫£n
M·ª•c ti√™u:
 Chuy·ªÉn ƒë·ªïi ƒëo·∫°n vƒÉn b·∫£n th√†nh c√°c ƒë∆°n v·ªã ki·∫øn th·ª©c (triple: subject ‚Äì predicate ‚Äì object) ƒë·ªÉ l∆∞u tr·ªØ trong b·ªô nh·ªõ d∆∞·ªõi d·∫°ng c√°c m·ªëi li√™n h·ªá ƒë∆°n gi·∫£n.

---
‚ùì Ph√° 1:
T·∫°i sao ch√∫ng ta c·∫ßn tr√≠ch xu·∫•t triple thay v√¨ ch·ªâ l∆∞u to√†n b·ªô ƒëo·∫°n vƒÉn?
‚úÖ ƒê√°p:
- Vi·ªác tr√≠ch xu·∫•t triple gi√∫p t√°ch th√¥ng tin quan tr·ªçng ra kh·ªèi m·ªõ vƒÉn b·∫£n d√†i.
- N√≥ cho ph√©p l∆∞u tr·ªØ d∆∞·ªõi d·∫°ng c√°c m·ªëi quan h·ªá r√µ r√†ng, gi·ªëng nh∆∞ ghi ch√∫ ng·∫Øn g·ªçn, d·ªÖ t√¨m l·∫°i v√† k·∫øt n·ªëi v·ªõi c√°c ki·∫øn th·ª©c kh√°c.
V√≠ d·ª• d·ªÖ hi·ªÉu:
 Gi·ªëng nh∆∞ khi h·ªçc, b·∫°n kh√¥ng c·∫ßn l∆∞u to√†n b·ªô b√†i gi·∫£ng m√† ch·ªâ c·∫ßn ghi l·∫°i c√°c √Ω ch√≠nh: ‚ÄúEinstein ‚Äì sinh ‚Äì Ulm‚Äù thay v√¨ ghi to√†n b·ªô ƒëo·∫°n vƒÉn d√†i.

---
‚ùì Ph√° 2:
T·∫°i sao d√πng LLM ƒë·ªÉ th·ª±c hi·ªán OpenIE, thay v√¨ d√πng c√°c c√¥ng c·ª• truy·ªÅn th·ªëng?
‚úÖ ƒê√°p:
- C√°c c√¥ng c·ª• OpenIE truy·ªÅn th·ªëng th∆∞·ªùng gi·ªõi h·∫°n ·ªü c√¢u ƒë∆°n gi·∫£n, kh√¥ng hi·ªÉu ƒë∆∞·ª£c nh·ªØng c√¢u ph·ª©c t·∫°p, m√¢u thu·∫´n hay ·∫©n d·ª•.
- LLM c√≥ kh·∫£ nƒÉng hi·ªÉu ng·ªØ c·∫£nh, x·ª≠ l√Ω c√¢u d√†i v√† ph·ª©c t·∫°p, t·ª´ ƒë√≥ tr√≠ch xu·∫•t triple ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß h∆°n.
V√≠ d·ª• h·ªçc sinh c·∫•p 2:
 N·∫øu ƒë·ªçc m·ªôt ƒëo·∫°n vƒÉn m√¥ t·∫£ "Marie Curie ƒë√£ kh√°m ph√° ra c√°c nguy√™n t·ªë m·ªõi v√† gi√†nh gi·∫£i Nobel trong nhi·ªÅu lƒ©nh v·ª±c", LLM c√≥ th·ªÉ t·∫°o ra nhi·ªÅu triple nh∆∞:
 ‚Üí (Marie Curie, kh√°m ph√°, nguy√™n t·ªë m·ªõi)
 ‚Üí (Marie Curie, gi√†nh, gi·∫£i Nobel)
 Nh∆∞ v·∫≠y, b·∫°n c√≥ th·ªÉ nh·ªõ nhi·ªÅu th√¥ng tin h∆°n ch·ªâ b·∫±ng m·ªôt c√¢u t√≥m t·∫Øt.

---
2. Synonym Detection ‚Äì Ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a
M·ª•c ti√™u:
 K·∫øt n·ªëi c√°c node (kh√°i ni·ªám, c·ª•m t·ª´) c√≥ √Ω nghƒ©a t∆∞∆°ng t·ª± trong KG, gi√∫p tƒÉng kh·∫£ nƒÉng li√™n k·∫øt gi·ªØa c√°c th√¥ng tin.

---
‚ùì Ph√° 1:
Synonym Detection d√πng thu·∫≠t to√°n n√†o ƒë·ªÉ ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a?
‚úÖ ƒê√°p:
- Th√¥ng th∆∞·ªùng, ta s·ª≠ d·ª•ng cosine similarity trong kh√¥ng gian embedding.
- M·ªói t·ª´ hay c·ª•m t·ª´ ƒë∆∞·ª£c chuy·ªÉn th√†nh m·ªôt vector, sau ƒë√≥ t√≠nh cosine similarity gi·ªØa c√°c vector. N·∫øu gi√° tr·ªã similarity v∆∞·ª£t qua m·ªôt ng∆∞·ª°ng ƒë·ªãnh tr∆∞·ªõc (v√≠ d·ª• 0.8), hai node ƒë∆∞·ª£c coi l√† ƒë·ªìng nghƒ©a v√† ƒë∆∞·ª£c n·ªëi v·ªõi nhau b·∫±ng m·ªôt ‚Äúsynonym edge.‚Äù

---
‚ùì Ph√° 2:
T·∫°i sao l·∫°i c·∫ßn n·ªëi c√°c node ƒë·ªìng nghƒ©a l·∫°i v·ªõi nhau?
‚úÖ ƒê√°p:
- ƒêi·ªÅu n√†y gi√∫p li√™n k·∫øt c√°c kh√°i ni·ªám c√≥ th·ªÉ ƒë∆∞·ª£c di·ªÖn ƒë·∫°t kh√°c nhau.
- Nh·ªù ƒë√≥, khi truy v·∫•n, n·∫øu t·ª´ kh√≥a ƒë∆∞·ª£c d√πng kh√°c v·ªõi c√°ch ghi ch√∫ ban ƒë·∫ßu nh∆∞ng c√≥ c√πng √Ω nghƒ©a, h·ªá th·ªëng v·∫´n c√≥ th·ªÉ li√™n k·∫øt ƒë√∫ng ki·∫øn th·ª©c.
V√≠ d·ª• d·ªÖ hi·ªÉu:
 N·∫øu trong vƒÉn b·∫£n c√≥ ‚Äúth·∫ßy gi√°o‚Äù v√† ‚Äúgi√°o vi√™n‚Äù ‚Äì m·∫∑c d√π hai t·ª´ kh√°c nhau, ch√∫ng ƒë·ªÅu n√≥i v·ªÅ ng∆∞·ªùi d·∫°y h·ªçc. K·∫øt n·ªëi ch√∫ng l·∫°i gi√∫p h·ªá th·ªëng hi·ªÉu r·∫±ng khi h·ªèi v·ªÅ ‚Äúgi√°o vi√™n‚Äù c≈©ng c√≥ th·ªÉ li√™n h·ªá ƒë·∫øn ‚Äúth·∫ßy gi√°o‚Äù.

---
‚ùì Ph√° 3:
C√≥ r·ªßi ro g√¨ khi d√πng cosine similarity?
‚úÖ ƒê√°p:
- C√≥ th·ªÉ x·∫£y ra tr∆∞·ªùng h·ª£p n·ªëi nh·∫ßm nh·ªØng t·ª´ c√≥ c√πng h√¨nh th·ª©c nh∆∞ng ng·ªØ nghƒ©a kh√°c (v√≠ d·ª•: ‚Äúbank‚Äù c√≥ th·ªÉ l√† ng√¢n h√†ng ho·∫∑c b·ªù s√¥ng).
- Tuy nhi√™n, v√¨ ƒë√¢y ch·ªâ l√† l·ªõp b·ªï tr·ª£ trong KG, n·∫øu c√°c b∆∞·ªõc sau (nh∆∞ triple filtering) ho·∫°t ƒë·ªông t·ªët, sai s√≥t n√†y s·∫Ω ƒë∆∞·ª£c gi·∫£m thi·ªÉu.

---
3. Dense‚ÄìSparse Integration ‚Äì T√≠ch h·ª£p c·∫£ Node "Phrase" v√† "Passage"
M·ª•c ti√™u:
 K·∫øt h·ª£p th√¥ng tin ng·∫Øn g·ªçn (phrase node) v√† th√¥ng tin ƒë·∫ßy ƒë·ªß (passage node) v√†o c√πng m·ªôt KG ƒë·ªÉ t·∫≠n d·ª•ng ∆∞u ƒëi·ªÉm c·ªßa c·∫£ hai.

---
‚ùì Ph√° 1:
T·∫°i sao kh√¥ng ch·ªâ l∆∞u th√¥ng tin d·∫°ng ‚Äúphrase‚Äù hay ‚Äúpassage‚Äù ri√™ng r·∫Ω?
‚úÖ ƒê√°p:
- Phrase node (sparse): R√∫t g·ªçn th√¥ng tin, d·ªÖ k·∫øt n·ªëi c√°c kh√°i ni·ªám c∆° b·∫£n, nhanh v√† hi·ªáu qu·∫£ cho reasoning.
- Passage node (dense): Gi·ªØ nguy√™n ng·ªØ c·∫£nh v√† chi ti·∫øt, c·∫ßn thi·∫øt cho c√°c t√°c v·ª• y√™u c·∫ßu ƒë·ªô ch√≠nh x√°c cao nh∆∞ factual QA.
V√≠ d·ª•:
 H·ªçc sinh khi √¥n b√†i to√°n: b·∫°n c√≥ th·ªÉ ghi l·∫°i ‚Äúc√¥ng th·ª©c Pythagoras‚Äù (phrase) v√† c√≥ s·ªï tay ch·ª©a b√†i gi·∫£i chi ti·∫øt (passage).
 C·∫£ hai ƒë·ªÅu quan tr·ªçng ƒë·ªÉ b·∫°n v·ª´a nh·ªõ nhanh, v·ª´a hi·ªÉu s√¢u.

---
‚ùì Ph√° 2:
How does integrating both dense and sparse information help in retrieval?
‚úÖ ƒê√°p:
- Khi c√≥ c·∫£ hai, h·ªá th·ªëng c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c phrase node ƒë·ªÉ x√°c ƒë·ªãnh c√°c kh√°i ni·ªám ch·ªß ƒë·∫°o v√† passage node ƒë·ªÉ l·∫•y chi ti·∫øt c·ª• th·ªÉ cho c√¢u tr·∫£ l·ªùi.
- ƒêi·ªÅu n√†y t·∫°o ra m·ªôt KG to√†n di·ªán, v·ª´a t·ªïng qu√°t v·ª´a chi ti·∫øt, gi√∫p tƒÉng hi·ªáu su·∫•t ·ªü c·∫£ factual QA v√† multi-hop reasoning.

---
4. Triple Retrieval ‚Äì L·∫•y ra c√°c Triple c√≥ li√™n quan v·ªõi Query
M·ª•c ti√™u:
 Khi c√≥ m·ªôt query, t√¨m ra c√°c triple t·ª´ KG ph√π h·ª£p ƒë·ªÉ d√πng cho qu√° tr√¨nh reasoning.

---
‚ùì Ph√° 1:
T·∫°i sao kh√¥ng ch·ªâ l·∫•y to√†n b·ªô passage m√† ph·∫£i l·∫•y ra triple?
‚úÖ ƒê√°p:
- Triple ch·ª©a m·ªëi li√™n h·ªá r√µ r√†ng v√† r√∫t g·ªçn, gi√∫p h·ªá th·ªëng d·ªÖ d√†ng ‚Äúƒëi theo‚Äù c√°c m·ªëi li√™n k·∫øt ƒë·ªÉ reasoning.
- Passage c√≥ th·ªÉ qu√° d√†i, ch·ª©a nhi·ªÖu; triple gi√∫p x√°c ƒë·ªãnh ‚Äúƒëi·ªÉm c·ªët l√µi‚Äù c·ªßa ki·∫øn th·ª©c.
V√≠ d·ª•:
 N·∫øu c√¢u h·ªèi l√† ‚ÄúBill Gates h·ªçc tr∆∞·ªùng n√†o?‚Äù, triple nh∆∞ (‚ÄúBill Gates‚Äù, ‚Äúh·ªçc t·∫°i‚Äù, ‚ÄúHarvard‚Äù) s·∫Ω gi√∫p h·ªá th·ªëng nhanh ch√≥ng t√¨m ra th√¥ng tin c·∫ßn thi·∫øt.

---
5. Triple Filtering (Recognition Memory) ‚Äì L·ªçc l·∫°i c√°c Triple
M·ª•c ti√™u:
 Sau khi l·∫•y ra c√°c triple ban ƒë·∫ßu, s·ª≠ d·ª•ng LLM ƒë·ªÉ x√°c nh·∫≠n l·∫°i t√≠nh li√™n quan c·ªßa ch√∫ng v·ªõi query, lo·∫°i b·ªè nh·ªØng triple kh√¥ng ph√π h·ª£p.

---
‚ùì Ph√° 1:
T·∫°i sao l·∫°i c·∫ßn b∆∞·ªõc l·ªçc triple n·∫øu ƒë√£ c√≥ triple retrieval?
‚úÖ ƒê√°p:
- Do b∆∞·ªõc retrieval ban ƒë·∫ßu c√≥ th·ªÉ mang l·∫°i nhi·ªÅu triple nhi·ªÖu ho·∫∑c kh√¥ng li√™n quan, v√¨ kh√¥ng ph·∫£i l√∫c n√†o c√°c vector embedding c≈©ng ho√†n h·∫£o.
- Recognition Memory gi√∫p ‚Äúƒë·ªçc l·∫°i‚Äù v√† x√°c nh·∫≠n, gi·ªëng nh∆∞ khi b·∫°n ki·ªÉm tra l·∫°i ghi ch√∫ c·ªßa m√¨nh ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ gi·ªØ l·∫°i nh·ªØng th√¥ng tin ƒë√∫ng c·∫ßn thi·∫øt.
V√≠ d·ª•:
 N·∫øu c√≥ 5 triple ƒë∆∞·ª£c l·∫•y ra, nh∆∞ng ch·ªâ c√≥ 2 th·ª±c s·ª± n√≥i v·ªÅ c√¢u h·ªèi, b∆∞·ªõc filtering s·∫Ω lo·∫°i b·ªè 3 triple kh√¥ng c·∫ßn thi·∫øt.

---
6. Seed Node Assignment ‚Äì G√°n Tr·ªçng S·ªë cho Seed Node
M·ª•c ti√™u:
 X√°c ƒë·ªãnh c√°c node ‚Äúg·ªëc‚Äù (seed node) t·ª´ query, v√† g√°n tr·ªçng s·ªë cho ch√∫ng nh·∫±m t·∫°o n·ªÅn t·∫£ng cho b∆∞·ªõc lan truy·ªÅn th√¥ng tin trong PPR.

---
‚ùì Ph√° 1:
Seed Node l√† g√¨?
‚úÖ ƒê√°p:
- Seed Node l√† nh·ªØng node trong KG m√† h·ªá th·ªëng x√°c ƒë·ªãnh c√≥ m·ªëi li√™n h·ªá tr·ª±c ti·∫øp v·ªõi query.
- Ch√∫ng l√† ƒëi·ªÉm kh·ªüi ƒë·∫ßu ƒë·ªÉ thu·∫≠t to√°n lan truy·ªÅn (PPR) ‚Äúkh√°m ph√°‚Äù c√°c node kh√°c c√≥ li√™n quan.
V√≠ d·ª•:
 N·∫øu c√¢u h·ªèi l√† ‚ÄúBill Gates h·ªçc ·ªü ƒë√¢u?‚Äù, seed node c√≥ th·ªÉ l√† ‚ÄúBill Gates‚Äù v√† ‚ÄúHarvard‚Äù n·∫øu ƒë√£ c√≥ triple li√™n quan.

---
‚ùì Ph√° 2:
T·∫°i sao c·∫ßn g√°n tr·ªçng s·ªë cho seed node?
‚úÖ ƒê√°p:
- Tr·ªçng s·ªë c√†ng cao, node ƒë√≥ s·∫Ω ·∫£nh h∆∞·ªüng m·∫°nh h∆°n trong qu√° tr√¨nh lan truy·ªÅn th√¥ng tin c·ªßa PageRank.
- Gi√∫p ∆∞u ti√™n c√°c node quan tr·ªçng, t·∫°o ra k·∫øt qu·∫£ retrieval ch√≠nh x√°c h∆°n.
V√≠ d·ª•:
 Gi·ªëng nh∆∞ khi b·∫°n ch·ªçn ƒëi·ªÉm m·∫•u ch·ªët trong s∆° ƒë·ªì t∆∞ duy, nh·ªØng ƒëi·ªÉm quan tr·ªçng ƒë∆∞·ª£c ƒë√°nh d·∫•u r√µ, ƒë·ªÉ khi ‚Äúƒëi theo s∆° ƒë·ªì‚Äù b·∫°n s·∫Ω ∆∞u ti√™n nh·ªØng ƒëi·ªÉm ƒë√≥.

---
7. PPR (Personalized PageRank) Graph Search ‚Äì T√¨m Ki·∫øm Trong Graph
M·ª•c ti√™u:
 S·ª≠ d·ª•ng thu·∫≠t to√°n PPR ƒë·ªÉ lan truy·ªÅn th√¥ng tin t·ª´ c√°c seed node v√† t√¨m ra c√°c node li√™n quan nh·∫•t trong KG.

---
‚ùì Ph√° 1:
Thu·∫≠t to√°n PageRank ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o v√† kh√°c g√¨ so v·ªõi t√¨m ki·∫øm th√¥ng th∆∞·ªùng?
‚úÖ ƒê√°p:
- PageRank ban ƒë·∫ßu ƒë∆∞·ª£c d√πng ƒë·ªÉ x·∫øp h·∫°ng c√°c trang web d·ª±a tr√™n s·ªë l∆∞·ª£ng v√† ch·∫•t l∆∞·ª£ng c·ªßa c√°c li√™n k·∫øt.
- Personalized PageRank (PPR) ƒëi·ªÅu ch·ªânh thu·∫≠t to√°n ƒë·ªÉ ∆∞u ti√™n c√°c node b·∫Øt ƒë·∫ßu t·ª´ seed node (ƒëi·ªÉm kh·ªüi ƒë·∫ßu t·ª´ query).
- Thay v√¨ ch·ªâ t√¨m ‚Äúƒëo·∫°n vƒÉn g·∫ßn nh·∫•t‚Äù, PPR lan truy·ªÅn tr·ªçng s·ªë qua c√°c li√™n k·∫øt trong graph, gi√∫p h·ªá th·ªëng reasoning multi-hop.
V√≠ d·ª•:
 Gi·ªëng nh∆∞ b·∫°n c√≥ m·ªôt b·∫£n ƒë·ªì m√™ cung v·ªõi c√°c l·ªëi ƒëi, PPR gi√∫p b·∫°n kh√¥ng ch·ªâ t√¨m l·ªëi ƒëi ng·∫Øn nh·∫•t m√† t√¨m l·ªëi ƒëi ‚Äúnhi·ªÅu d·∫•u hi·ªáu nh·∫•t‚Äù li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ c·ªßa c√¢u h·ªèi.

---
‚ùì Ph√° 2:
T·∫°i sao l·∫°i c·∫ßn b∆∞·ªõc n√†y khi c√≥ c√°c b∆∞·ªõc retrieval tr∆∞·ªõc ƒë√≥?
‚úÖ ƒê√°p:
- V√¨ PPR gi√∫p k·∫øt n·ªëi c√°c m·ªëi quan h·ªá trong KG, kh√¥ng ch·ªâ d·ª±a v√†o ƒë·ªô t∆∞∆°ng ƒë·ªìng ƒë∆°n thu·∫ßn m√† c√≤n hi·ªÉu ƒë∆∞·ª£c c·∫•u tr√∫c li√™n k·∫øt gi·ªØa c√°c th√¥ng tin.
- N√≥ cho ph√©p truy xu·∫•t c√°c th√¥ng tin li√™n quan t·ª´ xa trong graph, t·∫°o n√™n c√¢u tr·∫£ l·ªùi ch√≠nh x√°c cho c√°c c√¢u h·ªèi multi-hop.

---
8. QA Reader ‚Äì ƒê·ªçc v√† Sinh Ra C√¢u Tr·∫£ L·ªùi
M·ª•c ti√™u:
 Cu·ªëi c√πng, c√°c ƒëo·∫°n vƒÉn ƒë∆∞·ª£c l·∫•y ra (sau khi ƒë√£ qua c√°c b∆∞·ªõc x·ª≠ l√Ω) ƒë∆∞·ª£c ƒë∆∞a cho LLM ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi cu·ªëi c√πng cho query.

---
‚ùì Ph√° 1:
T·∫°i sao kh√¥ng s·ª≠ d·ª•ng tr·ª±c ti·∫øp c√°c triple l√†m c√¢u tr·∫£ l·ªùi?
‚úÖ ƒê√°p:
- Triple ch·ªâ l√† c√°c m·ªëi li√™n h·ªá r√∫t g·ªçn, ch∆∞a ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh c·∫ßn thi·∫øt ƒë·ªÉ tr·∫£ l·ªùi m·ªôt c√°ch m·∫°ch l·∫°c v√† t·ª± nhi√™n.
- Passage cung c·∫•p ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh, chi ti·∫øt gi√∫p LLM sinh ra c√¢u tr·∫£ l·ªùi trung th·ª±c, ƒë·∫ßy ƒë·ªß v√† t·ª± nhi√™n h∆°n.
V√≠ d·ª•:
 N·∫øu b·∫°n h·ªèi ‚ÄúBill Gates h·ªçc ·ªü ƒë√¢u?‚Äù, triple c√≥ th·ªÉ n√≥i ‚ÄúBill Gates ‚Äì h·ªçc t·∫°i ‚Äì Harvard‚Äù, nh∆∞ng passage c√≥ th·ªÉ cung c·∫•p th√™m chi ti·∫øt nh∆∞ ‚ÄúBill Gates t·ª´ng theo h·ªçc t·∫°i tr∆∞·ªùng Harvard, n∆°i √¥ng ƒë√£ ti·∫øp thu nhi·ªÅu ki·∫øn th·ª©c v·ªÅ kinh doanh v√† c√¥ng ngh·ªá‚Äù, gi√∫p c√¢u tr·∫£ l·ªùi phong ph√∫ h∆°n.

---
T·ªïng K·∫øt & Li√™n H·ªá:
M·ªói b∆∞·ªõc trong HippoRAG 2 ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ b√π tr·ª´ nh·ªØng ƒëi·ªÉm y·∫øu c·ªßa c√°c b∆∞·ªõc tr∆∞·ªõc v√† t·∫°o n√™n m·ªôt chu·ªói x·ª≠ l√Ω to√†n di·ªán:
- OpenIE: Tr√≠ch xu·∫•t ki·∫øn th·ª©c d·∫°ng triple gi√∫p ‚Äúghi nh·ªõ‚Äù c√°c m·ªëi quan h·ªá ch√≠nh.
- Synonym Detection: K·∫øt n·ªëi nh·ªØng kh√°i ni·ªám c√≥ √Ω nghƒ©a t∆∞∆°ng t·ª±, nh∆∞ vi·ªác n·ªëi c√°c t·ª´ ƒë·ªìng nghƒ©a trong t·ª´ ƒëi·ªÉn.
- Dense-Sparse Integration: Gi·ªØ ƒë∆∞·ª£c c·∫£ ‚Äúƒë·∫ßu d√≤ng‚Äù (phrase) v√† ‚Äún·ªôi dung chi ti·∫øt‚Äù (passage) ‚Äì gi·ªëng nh∆∞ ghi ch√∫ ng·∫Øn v√† s·ªï tay chi ti·∫øt.
- Triple Retrieval & Filtering: Nh·∫≠n ra c√°c ghi ch√∫ li√™n quan v√† lo·∫°i b·ªè nhi·ªÖu, nh∆∞ vi·ªác b·∫°n xem l·∫°i b√†i h·ªçc v√† ch·ªçn ra nh·ªØng ƒëi·ªÉm quan tr·ªçng.
- Seed Node Assignment & PPR Graph Search: Gi·ªëng nh∆∞ ch·ªçn c√°c ƒëi·ªÉm m·∫•u ch·ªët tr√™n s∆° ƒë·ªì t∆∞ duy r·ªìi ƒëi theo c√°c m·ªëi li√™n h·ªá ƒë·ªÉ t√¨m ra c√¢u tr·∫£ l·ªùi to√†n di·ªán.
- QA Reader: Cu·ªëi c√πng, t·ªïng h·ª£p c√°c th√¥ng tin ƒë√£ ƒë∆∞·ª£c l·ªçc v√† li√™n k·∫øt ƒë·ªÉ sinh ra c√¢u tr·∫£ l·ªùi t·ª± nhi√™n v√† ch√≠nh x√°c.

---
D∆∞·ªõi ƒë√¢y m√¨nh s·∫Ω ƒë√†o s√¢u v√†o kh√°i ni·ªám triple trong b∆∞·ªõc 1 c·ªßa HippoRAG (v√† HippoRAG 2), v·ªõi phong c√°ch ‚Äúm·ªôt ng∆∞·ªùi ph√° ‚Äì m·ªôt ng∆∞·ªùi tr·∫£ l·ªùi‚Äù ƒë·ªÉ l√†m r√µ t·ª´ng ƒëi·ªÉm.

---
1. Triple v√† Relation Edge
‚ùì Ph√°: Trong b∆∞·ªõc tr√≠ch xu·∫•t triple b·∫±ng OpenIE, c√°c triple ƒë∆∞·ª£c t·∫°o ra c√≥ ‚Äúh∆∞·ªõng‚Äù kh√¥ng?
Tr·∫£ l·ªùi:
- C√≥ h∆∞·ªõng:
 C√°c triple ƒë∆∞·ª£c tr√≠ch xu·∫•t theo ƒë·ªãnh d·∫°ng (subject, predicate, object) v·ªën ƒë√£ c√≥ th·ª© t·ª± r√µ r√†ng.
  - V√≠ d·ª•: t·ª´ c√¢u ‚ÄúMarie Curie won the Nobel Prize‚Äù ta c√≥ triple:
 (Marie Curie, won, Nobel Prize)
  - ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† m·ªëi quan h·ªá ‚Äúwon‚Äù ch·ªâ c√≥ √Ω nghƒ©a t·ª´ ‚ÄúMarie Curie‚Äù ƒë·∫øn ‚ÄúNobel Prize‚Äù.
- L√Ω do gi·ªØ h∆∞·ªõng:
  - Gi√∫p ph√¢n bi·ªát ƒë∆∞·ª£c ai l√†m g√¨ v·ªõi ai.
  - B·∫£o to√†n th·ª© t·ª± th√¥ng tin, r·∫•t quan tr·ªçng trong vi·ªác reasoning (v√≠ d·ª•: ‚Äúwon‚Äù kh√¥ng th·ªÉ ƒë·∫£o ng∆∞·ª£c th√†nh ‚ÄúNobel Prize won Marie Curie‚Äù n·∫øu kh√¥ng c√≥ h∆∞·ªõng).

---
‚ùì Ph√°: N·∫øu b·ªè ƒëi h∆∞·ªõng c·ªßa c√°c triple, li·ªáu c√≥ l√†m m·∫•t ƒëi th√¥ng tin quan tr·ªçng kh√¥ng?
Tr·∫£ l·ªùi:
- C√≥ m·∫•t:
  - H∆∞·ªõng c·ªßa edge x√°c ƒë·ªãnh m·ªëi li√™n h·ªá theo th·ª© t·ª± logic.
  - V√≠ d·ª•:
    - Triple: (Marie Curie, won, Nobel Prize) mang nghƒ©a r√µ r√†ng.
    - N·∫øu tr·ªü th√†nh edge kh√¥ng h∆∞·ªõng, ta ch·ªâ bi·∫øt ‚ÄúMarie Curie‚Äù v√† ‚ÄúNobel Prize‚Äù li√™n quan nhau nh∆∞ng kh√¥ng bi·∫øt ai l√† t√°c nh√¢n hay ƒë·ªëi t∆∞·ª£ng c·ªßa h√†nh ƒë·ªông.
- K·∫øt qu·∫£:
  - Vi·ªác gi·ªØ h∆∞·ªõng l√† c·∫ßn thi·∫øt ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c c·ªßa ng·ªØ nghƒ©a ban ƒë·∫ßu t·ª´ vƒÉn b·∫£n.

---
‚ùì Ph√°: Trong ƒë·ªì th·ªã ki·∫øn th·ª©c, vai tr√≤ c·ªßa relation edge c√≥ h∆∞·ªõng l√† g√¨ trong qu√° tr√¨nh reasoning?
Tr·∫£ l·ªùi:
- Gi√∫p ƒë·ªãnh h∆∞·ªõng th√¥ng tin:
  - Khi th·ª±c hi·ªán Personalized PageRank (PPR), h∆∞·ªõng c·ªßa edge gi√∫p x√°c ƒë·ªãnh lu·ªìng th√¥ng tin t·ª´ c√°c node g·ªëc (seed nodes) sang c√°c node li√™n quan.
- H·ªó tr·ª£ multi-hop reasoning:
  - V√≠ d·ª•: t·ª´ node ‚ÄúMarie Curie‚Äù qua edge ‚Äúwon‚Äù ƒë·∫øn ‚ÄúNobel Prize‚Äù, sau ƒë√≥ n·∫øu c√≥ edge kh√°c n·ªëi ‚ÄúNobel Prize‚Äù v·ªõi ‚ÄúPhysics‚Äù (n·∫øu ƒë∆∞·ª£c tr√≠ch xu·∫•t th√™m), h·ªá th·ªëng s·∫Ω hi·ªÉu ƒë∆∞·ª£c m·ªëi li√™n h·ªá c√≥ th·ª© t·ª± gi·ªØa c√°c kh√°i ni·ªám.
- Gi·ªØ nguy√™n ng·ªØ nghƒ©a:
  - B·∫±ng c√°ch kh√¥ng ƒë·∫£o ng∆∞·ª£c m·ªëi quan h·ªá, m√¥ h√¨nh c√≥ th·ªÉ truy xu·∫•t c√°c th√¥ng tin li√™n quan ch√≠nh x√°c h∆°n.

---
2. Synonym Edge
‚ùì Ph√°: Synonym edge ƒë∆∞·ª£c t·∫°o ra d·ª±a tr√™n thu·∫≠t to√°n g√¨?
Tr·∫£ l·ªùi:
- D·ª±a tr√™n cosine similarity trong embedding space:
  - M·ªói phrase (hay c·ª•m t·ª´) ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng vector.
  - N·∫øu cosine similarity gi·ªØa hai vector v∆∞·ª£t qua m·ªôt ng∆∞·ª°ng ƒë·ªãnh tr∆∞·ªõc, th√¨ hai node ƒë√≥ ƒë∆∞·ª£c n·ªëi b·∫±ng m·ªôt synonym edge.
- ƒê·∫∑c ƒëi·ªÉm c·ªßa synonym edge:
  - Kh√¥ng c√≥ h∆∞·ªõng: V√¨ n·∫øu hai t·ª´ l√† ƒë·ªìng nghƒ©a, m·ªëi quan h·ªá c·ªßa ch√∫ng th∆∞·ªùng l√† ƒë·ªëi x·ª©ng (v√≠ d·ª•: ‚Äúteacher‚Äù v√† ‚Äúinstructor‚Äù c√≥ th·ªÉ ƒë∆∞·ª£c n·ªëi v·ªõi nhau m√† kh√¥ng c·∫ßn ph√¢n bi·ªát th·ª© t·ª±).

---
‚ùì Ph√°: T·∫°i sao l·∫°i s·ª≠ d·ª•ng thu·∫≠t to√°n embedding cho synonym thay v√¨ d√πng LLM tr·ª±c ti·∫øp?
Tr·∫£ l·ªùi:
- Hi·ªáu qu·∫£ v√† t·ªëc ƒë·ªô:
  - T√≠nh to√°n cosine similarity gi·ªØa vector nhanh v√† hi·ªáu qu·∫£, kh√¥ng c·∫ßn g·ªçi LLM nhi·ªÅu l·∫ßn.
- ƒê·ªô ch√≠nh x√°c cao:
  - C√°c m√¥ h√¨nh embedding hi·ªán nay c√≥ kh·∫£ nƒÉng b·∫Øt ƒë∆∞·ª£c ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a r·∫•t t·ªët, cho n√™n ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a l√† kh√° ch√≠nh x√°c.
- Gi·∫£m t·∫£i cho LLM:
  - Gi·∫£m chi ph√≠ t√≠nh to√°n, v√¨ kh√¥ng c·∫ßn LLM ph·∫£i x·ª≠ l√Ω t·∫•t c·∫£ c√°c c·∫∑p t·ª´ trong corpus.

---
3. Seed Node
‚ùì Ph√°: Seed node l√† g√¨ trong b·ªëi c·∫£nh c·ªßa HippoRAG?
Tr·∫£ l·ªùi:
- Seed Node:
  - L√† nh·ªØng node trong ki·∫øn th·ª©c ƒë·ªì th·ªã ƒë∆∞·ª£c ch·ªçn l√†m ƒëi·ªÉm kh·ªüi ƒë·∫ßu cho qu√° tr√¨nh t√¨m ki·∫øm v√† lan truy·ªÅn th√¥ng tin (th√¥ng qua PPR) khi c√≥ query.
- Vai tr√≤ c·ªßa Seed Node:
  - Ch√∫ng ƒë·∫°i di·ªán cho c√°c th√†nh ph·∫ßn quan tr·ªçng li√™n quan ƒë·∫øn c√¢u h·ªèi ƒë∆∞·ª£c ƒë·∫∑t ra.
  - V√≠ d·ª•: N·∫øu c√¢u h·ªèi l√† ‚ÄúBill Gates h·ªçc ·ªü ƒë√¢u?‚Äù, c√°c node ch·ª©a ‚ÄúBill Gates‚Äù ho·∫∑c c√°c th√¥ng tin li√™n quan s·∫Ω ƒë∆∞·ª£c ch·ªçn l√†m seed.

---
‚ùì Ph√°: T·∫°i sao ph·∫£i g√°n tr·ªçng s·ªë (seed node weights) cho c√°c Seed Node?
Tr·∫£ l·ªùi:
- G√°n tr·ªçng s·ªë:
  - Gi√∫p h·ªá th·ªëng bi·∫øt ƒë∆∞·ª£c nh·ªØng node n√†o quan tr·ªçng h∆°n khi th·ª±c hi·ªán lan truy·ªÅn th√¥ng tin trong ƒë·ªì th·ªã.
- L·ª£i √≠ch:
  - C√°c Seed Node c√≥ tr·ªçng s·ªë cao s·∫Ω ‚Äúph√°t s√°ng‚Äù h∆°n trong qu√° tr√¨nh PPR, t·ª´ ƒë√≥ gi√∫p t√¨m ra c√°c node li√™n quan t·ªët h∆°n, ƒë·∫£m b·∫£o c√¢u tr·∫£ l·ªùi ƒë√∫ng v√† ƒë·∫ßy ƒë·ªß.
- V√≠ d·ª• ƒë∆°n gi·∫£n:
  - Trong m·ªôt b·∫£n ƒë·ªì t∆∞ duy, n·∫øu b·∫°n ƒë√°nh d·∫•u m·ªôt s·ªë √Ω ch√≠nh v·ªõi b√∫t d·∫°, nh·ªØng √Ω n√†y s·∫Ω ƒë∆∞·ª£c ch√∫ √Ω nhi·ªÅu h∆°n khi b·∫°n t·ªïng h·ª£p l·∫°i ki·∫øn th·ª©c.

---
4. Relation Edge v√† C√°c Kh√°i Ni·ªám Kh√°c
‚ùì Ph√°: Relation edge c√≥ h∆∞·ªõng nh∆∞ th·∫ø n√†o?
Tr·∫£ l·ªùi:
- Relation Edge:
  - L√† c√°c c·∫°nh n·ªëi gi·ªØa c√°c node trong KG, ƒë∆∞·ª£c t·∫°o ra t·ª´ c√°c triple (subject, predicate, object).
  - Ch√∫ng c√≥ h∆∞·ªõng, t·ª´ subject sang object, th√¥ng qua predicate.
- √ù nghƒ©a c·ªßa h∆∞·ªõng:
  - Gi√∫p x√°c ƒë·ªãnh ƒë√∫ng m·ªëi li√™n h·ªá ng·ªØ nghƒ©a (ai l√†m g√¨ v·ªõi g√¨).

---
‚ùì Ph√°: Ngo√†i c√°c kh√°i ni·ªám tr√™n, c√≤n kh√°i ni·ªám n√†o kh√°c c·∫ßn gi·∫£i th√≠ch chi ti·∫øt trong pipeline kh√¥ng?
Tr·∫£ l·ªùi:
- Context Edge:
  - N·ªëi c√°c Passage Node v·ªõi c√°c Phrase Node m√† ch√∫ng ch·ª©a ƒë·ª±ng.
  - √ù nghƒ©a: Gi√∫p gi·ªØ l·∫°i ng·ªØ c·∫£nh chi ti·∫øt c·ªßa vƒÉn b·∫£n, h·ªó tr·ª£ factual QA.
- Dense‚ÄìSparse Integration:
  - S·ª± k·∫øt h·ª£p gi·ªØa sparse coding (phrase node) v√† dense coding (passage node) nh·∫±m ƒë·∫£m b·∫£o v·ª´a c√≥ √Ω ch√≠nh t√≥m t·∫Øt, v·ª´a c√≥ ƒë·∫ßy ƒë·ªß chi ti·∫øt.
  - V√≠ d·ª• h·ªçc sinh: Nh∆∞ ƒë√£ n√≥i, gi·ªëng nh∆∞ v·ª´a ghi nh·ªõ ti√™u ƒë·ªÅ b√†i h·ªçc (sparse) v√† v·ª´a c√≥ to√†n b·ªô b√†i gi·∫£ng (dense).

---
K·∫æT LU·∫¨N
M·ªói kh√°i ni·ªám trong pipeline c·ªßa HippoRAG 2 ƒë·ªÅu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ b·∫£o to√†n v√† khai th√°c th√¥ng tin m·ªôt c√°ch t·ªëi ∆∞u:
- Triple extraction v·ªõi relation edge c√≥ h∆∞·ªõng gi·ªØ tr·∫≠t t·ª± v√† ng·ªØ nghƒ©a ban ƒë·∫ßu.
- Synonym edge (kh√¥ng c√≥ h∆∞·ªõng) n·ªëi c√°c c·ª•m t·ª´ c√≥ nghƒ©a t∆∞∆°ng ƒë·ªìng, gi√∫p li√™n k·∫øt th√¥ng tin gi·ªØa c√°c ƒëo·∫°n vƒÉn kh√°c nhau.
- Seed Node l√† c√°c ƒëi·ªÉm kh·ªüi ƒë·∫ßu quan tr·ªçng ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√° tr√¨nh t√¨m ki·∫øm th√¥ng tin trong KG, v√† vi·ªác g√°n tr·ªçng s·ªë cho ch√∫ng gi√∫p tƒÉng c∆∞·ªùng kh·∫£ nƒÉng lan truy·ªÅn th√¥ng tin trong qu√° tr√¨nh Personalized PageRank.
- Context edge v√† dense-sparse integration gi√∫p m√¥ h√¨nh kh√¥ng ch·ªâ ghi nh·ªõ √Ω ch√≠nh m√† c√≤n gi·ªØ nguy√™n ng·ªØ c·∫£nh chi ti·∫øt c·ªßa vƒÉn b·∫£n g·ªëc.
Qua t·ª´ng c√¢u h·ªèi ph·∫£n bi·ªán, ch√∫ng ta nh·∫≠n th·∫•y r·∫±ng vi·ªác duy tr√¨ h∆∞·ªõng trong c√°c relation edge l√† r·∫•t quan tr·ªçng cho vi·ªác x√¢y d·ª±ng m·ªôt ƒë·ªì th·ªã ki·∫øn th·ª©c ch√≠nh x√°c, t·ª´ ƒë√≥ h·ªó tr·ª£ qu√° tr√¨nh reasoning v√† retrieval trong to√†n h·ªá th·ªëng HippoRAG.
Ch√∫ng ta h√£y c√πng ph√¢n t√≠ch theo ki·ªÉu "m·ªôt ng∆∞·ªùi ph√° ‚Äì m·ªôt ng∆∞·ªùi tr·∫£ l·ªùi" ƒë·ªÉ hi·ªÉu s√¢u v·ªÅ c√°ch th√™m passage node sau khi ƒë√£ t√°ch ra b·ªô ba (triple) theo b√†i b√°o.

---
‚ùì Ph√°: Sau khi ƒë√£ s·ª≠ d·ª•ng OpenIE t√°ch ra c√°c triple (subject, predicate, object) t·ª´ m·ªói ƒëo·∫°n vƒÉn, l√†m th·∫ø n√†o ƒë·ªÉ "append" th√™m ph·∫ßn passage node v√†o ƒë·ªì th·ªã ki·∫øn th·ª©c?
Tr·∫£ l·ªùi:
 Theo b√†i b√°o, sau khi tr√≠ch xu·∫•t c√°c triple t·ª´ m·ªói passage, ta ti·∫øn h√†nh x√¢y d·ª±ng m·ªôt Knowledge Graph (KG) b·∫±ng c√°ch th·ª±c hi·ªán b∆∞·ªõc sau:
1. T·∫°o Phrase Nodes t·ª´ Triple:
  - M·ªói triple (v√≠ d·ª•: (Marie Curie, won, Nobel Prize)) s·∫Ω t·∫°o ra c√°c phrase nodes cho subject, predicate v√† object.
  - ƒê√¢y l√† c√°c n√∫t ƒë·∫°i di·ªán cho c√°c kh√°i ni·ªám tr·ª´u t∆∞·ª£ng c·ªßa th√¥ng tin.
2. Th√™m Passage Node:
  - M·ªói passage g·ªëc trong corpus ƒë∆∞·ª£c xem nh∆∞ m·ªôt passage node ri√™ng bi·ªát.
  - Sau ƒë√≥, append ph·∫ßn passage node n√†y v√†o ƒë·ªì th·ªã b·∫±ng c√°ch n·ªëi n√≥ v·ªõi t·∫•t c·∫£ c√°c phrase node ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ passage ƒë√≥.
  - C√°c k·∫øt n·ªëi n√†y th∆∞·ªùng ƒë∆∞·ª£c g·ªçi l√† context edge (ho·∫∑c edge "contains") v√¨ ch√∫ng bi·ªÉu th·ªã r·∫±ng passage ch·ª©a ƒë·ª±ng c√°c th√¥ng tin hay kh√°i ni·ªám ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi phrase nodes.

---
‚ùì Ph√°: T·∫°i sao c·∫ßn th√™m passage node? Kh√¥ng ƒë·ªß ch·ªâ c√≥ phrase nodes sao?
Tr·∫£ l·ªùi:
- B·∫£o to√†n ng·ªØ c·∫£nh:
  - Phrase nodes cung c·∫•p m·ªôt d·∫°ng tr·ª´u t∆∞·ª£ng, t·ªïng qu√°t c·ªßa th√¥ng tin. Tuy nhi√™n, th√¥ng tin n√†y c√≥ th·ªÉ m·∫•t ƒëi chi ti·∫øt ng·ªØ c·∫£nh quan tr·ªçng.
  - B·∫±ng c√°ch th√™m passage node, h·ªá th·ªëng l∆∞u gi·ªØ ƒë∆∞·ª£c to√†n b·ªô th√¥ng tin chi ti·∫øt c·ªßa ƒëo·∫°n vƒÉn g·ªëc.
- Dense‚ÄìSparse Integration:
  - Passage node ƒë·∫°i di·ªán cho ph·∫ßn dense coding (ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh, chi ti·∫øt), trong khi phrase nodes ƒë·∫°i di·ªán cho sparse coding (kh√°i qu√°t, d·ªÖ d√†ng reasoning).
  - Vi·ªác k·∫øt h·ª£p c·∫£ hai gi√∫p m√¥ h√¨nh c√≥ th·ªÉ th·ª±c hi·ªán t·ªët h∆°n trong vi·ªác tr·∫£ l·ªùi c√¢u h·ªèi y√™u c·∫ßu v·ª´a hi·ªÉu ƒë∆∞·ª£c √Ω ch√≠nh v·ª´a n·∫Øm b·∫Øt ƒë∆∞·ª£c chi ti·∫øt.

---
‚ùì Ph√°: C√°ch th·ª©c n·ªëi gi·ªØa passage node v√† phrase nodes ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o?
Tr·∫£ l·ªùi:
- M·ªói passage node s·∫Ω ƒë∆∞·ª£c n·ªëi v·ªõi t·∫•t c·∫£ c√°c phrase node ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ ch√≠nh passage ƒë√≥.
- C√°c k·∫øt n·ªëi n√†y (context edges) kh√¥ng c·∫ßn c√≥ h∆∞·ªõng r√µ r√†ng nh∆∞ c√°c relation edge c·ªßa triple, m√† ch·ªß y·∫øu nh·∫±m m·ª•c ƒë√≠ch th·ªÉ hi·ªán m·ªëi li√™n h·ªá v·ªÅ ng·ªØ c·∫£nh.
- Khi th·ª±c hi·ªán retrieval ho·∫∑c Personalized PageRank (PPR), passage node gi√∫p tƒÉng tr·ªçng s·ªë cho c√°c ƒëo·∫°n vƒÉn ch·ª©a ƒë·ª±ng th√¥ng tin chi ti·∫øt, t·ª´ ƒë√≥ c·∫£i thi·ªán kh·∫£ nƒÉng truy xu·∫•t v√† hi·ªÉu s√¢u c·ªßa m√¥ h√¨nh.

---
‚ùì Ph√°: T·∫°i sao kh√¥ng ch·ªâ d√πng phrase nodes m√† b·ªè qua passage node?
Tr·∫£ l·ªùi:
- N·∫øu ch·ªâ s·ª≠ d·ª•ng phrase nodes, ta ch·ªâ c√≥ ƒë∆∞·ª£c m·ªôt b·∫£n t√≥m t·∫Øt r·∫•t c√¥ ƒë·ªçng c·ªßa th√¥ng tin.
- Nhi·ªÅu tr∆∞·ªùng h·ª£p, ƒë·∫∑c bi·ªát l√† c√°c c√¢u h·ªèi factual hay y√™u c·∫ßu s·ª± li√™n k·∫øt s√¢u gi·ªØa c√°c √Ω, ƒë√≤i h·ªèi ph·∫£i c√≥ ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß t·ª´ ƒëo·∫°n vƒÉn g·ªëc.
- Passage node gi√∫p m√¥ h√¨nh truy xu·∫•t c√°c ƒëo·∫°n vƒÉn g·ªëc ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng m·∫•t ƒëi c√°c chi ti·∫øt c·∫ßn thi·∫øt cho vi·ªác tr·∫£ l·ªùi ch√≠nh x√°c.

---
T·ªïng K·∫øt
- B∆∞·ªõc 1: T√°ch c√°c triple (subject, predicate, object) t·ª´ m·ªói passage b·∫±ng OpenIE c·ªßa LLM ‚Üí t·∫°o ra c√°c phrase nodes.
- B∆∞·ªõc 2: V·ªõi m·ªói passage g·ªëc, t·∫°o passage node.
- B∆∞·ªõc 3: Append passage node v√†o ƒë·ªì th·ªã b·∫±ng c√°ch n·ªëi n√≥ v·ªõi t·∫•t c·∫£ c√°c phrase node t·ª´ passage ƒë√≥ th√¥ng qua context edges.
- M·ª•c ƒë√≠ch: Gi·ªØ l·∫°i c·∫£ √Ω ch√≠nh (phrase node) v√† th√¥ng tin chi ti·∫øt (passage node) ƒë·ªÉ h·ªó tr·ª£ qu√° tr√¨nh retrieval v√† reasoning, t·ª´ ƒë√≥ c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa to√†n h·ªá th·ªëng.
D∆∞·ªõi ƒë√¢y l√† c√°ch ti·∫øp c·∫≠n chi ti·∫øt theo d·∫°ng h·ªèi ‚Äì ƒë√°p ph·∫£n bi·ªán (m·ªôt ng∆∞·ªùi ‚Äúph√°‚Äù, m·ªôt ng∆∞·ªùi ‚Äúb√™nh‚Äù) ƒë·ªÉ m·ªï x·∫ª th·∫≠t s√¢u 5 b∆∞·ªõc trong Online Retrieval & QA c·ªßa HippoRAG/HippoRAG 2. M·ªói c√¢u tr·∫£ l·ªùi ƒë·ªÅu c·ªë g·∫Øng g·∫Øn v·ªõi l·∫≠p lu·∫≠n t·ª´ b√†i b√°o ho·∫∑c √Ω t∆∞·ªüng ch√≠nh (c√≥ d·∫´n ch·ª©ng).

---
(1) Retrieving Passages and Triples
Ph·∫£n bi·ªán (Q):
T·∫°i sao ph·∫£i l·∫•y c·∫£ c√°c ƒëo·∫°n vƒÉn (passages) v√† c√°c b·ªô ba (triples)? Li·ªáu ch·ªâ c·∫ßn retrieval embedding t·ªët ƒë·ªÉ l·∫•y ƒëo·∫°n vƒÉn ƒë√£ ƒë·ªß ch∆∞a?
Tr·∫£ l·ªùi (A):
- C·∫ßn c·∫£ passages: v√¨ c√°c ƒëo·∫°n vƒÉn mang ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh. N·∫øu ch·ªâ d·ª±a v√†o node (th∆∞·ªùng l√† m·ªôt c√¢u/triple r·ªùi r·∫°c) th√¨ c√≥ th·ªÉ thi·∫øu m·∫•t chi ti·∫øt quan tr·ªçng.
- C·∫ßn c·∫£ triples: v√¨ triple l√† ‚Äúƒë∆°n v·ªã th√¥ng tin‚Äù mang t√≠nh kh√°i qu√°t ‚Äì gi√∫p m√¥ ph·ªèng s·ª± li√™n k·∫øt (associativity). Khi ta k√©o ƒë∆∞·ª£c triple li√™n quan, n√≥ c√≥ th·ªÉ ‚Äúg·ª£i √Ω‚Äù nhi·ªÅu quan h·ªá s√¢u h∆°n m√† m·ªôt ƒëo·∫°n vƒÉn ch∆∞a ch·∫Øc ƒë√£ hi·ªÉn th·ªã ngay.
- D·∫´n ch·ª©ng: Ph·∫ßn ‚ÄúOnline Retrieval‚Äù trong b√†i HippoRAG/HippoRAG 2 cho th·∫•y, h·ªá th·ªëng ch·∫°y query-to-triple ƒë·ªÉ l·∫•y triple tr∆∞·ªõc, sau ƒë√≥ m·ªõi ch·∫°y ti·∫øp c√°c b∆∞·ªõc kh√°c. K·∫øt h·ª£p ‚Äúpassage node‚Äù v√† ‚Äúphrase node‚Äù (triple) trong ƒë·ªì th·ªã l√† c√°ch m√† m√¥ h√¨nh ‚Äútr·ªôn‚Äù gi·ªØa ‚Äúcontext‚Äù v√† ‚Äúconcepts‚Äù.

---
(2) Recognition Memory (Triple Filtering)
Ph·∫£n bi·ªán (Q):
T·∫°i sao l·∫°i c·∫ßn m·ªôt b∆∞·ªõc g·ªçi l√† ‚ÄúRecognition Memory‚Äù ƒë·ªÉ l·ªçc triple? Ch·∫≥ng ph·∫£i retrieval encoder ƒë√£ x·∫øp h·∫°ng ƒë∆∞·ª£c r·ªìi sao?
Tr·∫£ l·ªùi (A):
- Encoder-based ranking ch·ªâ d·ª±a tr√™n vector similarity, c√≥ th·ªÉ nh·∫∑t ph·∫£i nh·ªØng triple ‚Äúna n√°‚Äù v·ªÅ t·ª´ v·ª±ng nh∆∞ng kh√¥ng th·∫≠t s·ª± li√™n quan ng·ªØ nghƒ©a.
- LLM-based filtering (Recognition Memory) gi·ªëng nh∆∞ ‚Äúduy·ªát l·∫°i‚Äù nh·ªØng triple top-k, ƒë·ªÉ x√°c nh·∫≠n: ‚ÄúTriple n√†y th·ª±c s·ª± n√≥i v·ªÅ c√°i g√¨, c√≥ ƒÉn kh·ªõp v·ªõi c√¢u h·ªèi kh√¥ng?‚Äù ‚Äì T·ª´ ƒë√≥ lo·∫°i b·ªõt nhi·ªÖu.
- V√≠ d·ª•: C√≥ nh·ªØng triple m√† entity gi·ªëng nhau v·ªÅ m·∫∑t ng√¥n ng·ªØ (v√≠ d·ª• ‚ÄúParis‚Äù ‚Äì m·ªôt ng∆∞·ªùi t√™n Paris vs. th√†nh ph·ªë Paris). N·∫øu ch·ªâ d√πng embedding, ƒë√¥i khi b·ªã l·∫´n. B∆∞·ªõc filtering n√†y gi√∫p LLM d√πng ng·ªØ c·∫£nh ƒë·ªÉ ph√¢n bi·ªát.
- D·∫´n ch·ª©ng: B√†i b√°o gi·ªõi thi·ªáu c∆° ch·∫ø ‚Äútwo-step retrieval‚Äù: 1) Query-to-triple (embedding) v√† 2) Triple filter (LLM) ‚Äì m√¥ ph·ªèng ‚Äúrecall vs. recognition‚Äù trong tr√≠ nh·ªõ con ng∆∞·ªùi.

---
(3) Assigning Seed Node Weights
Ph·∫£n bi·ªán (Q):
Ph√¢n ph·ªëi tr·ªçng s·ªë (reset probability) cho c√°c ‚Äúseed node‚Äù ‚Äì ƒë·∫∑c bi·ªát l√† gi·ªØa ‚Äúpassage node‚Äù v√† ‚Äúphrase node‚Äù ‚Äì c√≥ th·ª±c s·ª± c·∫ßn thi·∫øt, hay ch·ªâ c·∫ßn g√°n ƒë·ªÅu l√† ƒë∆∞·ª£c?
Tr·∫£ l·ªùi (A):
- Kh√¥ng th·ªÉ g√°n ƒë·ªÅu: B·ªüi v√¨ ‚Äúphrase node‚Äù (c√°c kh√°i ni·ªám) v√† ‚Äúpassage node‚Äù (c√°c vƒÉn b·∫£n) ƒë√≥ng vai tr√≤ kh√°c nhau. N·∫øu g√°n tr·ªçng s·ªë gi·ªëng nhau, m√¥ h√¨nh c√≥ th·ªÉ nghi√™ng qu√° nhi·ªÅu v·ªÅ concept ho·∫∑c qu√° nhi·ªÅu v·ªÅ context.
- C·∫ßn c√¢n b·∫±ng: Ch·∫≥ng h·∫°n, khi c√¢u h·ªèi r·∫•t c·ª• th·ªÉ (v√≠ d·ª•: ‚ÄúT√°c gi·∫£ cu·ªën s√°ch X l√† ai?‚Äù), ‚Äúphrase node‚Äù v·ªÅ ‚Äúbook_author‚Äù c√≥ th·ªÉ quan tr·ªçng h∆°n. Nh∆∞ng n·∫øu c√¢u h·ªèi ph·ª©c t·∫°p, ‚Äúpassage node‚Äù c√≥ ng·ªØ c·∫£nh r·ªông h∆°n s·∫Ω c·∫ßn tr·ªçng s·ªë cao h∆°n ƒë·ªÉ PPR lan t·ªèa nhi·ªÅu h∆°n.
- D·∫´n ch·ª©ng: T√°c gi·∫£ c√≥ l√†m ablation study (v√≠ d·ª• trong ph·∫ßn Implementation & Discussion) ƒë·ªÉ ch·ª©ng minh ph·∫£i ch·ªçn ‚Äúweight factor‚Äù cho passage node h·ª£p l√Ω (0.01, 0.05, v.v.) th√¨ m·ªõi t·ªëi ∆∞u Recall@5.

---
(4) PPR Graph Search
Ph·∫£n bi·ªán (Q):
Personalized PageRank (PPR) th·ª±c s·ª± gi√∫p ƒë∆∞·ª£c g√¨? Li·ªáu m·ªôt v√≤ng l·∫∑p iterative retrieval (nh∆∞ IRCoT) kh√¥ng ph·∫£i ƒë√£ gi·∫£i quy·∫øt ƒë∆∞·ª£c multi-hop sao?
Tr·∫£ l·ªùi (A):
- PPR l√†m multi-hop ngay trong m·ªôt l∆∞·ª£t: Thay v√¨ ph·∫£i g·ªçi LLM li√™n ti·∫øp ƒë·ªÉ ‚Äúg·ª£i √Ω‚Äù truy v·∫•n ph·ª• (iterative retrieval), PPR cho ph√©p khu·∫øch t√°n x√°c su·∫•t t·ª´ c√°c seed node (passage/phrase) ƒë·∫øn l√¢n c·∫≠n trong KG.
- Hi·ªáu qu·∫£ chi ph√≠: Iterative retrieval ki·ªÉu IRCoT th∆∞·ªùng t·ªën nhi·ªÅu l∆∞·ª£t g·ªçi LLM. PPR l√† m·ªôt thu·∫≠t to√°n ƒë·ªì th·ªã ch·∫°y n·ªôi b·ªô, r·∫ª h∆°n v·ªÅ m·∫∑t inference.
- Ph√π h·ª£p v·ªõi ‚Äúassociative memory‚Äù: B√†i b√°o g·ª£i √Ω r·∫±ng PPR m√¥ ph·ªèng c√°ch hippocampus (n√£o b·ªô) k√≠ch ho·∫°t c√°c li√™n k·∫øt gi·ªØa kh√°i ni·ªám. T·ª´ 1 seed node, lan ra nhi·ªÅu n√∫t kh√°c theo x√°c su·∫•t.
- D·∫´n ch·ª©ng: HippoRAG/HippoRAG 2 so s√°nh v·ªõi IRCoT, cho th·∫•y PPR c√≥ th·ªÉ ‚Äúg√≥i‚Äù vi·ªác multi-hop reasoning trong m·ªôt l·∫ßn retrieval, nhanh h∆°n v√† √≠t t·ªën chi ph√≠ h∆°n.

---
(5) QA Reading with Selected Passages
Ph·∫£n bi·ªán (Q):
N·∫øu cu·ªëi c√πng v·∫´n ph·∫£i g·ªçi LLM ƒë·ªÉ ƒë·ªçc l·∫°i c√°c ƒëo·∫°n vƒÉn (QA reading), v·∫≠y vi·ªác x√¢y d·ª±ng KG hay PPR c√≥ th·∫≠t s·ª± quan tr·ªçng? Ch·∫≥ng ph·∫£i LLM gi·ªù ƒë√£ ‚Äús·∫µn s√†ng‚Äù tr·∫£ l·ªùi n·∫øu cho n√≥ ƒë·ªß context?
Tr·∫£ l·ªùi (A):
- LLM c·∫ßn ‚Äúƒë·∫ßu v√†o s·∫°ch‚Äù: D√π LLM m·∫°nh, nh∆∞ng n·∫øu input context b·ªã nhi·ªÖu (ch·ªçn sai passage) ho·∫∑c qu√° d√†n tr·∫£i, LLM d·ªÖ tr·∫£ l·ªùi sai, ƒë·∫∑c bi·ªát v·ªõi c√¢u h·ªèi c·∫ßn multi-hop.
- KG + PPR gi√∫p ‚Äút√≥m g·ªçn‚Äù ƒë√∫ng ƒëo·∫°n vƒÉn li√™n quan, thay v√¨ l√¥i t·∫•t c·∫£. LLM c√†ng c√≥ context ch√≠nh x√°c, tr·∫£ l·ªùi c√†ng t·ªët.
- T√≠nh m·ªü r·ªông: Khi t·∫≠p d·ªØ li·ªáu r·∫•t l·ªõn, ch·ªâ embedding retrieval c√≥ th·ªÉ b·ªè s√≥t ‚Äúchu·ªói logic‚Äù li√™n quan. B·ªï sung KG + PPR => M√¥ h√¨nh ‚Äúli√™n k·∫øt‚Äù t·ªët h∆°n, LLM ƒë·ª° b·ªã qu√° t·∫£i.
- D·∫´n ch·ª©ng: K·∫øt qu·∫£ th·ª±c nghi·ªám (b·∫£ng so s√°nh F1, EM) cho th·∫•y HippoRAG 2 sau khi ch·ªçn top passages qua PPR, r·ªìi cho LLM ƒë·ªçc, cho k·∫øt qu·∫£ cao h∆°n so v·ªõi retrieval th∆∞·ªùng.

---
T·ªïng k·∫øt
5 b∆∞·ªõc trong Online Retrieval & QA ƒë∆∞·ª£c thi·∫øt k·∫ø c√≥ ch·ªß ƒë√≠ch, m√¥ ph·ªèng c∆° ch·∫ø b·ªô nh·ªõ c·ªßa con ng∆∞·ªùi:
1. L·∫•y passage + triple song song (ƒë·∫£m b·∫£o v·ª´a c√≥ context v·ª´a c√≥ concept).
2. L·ªçc triple b·∫±ng LLM (Recognition Memory) ƒë·ªÉ lo·∫°i nhi·ªÖu.
3. G√°n tr·ªçng s·ªë seed node c·∫©n th·∫≠n (c√¢n b·∫±ng passage node v√† phrase node).
4. Ch·∫°y PPR ƒë·ªÉ th·ª±c hi·ªán multi-hop reasoning trong m·ªôt b∆∞·ªõc, ti·∫øt ki·ªám chi ph√≠ so v·ªõi iterative retrieval.
5. Cu·ªëi c√πng, LLM m·ªõi ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë√£ ch·ªçn ƒë·ªÉ tr·∫£ l·ªùi, t·ªëi ∆∞u ƒë·ªô ch√≠nh x√°c.
T·ª´ng b∆∞·ªõc ƒë·ªÅu c√≥ l√Ω do t·ªìn t·∫°i, v√† k·∫øt qu·∫£ th·ª±c nghi·ªám ch·ª©ng minh m√¥ h√¨nh n√†y tƒÉng hi·ªáu qu·∫£ truy xu·∫•t (retrieval) c≈©ng nh∆∞ ƒë·ªô ch√≠nh x√°c khi tr·∫£ l·ªùi c√¢u h·ªèi (QA).

---
(3) Assigning Seed Node Weights
ƒê·∫∑t c√¢u h·ªèi ph√° nhi·ªÅu h∆°n ·ªü b∆∞·ªõc n√†y 
- Ch·∫≥ng h·∫°n: l√∫c m√† x√¢y Graph c√°c nodes vai tr√≤ nh∆∞ nhau hay l√† Hippo RAG 2 c√≥ ƒë√°nh d·∫•u r√µ l√† phrase node hay passage node ? 
- C√°ch ƒë√°nh Assing Seed Node Weights nh∆∞ n√†o trong b√†i b√°o c√≥ n√≥i ko 
- ... nhi·ªÅu c√¢u n·ªØa nh√©

√Ω l√† ph√° 1, m√¨nh bi·∫øt l√† n√≥ ƒë∆∞·ª£c r√µ r√†ng vai tr√≤, nh∆∞ng l√∫c ƒë∆∞a l√™n ƒë·ªì th·ªã √Ω th√¨ n√≥ c√≥ ƒë∆∞·ª£c r√µ r√†ng v·∫≠y kh√¥ng ? ki·ªÉu h·ªç d√πng c√°ch n√†o ƒë·ªÉ ƒë√°nh d·∫•u 2 lo·∫°i node ƒë√≥ ƒë·ªÉ khi query d√πng

Trong qu√° tr√¨nh x√¢y d·ª±ng ƒë·ªì th·ªã, HippoRAG 2 kh√¥ng ch·ªâ tr√≠ch xu·∫•t c√°c node m√† c√≤n "ƒë√°nh d·∫•u" r√µ r√†ng t·ª´ng lo·∫°i node th√¥ng qua c·∫•u tr√∫c v√† c√°c lo·∫°i c·∫°nh n·ªëi k·∫øt:
- Ph√¢n lo·∫°i qua ngu·ªìn g·ªëc t·∫°o node:
  - Phrase node: ƒê∆∞·ª£c t·∫°o ra t·ª´ vi·ªác tr√≠ch xu·∫•t b·ªô ba (triple) c·ªßa c√°c kh√°i ni·ªám qua OpenIE. Nh·ªØng node n√†y ch·ªâ ch·ª©a th√¥ng tin "concept" r√∫t g·ªçn v√† li√™n quan ƒë·∫øn c√°c m·ªëi quan h·ªá gi·ªØa th·ª±c th·ªÉ.
  - Passage node: ƒê·∫°i di·ªán cho to√†n b·ªô ƒëo·∫°n vƒÉn, ch·ª©a ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh v√† th√¥ng tin chi ti·∫øt.
 Qua ƒë√≥, qu√° tr√¨nh offline indexing c·ªßa HippoRAG 2 ƒë√£ ph√¢n bi·ªát r√µ r√†ng ngu·ªìn g·ªëc c·ªßa c√°c node.
- ƒê√°nh d·∫•u qua lo·∫°i c·∫°nh (edge type):
  - C√°c passage node ƒë∆∞·ª£c li√™n k·∫øt v·ªõi c√°c phrase node th√¥ng qua c√°c c·∫°nh mang nh√£n "contains" (ho·∫∑c "context edge"), nh·∫±m th·ªÉ hi·ªán r·∫±ng ƒëo·∫°n vƒÉn ch·ª©a c√°c c·ª•m t·ª´ (phrase) ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ n√≥.
  - Trong khi ƒë√≥, c√°c phrase node c√≥ th·ªÉ ƒë∆∞·ª£c n·ªëi v·ªõi nhau qua c√°c c·∫°nh kh√°c nh∆∞ "synonym edge" hay "relation edge". Vi·ªác n√†y gi√∫p nh·∫•n m·∫°nh m·ªëi li√™n h·ªá gi·ªØa c√°c kh√°i ni·ªám tr·ª´u t∆∞·ª£ng, t√°ch bi·ªát v·ªõi th√¥ng tin ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß t·ª´ passage.
- T√°c ƒë·ªông khi query:
 Khi m·ªôt truy v·∫•n ƒë∆∞·ª£c ƒë∆∞a v√†o, h·ªá th·ªëng s·ª≠ d·ª•ng c√°c embedding v√† tr·ªçng s·ªë ri√™ng bi·ªát cho t·ª´ng lo·∫°i node. Nh·ªù v·∫≠y, vi·ªác so s√°nh cosine similarity ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n c·∫£ phrase node v√† passage node theo c√°ch ph√π h·ª£p v·ªõi ƒë·∫∑c tr∆∞ng c·ªßa ch√∫ng. C√°ch g√°n tr·ªçng s·ªë (seed node weights) c≈©ng ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ri√™ng cho t·ª´ng lo·∫°i, gi√∫p qu√° tr√¨nh Personalized PageRank lan t·ªèa th√¥ng tin ƒë√∫ng theo c·∫•u tr√∫c ƒë√£ ƒë∆∞·ª£c ƒë√°nh d·∫•u.
D·∫´n ch·ª©ng: Trong ph·∫ßn m√¥ t·∫£ c·ªßa HippoRAG 2, t√°c gi·∫£ tr√¨nh b√†y r·∫±ng m·ªói passage trong corpus ƒë∆∞·ª£c x·ª≠ l√Ω nh∆∞ m·ªôt passage node v√† ƒë∆∞·ª£c n·ªëi v·ªõi t·∫•t c·∫£ c√°c phrase ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ n√≥ th√¥ng qua c√°c "context edge". ƒêi·ªÅu n√†y kh√¥ng ch·ªâ gi√∫p x√¢y d·ª±ng m·ªôt ki·∫øn tr√∫c ƒë·ªì th·ªã r√µ r√†ng m√† c√≤n cho ph√©p h·ªá th·ªëng ph√¢n bi·ªát v√† √°p d·ª•ng tr·ªçng s·ªë kh√°c nhau cho m·ªói lo·∫°i node trong qu√° tr√¨nh truy xu·∫•t.
Nh∆∞ v·∫≠y, b·∫±ng c√°ch ph√¢n lo·∫°i qua ngu·ªìn g·ªëc t·∫°o node v√† ƒë√°nh d·∫•u th√¥ng qua c√°c lo·∫°i c·∫°nh n·ªëi k·∫øt, HippoRAG 2 ƒë·∫£m b·∫£o r·∫±ng trong qu√° tr√¨nh truy v·∫•n, h·ªá th·ªëng c√≥ th·ªÉ nh·∫≠n di·ªán v√† t·∫≠n d·ª•ng ƒë√∫ng vai tr√≤ c·ªßa t·ª´ng lo·∫°i node, t·ª´ ƒë√≥ c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa retrieval v√† QA.

D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• chi ti·∫øt, k√®m theo ƒë·ªëi tho·∫°i ‚Äúph·∫£n bi·ªán ‚Äì tr·∫£ l·ªùi‚Äù minh h·ªça r√µ r√†ng c√°ch Assigning Seed Node Weights ho·∫°t ƒë·ªông trong qu√° tr√¨nh Online Retrieval c·ªßa HippoRAG.

---
V√≠ d·ª• c·ª• th·ªÉ
Truy v·∫•n: "T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?"
Sau khi h·ªá th·ªëng x·ª≠ l√Ω truy v·∫•n, ta thu ƒë∆∞·ª£c c√°c th√†nh ph·∫ßn sau:
1. Tr√≠ch xu·∫•t t·ª´ truy v·∫•n (Query Extraction):
  - C√°c th·ª±c th·ªÉ ƒë∆∞·ª£c tr√≠ch xu·∫•t:
    - ‚ÄúThe Great Novel‚Äù
    - ‚Äút√°c gi·∫£‚Äù
2. Seed Nodes ƒë∆∞·ª£c x√°c ƒë·ªãnh trong Knowledge Graph (KG):
  - Phrase Nodes (ƒë·∫°i di·ªán cho kh√°i ni·ªám, th√¥ng tin tr√≠ch xu·∫•t t·ª´ c√°c triple):
    - Node A: "t√°c gi·∫£"
      - Gi√° tr·ªã similarity (v·ªõi "t√°c gi·∫£" truy v·∫•n): 0.72
    - Node B: "The Great Novel"
      - Gi√° tr·ªã similarity (v·ªõi ti√™u ƒë·ªÅ truy v·∫•n): 0.68
  - Passage Nodes (ƒëo·∫°n vƒÉn cung c·∫•p ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß):
    - Node C: ƒêo·∫°n vƒÉn 1: ‚ÄúThe Great Novel, m·ªôt t√°c ph·∫©m kinh ƒëi·ªÉn, ƒë∆∞·ª£c vi·∫øt b·ªüi John Doe ‚Äì t√°c gi·∫£ n·ªïi ti·∫øng v·ªõi nhi·ªÅu gi·∫£i th∆∞·ªüng vƒÉn h·ªçc."
      - Gi√° tr·ªã similarity (v·ªõi truy v·∫•n, d·ª±a tr√™n c·∫£ ng·ªØ c·∫£nh v√† t·ª´ kh√≥a): 0.80
    - Node D: ƒêo·∫°n vƒÉn 2: M·ªôt ƒëo·∫°n vƒÉn kh√°c n√≥i v·ªÅ John Doe nh∆∞ng kh√¥ng ƒë·ªÅ c·∫≠p r√µ r√†ng ƒë·∫øn cu·ªën s√°ch.
      - Gi√° tr·ªã similarity: 0.55

---
B∆∞·ªõc 1: X√°c ƒë·ªãnh Seed Nodes v√† G√°n Tr·ªçng S·ªë Ban ƒë·∫ßu
Gi·∫£ s·ª≠ h·ªá th·ªëng quy ∆∞·ªõc:
- Phrase Nodes nh·∫≠n tr·ªçng s·ªë m·∫∑c ƒë·ªãnh = 1.0
- Passage Nodes ban ƒë·∫ßu nh·∫≠n tr·ªçng s·ªë th·∫•p h∆°n, v√¨ ch√∫ng mang th√¥ng tin c·ª• th·ªÉ nh∆∞ng c·∫ßn ƒë∆∞·ª£c ‚Äúƒëi·ªÅu ch·ªânh‚Äù qua weight factor.
  - V√≠ d·ª•: h·ªá th·ªëng g√°n weight factor cho Passage Nodes = 0.05.
Do ƒë√≥, ta c√≥:
- Node A ("t√°c gi·∫£"):
  - Similarity score = 0.72 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.72 √ó 1.0 = 0.72
- Node B ("The Great Novel"):
  - Similarity score = 0.68 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.68 √ó 1.0 = 0.68
- Node C (Passage 1):
  - Similarity score = 0.80 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.80 √ó 0.05 = 0.04
- Node D (Passage 2):
  - Similarity score = 0.55 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.55 √ó 0.05 = 0.0275

---
B∆∞·ªõc 2: ƒê·ªëi tho·∫°i ph·∫£n bi·ªán ‚Äì Gi·∫£i th√≠ch chi ti·∫øt
Ph·∫£n bi·ªán 1: T·∫°i sao Passage Nodes l·∫°i ƒë∆∞·ª£c nh√¢n v·ªõi weight factor th·∫•p (0.05) trong khi Phrase Nodes gi·ªØ nguy√™n tr·ªçng s·ªë 1.0?
Q: N·∫øu Passage Nodes mang to√†n b·ªô ng·ªØ c·∫£nh c·ªßa ƒëo·∫°n vƒÉn, t·∫°i sao ch√∫ng l·∫°i ƒë∆∞·ª£c g√°n tr·ªçng s·ªë th·∫•p h∆°n so v·ªõi Phrase Nodes?
A:
- L√Ω do:
  - Phrase Nodes ƒë·∫°i di·ªán cho c√°c kh√°i ni·ªám tr·ª´u t∆∞·ª£ng, th∆∞·ªùng c√≥ t√≠nh kh√°i qu√°t v√† c√≥ th·ªÉ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn truy v·∫•n. Ch√∫ng c·∫ßn ƒë∆∞·ª£c ∆∞u ti√™n cao ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng c√°c kh√°i ni·ªám ch√≠nh ƒë∆∞·ª£c nh·∫≠n di·ªán.
  - Passage Nodes m·∫∑c d√π ch·ª©a ng·ªØ c·∫£nh chi ti·∫øt, nh∆∞ng n·∫øu g√°n tr·ªçng s·ªë qu√° cao, th√¥ng tin qu√° ‚Äúph√¢n t√°n‚Äù c√≥ th·ªÉ l√†m nhi·ªÖu k·∫øt qu·∫£ retrieval. Weight factor th·∫•p gi√∫p ki·ªÉm so√°t s·ª± lan t·ªèa c·ªßa th√¥ng tin c·ª• th·ªÉ, ch·ªâ khi m√† c√°c passage c√≥ nhi·ªÅu li√™n k·∫øt v·ªõi c√°c Phrase Nodes, ch√∫ng m·ªõi ‚Äúƒë∆∞·ª£c b√π ƒë·∫Øp‚Äù th√¥ng qua qu√° tr√¨nh PPR.
- D·∫´n ch·ª©ng th·ª±c nghi·ªám:
  - B√†i b√°o tr√¨nh b√†y c√°c th√≠ nghi·ªám ablation v·ªõi c√°c gi√° tr·ªã weight factor kh√°c nhau (v√≠ d·ª•: 0.01, 0.05, 0.1,...) cho Passage Nodes. K·∫øt qu·∫£ cho th·∫•y, khi ch·ªçn weight factor ph√π h·ª£p (0.05 trong v√≠ d·ª• n√†y), h·ªá th·ªëng ƒë·∫°t Recall@5 t·ªëi ∆∞u. ƒêi·ªÅu n√†y ch·ª©ng t·ªè r·∫±ng vi·ªác c√¢n b·∫±ng gi·ªØa kh√°i ni·ªám v√† ng·ªØ c·∫£nh l√† r·∫•t quan tr·ªçng.

---
Ph·∫£n bi·ªán 2: Vi·ªác nh√¢n weight factor ·∫£nh h∆∞·ªüng nh∆∞ th·∫ø n√†o ƒë·∫øn k·∫øt qu·∫£ ban ƒë·∫ßu c·ªßa PPR?
Q: V·ªõi c√°c tr·ªçng s·ªë ban ƒë·∫ßu nh∆∞ tr√™n, l√†m th·∫ø n√†o ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng Passage Node C ‚Äì m·∫∑c d√π c√≥ similarity score cao (0.80) ‚Äì c√≥ th·ªÉ c·∫°nh tranh ƒë∆∞·ª£c v·ªõi Phrase Nodes khi weight factor th·∫•p?
A:
- Gi·∫£i th√≠ch:
  - Ban ƒë·∫ßu, Passage Node C c√≥ gi√° tr·ªã tr·ªçng s·ªë ch·ªâ l√† 0.04, so v·ªõi 0.72 (Node A) v√† 0.68 (Node B).
  - Tuy nhi√™n, trong qu√° tr√¨nh PPR, tr·ªçng s·ªë n√†y s·∫Ω ƒë∆∞·ª£c ‚Äúlan t·ªèa‚Äù qua c√°c c·∫°nh trong Knowledge Graph. N·∫øu Passage Node C ƒë∆∞·ª£c li√™n k·∫øt m·∫°nh m·∫Ω v·ªõi c√°c Phrase Nodes c√≥ tr·ªçng s·ªë cao, c√°c c·∫°nh li√™n k·∫øt n√†y s·∫Ω gi√∫p ‚Äúk√©o‚Äù th√™m probability mass cho Node C.
  - V√≠ d·ª•: n·∫øu Node C c√≥ nhi·ªÅu li√™n k·∫øt ƒë·∫øn Node A v√† Node B, qu√° tr√¨nh PPR s·∫Ω c·ªông d·ªìn th√™m gi√° tr·ªã cho Node C, gi√∫p n√≥ ƒë∆∞·ª£c ∆∞u ti√™n khi x·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn cu·ªëi c√πng.
- D·∫´n ch·ª©ng:
  - B√†i b√°o cho bi·∫øt r·∫±ng k·∫øt h·ª£p gi·ªØa distribution t·ª´ PPR v√† ma tr·∫≠n xu·∫•t hi·ªán (P matrix) cho ph√©p c√°c passage nh·∫≠n ƒë∆∞·ª£c ƒëi·ªÉm t·ªïng h·ª£p cao h∆°n, n·∫øu ch√∫ng ƒë∆∞·ª£c li√™n k·∫øt ch·∫∑t ch·∫Ω v·ªõi c√°c Phrase Nodes c√≥ tr·ªçng s·ªë ban ƒë·∫ßu cao.

---
Ph·∫£n bi·ªán 3: Gi·∫£ s·ª≠ c√≥ m·ªôt Passage Node D c√≥ similarity score th·∫•p (0.55 ‚Üí 0.0275), li·ªáu n√≥ c√≥ b·ªã lo·∫°i b·ªè ho√†n to√†n hay v·∫´n c√≥ vai tr√≤?
Q: Passage Node D c√≥ ƒëi·ªÉm ban ƒë·∫ßu r·∫•t th·∫•p, nh∆∞ng li·ªáu n√≥ c√≥ ƒë√≥ng g√≥p hay kh√¥ng trong qu√° tr√¨nh retrieval?
A:
- Gi·∫£i th√≠ch:
  - M·∫∑c d√π Passage Node D c√≥ similarity score th·∫•p v√† tr·ªçng s·ªë ban ƒë·∫ßu ch·ªâ l√† 0.0275, n·∫øu n√≥ c√≥ m·ªôt s·ªë li√™n k·∫øt h·ªØu √≠ch trong KG, ƒë·∫∑c bi·ªát l√† v·ªõi c√°c Phrase Nodes c√≥ ƒëi·ªÉm cao, qu√° tr√¨nh PPR c√≥ th·ªÉ ‚Äún√¢ng ƒë·ª°‚Äù gi√° tr·ªã c·ªßa n√≥ qua vi·ªác c·ªông d·ªìn t·ª´ c√°c n√∫t l√¢n c·∫≠n.
  - Tuy nhi√™n, n·∫øu kh√¥ng c√≥ li√™n k·∫øt m·∫°nh m·∫Ω n√†o, Node D c√≥ th·ªÉ kh√¥ng ƒë√≥ng g√≥p nhi·ªÅu v√† s·∫Ω ƒë∆∞·ª£c x·∫øp h·∫°ng th·∫•p. ƒêi·ªÅu n√†y gi√∫p h·ªá th·ªëng l·ªçc ra c√°c passage kh√¥ng ƒë·ªß li√™n quan.
- D·∫´n ch·ª©ng:
  - K·∫øt qu·∫£ th·ª±c nghi·ªám v√† ablation study trong b√†i b√°o ch·ªâ ra r·∫±ng nh·ªØng passage kh√¥ng ƒë∆∞·ª£c li√™n k·∫øt ch·∫∑t ch·∫Ω s·∫Ω nh·∫≠n ƒëi·ªÉm th·∫•p sau PPR, t·ª´ ƒë√≥ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn k·∫øt qu·∫£ cu·ªëi c√πng.

---
B∆∞·ªõc 3: T√≠ch h·ª£p qua PPR
Sau khi c√°c seed node ƒë∆∞·ª£c g√°n tr·ªçng s·ªë, h·ªá th·ªëng ch·∫°y thu·∫≠t to√°n Personalized PageRank tr√™n KG. Qu√° tr√¨nh n√†y s·∫Ω:
- Lan t·ªèa probability mass: T·ª´ c√°c seed node ban ƒë·∫ßu, gi√° tr·ªã s·∫Ω ƒë∆∞·ª£c lan t·ªèa qua c√°c c·∫°nh li√™n k·∫øt.
- K·∫øt h·ª£p v·ªõi P matrix: Sau ƒë√≥, probability mass ƒë∆∞·ª£c nh√¢n v·ªõi ma tr·∫≠n P ch·ª©a th√¥ng tin t·∫ßn s·ªë xu·∫•t hi·ªán c·ªßa c√°c n√∫t trong c√°c ƒëo·∫°n vƒÉn, gi√∫p x√°c ƒë·ªãnh th·ª© h·∫°ng cu·ªëi c√πng cho c√°c passage.
K·∫øt qu·∫£ cu·ªëi c√πng, m·∫∑c d√π ban ƒë·∫ßu Passage Node C ch·ªâ c√≥ tr·ªçng s·ªë 0.04, nh∆∞ng n·∫øu li√™n k·∫øt t·ªët, n√≥ c√≥ th·ªÉ ƒë∆∞·ª£c ‚Äún√¢ng‚Äù l√™n v√† x·∫øp h·∫°ng cao, gi√∫p h·ªá th·ªëng truy xu·∫•t ƒëo·∫°n vƒÉn ch·ª©a th√¥ng tin ‚ÄúJohn Doe l√† t√°c gi·∫£ c·ªßa The Great Novel‚Äù m·ªôt c√°ch ch√≠nh x√°c.

---
T·ªïng k·∫øt v√≠ d·ª•
1. Tr√≠ch xu·∫•t v√† x√°c ƒë·ªãnh Seed Nodes:
  - Phrase Nodes: "t√°c gi·∫£" (0.72), "The Great Novel" (0.68)
  - Passage Nodes: Passage 1 (0.80 ‚Üí 0.04 sau nh√¢n), Passage 2 (0.55 ‚Üí 0.0275)
2. G√°n tr·ªçng s·ªë:
  - Phrase Nodes gi·ªØ tr·ªçng s·ªë ƒë·∫ßy ƒë·ªß (1.0)
  - Passage Nodes nh√¢n v·ªõi weight factor (0.05)
3. Qu√° tr√¨nh PPR:
  - Lan t·ªèa probability mass t·ª´ c√°c seed node qua c√°c c·∫°nh c·ªßa KG.
  - C√°c Passage Nodes, m·∫∑c d√π c√≥ tr·ªçng s·ªë ban ƒë·∫ßu th·∫•p, s·∫Ω nh·∫≠n ƒë∆∞·ª£c th√™m gi√° tr·ªã n·∫øu ƒë∆∞·ª£c li√™n k·∫øt m·∫°nh m·∫Ω v·ªõi c√°c Phrase Nodes.
4. K·∫øt qu·∫£ cu·ªëi c√πng:
  - Passage Node ch·ª©a ƒëo·∫°n vƒÉn ch√≠nh v·ªÅ John Doe s·∫Ω ƒë∆∞·ª£c x·∫øp h·∫°ng cao, ƒë·∫£m b·∫£o h·ªá th·ªëng tr·∫£ l·ªùi ch√≠nh x√°c cho truy v·∫•n "T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?".
Qua v√≠ d·ª• n√†y, ch√∫ng ta th·∫•y r·∫±ng vi·ªác Assigning Seed Node Weights kh√¥ng ch·ªâ l√† m·ªôt thao t√°c ƒë·ªãnh l∆∞·ª£ng ban ƒë·∫ßu, m√† c√≤n l√† b∆∞·ªõc quy·∫øt ƒë·ªãnh ƒë·ªÉ c√¢n b·∫±ng gi·ªØa th√¥ng tin kh√°i ni·ªám v√† ng·ªØ c·∫£nh, t·ª´ ƒë√≥ t·ªëi ∆∞u qu√° tr√¨nh PPR v√† c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa h·ªá th·ªëng truy xu·∫•t.
ƒê√∫ng v·∫≠y, khi n√≥i v·ªÅ tr·ªçng s·ªë m·∫∑c ƒë·ªãnh cho Phrase Nodes, ta c√≥ th·ªÉ ƒë·∫∑t weight factor l√† 1.0, nh∆∞ng ƒëi·ªÅu n√†y kh√¥ng c√≥ nghƒ©a l√† gi√° tr·ªã similarity c·ªßa t·ª´ng node s·∫Ω lu√¥n l√† 1.0.
C·ª• th·ªÉ:
- Tr·ªçng s·ªë (weight factor) c·ªßa Phrase Nodes: Khi g√°n tr·ªçng s·ªë cho c√°c Phrase Nodes, h·ªá th·ªëng s·ª≠ d·ª•ng h·ªá s·ªë 1.0 ƒë·ªÉ gi·ªØ nguy√™n gi√° tr·ªã similarity ban ƒë·∫ßu.
- Gi√° tr·ªã similarity: ƒê√¢y l√† m·ªôt ch·ªâ s·ªë ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa embedding c·ªßa th·ª±c th·ªÉ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ truy v·∫•n v√† embedding c·ªßa node trong Knowledge Graph. Gi√° tr·ªã n√†y th∆∞·ªùng n·∫±m trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1, v√† kh√¥ng ph·∫£i l√∫c n√†o c≈©ng b·∫±ng 1.0. V√≠ d·ª•, Node A c√≥ similarity l√† 0.72 v√† Node B c√≥ similarity l√† 0.68, ƒëi·ªÅu n√†y ph·∫£n √°nh m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng (kh√¥ng ph·∫£i l√† ‚Äútr·ªçng s·ªë‚Äù m√† l√† k·∫øt qu·∫£ c·ªßa ph√©p ƒëo similarity).
T√≥m l·∫°i, m·∫∑c ƒë·ªãnh weight factor c·ªßa Phrase Nodes l√† 1.0, nghƒ©a l√† ch√∫ng ta kh√¥ng ƒëi·ªÅu ch·ªânh th√™m gi√° tr·ªã similarity c·ªßa ch√∫ng, nh∆∞ng gi√° tr·ªã similarity th·ª±c s·ª± (0.72, 0.68, ‚Ä¶) ph·ª• thu·ªôc v√†o qu√° tr√¨nh t√≠nh to√°n t·ª´ c√°c embedding ch·ª© kh√¥ng ph·∫£i l√† 1.0 lu√¥n.

---
D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• chi ti·∫øt k√®m ƒë·ªëi tho·∫°i ph·∫£n bi·ªán ‚Äì ‚Äúng∆∞·ªùi ph√° ‚Äì ng∆∞·ªùi tr·∫£ l·ªùi‚Äù ‚Äì ƒë·ªÉ minh h·ªça c√°ch Seed Node Weights v√† thu·∫≠t to√°n Personalized PageRank (PPR) ho·∫°t ƒë·ªông c√πng nhau trong HippoRAG.

---
B·ªëi c·∫£nh v√≠ d·ª•
Gi·∫£ s·ª≠ truy v·∫•n l√†:
"T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?"
Sau khi tr√≠ch xu·∫•t th√¥ng tin t·ª´ truy v·∫•n v√† x√¢y d·ª±ng Knowledge Graph (KG), h·ªá th·ªëng x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c seed node v·ªõi c√°c gi√° tr·ªã similarity nh∆∞ sau:
- Phrase Node A: "t√°c gi·∫£"
  - Similarity score = 0.72
  - Weight factor (Phrase) = 1.0 ‚Üí Effective weight = 0.72
- Phrase Node B: "The Great Novel"
  - Similarity score = 0.68
  - Weight factor (Phrase) = 1.0 ‚Üí Effective weight = 0.68
- Passage Node C: ƒêo·∫°n vƒÉn 1 ch·ª©a th√¥ng tin:
 ‚ÄúThe Great Novel, m·ªôt t√°c ph·∫©m kinh ƒëi·ªÉn, ƒë∆∞·ª£c vi·∫øt b·ªüi John Doe ‚Äì t√°c gi·∫£ n·ªïi ti·∫øng‚Äù
  - Similarity score = 0.80
  - Weight factor (Passage) = 0.05 ‚Üí Effective weight = 0.80 √ó 0.05 = 0.04
- Passage Node D: ƒêo·∫°n vƒÉn 2 n√≥i chung v·ªÅ John Doe nh∆∞ng kh√¥ng nh·∫Øc tr·ª±c ti·∫øp ti√™u ƒë·ªÅ
  - Similarity score = 0.55
  - Weight factor (Passage) = 0.05 ‚Üí Effective weight = 0.55 √ó 0.05 = 0.0275

---
ƒê·ªëi tho·∫°i ph·∫£n bi·ªán ‚Äì C√°ch PPR ho·∫°t ƒë·ªông v·ªõi Seed Node Weights
Ph·∫£n bi·ªán 1: Kh·ªüi t·∫°o Seed Vector
Q: L√†m th·∫ø n√†o ta kh·ªüi t·∫°o vector seed cho PPR v·ªõi c√°c seed node tr√™n?
A:
- Ta t·∫°o m·ªôt vector ss ch·ª©a c√°c gi√° tr·ªã kh·ªüi t·∫°o cho c√°c node theo effective weight:
  - sA=0.72s_A = 0.72 (Phrase node A)
  - sB=0.68s_B = 0.68 (Phrase node B)
  - sC=0.04s_C = 0.04 (Passage node C)
  - sD=0.0275s_D = 0.0275 (Passage node D)
- Sau ƒë√≥, ta chu·∫©n h√≥a vector ss sao cho t·ªïng c√°c ph·∫ßn t·ª≠ b·∫±ng 1. V√≠ d·ª•:
  - T·ªïng S=0.72+0.68+0.04+0.0275=1.4675S = 0.72 + 0.68 + 0.04 + 0.0275 = 1.4675
  - Vector seed chu·∫©n h√≥a:
    - sA‚Ä≤=0.72/1.4675‚âà0.49s'_A = 0.72 / 1.4675 \approx 0.49
    - sB‚Ä≤=0.68/1.4675‚âà0.46s'_B = 0.68 / 1.4675 \approx 0.46
    - sC‚Ä≤=0.04/1.4675‚âà0.027s'_C = 0.04 / 1.4675 \approx 0.027
    - sD‚Ä≤=0.0275/1.4675‚âà0.019s'_D = 0.0275 / 1.4675 \approx 0.019
Nh∆∞ v·∫≠y, c√°c Phrase Nodes chi·∫øm ∆∞u th·∫ø ngay t·ª´ ƒë·∫ßu so v·ªõi c√°c Passage Nodes do effective weight c·ªßa ch√∫ng cao h∆°n.

---
Ph·∫£n bi·ªán 2: Qu√° tr√¨nh PPR ‚Äì Lan t·ªèa x√°c su·∫•t
Q: PPR s·∫Ω l√†m g√¨ v·ªõi vector seed ƒë√£ chu·∫©n h√≥a n√†y?
A:
- Thu·∫≠t to√°n PPR th·ª±c hi·ªán c·∫≠p nh·∫≠t x√°c su·∫•t theo c√¥ng th·ª©c:
- v(t+1)=(1‚àíd)‚ãÖs‚Ä≤+d‚ãÖT‚ãÖv(t)v^{(t+1)} = (1-d) \cdot s' + d \cdot T \cdot v^{(t)}
- trong ƒë√≥:
  - v(t)v^{(t)} l√† vector x√°c su·∫•t ·ªü v√≤ng l·∫∑p th·ª© tt
  - s‚Ä≤s' l√† vector seed (ƒë√£ chu·∫©n h√≥a)
  - TT l√† ma tr·∫≠n chuy·ªÉn ƒë·ªïi (transition matrix) c·ªßa KG, x√°c ƒë·ªãnh x√°c su·∫•t chuy·ªÉn t·ª´ node n√†y sang node kh√°c d·ª±a tr√™n c√°c c·∫°nh li√™n k·∫øt
  - dd l√† damping factor (th∆∞·ªùng kho·∫£ng 0.5‚Äì0.85)
- Qu√° tr√¨nh n√†y s·∫Ω ‚Äúlan t·ªèa‚Äù c√°c gi√° tr·ªã t·ª´ c√°c seed node ban ƒë·∫ßu theo c√°c li√™n k·∫øt trong ƒë·ªì th·ªã. C√°c Phrase Nodes v·ªõi gi√° tr·ªã cao ban ƒë·∫ßu (0.49 v√† 0.46) s·∫Ω ‚Äúƒë·∫©y‚Äù x√°c su·∫•t sang c√°c n√∫t l√¢n c·∫≠n. N·∫øu Passage Node C (v·ªõi gi√° tr·ªã ban ƒë·∫ßu 0.027) c√≥ c√°c c·∫°nh m·∫°nh li√™n k·∫øt v·ªõi Phrase Nodes, n√≥ c√≥ th·ªÉ nh·∫≠n th√™m x√°c su·∫•t t·ª´ ch√∫ng.

---
Ph·∫£n bi·ªán 3: V√≠ d·ª• s·ªë h·ªçc trong qu√° tr√¨nh PPR
Q: H√£y l·∫•y m·ªôt v√≤ng l·∫∑p PPR minh h·ªça c·ª• th·ªÉ cho v√≠ d·ª• n√†y.
A:
- Gi·∫£ s·ª≠ ma tr·∫≠n chuy·ªÉn ƒë·ªïi TT c·ªßa m·ªôt ph·∫ßn nh·ªè trong KG ƒë∆∞·ª£c ƒë∆°n gi·∫£n ho√° nh∆∞ sau (ch·ªâ x√©t 4 node tr√™n):
  - TABT_{AB} l√† x√°c su·∫•t t·ª´ node A sang B, TACT_{AC} t·ª´ A sang C, v.v‚Ä¶
- V√≠ d·ª•, ta gi·∫£ s·ª≠:
  - Node A (Phrase "t√°c gi·∫£") c√≥ li√™n k·∫øt:
    - 60% ƒë·∫øn Node B ("The Great Novel")
    - 40% ƒë·∫øn Node C (Passage ch·ª©a th√¥ng tin ch√≠nh)
  - Node B ("The Great Novel") li√™n k·∫øt:
    - 70% ƒë·∫øn Node A
    - 30% ƒë·∫øn Node D (Passage ph·ª•)
  - Node C (Passage 1) li√™n k·∫øt:
    - 100% ƒë·∫øn Node A (gi·∫£ s·ª≠ c√≥ m·ªëi li√™n h·ªá m·∫°nh v·ªõi kh√°i ni·ªám "t√°c gi·∫£")
  - Node D (Passage 2) li√™n k·∫øt:
    - 100% ƒë·∫øn Node B
- Ta c√≥ ma tr·∫≠n TT (ƒë∆°n gi·∫£n ho√°) nh∆∞ sau:
- T=[00.600.4000.70000.301.000001.000]T = \begin{bmatrix} 0   & 0.60 & 0.40 & 0 \\ 0.70 & 0   & 0    & 0.30 \\ 1.0  & 0   & 0    & 0 \\ 0   & 1.0 & 0    & 0  \end{bmatrix}
- Gi·∫£ s·ª≠ damping factor d=0.5d = 0.5. V·ªõi vector seed chu·∫©n h√≥a s‚Ä≤=[0.49,0.46,0.027,0.019]‚ä§s' = [0.49, 0.46, 0.027, 0.019]^\top, ta kh·ªüi t·∫°o v(0)=s‚Ä≤v^{(0)} = s'.
- V√≤ng l·∫∑p th·ª© 1:
 T√≠nh v(1)=0.5‚ãÖs‚Ä≤+0.5‚ãÖT‚ãÖv(0)v^{(1)} = 0.5 \cdot s' + 0.5 \cdot T \cdot v^{(0)}
- Tr∆∞·ªõc h·∫øt, t√≠nh T‚ãÖv(0)T \cdot v^{(0)}:
  - (T‚ãÖv(0))A=0‚àó0.49+0.60‚àó0.46+0.40‚àó0.027+0‚àó0.019‚âà0.60√ó0.46+0.40√ó0.027‚âà0.276+0.0108‚âà0.287(T \cdot v^{(0)})_A = 0*0.49 + 0.60*0.46 + 0.40*0.027 + 0*0.019 \approx 0.60 \times 0.46 + 0.40 \times 0.027 \approx 0.276 + 0.0108 \approx 0.287
  - (T‚ãÖv(0))B=0.70‚àó0.49+0‚àó0.46+0‚àó0.027+0.30‚àó0.019‚âà0.343+0.0057‚âà0.3487(T \cdot v^{(0)})_B = 0.70*0.49 + 0*0.46 + 0*0.027 + 0.30*0.019 \approx 0.343 + 0.0057 \approx 0.3487
  - (T‚ãÖv(0))C=1.0‚àó0.49+0‚àó0.46+0‚àó0.027+0‚àó0.019=0.49(T \cdot v^{(0)})_C = 1.0*0.49 + 0*0.46 + 0*0.027 + 0*0.019 = 0.49
  - (T‚ãÖv(0))D=0‚àó0.49+1.0‚àó0.46+0‚àó0.027+0‚àó0.019=0.46(T \cdot v^{(0)})_D = 0*0.49 + 1.0*0.46 + 0*0.027 + 0*0.019 = 0.46
- Sau ƒë√≥,
- v(1)=0.5‚ãÖ[0.49,0.46,0.027,0.019]‚ä§+0.5‚ãÖ[0.287,0.3487,0.49,0.46]‚ä§v^{(1)} = 0.5 \cdot [0.49, 0.46, 0.027, 0.019]^\top + 0.5 \cdot [0.287, 0.3487, 0.49, 0.46]^\top 
- =
- [0.245+0.1435,‚Äâ0.23+0.17435,‚Äâ0.0135+0.245,‚Äâ0.0095+0.23][0.245 + 0.1435,\, 0.23 + 0.17435,\, 0.0135 + 0.245,\, 0.0095 + 0.23]
- =
- [0.3885,‚Äâ0.40435,‚Äâ0.2585,‚Äâ0.2395][0.3885,\, 0.40435,\, 0.2585,\, 0.2395]
- Gi·∫£i th√≠ch:
 Nh√¨n v√†o v(1)v^{(1)}:
  - Node A v√† Node B (Phrase Nodes) c√≥ gi√° tr·ªã v·∫´n cao (0.3885 v√† 0.40435).
  - Node C (Passage 1) tƒÉng t·ª´ 0.027 l√™n 0.2585 nh·ªù nh·∫≠n ƒë∆∞·ª£c th√¥ng tin t·ª´ Node A (v√¨ c√≥ li√™n k·∫øt 100% t·ª´ Node C sang A) v√† t·ª´ m·ªëi li√™n k·∫øt c·ªßa A ƒë·∫øn C.
  - Node D (Passage 2) c√≥ gi√° tr·ªã 0.2395 nh·ªù li√™n k·∫øt v·ªõi Node B.
Qu√° tr√¨nh n√†y s·∫Ω l·∫∑p l·∫°i cho ƒë·∫øn khi h·ªôi t·ª•. K·∫øt qu·∫£ cu·ªëi c√πng s·∫Ω l√† vector x√°c su·∫•t ph√¢n b·ªï qua to√†n b·ªô KG, trong ƒë√≥ c√°c node c√≥ m·ªëi li√™n k·∫øt m·∫°nh v·ªõi seed nodes ban ƒë·∫ßu s·∫Ω nh·∫≠n ƒë∆∞·ª£c x√°c su·∫•t cao h∆°n.

---
Ph·∫£n bi·ªán 4: ·∫¢nh h∆∞·ªüng c·ªßa PPR ƒë·∫øn vi·ªác x·∫øp h·∫°ng c√°c Passage
Q: Sau khi vector x√°c su·∫•t ƒë∆∞·ª£c t√≠nh xong, l√†m sao c√°c gi√° tr·ªã n√†y chuy·ªÉn th√†nh vi·ªác x·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn tr·∫£ l·ªùi truy v·∫•n?
A:
- Sau khi c√≥ vector vv cho c√°c node trong KG, h·ªá th·ªëng s·∫Ω nh√¢n c√°c gi√° tr·ªã n√†y v·ªõi ma tr·∫≠n P (P matrix), m√† trong ƒë√≥ m·ªói ph·∫ßn t·ª≠ bi·ªÉu th·ªã s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªôt n√∫t (phrase ho·∫∑c passage) trong c√°c ƒëo·∫°n vƒÉn g·ªëc.
- T·ªïng h·ª£p gi√° tr·ªã t·ª´ c√°c node li√™n quan s·∫Ω cho ƒëi·ªÉm x·∫øp h·∫°ng cho t·ª´ng ƒëo·∫°n vƒÉn. C√°c ƒëo·∫°n vƒÉn c√≥ t·ªïng x√°c su·∫•t cao (ƒë·∫∑c bi·ªát l√† do nh·∫≠n ƒë∆∞·ª£c ‚Äúƒëi·ªÉm c·ªông‚Äù t·ª´ c√°c Phrase Nodes m·∫°nh v√† Passage Nodes c√≥ li√™n k·∫øt t·ªët) s·∫Ω ƒë∆∞·ª£c x·∫øp h·∫°ng ∆∞u ti√™n.
V√≠ d·ª•:
- Passage Node C, sau PPR, c√≥ gi√° tr·ªã tƒÉng l√™n ƒë√°ng k·ªÉ (v√≠ d·ª• 0.2585), khi nh√¢n v·ªõi s·ªë l·∫ßn xu·∫•t hi·ªán v√† c√°c li√™n k·∫øt li√™n quan, n√≥ c√≥ th·ªÉ ƒë∆∞·ª£c x·∫øp h·∫°ng cao v√† ch·ªçn l√†m c√¢u tr·∫£ l·ªùi cho truy v·∫•n "T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?".

---
T·ªïng k·∫øt
Qua v√≠ d·ª• chi ti·∫øt tr√™n, ta th·∫•y r·∫±ng:
1. Seed Node Weights ƒë∆∞·ª£c x√°c ƒë·ªãnh d·ª±a tr√™n similarity score v√† weight factor ri√™ng bi·ªát cho Phrase Nodes (1.0) v√† Passage Nodes (0.05).
2. Vector seed sau khi chu·∫©n h√≥a cung c·∫•p ƒëi·ªÉm kh·ªüi ƒë·∫ßu cho PPR.
3. PPR lan t·ªèa x√°c su·∫•t t·ª´ c√°c seed node theo c√°c li√™n k·∫øt trong KG, gi√∫p c√°c Passage Nodes nh·∫≠n ƒë∆∞·ª£c ‚Äúƒëi·ªÉm c·ªông‚Äù t·ª´ c√°c Phrase Nodes n·∫øu ch√∫ng c√≥ li√™n k·∫øt ch·∫∑t ch·∫Ω.
4. K·∫øt qu·∫£ cu·ªëi c√πng sau PPR v√† k·∫øt h·ª£p v·ªõi ma tr·∫≠n P s·∫Ω x·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi truy v·∫•n.
Qua ƒë√≥, ta th·∫•y r·∫±ng vi·ªác g√°n tr·ªçng s·ªë seed node v√† ·ª©ng d·ª•ng PPR kh√¥ng ch·ªâ gi√∫p c√¢n b·∫±ng gi·ªØa th√¥ng tin kh√°i ni·ªám v√† ng·ªØ c·∫£nh, m√† c√≤n t·∫°o ƒëi·ªÅu ki·ªán cho qu√° tr√¨nh multi-hop retrieval di·ªÖn ra hi·ªáu qu·∫£ v√† ch√≠nh x√°c.

---
1. MOTIVATION R√ï R√ÄNG => M·ªöI TRI·ªÇN?  
2. DATASET? C√ÅCH H·ªå ƒê√ÅNH GI√Å NH∆Ø N√ÄO?
3. METHOD? K·ª∏ THU·∫¨T ƒê√ì L√Ä G√å? ? K·ª∏ THU·∫¨T ƒê√ì NH∆Ø N√ÄO? => ƒê·ªäNH NGHƒ®A ƒê∆Ø·ª¢C C√ÅC KH√ÅI NI·ªÜM => M·ªöI C√ì K·ª∏ THU·∫¨T.  

---
Prompt 1: T√¥i mu·ªën b·∫°n ph·∫£i hi·ªÉu th·∫≠t s√¢u. m·ªói ph·∫ßn ƒë·∫∑t c√°c c√¢u h·ªèi ph·∫£n bi·ªán s·∫Øc x·∫£o ƒë·ªÉ t·ª± tr·∫£ l·ªùi. Gi·ªëng nh∆∞ 1 ng∆∞·ªùi ph√°, 1 ng∆∞·ªùi tr·∫£ l·ªùi, l·∫•y d·∫´n ch·ª©ng
Prompt 2: Gi·∫£i t√≠ch t·ª´ng kh√°i ni·ªám v√† li√™n h·ªá n√≥ v·ªõi th·ª© th·∫≠t d·ªÖ nh·ªõ ƒë∆°n gi·∫£n cho h·ªçc sinh c·∫•p 2

üß† PH·∫¶N 1: V·∫•n ƒë·ªÅ c·ªßa RAG hi·ªán t·∫°i & S·ª± xu·∫•t hi·ªán c·ªßa HippoRAG
üîç Gi·∫£i th√≠ch l·∫°i:
RAG hi·ªán t·∫°i ho·∫°t ƒë·ªông theo c∆° ch·∫ø:
- T√°ch vƒÉn b·∫£n th√†nh c√°c chunk nh·ªè
- D√πng embedding ‚Üí l∆∞u v√†o VectorDB
- Khi c√≥ query ‚Üí t√¨m vector t∆∞∆°ng ƒë·ªìng ‚Üí l·∫•y context ‚Üí ƒë∆∞a v√†o LLM
Nh∆∞ng v·ªõi c√°c t√°c v·ª• multi-hop reasoning, nhi·ªÅu th√¥ng tin ·ªü r·∫£i r√°c c√°c ƒëo·∫°n kh√°c nhau th√¨ RAG hi·ªán t·∫°i kh√¥ng ƒë·ªß hi·ªáu qu·∫£.
Ph√° 1.1 T·∫°i sao n√≥i v·ªõi c√°c t√°c v·ª• multi-hop reasoning, nhi·ªÅu th√¥ng tin ·ªü r·∫£i r√°c c√°c ƒëo·∫°n kh√°c nhau th√¨ RAG hi·ªán t·∫°i kh√¥ng ƒë·ªß hi·ªáu qu·∫£? 
Gi·∫£ s·ª≠ RAG truy·ªÅn th·ªëng retrieve ƒë∆∞·ª£c c·∫£ hai ƒëo·∫°n r·ªùi r·∫°c, th√¨ n√≥ v·∫´n n√©m h·∫øt v√†o context window c·ªßa LLM m√†? L√∫c ƒë√≥ LLM n·ªëi l·∫°i logic l√† xong ch·ª©?

Ph√° 1: RAG truy·ªÅn th·ªëng, trong l√∫c query ra c√°c ƒëo·∫°n chunking th√¨ n√≥ ch·∫≥ng nh·∫ª kh√¥ng c√≥ t√≠nh m√≥c n·ªëi?
‚ÄúGi·∫£ s·ª≠ RAG truy·ªÅn th·ªëng retrieve ƒë∆∞·ª£c c·∫£ hai ƒëo·∫°n r·ªùi r·∫°c, th√¨ n√≥ v·∫´n n√©m h·∫øt v√†o context window c·ªßa LLM m√†? L√∫c ƒë√≥ LLM n·ªëi l·∫°i logic l√† xong ch·ª©, c·∫ßn g√¨ ph·∫£i HippoRAG r∆∞·ªùm r√†?‚Äù 
üéØ 1. Kh√¥ng c√≥ g√¨ ƒë·∫£m b·∫£o RAG s·∫Ω retrieve ƒë√∫ng 2 ƒëo·∫°n
V·∫•n ƒë·ªÅ l·ªõn nh·∫•t c·ªßa RAG:
- Retrieval = vector similarity ‚Üí ch·ªâ ch·ªçn Top-K
- N·∫øu th√¥ng tin n·∫±m ·ªü 2 ƒëo·∫°n kh√°c nhau, v√† m·ªói c√°i ƒë·ªÅu ‚Äúh∆°i li√™n quan‚Äù, n√≥ c√≥ th·ªÉ b·ªã ƒë·∫©y ra kh·ªèi Top-K
V√≠ d·ª• c·ª• th·ªÉ:
Top-K = 3
 B·∫°n c·∫ßn ƒëo·∫°n A v√† ƒëo·∫°n B
 Nh∆∞ng c√≥ ƒëo·∫°n C, D r·∫•t gi·ªëng c√¢u h·ªèi v·ªÅ m·∫∑t ng·ªØ nghƒ©a ‚Üí l√™n Top
 A ho·∫∑c B b·ªã lo·∫°i v√¨ vector kh√¥ng ƒë·ªß g·∫ßn
üí• ‚Üí L√∫c n√†y LLM kh√¥ng c√≥ nguy√™n li·ªáu ƒë·ªÉ suy lu·∫≠n, d√π n√≥ gi·ªèi c·ª° n√†o. 

TƒÉng k. 
D·ªØ li·ªáu n·∫±m r·∫£i r√°c t·∫°i nhi·ªÅu n∆°i. 

üß† 2. LLM suy lu·∫≠n = t·ªën compute + d·ªÖ l·ªói logic
C√¢u h·ªèi:
"Which Stanford researcher who is not Alice but studies Alzheimer's worked at the same place as Thomas?"
Dataset:
902. Alice is a Stanford professor.
903. Thomas researches Alzheimer‚Äôs at UCSF.
904. Mike works at UCSF.
905. Sarah studies Alzheimer‚Äôs.
906. Sarah and Thomas are labmates at UCSF.

‚ö†Ô∏è V·∫•n ƒë·ªÅ c·ªßa RAG khi retrieve Top-K
Gi·∫£ s·ª≠ retrieve ƒë∆∞·ª£c 3 ƒëo·∫°n:
- (2) Thomas ‚Üí Alzheimer + UCSF
- (4) Sarah ‚Üí Alzheimer
- (5) Sarah ‚Üí labmate of Thomas ‚Üí UCSF
LLM ph·∫£i l√†m:
1267. G·∫Øn Sarah v·ªõi UCSF (t·ª´ labmate)
1268. Tr·ª´ Alice kh·ªèi k·∫øt qu·∫£
1269. Ch·ªçn ng∆∞·ªùi v·ª´a Stanford researcher, v·ª´a Alzheimer, v·ª´a ·ªü UCSF, nh∆∞ng kh√¥ng ph·∫£i Alice

---
‚ú≥Ô∏è ƒê√¢y l√† 3-hop reasoning. Kh√¥ng ph·∫£i v√¨ LLM kh√¥ng th·ªÉ, m√† v√¨:
- N√≥ ph·∫£i gi·ªØ ·ªïn ƒë·ªãnh attention gi·ªØa 4-5 ƒëo·∫°n vƒÉn
- N√≥ kh√¥ng bi·∫øt ch·∫Øc m·ªëi li√™n k·∫øt l√† quan tr·ªçng hay ch·ªâ l√† ‚Äúnhi·ªÖu‚Äù
- Kh√¥ng c√≥ logic structure r√µ r√†ng ƒë·ªÉ l·∫ßn t·ª´ A ‚Üí B ‚Üí C
‚û°Ô∏è Khi n√†y, retrieval d·∫°ng vector (d√π chunk t·ªët) kh√¥ng cung c·∫•p structure reasoning.
 Ch·ªâ n√©m ƒëo·∫°n v√†o context ‚Üí LLM ph·∫£i lo c·∫£ ƒë·ªçc, t√¨m logic, t·ªïng h·ª£p, tr√°nh sai l·ªách.

C√¢u h·ªèi ti·∫øp ƒë·∫∑t ra l√†: ‚ùó‚ÄúB√¢y gi·ªù c√≥ model 4o, Claude Opus, GPT-4-Turbo‚Ä¶ n√≥ c√≥ th·ªÉ context window 128k tokens, b·∫°n n√©m s·∫°ch v√¥ l√† n√≥ l√†m reasoning c·ª±c t·ªët. Th·∫ø th√¨ l√†m retrieval ƒë·ªÉ l√†m g√¨ n·ªØa? HippoRAG c√≥ c√≤n gi√° tr·ªã kh√¥ng?‚Äù
Tr·∫£ l·ªùi: ƒê√∫ng! Nh∆∞ng ch·ªâ khi b·∫°n s·ªëng trong th·∫ø gi·ªõi l√Ω t∆∞·ªüng.

‚ú≥Ô∏è PH·∫¶N I: Th·ª´a nh·∫≠n s·ª± th·∫≠t
‚úÖ ƒê√∫ng, model 4o, Claude Opus, GPT-4 Turbo hi·ªán nay ƒë√£ c√≥ kh·∫£ nƒÉng context-wide reasoning r·∫•t m·∫°nh.
üìå B·∫°n c√≥ th·ªÉ ƒë∆∞a v√†o:
- M·ªôt dataset 80k tokens g·ªìm nhi·ªÅu t√†i li·ªáu
- M·ªôt c√¢u h·ªèi c·∫ßn n·ªëi 3‚Äì4 th√¥ng tin l·∫°i
- V√† model s·∫Ω tr·∫£ l·ªùi ƒë√∫ng ‚Äì th·∫≠m ch√≠ c√≤n gi·∫£i th√≠ch t·∫°i sao.

---
‚ú≥Ô∏è PH·∫¶N II: Nh∆∞ng ƒë√≥ kh√¥ng ph·∫£i l√† c√°ch l√†m AI hi·ªáu qu·∫£ ho·∫∑c quy m√¥
‚ùó L√Ω do 1: Context-wide LLM reasoning = t·ªën compute kh·ªßng khi·∫øp
N√©m v√†o 128k tokens = gi√° infer cao, latency cao
- Claude Opus ho·∫∑c GPT-4 Turbo 128k input ‚Üí th·ªùi gian tr·∫£ l·ªùi l√¢u h∆°n, chi ph√≠ t√≠nh theo token c·ª±c l·ªõn (v√†i $ m·ªói l∆∞·ª£t n·∫øu full)
- B·∫°n kh√¥ng th·ªÉ scale 1 tri·ªáu c√¢u h·ªèi ki·ªÉu ƒë√≥

---
‚ùó L√Ω do 2: LLM = black box reasoning, kh√¥ng th·ªÉ debug, kh√¥ng th·ªÉ ki·ªÉm so√°t
- LLM kh√¥ng cho b·∫°n bi·∫øt reasoning path
- B·∫°n kh√¥ng bi·∫øt model n·ªëi A ‚Üí B ‚Üí C nh∆∞ th·∫ø n√†o
- N·∫øu n√≥ tr·∫£ sai? B·∫°n kh√¥ng bi·∫øt s·ª≠a t·ª´ ƒë√¢u
‚û°Ô∏è Retrieval reasoning = transparent, modular, debuggable
‚Üí HippoRAG = ta l·∫ßn ƒë∆∞·ª£c ƒë∆∞·ªùng ƒëi qua graph
 ‚Üí C√≥ th·ªÉ ghi log reasoning path
 ‚Üí C√≥ th·ªÉ gi·∫£i th√≠ch cho ng∆∞·ªùi d√πng ho·∫∑c ki·ªÉm tra b·∫±ng rule engine

---
‚ùó L√Ω do 3: Kh√¥ng ph·∫£i l√∫c n√†o c≈©ng c√≥ quy·ªÅn ‚Äún√©m h·∫øt v√†o‚Äù
- N·∫øu user h·ªèi li√™n t·ª•c, m·ªói c√¢u ƒë·ªÅu c·∫ßn to√†n b·ªô t√†i li·ªáu ‚Üí c·ª±c k·ª≥ l√£ng ph√≠
- V√≠ d·ª•: app legal, ho·∫∑c chatbot h·ªçc thu·∫≠t, ho·∫∑c QA y khoa
B·∫°n c·∫ßn selective retrieval ‚Üí RAG gi·∫£i quy·∫øt
 B·∫°n c·∫ßn reasoning t·ª´ c√°c concept, entity ‚Üí kh√¥ng c√≥ s·∫µn text chunk ‚Üí HippoRAG gi·∫£i quy·∫øt

üß† 3. HippoRAG cho ph√©p retrieval ch√≠nh l√† reasoning
ƒê√¢y l√† ƒëi·ªÉm ‚Äúƒë·∫≠p n√°t ph·∫£n bi·ªán‚Äù:
üß† HippoRAG kh√¥ng ch·ªù ƒë·∫øn khi LLM l√†m reasoning, m√† n√≥ ƒë√£ reasoning ngay t·ª´ b∆∞·ªõc truy h·ªìi.
- V·ªõi Personalized PageRank ‚Üí n√≥ t√¨m ra node trung gian (John Smith) tr∆∞·ªõc khi v√†o LLM
- Tr·∫£ v·ªÅ ƒëo·∫°n ƒë√£ ƒë·ªß logic ‚Üí LLM ch·ªâ c·∫ßn n√≥i l·∫°i
üí° ƒêi·ªÅu n√†y bi·∫øn retrieval th√†nh m·ªôt ‚Äúm√¥-ƒëun l·∫≠p lu·∫≠n‚Äù (reasoning module), kh√¥ng ch·ªâ l√† ‚Äúscanner ƒëo·∫°n vƒÉn‚Äù.

Ph√° 1.2 ƒê·∫∑t v·∫•n ƒë·ªÅ? 
Tuy·ªát v·ªùi, Qu·ªëc ƒëang d√πng k·ªπ thu·∫≠t ph·∫£n bi·ªán c·ª±c k·ª≥ hay ƒë·ªÉ ‚Äúƒë·ªçc s√¢u‚Äù ‚Äî ki·ªÉu Socratic method trong nghi√™n c·ª©u. M√¨nh s·∫Ω ƒë√≥ng c·∫£ ng∆∞·ªùi h·ªèi s·∫Øc s·∫£o v√† ng∆∞·ªùi tr·∫£ l·ªùi ch·∫∑t ch·∫Ω, b√°m s√°t ph·∫ßn ‚Äú1. Introduction‚Äù c·ªßa b√†i b√°o HippoRAG 2.

---
üß© PH√ÇN T√çCH PH·∫¶N ƒê·∫∂T V·∫§N ƒê·ªÄ (INTRODUCTION - HippoRAG 2)

---
‚öîÔ∏è Ph·∫£n bi·ªán 1:
H·ªèi: T·∫°i sao c·ª© ph·∫£i b·∫Øt AI h·ªçc nh∆∞ con ng∆∞·ªùi? Vi·ªác m√¥ ph·ªèng ‚Äúlong-term memory‚Äù c·ªßa ng∆∞·ªùi c√≥ th·∫≠t s·ª± c·∫ßn thi·∫øt cho LLM kh√¥ng?
Tr·∫£ l·ªùi: V√¨ m·ª•c ti√™u cu·ªëi c√πng c·ªßa LLM l√† tr·ªü th√†nh tr·ª£ l√Ω th√¥ng minh v√† hi·ªáu qu·∫£ nh∆∞ con ng∆∞·ªùi. Trong th·∫ø gi·ªõi th·ª±c, con ng∆∞·ªùi li√™n t·ª•c h·ªçc, t√≠ch h·ª£p th√¥ng tin m·ªõi m√† kh√¥ng qu√™n c√°i c≈© ‚Äì ƒë√¢y l√† ƒëi·ªÅu LLM hi·ªán t·∫°i r·∫•t y·∫øu.
B·∫±ng ch·ª©ng:
- LLM g·∫∑p v·∫•n ƒë·ªÅ catastrophic forgetting n·∫øu fine-tune.
- RAG tuy ti·∫øp c·∫≠n t·ªët h∆°n nh∆∞ng v·∫´n kh√¥ng c√≥ kh·∫£ nƒÉng k·∫øt n·ªëi m·∫°ch l·∫°c, nh∆∞ ng∆∞·ªùi nh·ªõ chuy·ªán ƒë√£ x·∫£y ra t·ª´ l√¢u v√† li√™n k·∫øt n√≥ v·ªõi b·ªëi c·∫£nh hi·ªán t·∫°i.
V√¨ th·∫ø, mu·ªën LLM tr·ªü th√†nh tr·ª£ l√Ω l√¢u d√†i, c·∫≠p nh·∫≠t li√™n t·ª•c, n√≥ c·∫ßn m·ªôt h·ªá th·ªëng b·ªô nh·ªõ d√†i h·∫°n linh ho·∫°t v√† li√™n k·∫øt ‚Äì gi·ªëng nh∆∞ c·ªßa ng∆∞·ªùi.

---
‚öîÔ∏è Ph·∫£n bi·ªán 2:
H·ªèi: RAG ƒëang ch·∫°y t·ªët m√†. T·∫°i sao l·∫°i n√≥i RAG truy·ªÅn th·ªëng kh√¥ng ƒë·ªß? C√≥ b·∫±ng ch·ª©ng g√¨?
Tr·∫£ l·ªùi: RAG truy·ªÅn th·ªëng ch·ªß y·∫øu d·ª±a v√†o vector retrieval ƒë·ªôc l·∫≠p, m·ªói chunk ƒë∆∞·ª£c x·∫øp h·∫°ng d·ª±a v√†o ƒë·ªô t∆∞∆°ng t·ª± embedding v·ªõi query ‚Äì kh√¥ng c√≥ c∆° ch·∫ø ƒë·ªÉ hi·ªÉu m·ªëi quan h·ªá gi·ªØa c√°c chunk, kh√¥ng hi·ªÉu m·∫°ch truy·ªán, kh√¥ng l√†m multi-hop reasoning.
B·∫±ng ch·ª©ng c·ª• th·ªÉ trong b√†i:
‚Äú...inability to capture two vital aspects: sense-making (hi·ªÉu ng·ªØ c·∫£nh ph·ª©c t·∫°p) v√† associativity (k·∫øt n·ªëi ƒëa b∆∞·ªõc gi·ªØa th√¥ng tin r·ªùi r·∫°c)‚Äù ‚Äì (trang 1)
V√¨ v·∫≠y, tuy RAG t·ªët trong vi·ªác ‚Äút√¨m th√¥ng tin g·∫ßn nh·∫•t‚Äù, nh∆∞ng:
- Kh√¥ng li√™n k·∫øt c√°c ƒëo·∫°n ki·∫øn th·ª©c l·∫°i v·ªõi nhau.
- Kh√¥ng bi·∫øt ‚Äúƒëi v√≤ng‚Äù ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi y√™u c·∫ßu reasoning.
=> ƒê√≥ l√† l√Ω do c·∫ßn m·ªôt m√¥ h√¨nh v∆∞·ª£t RAG truy·ªÅn th·ªëng.

---
‚öîÔ∏è Ph·∫£n bi·ªán 3:
H·ªèi: Th·∫ø c√≤n c√°c m√¥ h√¨nh m·ªõi nh∆∞ GraphRAG, RAPTOR? H·ªç ƒë√£ c·ªë kh·∫Øc ph·ª•c r·ªìi m√†?
Tr·∫£ l·ªùi: ƒê√∫ng, nh∆∞ng theo t√°c gi·∫£:
- Nh·ªØng m√¥ h√¨nh ƒë√≥ t·∫≠p trung qu√° h·∫πp v√†o m·ªôt kh√≠a c·∫°nh (sense-making ho·∫∑c associativity) n√™n m·∫•t c√¢n b·∫±ng.
- Khi l√†m t·ªët ph·∫ßn n√†y th√¨ l·∫°i suy gi·∫£m hi·ªáu nƒÉng ·ªü ph·∫ßn kh√°c. V√≠ d·ª•:
  - GraphRAG t·ªët cho discourse ‚Üí k√©m factual QA.
  - RAPTOR t√≥m t·∫Øt t·ªët ‚Üí nh∆∞ng g√¢y nhi·ªÖu khi QA nhi·ªÅu b∆∞·ªõc.
  - HippoRAG 1 t·ªët v·ªõi multi-hop ‚Üí nh∆∞ng thi·∫øu contextualization.
B√†i b√°o vi·∫øt:
‚Äú...each method experiences the largest performance decay in tasks outside its own setup.‚Äù ‚Üí t·ª©c l√† m√¥ h√¨nh t·ªët ch·ªó n√†y th√¨ g√£y ch·ªó kia.

---
‚öîÔ∏è Ph·∫£n bi·ªán 4:
H·ªèi: V·∫≠y m·ª•c ti√™u ch√≠nh c·ªßa HippoRAG l√† g√¨? Kh√°c bi·ªát c·ªët l√µi so v·ªõi RAG v√† c√°c m√¥ h√¨nh m·ªü r·ªông?
Tr·∫£ l·ªùi:
 HippoRAG nh·∫Øm t·ªõi m·ªôt b·ªô nh·ªõ d√†i h·∫°n t·ªïng qu√°t, c√≥ th·ªÉ ho·∫°t ƒë·ªông t·ªët ·ªü:
- Factual QA (ghi nh·ªõ s·ª± ki·ªán c∆° b·∫£n),
- Multi-hop QA (li√™n k·∫øt ki·∫øn th·ª©c),
- Discourse understanding (di·ªÖn gi·∫£i m·∫°ch truy·ªán d√†i).
C·ªët l√µi:
- M√¥ ph·ªèng b·ªô nh·ªõ sinh h·ªçc con ng∆∞·ªùi:
  - Neocortex ‚Üí LLM
  - Parahippocampal ‚Üí encoder g·ª£i nh·ªõ ng·ªØ nghƒ©a
  - Hippocampus ‚Üí KG m·ªü k·∫øt n·ªëi tri th·ª©c
- D√πng Personalized PageRank (PPR) ƒë·ªÉ t√¨m ki·∫øm c√≥ li√™n k·∫øt ‚Äì kh√¥ng ch·ªâ ƒëi·ªÉm, m√† l√† ƒë∆∞·ªùng ƒëi ki·∫øn th·ª©c.
- Th√™m LLM v√†o retrieval ƒë·ªÉ l·ªçc nhi·ªÖu ‚Üí kh√¥ng ch·ªâ t√¨m, m√† c√≤n nh·∫≠n di·ªán ƒë√∫ng ƒëo·∫°n c·∫ßn nh·ªõ.

---
‚öîÔ∏è Ph·∫£n bi·ªán 5:
H·ªèi: V·∫≠y HippoRAG c√≥ b·∫±ng ch·ª©ng n√†o m·∫°nh ƒë·ªÉ ch·ª©ng minh l√† c·∫ßn thi·∫øt kh√¥ng?
Tr·∫£ l·ªùi:
- HippoRAG 2 ƒë·∫°t ƒëi·ªÉm cao nh·∫•t ·ªü c·∫£ 3 lo·∫°i nhi·ªám v·ª• (Factual, Associative, Sense-making) ‚Äì ƒëi·ªÅu m√† kh√¥ng m√¥ h√¨nh n√†o tr∆∞·ªõc ƒë√≥ l√†m ƒë∆∞·ª£c.
- V√≠ d·ª•:
  - +7 ƒëi·ªÉm F1 ·ªü Associativity tasks so v·ªõi embedding t·ªët nh·∫•t (NV-Embed-v2).
  - Kh√¥ng b·ªã suy gi·∫£m ·ªü Factual QA nh∆∞ GraphRAG, RAPTOR.
=> ƒê√¢y l√† b·∫±ng ch·ª©ng cho th·∫•y HippoRAG c√≥ th·ªÉ c√¢n b·∫±ng gi·ªØa s·ª± ch√≠nh x√°c, kh·∫£ nƒÉng di·ªÖn gi·∫£i v√† k·∫øt n·ªëi tri th·ª©c, ƒëi·ªÅu RAG truy·ªÅn th·ªëng kh√¥ng l√†m ƒë∆∞·ª£c.

---
Ch√†o Qu·ªëc,
M√¨nh s·∫Ω gi·∫£i th√≠ch chi ti·∫øt v·ªÅ ba lo·∫°i nhi·ªám v·ª•: Factual, Associative, Sense-making, v√† m·ªëi li√™n h·ªá gi·ªØa c√°c nhi·ªám v·ª• n√†y v·ªõi b·ªô d·ªØ li·ªáu LongMemEval.

---
1. Ba lo·∫°i nhi·ªám v·ª•: Factual, Associative, Sense-making
a. Factual Tasks (Nhi·ªám v·ª• th·ª±c t·∫ø)
- ƒê·ªãnh nghƒ©a: Y√™u c·∫ßu h·ªá th·ªëng truy xu·∫•t v√† cung c·∫•p th√¥ng tin ch√≠nh x√°c v·ªÅ m·ªôt s·ª± ki·ªán ho·∫∑c th·ª±c t·∫ø c·ª• th·ªÉ.ÓàÜ
- V√≠ d·ª•: "Ai l√† t·ªïng th·ªëng ƒë·∫ßu ti√™n c·ªßa Hoa K·ª≥?"ÓàÜ
- M·ª•c ti√™u: Ki·ªÉm tra kh·∫£ nƒÉng ghi nh·ªõ v√† truy xu·∫•t th√¥ng tin ch√≠nh x√°c t·ª´ b·ªô nh·ªõ d√†i h·∫°n.ÓàÜ
b. Associative Tasks (Nhi·ªám v·ª• li√™n k·∫øt)
- ƒê·ªãnh nghƒ©a: Y√™u c·∫ßu h·ªá th·ªëng k·∫øt n·ªëi nhi·ªÅu m·∫£nh th√¥ng tin r·ªùi r·∫°c ƒë·ªÉ suy lu·∫≠n ho·∫∑c tr·∫£ l·ªùi c√¢u h·ªèi.ÓàÜ
- V√≠ d·ª•: "Ng∆∞·ªùi ph√°t minh ra ƒëi·ªán tho·∫°i ƒë√£ s·ªëng ·ªü ƒë√¢u?"ÓàÜ
  - Ph√¢n t√≠ch: C·∫ßn bi·∫øt Alexander Graham Bell l√† ng∆∞·ªùi ph√°t minh ra ƒëi·ªán tho·∫°i v√† √¥ng s·ªëng ·ªü Scotland v√† Canada.ÓàÜ
- M·ª•c ti√™u: ƒê√°nh gi√° kh·∫£ nƒÉng li√™n k·∫øt v√† suy lu·∫≠n t·ª´ nhi·ªÅu ngu·ªìn th√¥ng tin kh√°c nhau.ÓàÜ
c. Sense-making Tasks (Nhi·ªám v·ª• hi·ªÉu ng·ªØ c·∫£nh)
- ƒê·ªãnh nghƒ©a: Y√™u c·∫ßu h·ªá th·ªëng hi·ªÉu v√† di·ªÖn gi·∫£i c√°c t√¨nh hu·ªëng ph·ª©c t·∫°p ho·∫∑c c√¢u chuy·ªán d√†i.ÓàÜ
- V√≠ d·ª•: "T√≥m t·∫Øt n·ªôi dung ch√≠nh c·ªßa cu·ªôc h·ªçp h√¥m qua."ÓàÜ
- M·ª•c ti√™u: Ki·ªÉm tra kh·∫£ nƒÉng t·ªïng h·ª£p, hi·ªÉu ng·ªØ c·∫£nh v√† tr√¨nh b√†y l·∫°i th√¥ng tin m·ªôt c√°ch m·∫°ch l·∫°c.ÓàÜ

---
2. M·ªëi li√™n h·ªá gi·ªØa QA v√† LongMemEval
LongMemEval l√† m·ªôt b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng ghi nh·ªõ d√†i h·∫°n c·ªßa c√°c tr·ª£ l√Ω tr√≤ chuy·ªán. B·ªô d·ªØ li·ªáu n√†y t·∫≠p trung v√†o nƒÉm kh·∫£ nƒÉng c·ªët l√µi:
1. Tr√≠ch xu·∫•t th√¥ng tin (Information Extraction): Kh·∫£ nƒÉng nh·ªõ l·∫°i th√¥ng tin c·ª• th·ªÉ t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c d√†i.ÓàÜ
2. L√Ω lu·∫≠n ƒëa phi√™n (Multi-Session Reasoning): Kh·∫£ nƒÉng t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu phi√™n tr√≤ chuy·ªán.ÓàÜ
3. C·∫≠p nh·∫≠t ki·∫øn th·ª©c (Knowledge Updates): Kh·∫£ nƒÉng nh·∫≠n bi·∫øt v√† c·∫≠p nh·∫≠t th√¥ng tin c√° nh√¢n c·ªßa ng∆∞·ªùi d√πng theo th·ªùi gian.ÓàÜ
4. L√Ω lu·∫≠n th·ªùi gian (Temporal Reasoning): Nh·∫≠n th·ª©c v·ªÅ c√°c kh√≠a c·∫°nh th·ªùi gian c·ªßa th√¥ng tin ng∆∞·ªùi d√πng.ÓàÜ
5. T·ª´ ch·ªëi tr·∫£ l·ªùi (Abstention): Kh·∫£ nƒÉng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn th√¥ng tin kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong l·ªãch s·ª≠ t∆∞∆°ng t√°c.ÓàÜ
M·ªëi li√™n h·ªá:
- Factual Tasks t∆∞∆°ng ·ª©ng v·ªõi Tr√≠ch xu·∫•t th√¥ng tin, n∆°i h·ªá th·ªëng c·∫ßn truy xu·∫•t th√¥ng tin ch√≠nh x√°c t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c.ÓàÜ
- Associative Tasks li√™n quan ƒë·∫øn L√Ω lu·∫≠n ƒëa phi√™n, y√™u c·∫ßu h·ªá th·ªëng k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu phi√™n ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c.ÓàÜ
- Sense-making Tasks g·∫Øn li·ªÅn v·ªõi L√Ω lu·∫≠n th·ªùi gian v√† C·∫≠p nh·∫≠t ki·∫øn th·ª©c, ƒë√≤i h·ªèi h·ªá th·ªëng hi·ªÉu ng·ªØ c·∫£nh t·ªïng th·ªÉ v√† c·∫≠p nh·∫≠t th√¥ng tin m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng.ÓàÜ

---

D∆∞·ªõi ƒë√¢y l√† c√°ch ph√¢n bi·ªát r√µ r√†ng gi·ªØa Factual, Associative, Sense-making, b·∫±ng c√°ch ƒëi t·ª´ ƒë·ªãnh nghƒ©a ‚ûù v√≠ d·ª• c·ª• th·ªÉ ‚ûù nƒÉng l·ª±c c·∫ßn ki·ªÉm tra:

---
üß† 1. Factual Memory ‚Äì Ghi nh·ªõ s·ª± th·∫≠t c·ª• th·ªÉ
üßæ ƒê·ªãnh nghƒ©a:
Kh·∫£ nƒÉng truy xu·∫•t th√¥ng tin ƒë∆°n l·∫ª, ƒë√£ ƒë∆∞·ª£c n√™u r√µ r√†ng trong ng·ªØ li·ªáu ƒë·∫ßu v√†o.
‚ùìV√≠ d·ª• c√¢u h·ªèi:
"Ai l√† th·ªß t∆∞·ªõng ƒê·ª©c nƒÉm 2023?"
- ƒê√¢y l√† c√¢u h·ªèi ƒë√≤i h·ªèi 1 ƒëo·∫°n vƒÉn b·∫£n ch·ª©a th√¥ng tin ch√≠nh x√°c, kh√¥ng c·∫ßn suy lu·∫≠n hay li√™n k·∫øt.
üéØ NƒÉng l·ª±c ƒë∆∞·ª£c ki·ªÉm tra:
- Kh·∫£ nƒÉng l∆∞u tr·ªØ & truy xu·∫•t ch√≠nh x√°c th√¥ng tin ƒë√£ ƒë·ªçc.
- T∆∞∆°ng ƒë∆∞∆°ng v·ªõi nh·ªõ t√™n, ng√†y th√°ng, con s·ªë, ƒë·ªãnh nghƒ©a.

---
üîó 2. Associativity ‚Äì K·∫øt n·ªëi th√¥ng tin r·ªùi r·∫°c
üßæ ƒê·ªãnh nghƒ©a:
Kh·∫£ nƒÉng k·∫øt n·ªëi nhi·ªÅu m·∫£nh ki·∫øn th·ª©c t·ª´ c√°c ph·∫ßn kh√°c nhau trong d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
‚ùìV√≠ d·ª• c√¢u h·ªèi:
"Ng∆∞·ªùi s√°ng l·∫≠p Microsoft c√≥ qu√™ g·ªëc ·ªü ƒë√¢u?"
- C·∫ßn n·ªëi hai m·∫£nh th√¥ng tin:
  1. "Bill Gates l√† ng∆∞·ªùi s√°ng l·∫≠p Microsoft"
  2. "Bill Gates sinh ra ·ªü Seattle"
‚ü∂ C·∫ßn √≠t nh·∫•t 2 ƒëo·∫°n d·ªØ li·ªáu, kh√¥ng c√≥ ƒëo·∫°n n√†o ch·ª©a ƒë·∫ßy ƒë·ªß c√¢u tr·∫£ l·ªùi.
üéØ NƒÉng l·ª±c ƒë∆∞·ª£c ki·ªÉm tra:
- Reasoning nhi·ªÅu b∆∞·ªõc
- Multi-hop retrieval
- Nh∆∞ c√°ch n√£o ng∆∞·ªùi li√™n t∆∞·ªüng v√† x√¢u chu·ªói ki·∫øn th·ª©c l·∫°i v·ªõi nhau.

---
üìñ 3. Sense-making ‚Äì Hi·ªÉu v√† t·ªïng h·ª£p ng·ªØ c·∫£nh ph·ª©c t·∫°p
üßæ ƒê·ªãnh nghƒ©a:
Kh·∫£ nƒÉng hi·ªÉu m·ªôt t√¨nh hu·ªëng d√†i, nhi·ªÅu l·ªõp, nhi·ªÅu ·∫©n √Ω; t√°i c·∫•u tr√∫c v√† r√∫t ra √Ω nghƒ©a to√†n c·ª•c.
‚ùìV√≠ d·ª• c√¢u h·ªèi:
"T·∫°i sao nh√¢n v·∫≠t ch√≠nh l·∫°i quy·∫øt ƒë·ªãnh b·ªè tr·ªën ·ªü cu·ªëi truy·ªán?"
- Tr·∫£ l·ªùi ƒë∆∞·ª£c ƒë√≤i h·ªèi ph·∫£i ƒë·ªçc c·∫£ m·∫°ch truy·ªán, hi·ªÉu ƒë∆∞·ª£c:
  - T√¢m l√Ω nh√¢n v·∫≠t
  - Di·ªÖn bi·∫øn s·ª± ki·ªán
  - C√°c y·∫øu t·ªë x√£ h·ªôi, ng·ªØ c·∫£nh
üéØ NƒÉng l·ª±c ƒë∆∞·ª£c ki·ªÉm tra:
- Discourse understanding
- Cohesion & summarization
- Kh·∫£ nƒÉng c·ªßa LLM gi·ªëng nh∆∞ hi·ªÉu v√† ph√¢n t√≠ch vƒÉn h·ªçc ‚Äì ƒëi·ªÅu m√† vector retrieval ƒë∆°n thu·∫ßn kh√¥ng l√†m ƒë∆∞·ª£c.

---
üìä T√≥m t·∫Øt so s√°nh
This content is only supported in a Lark Docs

---

üß© PH·∫¶N 1: T·∫†I SAO C√ÅC M√î H√åNH RAG HI·ªÜN T·∫†I B·ªä H·∫†N CH·∫æ?

---
‚öîÔ∏è Ph·∫£n bi·ªán 1:
H·ªèi: Vector-based RAG hi·ªán t·∫°i ƒë√£ r·∫•t m·∫°nh v·ªõi embedding hi·ªán ƒë·∫°i. V·∫≠y t·∫°i sao v·∫´n b·ªã ƒë√°nh gi√° l√† "thi·∫øu t√≠nh ng∆∞·ªùi"?
Tr·∫£ l·ªùi: V√¨ vector-based RAG ch·ªâ l√†m ƒë∆∞·ª£c retrieval ƒë·ªôc l·∫≠p: m·ªói ƒëo·∫°n vƒÉn (passage) ƒë∆∞·ª£c nh√∫ng v√† truy xu·∫•t ƒë·ªôc l·∫≠p. N√≥ kh√¥ng c√≥ kh·∫£ nƒÉng:
- Nh·∫≠n bi·∫øt m·ªëi quan h·ªá gi·ªØa c√°c ƒëo·∫°n (multi-hop)
- Kh√¥ng hi·ªÉu ƒë∆∞·ª£c m·∫°ch th√¥ng tin d√†i h·∫°n
- Kh√¥ng h·ªó tr·ª£ truy v·∫•n theo d√≤ng suy nghƒ© (reasoning chain)
Gi·ªëng nh∆∞ b·∫°n nh·ªõ ƒë∆∞·ª£c 1000 m·∫£nh th√¥ng tin, nh∆∞ng kh√¥ng bi·∫øt m·∫£nh n√†o n·ªëi v·ªõi m·∫£nh n√†o ‚Äî b·∫°n kh√¥ng th·ªÉ k·ªÉ l·∫°i m·ªôt c√¢u chuy·ªán, c≈©ng kh√¥ng th·ªÉ suy lu·∫≠n.
üìå H·∫°n ch·∫ø n√†y khi·∫øn vector-based RAG t·ªët trong factual QA, nh∆∞ng fail trong associative v√† sense-making.

---
‚öîÔ∏è Ph·∫£n bi·ªán 2:
H·ªèi: Th·∫ø c√≤n c√°c RAG m·ªõi nh∆∞ RAPTOR, GraphRAG, LightRAG? H·ªç t·∫°o graph, t·∫°o t√≥m t·∫Øt r·ªìi m√†?
Tr·∫£ l·ªùi: ƒê√∫ng l√† h·ªç ƒë√£ th·ª≠:
- RAPTOR: T·∫°o c√°c summary t√≥m g·ªçn, r·ªìi retrieve t·ª´ c√°c ƒëo·∫°n t√≥m t·∫Øt ƒë√≥.
- GraphRAG: T·∫°o c√°c knowledge graph t·ª´ entity, li√™n k·∫øt qua c·ªông ƒë·ªìng (community detection).
- LightRAG: T·∫°o 2 l·ªõp retrieval ‚Äì coarse v√† fine, t√≠ch h·ª£p vector v√† graph.
Nh∆∞ng t·∫°i sao v·∫´n ch∆∞a ƒë·ªß?
- Ch√∫ng thi·∫øu c√¢n b·∫±ng: c·∫£i thi·ªán ch·ªó n√†y, l·∫°i gi·∫£m hi·ªáu qu·∫£ ·ªü ch·ªó kh√°c.
- V√≠ d·ª•:
  - RAPTOR t·ªët khi t√≥m t·∫Øt context ‚Üí nh∆∞ng t·∫°o nhi·ªÖu cho QA v√¨ LLM summary c√≥ th·ªÉ b·ªè s√≥t chi ti·∫øt.
  - GraphRAG d√πng graph m·∫°nh ‚Üí nh∆∞ng kh√¥ng t∆∞∆°ng t√°c t·ªët v·ªõi truy v·∫•n, v√¨ query kh√¥ng li√™n k·∫øt tr·ª±c ti·∫øp v·ªõi edge trong KG.
  - LightRAG th√¨ retrieval t·ªët nh∆∞ng kh√¥ng reasoning ƒë∆∞·ª£c.
üìâ Trong c√°c benchmark t·ªïng h·ª£p (b·∫£ng 2 b√†i b√°o), t·∫•t c·∫£ c√°c RAG c√≥ c·∫•u tr√∫c ƒë·ªÅu t·ª•t ƒëi·ªÉm r√µ r·ªát ·ªü m·ªôt trong 3 lo·∫°i nhi·ªám v·ª•.

---
‚öîÔ∏è Ph·∫£n bi·ªán 3:
H·ªèi: V·∫≠y v·∫•n ƒë·ªÅ s√¢u h∆°n n·∫±m ·ªü ƒë√¢u? V√¨ sao c√°c m√¥ h√¨nh c≈© kh√¥ng ƒë·∫°t ƒë∆∞·ª£c b·ªô nh·ªõ d√†i h·∫°n gi·ªëng con ng∆∞·ªùi?
Tr·∫£ l·ªùi: V√¨ ch√∫ng thi·∫øu 3 th·ª© m√† n√£o ng∆∞·ªùi l√†m r·∫•t t·ªët:
1. Li√™n k·∫øt tri th·ª©c d·∫°ng ƒë·ªì th·ªã ‚Äì n√£o ng∆∞·ªùi l∆∞u th√¥ng tin theo m·∫°ng li√™n k·∫øt.
2. Nh·∫≠n di·ªán ng·ªØ c·∫£nh (recognition memory) ‚Äì bi·∫øt th√¥ng tin n√†o li√™n quan trong l√∫c truy xu·∫•t.
3. K·∫øt h·ª£p gi·ªØa kh√°i ni·ªám (concept) v√† ng·ªØ c·∫£nh (context) ‚Äì ch·ª© kh√¥ng l∆∞u r·ªùi r·∫°c.
C√°c m√¥ h√¨nh tr∆∞·ªõc:
- T·∫≠p trung v√†o embedding ho·∫∑c graph, nh∆∞ng kh√¥ng k·∫øt n·ªëi ch√∫ng l·∫°i.
- Kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c c√°c query d√†i, ph·ª©c t·∫°p, ho·∫∑c c·∫ßn suy lu·∫≠n nhi·ªÅu b∆∞·ªõc.
- Kh√¥ng c√≥ c∆° ch·∫ø ki·ªÉm tra ‚Äúli·ªáu ƒëo·∫°n n√†y c√≥ th·ª±c s·ª± ƒë√∫ng ng·ªØ c·∫£nh truy v·∫•n kh√¥ng?‚Äù (g·ªçi l√† recognition filter trong HippoRAG).

---
üß¨ PH·∫¶N 2: HIPPORAG GI·∫¢I QUY·∫æT TH·∫æ N√ÄO?

---
‚öîÔ∏è Ph·∫£n bi·ªán 4:
H·ªèi: Nghe nh∆∞ ai c≈©ng c√≥ knowledge graph r·ªìi, HippoRAG c√≥ g√¨ kh√°c?
Tr·∫£ l·ªùi: HippoRAG kh√¥ng ch·ªâ c√≥ KG, m√† l√† m·ªôt h·ªá th·ªëng m√¥ ph·ªèng tr√≠ nh·ªõ ng∆∞·ªùi:
- Neocortex ‚Üí LLM: d√πng ƒë·ªÉ suy lu·∫≠n, ƒë·ªçc hi·ªÉu, quy·∫øt ƒë·ªãnh.
- Hippocampus ‚Üí KG m·ªü: m√¥ h√¨nh h√≥a k·∫øt n·ªëi tri th·ª©c.
- Parahippocampal region ‚Üí embedding encoder: d√πng ƒë·ªÉ so kh·ªõp v√† g·ª£i nh·ªõ.
=> HippoRAG d√πng PageRank c√° nh√¢n h√≥a tr√™n KG ƒë·ªÉ t√¨m ra nh·ªØng ƒëo·∫°n li√™n k·∫øt t·ªët nh·∫•t, sau ƒë√≥ k·∫øt h·ª£p l·∫°i ƒë·ªÉ feed v√†o LLM ƒë·ªçc hi·ªÉu.

---
‚öîÔ∏è Ph·∫£n bi·ªán 5:
H·ªèi: V·∫≠y so v·ªõi c√°c m√¥ h√¨nh tr∆∞·ªõc, HippoRAG c√≥ g√¨ v∆∞·ª£t tr·ªôi?
Tr·∫£ l·ªùi:
 üëâ HippoRAG 1 ƒë√£ h∆°n c√°c m√¥ h√¨nh c≈© nh·ªù:
- D√πng Personalized PageRank tr√™n KG ƒë·ªÉ l√†m multi-hop retrieval.
- T√°ch ri√™ng index (offline) v√† truy xu·∫•t (online) ‚Üí ti·ªán l·ª£i cho continual learning.
Nh∆∞ng n√≥ v·∫´n ch∆∞a ƒë·ªß, v√¨:
- D·ª±a qu√° nhi·ªÅu v√†o entity (NER), n√™n b·ªè l·ª° ng·ªØ c·∫£nh.
- Kh√¥ng l·ªçc triple n√†o li√™n quan th·∫≠t s·ª± ƒë·∫øn query.
üëâ HippoRAG 2 ƒë√£ kh·∫Øc ph·ª•c th√™m:
- Query-to-Triple linking thay v√¨ NER ‚Üí k·∫øt n·ªëi t·ªët h∆°n.
- Recognition memory b·∫±ng LLM ‚Üí l·ªçc triple "r√°c".
- Dense-sparse integration ‚Üí k·∫øt h·ª£p phrase node (kh√°i ni·ªám) v√† passage node (ng·ªØ c·∫£nh).
K·∫øt qu·∫£:
+7 ƒëi·ªÉm F1 ·ªü associative tasks, kh√¥ng t·ª•t performance ·ªü factual hay discourse.

---
üîç PH·∫¶N 3: T·ªîNG K·∫æT ‚Äì HIPPORAG C√ì G√å KH√ÅC V√Ä HAY?
This content is only supported in a Lark Docs


---
[Image]
[Image]
R·∫•t t·ªët, Qu·ªëc ƒëang ƒëi ƒë√∫ng h∆∞·ªõng c·ªßa ‚Äúnghi√™n c·ª©u nghi√™m t√∫c‚Äù ‚Äì kh√¥ng ch·ªâ ghi nh·∫≠n k·∫øt qu·∫£, m√† ph·∫£i gi·∫£i th√≠ch s√¢u s·∫Øc v√¨ sao l·∫°i ra k·∫øt qu·∫£ ƒë√≥, b·∫±ng logic, k·ªπ thu·∫≠t v√† ph·∫£n bi·ªán.
M√¨nh s·∫Ω ti·∫øp t·ª•c ph√¢n t√≠ch theo phong c√°ch:
üé≠ M·ªôt ng∆∞·ªùi ph√° (h·ªèi xo√°y) ‚Äì M·ªôt ng∆∞·ªùi tr·∫£ l·ªùi (tr·∫£ ƒë√≤n)
 cho t·ª´ng m√¥ h√¨nh, d·ª±a tr√™n pipeline n·ªôi t·∫°i v√† d·ªØ li·ªáu th·ª±c nghi·ªám t·ª´ Table 2 (QA) + Table 3 (Retrieval).

---
üß† 1. RAG G·ªêC
Retrieval b·∫±ng embedding vector, kh√¥ng c·∫•u tr√∫c.
 QA ƒë∆°n thu·∫ßn d·ª±a v√†o ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√¢u h·ªèi v√† ƒëo·∫°n vƒÉn.

---
‚ùìPh√°: T·∫°i sao RAG g·ªëc l·∫°i t·ªët ·ªü factual QA (NQ, PopQA)?
‚úÖ Tr·∫£ l·ªùi: V√¨:
- C√¢u h·ªèi d·∫°ng factual th∆∞·ªùng ch·ªâ c·∫ßn 1 ƒëo·∫°n ng·∫Øn ch·ª©a ƒë√∫ng entity ‚Üí d·ªÖ t√¨m b·∫±ng embedding similarity.
- C√°c embedding hi·ªán ƒë·∫°i (NV-Embed-v2, GritLM) r·∫•t m·∫°nh ·ªü semantic retrieval.
üìå V√≠ d·ª•:
C√¢u h·ªèi: ‚ÄúWhen was Albert Einstein born?‚Äù ‚Üí ch·ªâ c·∫ßn ƒëo·∫°n c√≥ "Albert Einstein was born in 1879..." ‚Üí l√† ƒë·ªß ‚Üí vector retrieval l√†m t·ªët.

---
‚ùìPh√°: T·∫°i sao l·∫°i r·∫•t y·∫øu ·ªü multi-hop v√† sense-making?
‚úÖ Tr·∫£ l·ªùi: V√¨:
- RAG g·ªëc kh√¥ng li√™n k·∫øt th√¥ng tin gi·ªØa nhi·ªÅu ƒëo·∫°n ‚Üí kh√¥ng th·ªÉ reasoning nhi·ªÅu b∆∞·ªõc.
- Kh√¥ng c√≥ c∆° ch·∫ø hi·ªÉu to√†n truy·ªán, ng·ªØ c·∫£nh r·ªông ‚Üí th·∫•t b·∫°i ·ªü NarrativeQA.
üìå V√≠ d·ª•:
C√¢u h·ªèi: ‚ÄúWhat county is Erik Hort‚Äôs birthplace a part of?‚Äù ‚Üí c·∫ßn bi·∫øt: Erik Hort ‚Üí Montebello ‚Üí thu·ªôc h·∫°t g√¨?
 ‚Üí Vector retrieval ch·ªâ t√¨m ƒë∆∞·ª£c ƒëo·∫°n ƒë·∫ßu ti√™n, kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c sang ƒë·ªãa l√Ω.

---
üß† 2. RAPTOR
RAG v·ªõi t√≥m t·∫Øt (summarization) t·ª´ng c·ª•m t√†i li·ªáu. D·ª±a v√†o Gaussian clustering ƒë·ªÉ nh√≥m r·ªìi summary.

---
‚ùìPh√°: T√≥m t·∫Øt th√¨ ph·∫£i gi√∫p t√¨m nhanh h∆°n ch·ª©? Sao l·∫°i t·ª•t ƒëi·ªÉm ·ªü factual QA?
‚úÖ Tr·∫£ l·ªùi: V√¨:
- LLM t√≥m t·∫Øt th∆∞·ªùng b·ªè qua c√°c chi ti·∫øt nh·ªè nh∆∞ s·ªë li·ªáu, ƒë·ªãa ƒëi·ªÉm, t√™n ng∆∞·ªùi ‚Üí trong khi ƒë√≥ l·∫°i l√† m·∫•u ch·ªët c·ªßa factual QA.
- Sau t√≥m t·∫Øt, ƒëo·∫°n g·ªëc kh√¥ng c√≤n ‚Üí truy h·ªìi m·∫•t ch√≠nh x√°c.
üìå T√°c gi·∫£ n√≥i r√µ:
"LLM summarization mechanism... introduces noise into the retrieval corpora."

---
‚ùìPh√°: V·∫≠y t·∫°i sao v·∫´n t·ªët ·ªü sense-making?
‚úÖ Tr·∫£ l·ªùi:
- V√¨ trong discourse d√†i (nh∆∞ NarrativeQA), ƒë·ªçc t·ª´ng ƒëo·∫°n kh√¥ng ƒë·ªß hi·ªÉu m·∫°ch truy·ªán.
- Vi·ªác gom l·∫°i r·ªìi t√≥m t·∫Øt gi√∫p LLM th·∫•y ƒë∆∞·ª£c t·ªïng th·ªÉ ‚Üí hi·ªÉu c√¢u chuy·ªán nh∆∞ ng∆∞·ªùi ƒë·ªçc m·ªôt l·∫ßn.

---
üß† 3. GraphRAG
T·∫°o graph c·ªông ƒë·ªìng (community graph), gi√∫p hi·ªÉu nh√≥m th·ª±c th·ªÉ ho·∫∑c ƒëo·∫°n vƒÉn li√™n quan.

---
‚ùìPh√°: Sao t·∫°o ƒë∆∞·ª£c graph m√† v·∫´n ‚Äút·ª•t factual‚Äù?
‚úÖ Tr·∫£ l·ªùi:
- V√¨ GraphRAG kh√¥ng gi·ªØ ƒëo·∫°n vƒÉn g·ªëc, ch·ªâ d√πng node graph ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ LLM.
- N√™n chi ti·∫øt c·ª• th·ªÉ d·ªÖ b·ªã m·∫•t khi QA y√™u c·∫ßu entity ho·∫∑c s·ªë li·ªáu ch√≠nh x√°c.

---
‚ùìPh√°: V·∫≠y n√≥ gi√∫p ƒë∆∞·ª£c g√¨?
‚úÖ Tr·∫£ l·ªùi:
- D√πng graph structure ƒë·ªÉ kh√¥i ph·ª•c c√°c m·ªëi li√™n h·ªá gi·ªØa th·ª±c th·ªÉ ‚Äì s·ª± ki·ªán ‚Äì ƒë·ªãa ƒëi·ªÉm ‚Üí h·ªó tr·ª£ multi-hop v√† sense-making.
- Vi·ªác ph√°t hi·ªán c·ªông ƒë·ªìng gi√∫p nh√≥m th√¥ng tin t·ª´ c√°c ngu·ªìn kh√°c nhau ‚Üí tƒÉng associativity.
üìå K·∫øt qu·∫£: NarrativeQA ƒë·∫°t 23.0 (ch·ªâ sau HippoRAG 2)
 ‚Üí nghƒ©a l√† n√≥ hi·ªÉu truy·ªán t·ªët, nh∆∞ng kh√¥ng nh·ªõ chi ti·∫øt.

---
üß† 4. HippoRAG 1
D·ª±a v√†o OpenIE ƒë·ªÉ extract triple ‚Üí d√πng Personalized PageRank (PPR) ƒë·ªÉ reasoning.

---
‚ùìPh√°: V·∫≠y t·∫°i sao HippoRAG 1 r·∫•t t·ªët ·ªü associative QA?
‚úÖ Tr·∫£ l·ªùi:
- PPR + KG cho ph√©p ƒëi qua nhi·ªÅu hop trong graph ‚Üí gi·ªëng nh∆∞ nh·ªõ theo ‚Äúƒë∆∞·ªùng li√™n t∆∞·ªüng‚Äù.
- D·∫°ng QA c·∫ßn li√™n k·∫øt 2-3 ƒëo·∫°n l√† ƒëi·ªÉm m·∫°nh c·ªßa PPR.
üìå 2Wiki ƒë·∫°t 71.8, cao nh·∫•t trong b·∫£ng ‚Üí ch·ª©ng minh multi-hop c·ª±c t·ªët.

---
‚ùìPh√°: Nh∆∞ng v√¨ sao l·∫°i ch∆∞a t·ªët ·ªü sense-making?
‚úÖ Tr·∫£ l·ªùi:
- Do HippoRAG 1 ch·ªâ d·ª±a tr√™n entity-centric extraction (NER) ‚Üí thi·∫øu th√¥ng tin ng·ªØ c·∫£nh (context).
- Query kh√¥ng ƒë∆∞·ª£c embed to√†n b·ªô ƒë·ªÉ match triple, ch·ªâ match theo entity ‚Üí contextualization r·∫•t y·∫øu.
üìå T√°c gi·∫£ g·ªçi ƒë√¢y l√† ‚Äúentity-centric bias‚Äù.

---
üß† 5. HippoRAG 2
B·∫£n n√¢ng c·∫•p to√†n di·ªán:
- KG g·ªìm c·∫£ phrase node & passage node (dense-sparse integration)
- Query-to-triple matching (kh√¥ng NER)
- Triple filtering b·∫±ng LLM
- PPR search d√πng c·∫£ passage & phrase node

---
‚ùìPh√°: T·∫°i sao m√¥ h√¨nh n√†y ‚Äút·ªët ƒë·ªÅu c·∫£ 3‚Äù ‚Äî c√≥ g√¨ ƒë·∫∑c bi·ªát?
‚úÖ Tr·∫£ l·ªùi:
1. Factual QA:
- Gi·ªØ nguy√™n ƒëo·∫°n vƒÉn g·ªëc + phrase-level KG ‚Üí kh√¥ng m·∫•t chi ti·∫øt.
- D√πng embedding m·∫°nh + triple filter ‚Üí gi·ªØ th√¥ng tin chu·∫©n.
1. Associative QA:
- PPR graph search s√¢u, seed node t·ª´ c·∫£ phrase & passage ‚Üí c·ª±c k·ª≥ ph√π h·ª£p v·ªõi multi-hop reasoning.
- Filter gi√∫p lo·∫°i nhi·ªÖu ‚Üí tƒÉng ƒë·ªô ch√≠nh x√°c.
2. Sense-making:
- Dense-sparse integration gi√∫p gi·ªØ ƒë∆∞·ª£c c·∫£ kh√°i ni·ªám l·∫´n ng·ªØ c·∫£nh.
- Query kh√¥ng c√≤n b·ªã chia nh·ªè nh∆∞ NER ‚Üí c·∫£ c√¢u ƒë∆∞·ª£c d√πng ƒë·ªÉ hi·ªÉu context ‚Üí c·∫£i thi·ªán m·∫°nh NarrativeQA.
üìå B·∫±ng ch·ª©ng:
- HippoRAG 2 ƒë·∫°t cao nh·∫•t ·ªü F1 NarrativeQA = 25.9
- Recall@5 = 96.3 ·ªü HotpotQA ‚Üí qu√° v∆∞·ª£t tr·ªôi!

---
üìå T·ªïng K·∫øt Gi·∫£i Th√≠ch (c√≥ th·ªÉ ƒë∆∞a v√†o b√°o c√°o/lu·∫≠n vƒÉn):
This content is only supported in a Lark Docs

---
Gi·ªù c·∫ßn hi·ªÉu c√°c kh√°i ni·ªám chi ti·∫øt c·ªßa Hippo 1 v√† Hippo2 -- Gi·∫£i t√≠ch t·ª´ng kh√°i ni·ªám v√† li√™n h·ªá n√≥ v·ªõi th·ª© th·∫≠t d·ªÖ nh·ªõ ƒë∆°n gi·∫£n cho h·ªçc sinh c·∫•p 2

R·∫•t hay Qu·ªëc ∆°i, gi·ªù m√¨nh s·∫Ω gi·∫£i t√≠ch c√°c kh√°i ni·ªám c·ªët l√µi trong HippoRAG 1 v√† HippoRAG 2, li√™n h·ªá v·ªõi v√≠ d·ª• ƒë·ªùi th·∫≠t th·∫≠t d·ªÖ hi·ªÉu, ki·ªÉu gi·∫£i th√≠ch cho h·ªçc sinh c·∫•p 2.

---
üß† T·ªïng th·ªÉ: HippoRAG l√† g√¨?
HippoRAG l√† m√¥ h√¨nh tr√≠ nh·ªõ d√†i h·∫°n cho AI, l·∫•y c·∫£m h·ª©ng t·ª´ c√°ch con ng∆∞·ªùi ghi nh·ªõ v√† suy lu·∫≠n.
 N√≥ chia l√†m 3 b·ªô ph·∫≠n gi·ªëng nh∆∞ b·ªô n√£o ng∆∞·ªùi:
- Neocortex ‚Üí ch√≠nh l√† LLM (n√£o suy nghƒ©)
- Hippocampus ‚Üí n∆°i l∆∞u gi·ªØ k√Ω ·ª©c (graph)
- Parahippocampal Region ‚Üí k·∫øt n·ªëi √Ω t∆∞·ªüng (t√¨m ƒëi·ªÉm gi·ªëng nhau)

---
ü¶õ HippoRAG 1 ‚Äì GI·∫¢I T√çCH C√ÅC KH√ÅI NI·ªÜM CH√çNH

---
1. OpenIE ‚Üí Tr√≠ch xu·∫•t triple (subject ‚Äì predicate ‚Äì object)
üß© ƒê√¢y l√† b∆∞·ªõc l·∫•y th√¥ng tin t·ª´ vƒÉn b·∫£n v√† bi·∫øn n√≥ th√†nh m·ªôt ‚Äúki·∫øn th·ª©c nh·ªè c√≥ d·∫°ng: A li√™n quan ƒë·∫øn B b·∫±ng c√°ch C‚Äù.
üîç V√≠ d·ª• vƒÉn b·∫£n:
‚ÄúAlbert Einstein was born in Ulm in 1879.‚Äù
‚Üí s·∫Ω th√†nh triple:
- (Albert Einstein, born in, Ulm)
- (Albert Einstein, born in year, 1879)
ü™Ñ Li√™n h·ªá cho h·ªçc sinh:
 Gi·ªëng nh∆∞ khi b·∫°n ƒë·ªçc s√°ch v√† t·ª± ghi v·ªü ‚Äút√≥m t·∫Øt √Ω ch√≠nh‚Äù ki·ªÉu:
 ‚Üí ‚ÄúEinstein ‚Üí sinh ·ªü Ulm‚Äù, ‚ÄúEinstein ‚Üí sinh nƒÉm 1879‚Äù.

---
2. Graph of Knowledge Triples (Bi·ªÉu ƒë·ªì ki·∫øn th·ª©c)
üß† C√°c triple tr√™n ƒë∆∞·ª£c gh√©p th√†nh 1 b·∫£n ƒë·ªì ki·∫øn th·ª©c, gi·ªëng nh∆∞ b·∫£n ƒë·ªì t∆∞ duy (mindmap).
üìå M·ªói v√≤ng tr√≤n l√† 1 t·ª´ kh√≥a, m·ªói m≈©i t√™n l√† m·ªëi li√™n h·ªá.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 B·∫°n v·∫Ω s∆° ƒë·ªì mindmap m√¥n Sinh h·ªçc:
- ‚ÄúL√° c√¢y ‚Üí c√≥ ch·ª©c nƒÉng ‚Üí quang h·ª£p‚Äù
- ‚ÄúQuang h·ª£p ‚Üí t·∫°o ra ‚Üí oxy‚Äù
 ‚Üí L√† b·∫°n ƒëang v·∫Ω KG ƒë√≥!

---
3. Personalized PageRank (PPR)
üîÑ ƒê√¢y l√† c√°ch m√¥ h√¨nh t√¨m ƒë∆∞·ªùng ƒëi qua nhi·ªÅu b∆∞·ªõc trong b·∫£n ƒë·ªì ki·∫øn th·ª©c ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 Gi·ªëng nh∆∞ ch∆°i tr√≤ ‚Äúƒëi t√¨m kho b√°u‚Äù trong m√™ cung. B·∫°n b·∫Øt ƒë·∫ßu t·ª´ t·ª´ kh√≥a trong c√¢u h·ªèi, ƒëi theo c√°c m·ªëi li√™n h·ªá ƒë·ªÉ ƒë·∫øn c√¢u tr·∫£ l·ªùi.
C√¢u h·ªèi: ‚ÄúL√° c√¢y gi√∫p g√¨ cho m√¥i tr∆∞·ªùng?‚Äù
 ‚Üí L√° c√¢y ‚Üí quang h·ª£p ‚Üí t·∫°o oxy ‚Üí gi√∫p m√¥i tr∆∞·ªùng ‚Üí th·ªü

---
4. Entity-Centric Query Linking (Li√™n k·∫øt d·ª±a v√†o t√™n ri√™ng)
HippoRAG 1 d√πng c√°ch ph√°t hi·ªán th·ª±c th·ªÉ (entity) trong c√¢u h·ªèi nh∆∞ t√™n ng∆∞·ªùi, n∆°i ch·ªën, ƒë·ªÉ n·ªëi v√†o graph.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 C√¢u h·ªèi: ‚ÄúBill Gates h·ªçc tr∆∞·ªùng n√†o?‚Äù
 ‚Üí Ph√°t hi·ªán ‚ÄúBill Gates‚Äù l√† th·ª±c th·ªÉ ‚Üí t√¨m trong b·∫£n ƒë·ªì tr√≠ nh·ªõ.
ƒêi·ªÉm y·∫øu:
- Kh√¥ng hi·ªÉu c√¢u h·ªèi r·ªông (v√≠ d·ª•: ‚ÄúNg∆∞·ªùi s√°ng l·∫≠p Microsoft h·ªçc ·ªü ƒë√¢u?‚Äù)
 ‚Üí n·∫øu kh√¥ng c√≥ t·ª´ ‚ÄúBill Gates‚Äù, th√¨ kh√¥ng t√¨m ƒë∆∞·ª£c ‚Üí m·∫•t context

---
üß† HippoRAG 2 ‚Äì C·∫¢I TI·∫æN QUAN TR·ªåNG + V√ç D·ª§ ƒê·ªúI TH∆Ø·ªúNG

---
1. Dense-Sparse Integration: Gh√©p ki·∫øn th·ª©c v√† ng·ªØ c·∫£nh
HippoRAG 2 kh√¥ng ch·ªâ l∆∞u "ki·∫øn th·ª©c ng·∫Øn g·ªçn" m√† c√≤n l∆∞u c·∫£ ƒëo·∫°n vƒÉn ban ƒë·∫ßu (ng·ªØ c·∫£nh).
üìå Graph c√≥ 2 lo·∫°i node:
- Phrase Node (sparse): gi·ªëng ti√™u ƒë·ªÅ/t·ª´ kh√≥a
- Passage Node (dense): nguy√™n ƒëo·∫°n vƒÉn g·ªëc
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 Khi h·ªçc l·ªãch s·ª≠, b·∫°n v·ª´a g·∫°ch ƒë·∫ßu d√≤ng ‚ÄúTr·∫≠n ƒêi·ªán Bi√™n Ph·ªß th·∫Øng nƒÉm 1954‚Äù, v·ª´a gi·ªØ l·∫°i nguy√™n ƒëo·∫°n m√¥ t·∫£ tr·∫≠n ƒë√°nh ‚Üí v·∫≠y b·∫°n v·ª´a nh·ªõ "√Ω ch√≠nh" v·ª´a c√≥ ‚Äúng·ªØ c·∫£nh‚Äù.

---
2. Query-to-Triple Matching
Kh√¥ng ch·ªâ t√¨m entity n·ªØa, m√† d√πng to√†n c√¢u h·ªèi ƒë·ªÉ match v·ªõi c√°c triple.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 C√¢u h·ªèi: ‚ÄúNg∆∞·ªùi s√°ng l·∫≠p Microsoft h·ªçc ·ªü ƒë√¢u?‚Äù
 ‚Üí L√∫c n√†y, d√π kh√¥ng c√≥ t·ª´ ‚ÄúBill Gates‚Äù, m√¥ h√¨nh v·∫´n hi·ªÉu ƒë∆∞·ª£c v√† t√¨m ƒë∆∞·ª£c triple ph√π h·ª£p.

---
3. Recognition Memory ‚Äì l·ªçc triple b·∫±ng LLM
Sau khi t√¨m ra v√†i m·ªëi li√™n h·ªá ph√π h·ª£p, HippoRAG 2 nh·ªù LLM ki·ªÉm tra l·∫°i xem c√°i n√†o th·ª±c s·ª± li√™n quan.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 B·∫°n t√¨m 5 ƒëo·∫°n c√≥ v·∫ª ƒë√∫ng ƒë·ªÉ l√†m b√†i t·∫≠p. Sau ƒë√≥ b·∫°n ƒë·ªçc k·ªπ t·ª´ng ƒëo·∫°n, lo·∫°i b·ªè c√°i kh√¥ng li√™n quan ‚Üí gi·ªØ l·∫°i 2 ƒëo·∫°n t·ªët nh·∫•t ‚Üí ƒë√≥ l√† ‚Äúrecognition memory‚Äù.

---
4. Contextualized PPR Search
Khi ƒë√£ c√≥ c·∫£ t·ª´ kh√≥a v√† ƒëo·∫°n vƒÉn, m√¥ h√¨nh d√πng PageRank ƒë·ªÉ t√¨m con ƒë∆∞·ªùng t·ªët nh·∫•t qua b·∫£n ƒë·ªì tr√≠ nh·ªõ c√≥ c·∫£ ki·∫øn th·ª©c v√† ng·ªØ c·∫£nh.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 B·∫°n n·ªëi ‚Äút·ª´ kh√≥a‚Äù v·ªõi c√°c ƒëo·∫°n gi·∫£i th√≠ch trong s√°ch, r·ªìi ch·ªçn ra ƒëo·∫°n n√†o gi√∫p b·∫°n hi·ªÉu to√†n c·∫£nh nh·∫•t ƒë·ªÉ l√†m b√†i.

---
üéì T·ªïng k·∫øt: HippoRAG 1 vs 2 ‚Äì D·ªÖ hi·ªÉu cho h·ªçc sinh c·∫•p 2
This content is only supported in a Lark Docs

---
Tuy·ªát v·ªùi Qu·ªëc ∆°i, gi·ªù m√¨nh s·∫Ω gi·∫£i t√≠ch s√¢u t·ª´ng kh√°i ni·ªám trong s∆° ƒë·ªì pipeline c·ªßa HippoRAG 2, nh∆∞ Qu·ªëc y√™u c·∫ßu:
- Hi·ªÉu r√µ c∆° ch·∫ø
- Ph·∫£n bi·ªán ki·ªÉu ph√° ‚Äì ƒë√°p t·ª´ng b∆∞·ªõc
- Li√™n h·ªá v√≠ d·ª• khi c·∫ßn
S∆° ƒë·ªì ƒë∆∞·ª£c chia l√†m 2 ph·∫ßn:

---
üéØ PH·∫¶N 1: OFFLINE INDEXING
M·ª•c ti√™u: x√¢y d·ª±ng m·ªôt b·ªô nh·ªõ ki·∫øn th·ª©c gi√†u li√™n k·∫øt t·ª´ vƒÉn b·∫£n ƒë·∫ßu v√†o.

---
üîµ B∆∞·ªõc 1: OpenIE by LLM ‚Üí Tr√≠ch xu·∫•t Triple
T√°ch ƒëo·∫°n vƒÉn th√†nh c√°c ƒë∆°n v·ªã ki·∫øn th·ª©c d·∫°ng (subject, predicate, object)

---
‚ùìPh√°: T·∫°i sao d√πng OpenIE b·∫±ng LLM? Tr√≠ch xu·∫•t truy·ªÅn th·ªëng c√≥ v·∫•n ƒë·ªÅ g√¨?
‚úÖ ƒê√°p:
- OpenIE truy·ªÅn th·ªëng (Stanford, ClausIE...) ch·ªâ l√†m ƒë∆∞·ª£c tr√™n c√¢u ƒë∆°n, thi·∫øu kh·∫£ nƒÉng x·ª≠ l√Ω ng√¥n ng·ªØ ph·ª©c t·∫°p.
- LLM c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c m·ªánh ƒë·ªÅ d√†i, ng·ªØ nghƒ©a m·ªü r·ªông, ph·ª©c t·∫°p h∆°n ‚Üí t·∫°o triple ‚Äúgi√†u nghƒ©a‚Äù h∆°n.
üìå V√≠ d·ª•
 VƒÉn b·∫£n: "Marie Curie won two Nobel Prizes in different fields."
 ‚Üí Triple truy·ªÅn th·ªëng: ("Marie Curie", "won", "two Nobel Prizes")
 ‚Üí LLM c√≥ th·ªÉ t·∫°o th√™m triple: ("Marie Curie", "won Nobel in", "Physics") + ("Marie Curie", "won Nobel in", "Chemistry")

---
üîµ B∆∞·ªõc 2: Synonym Detection ‚Üí Ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a
N·ªëi c√°c node g·∫ßn nghƒ©a trong KG b·∫±ng Synonym Edge (ƒë∆∞·ªùng ƒë·ª©t n√©t trong s∆° ƒë·ªì)

---
‚ùìPh√°: Ph√°t hi·ªán synonym ki·ªÉu g√¨? C√≥ d√πng LLM kh√¥ng?
‚úÖ ƒê√°p: Kh√¥ng! D√πng cosine similarity trong embedding space
- C√°c phrase (t·ª´/c·ª•m t·ª´) ƒë∆∞·ª£c nh√∫ng (embedding)
- N·∫øu cosine similarity > ng∆∞·ª°ng ‚Üí ƒë∆∞·ª£c n·ªëi b·∫±ng "synonym edge"
üìå T√°c gi·∫£ kh√¥ng d√πng LLM v√¨:
- LLM ch·∫≠m h∆°n r·∫•t nhi·ªÅu
- Embedding-based synonym c√≥ th·ªÉ c·∫≠p nh·∫≠t li√™n t·ª•c, ƒë∆°n gi·∫£n h∆°n

---
‚ùìPh√°: C√≥ r·ªßi ro g√¨ kh√¥ng khi d√πng embedding ƒë·ªÉ n·ªëi synonym?
‚úÖ ƒê√°p:
- C√≥! C√≥ th·ªÉ n·ªëi nh·∫ßm c√°c t·ª´ g·∫ßn nghƒ©a nh∆∞ng kh√°c ng·ªØ c·∫£nh ("doctor" v√† "professor" ch·∫≥ng h·∫°n).
- Nh∆∞ng v√¨ ƒë√¢y l√† l·ªõp b·ªï tr·ª£ (synonym edge), kh√¥ng ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp n·∫øu downstream filter l√†m t·ªët.

---
üîµ B∆∞·ªõc 3: Dense‚ÄìSparse Integration
T√≠ch h·ª£p c·∫£ node "phrase" v√† node "passage" trong KG ‚Üí hybrid graph

---
‚ùìPh√°: T·∫°i sao c·∫ßn v·ª´a sparse (phrase) v·ª´a dense (passage)? Kh√¥ng gi·ªØ m·ªôt lo·∫°i cho g·ªçn?
‚úÖ ƒê√°p:
- Sparse (phrase): D·ªÖ truy h·ªìi, bi·ªÉu di·ªÖn kh√°i ni·ªám ‚Üí t·ªët cho reasoning
- Dense (passage): Gi·ªØ th√¥ng tin chi ti·∫øt ‚Üí t·ªët cho factual QA
 ‚Üí Gh√©p c·∫£ hai t·∫°o ra m·ªôt KG v·ª´a kh√°i qu√°t v·ª´a gi√†u ng·ªØ c·∫£nh.
ü™Ñ Li√™n h·ªá d·ªÖ hi·ªÉu: Gi·ªëng h·ªçc Sinh h·ªçc b·∫±ng s∆° ƒë·ªì + s√°ch gi√°o khoa:
- S∆° ƒë·ªì (phrase node) gi√∫p nh·ªõ nhanh
- SGK (passage node) gi√∫p hi·ªÉu s√¢u ‚Üí c·∫ßn c·∫£ hai ƒë·ªÉ thi ƒëi·ªÉm cao

---
‚úÖ T·ªïng k·∫øt PH·∫¶N 1: Graph c√≥ c√°c lo·∫°i node v√† edge sau:
This content is only supported in a Lark Docs

---
üîç PH·∫¶N 2: ONLINE RETRIEVAL & QA
M·ª•c ti√™u: Khi c√≥ query ‚Üí t√¨m ƒë∆∞·ªùng trong graph ‚Üí tr·∫£ l·ªùi.

---
üü¢ B∆∞·ªõc 1: Retrieving Passages and Triples

---
‚ùìPh√°: Sao ph·∫£i l·∫•y c·∫£ ƒëo·∫°n vƒÉn l·∫´n triple? Kh√¥ng ch·ªçn 1 c√°i cho nhanh?
‚úÖ ƒê√°p:
- Triple ƒë·ªÉ l√†m reasoning, k·∫øt n·ªëi m·∫°ch ki·∫øn th·ª©c
- Passage ƒë·ªÉ c√≥ th√¥ng tin chi ti·∫øt, l√†m QA cu·ªëi c√πng
‚Üí Hai l·ªõp retrieval b·ªï tr·ª£ nhau, kh√¥ng th·ªÉ thi·∫øu c√°i n√†o.

---
üü¢ B∆∞·ªõc 2: Triple Filtering = Recognition Memory
Sau khi l·∫•y triple v·ªÅ, ta l·ªçc xem c√°i n√†o th·ª±c s·ª± li√™n quan ƒë·∫øn query

---
‚ùìPh√°: T·∫°i sao c·∫ßn b∆∞·ªõc n√†y? Triple ƒë√£ ch·ªçn r·ªìi m√†?
‚úÖ ƒê√°p: V√¨:
- Retrieval ban ƒë·∫ßu b·∫±ng embedding ‚Üí c√≥ th·ªÉ nhi·ªÖu
- Recognition l√† b∆∞·ªõc gi·ªëng b·ªô nh·ªõ con ng∆∞·ªùi:
 ‚Üí ‚Äúm√¨nh t·ª´ng th·∫•y c√°i n√†y r·ªìi‚Ä¶ nh∆∞ng c√≥ ƒë√∫ng c√°i m√¨nh c·∫ßn kh√¥ng?‚Äù
üìå D√πng LLM ƒë·ªÉ so s√°nh l·∫°i t·ª´ng triple v·ªõi query ‚Üí gi·ªØ l·∫°i nh·ªØng c√°i t·ªët nh·∫•t
ü™Ñ Li√™n h·ªá h·ªçc sinh: Gi·ªëng ƒë·ªçc l·∫°i 5 ƒëo·∫°n b·∫°n highlight khi h·ªçc ‚Üí lo·∫°i 3 c√°i kh√¥ng li√™n quan, gi·ªØ l·∫°i 2 ƒëo·∫°n th·∫≠t s·ª± ƒë√∫ng ƒë·ªÅ.

---
üî¥ B∆∞·ªõc 3: Assigning Seed Node Weights
G√°n ƒëi·ªÉm cho c√°c node ‚Äúg·ªëc‚Äù trong KG ‚Üí chu·∫©n b·ªã cho b∆∞·ªõc lan truy·ªÅn PageRank

---
‚ùìPh√°: Seed Node l√† g√¨? T·∫°i sao ph·∫£i g√°n tr·ªçng s·ªë?
‚úÖ ƒê√°p:
- Seed Node l√† node trong graph c√≥ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn query
- Tr·ªçng s·ªë cao th√¨ node ƒë√≥ lan truy·ªÅn ·∫£nh h∆∞·ªüng m·∫°nh h∆°n trong PPR
üìå D√πng c·∫£:
- Passage t·ª´ query retrieval
- Phrase t·ª´ triple l·ªçc
 ‚Üí t·∫•t c·∫£ ƒë·ªÅu tr·ªü th√†nh seed node

---
üîÑ B∆∞·ªõc 4: PPR Graph Search
Personalized PageRank t√¨m ƒë∆∞·ªùng trong KG t·ª´ c√°c seed node

---
‚ùìPh√°: Kh√°c g√¨ so v·ªõi search b√¨nh th∆∞·ªùng?
‚úÖ ƒê√°p:
- PageRank lan t·ªèa ·∫£nh h∆∞·ªüng ‚Üí kh√¥ng t√¨m ƒë∆∞·ªùng ng·∫Øn nh·∫•t, m√† t√¨m ƒëo·∫°n c√≥ li√™n quan nh·∫•t trong to√†n b·ªô c·∫•u tr√∫c
- Personalized ‚Üí ∆∞u ti√™n c√°c v√πng g·∫ßn v·ªõi seed node
ü™Ñ Li√™n h·ªá h·ªçc sinh: Gi·ªëng h·ªçc theo s∆° ƒë·ªì t∆∞ duy, b·∫°n ∆∞u ti√™n ƒë·ªçc ph·∫ßn g·∫ßn ch·ªß ƒë·ªÅ ch√≠nh ‚Üí kh√¥ng l·∫≠t s√°ch lung tung

---
‚úÖ B∆∞·ªõc 5: QA Reading with Selected Passages
LLM ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë√£ ch·ªçn ‚Üí sinh ra c√¢u tr·∫£ l·ªùi.

---
‚ùìPh√°: N·∫øu ch·ªâ d√πng ƒëo·∫°n vƒÉn m√† kh√¥ng graph th√¨ sao?
‚úÖ ƒê√°p:
 ‚Üí Gi·ªëng RAG truy·ªÅn th·ªëng ‚Üí b·ªã l·ªách, thi·∫øu reasoning
 ‚Üí Nh·ªù b∆∞·ªõc graph search, m√¥ h√¨nh nh·∫≠n ƒë∆∞·ª£c ƒëo·∫°n vƒÉn ‚Äúgi√†u li√™n k·∫øt nh·∫•t‚Äù ‚Üí tr·∫£ l·ªùi m·∫°ch l·∫°c h∆°n, ch√≠nh x√°c h∆°n.

---
üîö T·ªïng K·∫øt: Pipeline HippoRAG 2 ‚Äì Hi·ªÉu s√¢u + v√≠ d·ª• h√≥a
This content is only supported in a Lark Docs

---
D∆∞·ªõi ƒë√¢y l√† ph·∫ßn gi·∫£i th√≠ch s√¢u t·ª´ng b∆∞·ªõc trong pipeline c·ªßa HippoRAG 2, k√®m theo c√°c c√¢u h·ªèi ‚Äúph√°‚Äù ‚Äì ‚Äúƒë√°p‚Äù ƒë·ªÉ ƒë√†o s√¢u hi·ªÉu bi·∫øt, gi·ªëng nh∆∞ ƒë·ªëi tho·∫°i gi·ªØa ng∆∞·ªùi ƒë·∫∑t c√¢u h·ªèi s·∫Øc s·∫£o v√† ng∆∞·ªùi tr·∫£ l·ªùi ch·∫∑t ch·∫Ω.

---
1. OpenIE by LLM ‚Äì Tr√≠ch xu·∫•t Triple t·ª´ vƒÉn b·∫£n
M·ª•c ti√™u:
 Chuy·ªÉn ƒë·ªïi ƒëo·∫°n vƒÉn b·∫£n th√†nh c√°c ƒë∆°n v·ªã ki·∫øn th·ª©c (triple: subject ‚Äì predicate ‚Äì object) ƒë·ªÉ l∆∞u tr·ªØ trong b·ªô nh·ªõ d∆∞·ªõi d·∫°ng c√°c m·ªëi li√™n h·ªá ƒë∆°n gi·∫£n.

---
‚ùì Ph√° 1:
T·∫°i sao ch√∫ng ta c·∫ßn tr√≠ch xu·∫•t triple thay v√¨ ch·ªâ l∆∞u to√†n b·ªô ƒëo·∫°n vƒÉn?
‚úÖ ƒê√°p:
- Vi·ªác tr√≠ch xu·∫•t triple gi√∫p t√°ch th√¥ng tin quan tr·ªçng ra kh·ªèi m·ªõ vƒÉn b·∫£n d√†i.
- N√≥ cho ph√©p l∆∞u tr·ªØ d∆∞·ªõi d·∫°ng c√°c m·ªëi quan h·ªá r√µ r√†ng, gi·ªëng nh∆∞ ghi ch√∫ ng·∫Øn g·ªçn, d·ªÖ t√¨m l·∫°i v√† k·∫øt n·ªëi v·ªõi c√°c ki·∫øn th·ª©c kh√°c.
V√≠ d·ª• d·ªÖ hi·ªÉu:
 Gi·ªëng nh∆∞ khi h·ªçc, b·∫°n kh√¥ng c·∫ßn l∆∞u to√†n b·ªô b√†i gi·∫£ng m√† ch·ªâ c·∫ßn ghi l·∫°i c√°c √Ω ch√≠nh: ‚ÄúEinstein ‚Äì sinh ‚Äì Ulm‚Äù thay v√¨ ghi to√†n b·ªô ƒëo·∫°n vƒÉn d√†i.

---
‚ùì Ph√° 2:
T·∫°i sao d√πng LLM ƒë·ªÉ th·ª±c hi·ªán OpenIE, thay v√¨ d√πng c√°c c√¥ng c·ª• truy·ªÅn th·ªëng?
‚úÖ ƒê√°p:
- C√°c c√¥ng c·ª• OpenIE truy·ªÅn th·ªëng th∆∞·ªùng gi·ªõi h·∫°n ·ªü c√¢u ƒë∆°n gi·∫£n, kh√¥ng hi·ªÉu ƒë∆∞·ª£c nh·ªØng c√¢u ph·ª©c t·∫°p, m√¢u thu·∫´n hay ·∫©n d·ª•.
- LLM c√≥ kh·∫£ nƒÉng hi·ªÉu ng·ªØ c·∫£nh, x·ª≠ l√Ω c√¢u d√†i v√† ph·ª©c t·∫°p, t·ª´ ƒë√≥ tr√≠ch xu·∫•t triple ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß h∆°n.
V√≠ d·ª• h·ªçc sinh c·∫•p 2:
 N·∫øu ƒë·ªçc m·ªôt ƒëo·∫°n vƒÉn m√¥ t·∫£ "Marie Curie ƒë√£ kh√°m ph√° ra c√°c nguy√™n t·ªë m·ªõi v√† gi√†nh gi·∫£i Nobel trong nhi·ªÅu lƒ©nh v·ª±c", LLM c√≥ th·ªÉ t·∫°o ra nhi·ªÅu triple nh∆∞:
 ‚Üí (Marie Curie, kh√°m ph√°, nguy√™n t·ªë m·ªõi)
 ‚Üí (Marie Curie, gi√†nh, gi·∫£i Nobel)
 Nh∆∞ v·∫≠y, b·∫°n c√≥ th·ªÉ nh·ªõ nhi·ªÅu th√¥ng tin h∆°n ch·ªâ b·∫±ng m·ªôt c√¢u t√≥m t·∫Øt.

---
2. Synonym Detection ‚Äì Ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a
M·ª•c ti√™u:
 K·∫øt n·ªëi c√°c node (kh√°i ni·ªám, c·ª•m t·ª´) c√≥ √Ω nghƒ©a t∆∞∆°ng t·ª± trong KG, gi√∫p tƒÉng kh·∫£ nƒÉng li√™n k·∫øt gi·ªØa c√°c th√¥ng tin.

---
‚ùì Ph√° 1:
Synonym Detection d√πng thu·∫≠t to√°n n√†o ƒë·ªÉ ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a?
‚úÖ ƒê√°p:
- Th√¥ng th∆∞·ªùng, ta s·ª≠ d·ª•ng cosine similarity trong kh√¥ng gian embedding.
- M·ªói t·ª´ hay c·ª•m t·ª´ ƒë∆∞·ª£c chuy·ªÉn th√†nh m·ªôt vector, sau ƒë√≥ t√≠nh cosine similarity gi·ªØa c√°c vector. N·∫øu gi√° tr·ªã similarity v∆∞·ª£t qua m·ªôt ng∆∞·ª°ng ƒë·ªãnh tr∆∞·ªõc (v√≠ d·ª• 0.8), hai node ƒë∆∞·ª£c coi l√† ƒë·ªìng nghƒ©a v√† ƒë∆∞·ª£c n·ªëi v·ªõi nhau b·∫±ng m·ªôt ‚Äúsynonym edge.‚Äù

---
‚ùì Ph√° 2:
T·∫°i sao l·∫°i c·∫ßn n·ªëi c√°c node ƒë·ªìng nghƒ©a l·∫°i v·ªõi nhau?
‚úÖ ƒê√°p:
- ƒêi·ªÅu n√†y gi√∫p li√™n k·∫øt c√°c kh√°i ni·ªám c√≥ th·ªÉ ƒë∆∞·ª£c di·ªÖn ƒë·∫°t kh√°c nhau.
- Nh·ªù ƒë√≥, khi truy v·∫•n, n·∫øu t·ª´ kh√≥a ƒë∆∞·ª£c d√πng kh√°c v·ªõi c√°ch ghi ch√∫ ban ƒë·∫ßu nh∆∞ng c√≥ c√πng √Ω nghƒ©a, h·ªá th·ªëng v·∫´n c√≥ th·ªÉ li√™n k·∫øt ƒë√∫ng ki·∫øn th·ª©c.
V√≠ d·ª• d·ªÖ hi·ªÉu:
 N·∫øu trong vƒÉn b·∫£n c√≥ ‚Äúth·∫ßy gi√°o‚Äù v√† ‚Äúgi√°o vi√™n‚Äù ‚Äì m·∫∑c d√π hai t·ª´ kh√°c nhau, ch√∫ng ƒë·ªÅu n√≥i v·ªÅ ng∆∞·ªùi d·∫°y h·ªçc. K·∫øt n·ªëi ch√∫ng l·∫°i gi√∫p h·ªá th·ªëng hi·ªÉu r·∫±ng khi h·ªèi v·ªÅ ‚Äúgi√°o vi√™n‚Äù c≈©ng c√≥ th·ªÉ li√™n h·ªá ƒë·∫øn ‚Äúth·∫ßy gi√°o‚Äù.

---
‚ùì Ph√° 3:
C√≥ r·ªßi ro g√¨ khi d√πng cosine similarity?
‚úÖ ƒê√°p:
- C√≥ th·ªÉ x·∫£y ra tr∆∞·ªùng h·ª£p n·ªëi nh·∫ßm nh·ªØng t·ª´ c√≥ c√πng h√¨nh th·ª©c nh∆∞ng ng·ªØ nghƒ©a kh√°c (v√≠ d·ª•: ‚Äúbank‚Äù c√≥ th·ªÉ l√† ng√¢n h√†ng ho·∫∑c b·ªù s√¥ng).
- Tuy nhi√™n, v√¨ ƒë√¢y ch·ªâ l√† l·ªõp b·ªï tr·ª£ trong KG, n·∫øu c√°c b∆∞·ªõc sau (nh∆∞ triple filtering) ho·∫°t ƒë·ªông t·ªët, sai s√≥t n√†y s·∫Ω ƒë∆∞·ª£c gi·∫£m thi·ªÉu.

---
3. Dense‚ÄìSparse Integration ‚Äì T√≠ch h·ª£p c·∫£ Node "Phrase" v√† "Passage"
M·ª•c ti√™u:
 K·∫øt h·ª£p th√¥ng tin ng·∫Øn g·ªçn (phrase node) v√† th√¥ng tin ƒë·∫ßy ƒë·ªß (passage node) v√†o c√πng m·ªôt KG ƒë·ªÉ t·∫≠n d·ª•ng ∆∞u ƒëi·ªÉm c·ªßa c·∫£ hai.

---
‚ùì Ph√° 1:
T·∫°i sao kh√¥ng ch·ªâ l∆∞u th√¥ng tin d·∫°ng ‚Äúphrase‚Äù hay ‚Äúpassage‚Äù ri√™ng r·∫Ω?
‚úÖ ƒê√°p:
- Phrase node (sparse): R√∫t g·ªçn th√¥ng tin, d·ªÖ k·∫øt n·ªëi c√°c kh√°i ni·ªám c∆° b·∫£n, nhanh v√† hi·ªáu qu·∫£ cho reasoning.
- Passage node (dense): Gi·ªØ nguy√™n ng·ªØ c·∫£nh v√† chi ti·∫øt, c·∫ßn thi·∫øt cho c√°c t√°c v·ª• y√™u c·∫ßu ƒë·ªô ch√≠nh x√°c cao nh∆∞ factual QA.
V√≠ d·ª•:
 H·ªçc sinh khi √¥n b√†i to√°n: b·∫°n c√≥ th·ªÉ ghi l·∫°i ‚Äúc√¥ng th·ª©c Pythagoras‚Äù (phrase) v√† c√≥ s·ªï tay ch·ª©a b√†i gi·∫£i chi ti·∫øt (passage).
 C·∫£ hai ƒë·ªÅu quan tr·ªçng ƒë·ªÉ b·∫°n v·ª´a nh·ªõ nhanh, v·ª´a hi·ªÉu s√¢u.

---
‚ùì Ph√° 2:
How does integrating both dense and sparse information help in retrieval?
‚úÖ ƒê√°p:
- Khi c√≥ c·∫£ hai, h·ªá th·ªëng c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c phrase node ƒë·ªÉ x√°c ƒë·ªãnh c√°c kh√°i ni·ªám ch·ªß ƒë·∫°o v√† passage node ƒë·ªÉ l·∫•y chi ti·∫øt c·ª• th·ªÉ cho c√¢u tr·∫£ l·ªùi.
- ƒêi·ªÅu n√†y t·∫°o ra m·ªôt KG to√†n di·ªán, v·ª´a t·ªïng qu√°t v·ª´a chi ti·∫øt, gi√∫p tƒÉng hi·ªáu su·∫•t ·ªü c·∫£ factual QA v√† multi-hop reasoning.

---
4. Triple Retrieval ‚Äì L·∫•y ra c√°c Triple c√≥ li√™n quan v·ªõi Query
M·ª•c ti√™u:
 Khi c√≥ m·ªôt query, t√¨m ra c√°c triple t·ª´ KG ph√π h·ª£p ƒë·ªÉ d√πng cho qu√° tr√¨nh reasoning.

---
‚ùì Ph√° 1:
T·∫°i sao kh√¥ng ch·ªâ l·∫•y to√†n b·ªô passage m√† ph·∫£i l·∫•y ra triple?
‚úÖ ƒê√°p:
- Triple ch·ª©a m·ªëi li√™n h·ªá r√µ r√†ng v√† r√∫t g·ªçn, gi√∫p h·ªá th·ªëng d·ªÖ d√†ng ‚Äúƒëi theo‚Äù c√°c m·ªëi li√™n k·∫øt ƒë·ªÉ reasoning.
- Passage c√≥ th·ªÉ qu√° d√†i, ch·ª©a nhi·ªÖu; triple gi√∫p x√°c ƒë·ªãnh ‚Äúƒëi·ªÉm c·ªët l√µi‚Äù c·ªßa ki·∫øn th·ª©c.
V√≠ d·ª•:
 N·∫øu c√¢u h·ªèi l√† ‚ÄúBill Gates h·ªçc tr∆∞·ªùng n√†o?‚Äù, triple nh∆∞ (‚ÄúBill Gates‚Äù, ‚Äúh·ªçc t·∫°i‚Äù, ‚ÄúHarvard‚Äù) s·∫Ω gi√∫p h·ªá th·ªëng nhanh ch√≥ng t√¨m ra th√¥ng tin c·∫ßn thi·∫øt.

---
5. Triple Filtering (Recognition Memory) ‚Äì L·ªçc l·∫°i c√°c Triple
M·ª•c ti√™u:
 Sau khi l·∫•y ra c√°c triple ban ƒë·∫ßu, s·ª≠ d·ª•ng LLM ƒë·ªÉ x√°c nh·∫≠n l·∫°i t√≠nh li√™n quan c·ªßa ch√∫ng v·ªõi query, lo·∫°i b·ªè nh·ªØng triple kh√¥ng ph√π h·ª£p.

---
‚ùì Ph√° 1:
T·∫°i sao l·∫°i c·∫ßn b∆∞·ªõc l·ªçc triple n·∫øu ƒë√£ c√≥ triple retrieval?
‚úÖ ƒê√°p:
- Do b∆∞·ªõc retrieval ban ƒë·∫ßu c√≥ th·ªÉ mang l·∫°i nhi·ªÅu triple nhi·ªÖu ho·∫∑c kh√¥ng li√™n quan, v√¨ kh√¥ng ph·∫£i l√∫c n√†o c√°c vector embedding c≈©ng ho√†n h·∫£o.
- Recognition Memory gi√∫p ‚Äúƒë·ªçc l·∫°i‚Äù v√† x√°c nh·∫≠n, gi·ªëng nh∆∞ khi b·∫°n ki·ªÉm tra l·∫°i ghi ch√∫ c·ªßa m√¨nh ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ gi·ªØ l·∫°i nh·ªØng th√¥ng tin ƒë√∫ng c·∫ßn thi·∫øt.
V√≠ d·ª•:
 N·∫øu c√≥ 5 triple ƒë∆∞·ª£c l·∫•y ra, nh∆∞ng ch·ªâ c√≥ 2 th·ª±c s·ª± n√≥i v·ªÅ c√¢u h·ªèi, b∆∞·ªõc filtering s·∫Ω lo·∫°i b·ªè 3 triple kh√¥ng c·∫ßn thi·∫øt.

---
6. Seed Node Assignment ‚Äì G√°n Tr·ªçng S·ªë cho Seed Node
M·ª•c ti√™u:
 X√°c ƒë·ªãnh c√°c node ‚Äúg·ªëc‚Äù (seed node) t·ª´ query, v√† g√°n tr·ªçng s·ªë cho ch√∫ng nh·∫±m t·∫°o n·ªÅn t·∫£ng cho b∆∞·ªõc lan truy·ªÅn th√¥ng tin trong PPR.

---
‚ùì Ph√° 1:
Seed Node l√† g√¨?
‚úÖ ƒê√°p:
- Seed Node l√† nh·ªØng node trong KG m√† h·ªá th·ªëng x√°c ƒë·ªãnh c√≥ m·ªëi li√™n h·ªá tr·ª±c ti·∫øp v·ªõi query.
- Ch√∫ng l√† ƒëi·ªÉm kh·ªüi ƒë·∫ßu ƒë·ªÉ thu·∫≠t to√°n lan truy·ªÅn (PPR) ‚Äúkh√°m ph√°‚Äù c√°c node kh√°c c√≥ li√™n quan.
V√≠ d·ª•:
 N·∫øu c√¢u h·ªèi l√† ‚ÄúBill Gates h·ªçc ·ªü ƒë√¢u?‚Äù, seed node c√≥ th·ªÉ l√† ‚ÄúBill Gates‚Äù v√† ‚ÄúHarvard‚Äù n·∫øu ƒë√£ c√≥ triple li√™n quan.

---
‚ùì Ph√° 2:
T·∫°i sao c·∫ßn g√°n tr·ªçng s·ªë cho seed node?
‚úÖ ƒê√°p:
- Tr·ªçng s·ªë c√†ng cao, node ƒë√≥ s·∫Ω ·∫£nh h∆∞·ªüng m·∫°nh h∆°n trong qu√° tr√¨nh lan truy·ªÅn th√¥ng tin c·ªßa PageRank.
- Gi√∫p ∆∞u ti√™n c√°c node quan tr·ªçng, t·∫°o ra k·∫øt qu·∫£ retrieval ch√≠nh x√°c h∆°n.
V√≠ d·ª•:
 Gi·ªëng nh∆∞ khi b·∫°n ch·ªçn ƒëi·ªÉm m·∫•u ch·ªët trong s∆° ƒë·ªì t∆∞ duy, nh·ªØng ƒëi·ªÉm quan tr·ªçng ƒë∆∞·ª£c ƒë√°nh d·∫•u r√µ, ƒë·ªÉ khi ‚Äúƒëi theo s∆° ƒë·ªì‚Äù b·∫°n s·∫Ω ∆∞u ti√™n nh·ªØng ƒëi·ªÉm ƒë√≥.

---
7. PPR (Personalized PageRank) Graph Search ‚Äì T√¨m Ki·∫øm Trong Graph
M·ª•c ti√™u:
 S·ª≠ d·ª•ng thu·∫≠t to√°n PPR ƒë·ªÉ lan truy·ªÅn th√¥ng tin t·ª´ c√°c seed node v√† t√¨m ra c√°c node li√™n quan nh·∫•t trong KG.

---
‚ùì Ph√° 1:
Thu·∫≠t to√°n PageRank ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o v√† kh√°c g√¨ so v·ªõi t√¨m ki·∫øm th√¥ng th∆∞·ªùng?
‚úÖ ƒê√°p:
- PageRank ban ƒë·∫ßu ƒë∆∞·ª£c d√πng ƒë·ªÉ x·∫øp h·∫°ng c√°c trang web d·ª±a tr√™n s·ªë l∆∞·ª£ng v√† ch·∫•t l∆∞·ª£ng c·ªßa c√°c li√™n k·∫øt.
- Personalized PageRank (PPR) ƒëi·ªÅu ch·ªânh thu·∫≠t to√°n ƒë·ªÉ ∆∞u ti√™n c√°c node b·∫Øt ƒë·∫ßu t·ª´ seed node (ƒëi·ªÉm kh·ªüi ƒë·∫ßu t·ª´ query).
- Thay v√¨ ch·ªâ t√¨m ‚Äúƒëo·∫°n vƒÉn g·∫ßn nh·∫•t‚Äù, PPR lan truy·ªÅn tr·ªçng s·ªë qua c√°c li√™n k·∫øt trong graph, gi√∫p h·ªá th·ªëng reasoning multi-hop.
V√≠ d·ª•:
 Gi·ªëng nh∆∞ b·∫°n c√≥ m·ªôt b·∫£n ƒë·ªì m√™ cung v·ªõi c√°c l·ªëi ƒëi, PPR gi√∫p b·∫°n kh√¥ng ch·ªâ t√¨m l·ªëi ƒëi ng·∫Øn nh·∫•t m√† t√¨m l·ªëi ƒëi ‚Äúnhi·ªÅu d·∫•u hi·ªáu nh·∫•t‚Äù li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ c·ªßa c√¢u h·ªèi.

---
‚ùì Ph√° 2:
T·∫°i sao l·∫°i c·∫ßn b∆∞·ªõc n√†y khi c√≥ c√°c b∆∞·ªõc retrieval tr∆∞·ªõc ƒë√≥?
‚úÖ ƒê√°p:
- V√¨ PPR gi√∫p k·∫øt n·ªëi c√°c m·ªëi quan h·ªá trong KG, kh√¥ng ch·ªâ d·ª±a v√†o ƒë·ªô t∆∞∆°ng ƒë·ªìng ƒë∆°n thu·∫ßn m√† c√≤n hi·ªÉu ƒë∆∞·ª£c c·∫•u tr√∫c li√™n k·∫øt gi·ªØa c√°c th√¥ng tin.
- N√≥ cho ph√©p truy xu·∫•t c√°c th√¥ng tin li√™n quan t·ª´ xa trong graph, t·∫°o n√™n c√¢u tr·∫£ l·ªùi ch√≠nh x√°c cho c√°c c√¢u h·ªèi multi-hop.

---
8. QA Reader ‚Äì ƒê·ªçc v√† Sinh Ra C√¢u Tr·∫£ L·ªùi
M·ª•c ti√™u:
 Cu·ªëi c√πng, c√°c ƒëo·∫°n vƒÉn ƒë∆∞·ª£c l·∫•y ra (sau khi ƒë√£ qua c√°c b∆∞·ªõc x·ª≠ l√Ω) ƒë∆∞·ª£c ƒë∆∞a cho LLM ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi cu·ªëi c√πng cho query.

---
‚ùì Ph√° 1:
T·∫°i sao kh√¥ng s·ª≠ d·ª•ng tr·ª±c ti·∫øp c√°c triple l√†m c√¢u tr·∫£ l·ªùi?
‚úÖ ƒê√°p:
- Triple ch·ªâ l√† c√°c m·ªëi li√™n h·ªá r√∫t g·ªçn, ch∆∞a ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh c·∫ßn thi·∫øt ƒë·ªÉ tr·∫£ l·ªùi m·ªôt c√°ch m·∫°ch l·∫°c v√† t·ª± nhi√™n.
- Passage cung c·∫•p ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh, chi ti·∫øt gi√∫p LLM sinh ra c√¢u tr·∫£ l·ªùi trung th·ª±c, ƒë·∫ßy ƒë·ªß v√† t·ª± nhi√™n h∆°n.
V√≠ d·ª•:
 N·∫øu b·∫°n h·ªèi ‚ÄúBill Gates h·ªçc ·ªü ƒë√¢u?‚Äù, triple c√≥ th·ªÉ n√≥i ‚ÄúBill Gates ‚Äì h·ªçc t·∫°i ‚Äì Harvard‚Äù, nh∆∞ng passage c√≥ th·ªÉ cung c·∫•p th√™m chi ti·∫øt nh∆∞ ‚ÄúBill Gates t·ª´ng theo h·ªçc t·∫°i tr∆∞·ªùng Harvard, n∆°i √¥ng ƒë√£ ti·∫øp thu nhi·ªÅu ki·∫øn th·ª©c v·ªÅ kinh doanh v√† c√¥ng ngh·ªá‚Äù, gi√∫p c√¢u tr·∫£ l·ªùi phong ph√∫ h∆°n.

---
T·ªïng K·∫øt & Li√™n H·ªá:
M·ªói b∆∞·ªõc trong HippoRAG 2 ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ b√π tr·ª´ nh·ªØng ƒëi·ªÉm y·∫øu c·ªßa c√°c b∆∞·ªõc tr∆∞·ªõc v√† t·∫°o n√™n m·ªôt chu·ªói x·ª≠ l√Ω to√†n di·ªán:
- OpenIE: Tr√≠ch xu·∫•t ki·∫øn th·ª©c d·∫°ng triple gi√∫p ‚Äúghi nh·ªõ‚Äù c√°c m·ªëi quan h·ªá ch√≠nh.
- Synonym Detection: K·∫øt n·ªëi nh·ªØng kh√°i ni·ªám c√≥ √Ω nghƒ©a t∆∞∆°ng t·ª±, nh∆∞ vi·ªác n·ªëi c√°c t·ª´ ƒë·ªìng nghƒ©a trong t·ª´ ƒëi·ªÉn.
- Dense-Sparse Integration: Gi·ªØ ƒë∆∞·ª£c c·∫£ ‚Äúƒë·∫ßu d√≤ng‚Äù (phrase) v√† ‚Äún·ªôi dung chi ti·∫øt‚Äù (passage) ‚Äì gi·ªëng nh∆∞ ghi ch√∫ ng·∫Øn v√† s·ªï tay chi ti·∫øt.
- Triple Retrieval & Filtering: Nh·∫≠n ra c√°c ghi ch√∫ li√™n quan v√† lo·∫°i b·ªè nhi·ªÖu, nh∆∞ vi·ªác b·∫°n xem l·∫°i b√†i h·ªçc v√† ch·ªçn ra nh·ªØng ƒëi·ªÉm quan tr·ªçng.
- Seed Node Assignment & PPR Graph Search: Gi·ªëng nh∆∞ ch·ªçn c√°c ƒëi·ªÉm m·∫•u ch·ªët tr√™n s∆° ƒë·ªì t∆∞ duy r·ªìi ƒëi theo c√°c m·ªëi li√™n h·ªá ƒë·ªÉ t√¨m ra c√¢u tr·∫£ l·ªùi to√†n di·ªán.
- QA Reader: Cu·ªëi c√πng, t·ªïng h·ª£p c√°c th√¥ng tin ƒë√£ ƒë∆∞·ª£c l·ªçc v√† li√™n k·∫øt ƒë·ªÉ sinh ra c√¢u tr·∫£ l·ªùi t·ª± nhi√™n v√† ch√≠nh x√°c.

---
D∆∞·ªõi ƒë√¢y m√¨nh s·∫Ω ƒë√†o s√¢u v√†o kh√°i ni·ªám triple trong b∆∞·ªõc 1 c·ªßa HippoRAG (v√† HippoRAG 2), v·ªõi phong c√°ch ‚Äúm·ªôt ng∆∞·ªùi ph√° ‚Äì m·ªôt ng∆∞·ªùi tr·∫£ l·ªùi‚Äù ƒë·ªÉ l√†m r√µ t·ª´ng ƒëi·ªÉm.

---
1. Triple v√† Relation Edge
‚ùì Ph√°: Trong b∆∞·ªõc tr√≠ch xu·∫•t triple b·∫±ng OpenIE, c√°c triple ƒë∆∞·ª£c t·∫°o ra c√≥ ‚Äúh∆∞·ªõng‚Äù kh√¥ng?
Tr·∫£ l·ªùi:
- C√≥ h∆∞·ªõng:
 C√°c triple ƒë∆∞·ª£c tr√≠ch xu·∫•t theo ƒë·ªãnh d·∫°ng (subject, predicate, object) v·ªën ƒë√£ c√≥ th·ª© t·ª± r√µ r√†ng.
  - V√≠ d·ª•: t·ª´ c√¢u ‚ÄúMarie Curie won the Nobel Prize‚Äù ta c√≥ triple:
 (Marie Curie, won, Nobel Prize)
  - ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† m·ªëi quan h·ªá ‚Äúwon‚Äù ch·ªâ c√≥ √Ω nghƒ©a t·ª´ ‚ÄúMarie Curie‚Äù ƒë·∫øn ‚ÄúNobel Prize‚Äù.
- L√Ω do gi·ªØ h∆∞·ªõng:
  - Gi√∫p ph√¢n bi·ªát ƒë∆∞·ª£c ai l√†m g√¨ v·ªõi ai.
  - B·∫£o to√†n th·ª© t·ª± th√¥ng tin, r·∫•t quan tr·ªçng trong vi·ªác reasoning (v√≠ d·ª•: ‚Äúwon‚Äù kh√¥ng th·ªÉ ƒë·∫£o ng∆∞·ª£c th√†nh ‚ÄúNobel Prize won Marie Curie‚Äù n·∫øu kh√¥ng c√≥ h∆∞·ªõng).

---
‚ùì Ph√°: N·∫øu b·ªè ƒëi h∆∞·ªõng c·ªßa c√°c triple, li·ªáu c√≥ l√†m m·∫•t ƒëi th√¥ng tin quan tr·ªçng kh√¥ng?
Tr·∫£ l·ªùi:
- C√≥ m·∫•t:
  - H∆∞·ªõng c·ªßa edge x√°c ƒë·ªãnh m·ªëi li√™n h·ªá theo th·ª© t·ª± logic.
  - V√≠ d·ª•:
    - Triple: (Marie Curie, won, Nobel Prize) mang nghƒ©a r√µ r√†ng.
    - N·∫øu tr·ªü th√†nh edge kh√¥ng h∆∞·ªõng, ta ch·ªâ bi·∫øt ‚ÄúMarie Curie‚Äù v√† ‚ÄúNobel Prize‚Äù li√™n quan nhau nh∆∞ng kh√¥ng bi·∫øt ai l√† t√°c nh√¢n hay ƒë·ªëi t∆∞·ª£ng c·ªßa h√†nh ƒë·ªông.
- K·∫øt qu·∫£:
  - Vi·ªác gi·ªØ h∆∞·ªõng l√† c·∫ßn thi·∫øt ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c c·ªßa ng·ªØ nghƒ©a ban ƒë·∫ßu t·ª´ vƒÉn b·∫£n.

---
‚ùì Ph√°: Trong ƒë·ªì th·ªã ki·∫øn th·ª©c, vai tr√≤ c·ªßa relation edge c√≥ h∆∞·ªõng l√† g√¨ trong qu√° tr√¨nh reasoning?
Tr·∫£ l·ªùi:
- Gi√∫p ƒë·ªãnh h∆∞·ªõng th√¥ng tin:
  - Khi th·ª±c hi·ªán Personalized PageRank (PPR), h∆∞·ªõng c·ªßa edge gi√∫p x√°c ƒë·ªãnh lu·ªìng th√¥ng tin t·ª´ c√°c node g·ªëc (seed nodes) sang c√°c node li√™n quan.
- H·ªó tr·ª£ multi-hop reasoning:
  - V√≠ d·ª•: t·ª´ node ‚ÄúMarie Curie‚Äù qua edge ‚Äúwon‚Äù ƒë·∫øn ‚ÄúNobel Prize‚Äù, sau ƒë√≥ n·∫øu c√≥ edge kh√°c n·ªëi ‚ÄúNobel Prize‚Äù v·ªõi ‚ÄúPhysics‚Äù (n·∫øu ƒë∆∞·ª£c tr√≠ch xu·∫•t th√™m), h·ªá th·ªëng s·∫Ω hi·ªÉu ƒë∆∞·ª£c m·ªëi li√™n h·ªá c√≥ th·ª© t·ª± gi·ªØa c√°c kh√°i ni·ªám.
- Gi·ªØ nguy√™n ng·ªØ nghƒ©a:
  - B·∫±ng c√°ch kh√¥ng ƒë·∫£o ng∆∞·ª£c m·ªëi quan h·ªá, m√¥ h√¨nh c√≥ th·ªÉ truy xu·∫•t c√°c th√¥ng tin li√™n quan ch√≠nh x√°c h∆°n.

---
2. Synonym Edge
‚ùì Ph√°: Synonym edge ƒë∆∞·ª£c t·∫°o ra d·ª±a tr√™n thu·∫≠t to√°n g√¨?
Tr·∫£ l·ªùi:
- D·ª±a tr√™n cosine similarity trong embedding space:
  - M·ªói phrase (hay c·ª•m t·ª´) ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng vector.
  - N·∫øu cosine similarity gi·ªØa hai vector v∆∞·ª£t qua m·ªôt ng∆∞·ª°ng ƒë·ªãnh tr∆∞·ªõc, th√¨ hai node ƒë√≥ ƒë∆∞·ª£c n·ªëi b·∫±ng m·ªôt synonym edge.
- ƒê·∫∑c ƒëi·ªÉm c·ªßa synonym edge:
  - Kh√¥ng c√≥ h∆∞·ªõng: V√¨ n·∫øu hai t·ª´ l√† ƒë·ªìng nghƒ©a, m·ªëi quan h·ªá c·ªßa ch√∫ng th∆∞·ªùng l√† ƒë·ªëi x·ª©ng (v√≠ d·ª•: ‚Äúteacher‚Äù v√† ‚Äúinstructor‚Äù c√≥ th·ªÉ ƒë∆∞·ª£c n·ªëi v·ªõi nhau m√† kh√¥ng c·∫ßn ph√¢n bi·ªát th·ª© t·ª±).

---
‚ùì Ph√°: T·∫°i sao l·∫°i s·ª≠ d·ª•ng thu·∫≠t to√°n embedding cho synonym thay v√¨ d√πng LLM tr·ª±c ti·∫øp?
Tr·∫£ l·ªùi:
- Hi·ªáu qu·∫£ v√† t·ªëc ƒë·ªô:
  - T√≠nh to√°n cosine similarity gi·ªØa vector nhanh v√† hi·ªáu qu·∫£, kh√¥ng c·∫ßn g·ªçi LLM nhi·ªÅu l·∫ßn.
- ƒê·ªô ch√≠nh x√°c cao:
  - C√°c m√¥ h√¨nh embedding hi·ªán nay c√≥ kh·∫£ nƒÉng b·∫Øt ƒë∆∞·ª£c ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a r·∫•t t·ªët, cho n√™n ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a l√† kh√° ch√≠nh x√°c.
- Gi·∫£m t·∫£i cho LLM:
  - Gi·∫£m chi ph√≠ t√≠nh to√°n, v√¨ kh√¥ng c·∫ßn LLM ph·∫£i x·ª≠ l√Ω t·∫•t c·∫£ c√°c c·∫∑p t·ª´ trong corpus.

---
3. Seed Node
‚ùì Ph√°: Seed node l√† g√¨ trong b·ªëi c·∫£nh c·ªßa HippoRAG?
Tr·∫£ l·ªùi:
- Seed Node:
  - L√† nh·ªØng node trong ki·∫øn th·ª©c ƒë·ªì th·ªã ƒë∆∞·ª£c ch·ªçn l√†m ƒëi·ªÉm kh·ªüi ƒë·∫ßu cho qu√° tr√¨nh t√¨m ki·∫øm v√† lan truy·ªÅn th√¥ng tin (th√¥ng qua PPR) khi c√≥ query.
- Vai tr√≤ c·ªßa Seed Node:
  - Ch√∫ng ƒë·∫°i di·ªán cho c√°c th√†nh ph·∫ßn quan tr·ªçng li√™n quan ƒë·∫øn c√¢u h·ªèi ƒë∆∞·ª£c ƒë·∫∑t ra.
  - V√≠ d·ª•: N·∫øu c√¢u h·ªèi l√† ‚ÄúBill Gates h·ªçc ·ªü ƒë√¢u?‚Äù, c√°c node ch·ª©a ‚ÄúBill Gates‚Äù ho·∫∑c c√°c th√¥ng tin li√™n quan s·∫Ω ƒë∆∞·ª£c ch·ªçn l√†m seed.

---
‚ùì Ph√°: T·∫°i sao ph·∫£i g√°n tr·ªçng s·ªë (seed node weights) cho c√°c Seed Node?
Tr·∫£ l·ªùi:
- G√°n tr·ªçng s·ªë:
  - Gi√∫p h·ªá th·ªëng bi·∫øt ƒë∆∞·ª£c nh·ªØng node n√†o quan tr·ªçng h∆°n khi th·ª±c hi·ªán lan truy·ªÅn th√¥ng tin trong ƒë·ªì th·ªã.
- L·ª£i √≠ch:
  - C√°c Seed Node c√≥ tr·ªçng s·ªë cao s·∫Ω ‚Äúph√°t s√°ng‚Äù h∆°n trong qu√° tr√¨nh PPR, t·ª´ ƒë√≥ gi√∫p t√¨m ra c√°c node li√™n quan t·ªët h∆°n, ƒë·∫£m b·∫£o c√¢u tr·∫£ l·ªùi ƒë√∫ng v√† ƒë·∫ßy ƒë·ªß.
- V√≠ d·ª• ƒë∆°n gi·∫£n:
  - Trong m·ªôt b·∫£n ƒë·ªì t∆∞ duy, n·∫øu b·∫°n ƒë√°nh d·∫•u m·ªôt s·ªë √Ω ch√≠nh v·ªõi b√∫t d·∫°, nh·ªØng √Ω n√†y s·∫Ω ƒë∆∞·ª£c ch√∫ √Ω nhi·ªÅu h∆°n khi b·∫°n t·ªïng h·ª£p l·∫°i ki·∫øn th·ª©c.

---
4. Relation Edge v√† C√°c Kh√°i Ni·ªám Kh√°c
‚ùì Ph√°: Relation edge c√≥ h∆∞·ªõng nh∆∞ th·∫ø n√†o?
Tr·∫£ l·ªùi:
- Relation Edge:
  - L√† c√°c c·∫°nh n·ªëi gi·ªØa c√°c node trong KG, ƒë∆∞·ª£c t·∫°o ra t·ª´ c√°c triple (subject, predicate, object).
  - Ch√∫ng c√≥ h∆∞·ªõng, t·ª´ subject sang object, th√¥ng qua predicate.
- √ù nghƒ©a c·ªßa h∆∞·ªõng:
  - Gi√∫p x√°c ƒë·ªãnh ƒë√∫ng m·ªëi li√™n h·ªá ng·ªØ nghƒ©a (ai l√†m g√¨ v·ªõi g√¨).

---
‚ùì Ph√°: Ngo√†i c√°c kh√°i ni·ªám tr√™n, c√≤n kh√°i ni·ªám n√†o kh√°c c·∫ßn gi·∫£i th√≠ch chi ti·∫øt trong pipeline kh√¥ng?
Tr·∫£ l·ªùi:
- Context Edge:
  - N·ªëi c√°c Passage Node v·ªõi c√°c Phrase Node m√† ch√∫ng ch·ª©a ƒë·ª±ng.
  - √ù nghƒ©a: Gi√∫p gi·ªØ l·∫°i ng·ªØ c·∫£nh chi ti·∫øt c·ªßa vƒÉn b·∫£n, h·ªó tr·ª£ factual QA.
- Dense‚ÄìSparse Integration:
  - S·ª± k·∫øt h·ª£p gi·ªØa sparse coding (phrase node) v√† dense coding (passage node) nh·∫±m ƒë·∫£m b·∫£o v·ª´a c√≥ √Ω ch√≠nh t√≥m t·∫Øt, v·ª´a c√≥ ƒë·∫ßy ƒë·ªß chi ti·∫øt.
  - V√≠ d·ª• h·ªçc sinh: Nh∆∞ ƒë√£ n√≥i, gi·ªëng nh∆∞ v·ª´a ghi nh·ªõ ti√™u ƒë·ªÅ b√†i h·ªçc (sparse) v√† v·ª´a c√≥ to√†n b·ªô b√†i gi·∫£ng (dense).

---
K·∫æT LU·∫¨N
M·ªói kh√°i ni·ªám trong pipeline c·ªßa HippoRAG 2 ƒë·ªÅu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ b·∫£o to√†n v√† khai th√°c th√¥ng tin m·ªôt c√°ch t·ªëi ∆∞u:
- Triple extraction v·ªõi relation edge c√≥ h∆∞·ªõng gi·ªØ tr·∫≠t t·ª± v√† ng·ªØ nghƒ©a ban ƒë·∫ßu.
- Synonym edge (kh√¥ng c√≥ h∆∞·ªõng) n·ªëi c√°c c·ª•m t·ª´ c√≥ nghƒ©a t∆∞∆°ng ƒë·ªìng, gi√∫p li√™n k·∫øt th√¥ng tin gi·ªØa c√°c ƒëo·∫°n vƒÉn kh√°c nhau.
- Seed Node l√† c√°c ƒëi·ªÉm kh·ªüi ƒë·∫ßu quan tr·ªçng ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√° tr√¨nh t√¨m ki·∫øm th√¥ng tin trong KG, v√† vi·ªác g√°n tr·ªçng s·ªë cho ch√∫ng gi√∫p tƒÉng c∆∞·ªùng kh·∫£ nƒÉng lan truy·ªÅn th√¥ng tin trong qu√° tr√¨nh Personalized PageRank.
- Context edge v√† dense-sparse integration gi√∫p m√¥ h√¨nh kh√¥ng ch·ªâ ghi nh·ªõ √Ω ch√≠nh m√† c√≤n gi·ªØ nguy√™n ng·ªØ c·∫£nh chi ti·∫øt c·ªßa vƒÉn b·∫£n g·ªëc.
Qua t·ª´ng c√¢u h·ªèi ph·∫£n bi·ªán, ch√∫ng ta nh·∫≠n th·∫•y r·∫±ng vi·ªác duy tr√¨ h∆∞·ªõng trong c√°c relation edge l√† r·∫•t quan tr·ªçng cho vi·ªác x√¢y d·ª±ng m·ªôt ƒë·ªì th·ªã ki·∫øn th·ª©c ch√≠nh x√°c, t·ª´ ƒë√≥ h·ªó tr·ª£ qu√° tr√¨nh reasoning v√† retrieval trong to√†n h·ªá th·ªëng HippoRAG.
Ch√∫ng ta h√£y c√πng ph√¢n t√≠ch theo ki·ªÉu "m·ªôt ng∆∞·ªùi ph√° ‚Äì m·ªôt ng∆∞·ªùi tr·∫£ l·ªùi" ƒë·ªÉ hi·ªÉu s√¢u v·ªÅ c√°ch th√™m passage node sau khi ƒë√£ t√°ch ra b·ªô ba (triple) theo b√†i b√°o.

---
‚ùì Ph√°: Sau khi ƒë√£ s·ª≠ d·ª•ng OpenIE t√°ch ra c√°c triple (subject, predicate, object) t·ª´ m·ªói ƒëo·∫°n vƒÉn, l√†m th·∫ø n√†o ƒë·ªÉ "append" th√™m ph·∫ßn passage node v√†o ƒë·ªì th·ªã ki·∫øn th·ª©c?
Tr·∫£ l·ªùi:
 Theo b√†i b√°o, sau khi tr√≠ch xu·∫•t c√°c triple t·ª´ m·ªói passage, ta ti·∫øn h√†nh x√¢y d·ª±ng m·ªôt Knowledge Graph (KG) b·∫±ng c√°ch th·ª±c hi·ªán b∆∞·ªõc sau:
1. T·∫°o Phrase Nodes t·ª´ Triple:
  - M·ªói triple (v√≠ d·ª•: (Marie Curie, won, Nobel Prize)) s·∫Ω t·∫°o ra c√°c phrase nodes cho subject, predicate v√† object.
  - ƒê√¢y l√† c√°c n√∫t ƒë·∫°i di·ªán cho c√°c kh√°i ni·ªám tr·ª´u t∆∞·ª£ng c·ªßa th√¥ng tin.
2. Th√™m Passage Node:
  - M·ªói passage g·ªëc trong corpus ƒë∆∞·ª£c xem nh∆∞ m·ªôt passage node ri√™ng bi·ªát.
  - Sau ƒë√≥, append ph·∫ßn passage node n√†y v√†o ƒë·ªì th·ªã b·∫±ng c√°ch n·ªëi n√≥ v·ªõi t·∫•t c·∫£ c√°c phrase node ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ passage ƒë√≥.
  - C√°c k·∫øt n·ªëi n√†y th∆∞·ªùng ƒë∆∞·ª£c g·ªçi l√† context edge (ho·∫∑c edge "contains") v√¨ ch√∫ng bi·ªÉu th·ªã r·∫±ng passage ch·ª©a ƒë·ª±ng c√°c th√¥ng tin hay kh√°i ni·ªám ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi phrase nodes.

---
‚ùì Ph√°: T·∫°i sao c·∫ßn th√™m passage node? Kh√¥ng ƒë·ªß ch·ªâ c√≥ phrase nodes sao?
Tr·∫£ l·ªùi:
- B·∫£o to√†n ng·ªØ c·∫£nh:
  - Phrase nodes cung c·∫•p m·ªôt d·∫°ng tr·ª´u t∆∞·ª£ng, t·ªïng qu√°t c·ªßa th√¥ng tin. Tuy nhi√™n, th√¥ng tin n√†y c√≥ th·ªÉ m·∫•t ƒëi chi ti·∫øt ng·ªØ c·∫£nh quan tr·ªçng.
  - B·∫±ng c√°ch th√™m passage node, h·ªá th·ªëng l∆∞u gi·ªØ ƒë∆∞·ª£c to√†n b·ªô th√¥ng tin chi ti·∫øt c·ªßa ƒëo·∫°n vƒÉn g·ªëc.
- Dense‚ÄìSparse Integration:
  - Passage node ƒë·∫°i di·ªán cho ph·∫ßn dense coding (ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh, chi ti·∫øt), trong khi phrase nodes ƒë·∫°i di·ªán cho sparse coding (kh√°i qu√°t, d·ªÖ d√†ng reasoning).
  - Vi·ªác k·∫øt h·ª£p c·∫£ hai gi√∫p m√¥ h√¨nh c√≥ th·ªÉ th·ª±c hi·ªán t·ªët h∆°n trong vi·ªác tr·∫£ l·ªùi c√¢u h·ªèi y√™u c·∫ßu v·ª´a hi·ªÉu ƒë∆∞·ª£c √Ω ch√≠nh v·ª´a n·∫Øm b·∫Øt ƒë∆∞·ª£c chi ti·∫øt.

---
‚ùì Ph√°: C√°ch th·ª©c n·ªëi gi·ªØa passage node v√† phrase nodes ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o?
Tr·∫£ l·ªùi:
- M·ªói passage node s·∫Ω ƒë∆∞·ª£c n·ªëi v·ªõi t·∫•t c·∫£ c√°c phrase node ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ ch√≠nh passage ƒë√≥.
- C√°c k·∫øt n·ªëi n√†y (context edges) kh√¥ng c·∫ßn c√≥ h∆∞·ªõng r√µ r√†ng nh∆∞ c√°c relation edge c·ªßa triple, m√† ch·ªß y·∫øu nh·∫±m m·ª•c ƒë√≠ch th·ªÉ hi·ªán m·ªëi li√™n h·ªá v·ªÅ ng·ªØ c·∫£nh.
- Khi th·ª±c hi·ªán retrieval ho·∫∑c Personalized PageRank (PPR), passage node gi√∫p tƒÉng tr·ªçng s·ªë cho c√°c ƒëo·∫°n vƒÉn ch·ª©a ƒë·ª±ng th√¥ng tin chi ti·∫øt, t·ª´ ƒë√≥ c·∫£i thi·ªán kh·∫£ nƒÉng truy xu·∫•t v√† hi·ªÉu s√¢u c·ªßa m√¥ h√¨nh.

---
‚ùì Ph√°: T·∫°i sao kh√¥ng ch·ªâ d√πng phrase nodes m√† b·ªè qua passage node?
Tr·∫£ l·ªùi:
- N·∫øu ch·ªâ s·ª≠ d·ª•ng phrase nodes, ta ch·ªâ c√≥ ƒë∆∞·ª£c m·ªôt b·∫£n t√≥m t·∫Øt r·∫•t c√¥ ƒë·ªçng c·ªßa th√¥ng tin.
- Nhi·ªÅu tr∆∞·ªùng h·ª£p, ƒë·∫∑c bi·ªát l√† c√°c c√¢u h·ªèi factual hay y√™u c·∫ßu s·ª± li√™n k·∫øt s√¢u gi·ªØa c√°c √Ω, ƒë√≤i h·ªèi ph·∫£i c√≥ ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß t·ª´ ƒëo·∫°n vƒÉn g·ªëc.
- Passage node gi√∫p m√¥ h√¨nh truy xu·∫•t c√°c ƒëo·∫°n vƒÉn g·ªëc ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng m·∫•t ƒëi c√°c chi ti·∫øt c·∫ßn thi·∫øt cho vi·ªác tr·∫£ l·ªùi ch√≠nh x√°c.

---
T·ªïng K·∫øt
- B∆∞·ªõc 1: T√°ch c√°c triple (subject, predicate, object) t·ª´ m·ªói passage b·∫±ng OpenIE c·ªßa LLM ‚Üí t·∫°o ra c√°c phrase nodes.
- B∆∞·ªõc 2: V·ªõi m·ªói passage g·ªëc, t·∫°o passage node.
- B∆∞·ªõc 3: Append passage node v√†o ƒë·ªì th·ªã b·∫±ng c√°ch n·ªëi n√≥ v·ªõi t·∫•t c·∫£ c√°c phrase node t·ª´ passage ƒë√≥ th√¥ng qua context edges.
- M·ª•c ƒë√≠ch: Gi·ªØ l·∫°i c·∫£ √Ω ch√≠nh (phrase node) v√† th√¥ng tin chi ti·∫øt (passage node) ƒë·ªÉ h·ªó tr·ª£ qu√° tr√¨nh retrieval v√† reasoning, t·ª´ ƒë√≥ c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa to√†n h·ªá th·ªëng.
D∆∞·ªõi ƒë√¢y l√† c√°ch ti·∫øp c·∫≠n chi ti·∫øt theo d·∫°ng h·ªèi ‚Äì ƒë√°p ph·∫£n bi·ªán (m·ªôt ng∆∞·ªùi ‚Äúph√°‚Äù, m·ªôt ng∆∞·ªùi ‚Äúb√™nh‚Äù) ƒë·ªÉ m·ªï x·∫ª th·∫≠t s√¢u 5 b∆∞·ªõc trong Online Retrieval & QA c·ªßa HippoRAG/HippoRAG 2. M·ªói c√¢u tr·∫£ l·ªùi ƒë·ªÅu c·ªë g·∫Øng g·∫Øn v·ªõi l·∫≠p lu·∫≠n t·ª´ b√†i b√°o ho·∫∑c √Ω t∆∞·ªüng ch√≠nh (c√≥ d·∫´n ch·ª©ng).

---
(1) Retrieving Passages and Triples
Ph·∫£n bi·ªán (Q):
T·∫°i sao ph·∫£i l·∫•y c·∫£ c√°c ƒëo·∫°n vƒÉn (passages) v√† c√°c b·ªô ba (triples)? Li·ªáu ch·ªâ c·∫ßn retrieval embedding t·ªët ƒë·ªÉ l·∫•y ƒëo·∫°n vƒÉn ƒë√£ ƒë·ªß ch∆∞a?
Tr·∫£ l·ªùi (A):
- C·∫ßn c·∫£ passages: v√¨ c√°c ƒëo·∫°n vƒÉn mang ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh. N·∫øu ch·ªâ d·ª±a v√†o node (th∆∞·ªùng l√† m·ªôt c√¢u/triple r·ªùi r·∫°c) th√¨ c√≥ th·ªÉ thi·∫øu m·∫•t chi ti·∫øt quan tr·ªçng.
- C·∫ßn c·∫£ triples: v√¨ triple l√† ‚Äúƒë∆°n v·ªã th√¥ng tin‚Äù mang t√≠nh kh√°i qu√°t ‚Äì gi√∫p m√¥ ph·ªèng s·ª± li√™n k·∫øt (associativity). Khi ta k√©o ƒë∆∞·ª£c triple li√™n quan, n√≥ c√≥ th·ªÉ ‚Äúg·ª£i √Ω‚Äù nhi·ªÅu quan h·ªá s√¢u h∆°n m√† m·ªôt ƒëo·∫°n vƒÉn ch∆∞a ch·∫Øc ƒë√£ hi·ªÉn th·ªã ngay.
- D·∫´n ch·ª©ng: Ph·∫ßn ‚ÄúOnline Retrieval‚Äù trong b√†i HippoRAG/HippoRAG 2 cho th·∫•y, h·ªá th·ªëng ch·∫°y query-to-triple ƒë·ªÉ l·∫•y triple tr∆∞·ªõc, sau ƒë√≥ m·ªõi ch·∫°y ti·∫øp c√°c b∆∞·ªõc kh√°c. K·∫øt h·ª£p ‚Äúpassage node‚Äù v√† ‚Äúphrase node‚Äù (triple) trong ƒë·ªì th·ªã l√† c√°ch m√† m√¥ h√¨nh ‚Äútr·ªôn‚Äù gi·ªØa ‚Äúcontext‚Äù v√† ‚Äúconcepts‚Äù.

---
(2) Recognition Memory (Triple Filtering)
Ph·∫£n bi·ªán (Q):
T·∫°i sao l·∫°i c·∫ßn m·ªôt b∆∞·ªõc g·ªçi l√† ‚ÄúRecognition Memory‚Äù ƒë·ªÉ l·ªçc triple? Ch·∫≥ng ph·∫£i retrieval encoder ƒë√£ x·∫øp h·∫°ng ƒë∆∞·ª£c r·ªìi sao?
Tr·∫£ l·ªùi (A):
- Encoder-based ranking ch·ªâ d·ª±a tr√™n vector similarity, c√≥ th·ªÉ nh·∫∑t ph·∫£i nh·ªØng triple ‚Äúna n√°‚Äù v·ªÅ t·ª´ v·ª±ng nh∆∞ng kh√¥ng th·∫≠t s·ª± li√™n quan ng·ªØ nghƒ©a.
- LLM-based filtering (Recognition Memory) gi·ªëng nh∆∞ ‚Äúduy·ªát l·∫°i‚Äù nh·ªØng triple top-k, ƒë·ªÉ x√°c nh·∫≠n: ‚ÄúTriple n√†y th·ª±c s·ª± n√≥i v·ªÅ c√°i g√¨, c√≥ ƒÉn kh·ªõp v·ªõi c√¢u h·ªèi kh√¥ng?‚Äù ‚Äì T·ª´ ƒë√≥ lo·∫°i b·ªõt nhi·ªÖu.
- V√≠ d·ª•: C√≥ nh·ªØng triple m√† entity gi·ªëng nhau v·ªÅ m·∫∑t ng√¥n ng·ªØ (v√≠ d·ª• ‚ÄúParis‚Äù ‚Äì m·ªôt ng∆∞·ªùi t√™n Paris vs. th√†nh ph·ªë Paris). N·∫øu ch·ªâ d√πng embedding, ƒë√¥i khi b·ªã l·∫´n. B∆∞·ªõc filtering n√†y gi√∫p LLM d√πng ng·ªØ c·∫£nh ƒë·ªÉ ph√¢n bi·ªát.
- D·∫´n ch·ª©ng: B√†i b√°o gi·ªõi thi·ªáu c∆° ch·∫ø ‚Äútwo-step retrieval‚Äù: 1) Query-to-triple (embedding) v√† 2) Triple filter (LLM) ‚Äì m√¥ ph·ªèng ‚Äúrecall vs. recognition‚Äù trong tr√≠ nh·ªõ con ng∆∞·ªùi.

---
(3) Assigning Seed Node Weights
Ph·∫£n bi·ªán (Q):
Ph√¢n ph·ªëi tr·ªçng s·ªë (reset probability) cho c√°c ‚Äúseed node‚Äù ‚Äì ƒë·∫∑c bi·ªát l√† gi·ªØa ‚Äúpassage node‚Äù v√† ‚Äúphrase node‚Äù ‚Äì c√≥ th·ª±c s·ª± c·∫ßn thi·∫øt, hay ch·ªâ c·∫ßn g√°n ƒë·ªÅu l√† ƒë∆∞·ª£c?
Tr·∫£ l·ªùi (A):
- Kh√¥ng th·ªÉ g√°n ƒë·ªÅu: B·ªüi v√¨ ‚Äúphrase node‚Äù (c√°c kh√°i ni·ªám) v√† ‚Äúpassage node‚Äù (c√°c vƒÉn b·∫£n) ƒë√≥ng vai tr√≤ kh√°c nhau. N·∫øu g√°n tr·ªçng s·ªë gi·ªëng nhau, m√¥ h√¨nh c√≥ th·ªÉ nghi√™ng qu√° nhi·ªÅu v·ªÅ concept ho·∫∑c qu√° nhi·ªÅu v·ªÅ context.
- C·∫ßn c√¢n b·∫±ng: Ch·∫≥ng h·∫°n, khi c√¢u h·ªèi r·∫•t c·ª• th·ªÉ (v√≠ d·ª•: ‚ÄúT√°c gi·∫£ cu·ªën s√°ch X l√† ai?‚Äù), ‚Äúphrase node‚Äù v·ªÅ ‚Äúbook_author‚Äù c√≥ th·ªÉ quan tr·ªçng h∆°n. Nh∆∞ng n·∫øu c√¢u h·ªèi ph·ª©c t·∫°p, ‚Äúpassage node‚Äù c√≥ ng·ªØ c·∫£nh r·ªông h∆°n s·∫Ω c·∫ßn tr·ªçng s·ªë cao h∆°n ƒë·ªÉ PPR lan t·ªèa nhi·ªÅu h∆°n.
- D·∫´n ch·ª©ng: T√°c gi·∫£ c√≥ l√†m ablation study (v√≠ d·ª• trong ph·∫ßn Implementation & Discussion) ƒë·ªÉ ch·ª©ng minh ph·∫£i ch·ªçn ‚Äúweight factor‚Äù cho passage node h·ª£p l√Ω (0.01, 0.05, v.v.) th√¨ m·ªõi t·ªëi ∆∞u Recall@5.

---
(4) PPR Graph Search
Ph·∫£n bi·ªán (Q):
Personalized PageRank (PPR) th·ª±c s·ª± gi√∫p ƒë∆∞·ª£c g√¨? Li·ªáu m·ªôt v√≤ng l·∫∑p iterative retrieval (nh∆∞ IRCoT) kh√¥ng ph·∫£i ƒë√£ gi·∫£i quy·∫øt ƒë∆∞·ª£c multi-hop sao?
Tr·∫£ l·ªùi (A):
- PPR l√†m multi-hop ngay trong m·ªôt l∆∞·ª£t: Thay v√¨ ph·∫£i g·ªçi LLM li√™n ti·∫øp ƒë·ªÉ ‚Äúg·ª£i √Ω‚Äù truy v·∫•n ph·ª• (iterative retrieval), PPR cho ph√©p khu·∫øch t√°n x√°c su·∫•t t·ª´ c√°c seed node (passage/phrase) ƒë·∫øn l√¢n c·∫≠n trong KG.
- Hi·ªáu qu·∫£ chi ph√≠: Iterative retrieval ki·ªÉu IRCoT th∆∞·ªùng t·ªën nhi·ªÅu l∆∞·ª£t g·ªçi LLM. PPR l√† m·ªôt thu·∫≠t to√°n ƒë·ªì th·ªã ch·∫°y n·ªôi b·ªô, r·∫ª h∆°n v·ªÅ m·∫∑t inference.
- Ph√π h·ª£p v·ªõi ‚Äúassociative memory‚Äù: B√†i b√°o g·ª£i √Ω r·∫±ng PPR m√¥ ph·ªèng c√°ch hippocampus (n√£o b·ªô) k√≠ch ho·∫°t c√°c li√™n k·∫øt gi·ªØa kh√°i ni·ªám. T·ª´ 1 seed node, lan ra nhi·ªÅu n√∫t kh√°c theo x√°c su·∫•t.
- D·∫´n ch·ª©ng: HippoRAG/HippoRAG 2 so s√°nh v·ªõi IRCoT, cho th·∫•y PPR c√≥ th·ªÉ ‚Äúg√≥i‚Äù vi·ªác multi-hop reasoning trong m·ªôt l·∫ßn retrieval, nhanh h∆°n v√† √≠t t·ªën chi ph√≠ h∆°n.

---
(5) QA Reading with Selected Passages
Ph·∫£n bi·ªán (Q):
N·∫øu cu·ªëi c√πng v·∫´n ph·∫£i g·ªçi LLM ƒë·ªÉ ƒë·ªçc l·∫°i c√°c ƒëo·∫°n vƒÉn (QA reading), v·∫≠y vi·ªác x√¢y d·ª±ng KG hay PPR c√≥ th·∫≠t s·ª± quan tr·ªçng? Ch·∫≥ng ph·∫£i LLM gi·ªù ƒë√£ ‚Äús·∫µn s√†ng‚Äù tr·∫£ l·ªùi n·∫øu cho n√≥ ƒë·ªß context?
Tr·∫£ l·ªùi (A):
- LLM c·∫ßn ‚Äúƒë·∫ßu v√†o s·∫°ch‚Äù: D√π LLM m·∫°nh, nh∆∞ng n·∫øu input context b·ªã nhi·ªÖu (ch·ªçn sai passage) ho·∫∑c qu√° d√†n tr·∫£i, LLM d·ªÖ tr·∫£ l·ªùi sai, ƒë·∫∑c bi·ªát v·ªõi c√¢u h·ªèi c·∫ßn multi-hop.
- KG + PPR gi√∫p ‚Äút√≥m g·ªçn‚Äù ƒë√∫ng ƒëo·∫°n vƒÉn li√™n quan, thay v√¨ l√¥i t·∫•t c·∫£. LLM c√†ng c√≥ context ch√≠nh x√°c, tr·∫£ l·ªùi c√†ng t·ªët.
- T√≠nh m·ªü r·ªông: Khi t·∫≠p d·ªØ li·ªáu r·∫•t l·ªõn, ch·ªâ embedding retrieval c√≥ th·ªÉ b·ªè s√≥t ‚Äúchu·ªói logic‚Äù li√™n quan. B·ªï sung KG + PPR => M√¥ h√¨nh ‚Äúli√™n k·∫øt‚Äù t·ªët h∆°n, LLM ƒë·ª° b·ªã qu√° t·∫£i.
- D·∫´n ch·ª©ng: K·∫øt qu·∫£ th·ª±c nghi·ªám (b·∫£ng so s√°nh F1, EM) cho th·∫•y HippoRAG 2 sau khi ch·ªçn top passages qua PPR, r·ªìi cho LLM ƒë·ªçc, cho k·∫øt qu·∫£ cao h∆°n so v·ªõi retrieval th∆∞·ªùng.

---
T·ªïng k·∫øt
5 b∆∞·ªõc trong Online Retrieval & QA ƒë∆∞·ª£c thi·∫øt k·∫ø c√≥ ch·ªß ƒë√≠ch, m√¥ ph·ªèng c∆° ch·∫ø b·ªô nh·ªõ c·ªßa con ng∆∞·ªùi:
1. L·∫•y passage + triple song song (ƒë·∫£m b·∫£o v·ª´a c√≥ context v·ª´a c√≥ concept).
2. L·ªçc triple b·∫±ng LLM (Recognition Memory) ƒë·ªÉ lo·∫°i nhi·ªÖu.
3. G√°n tr·ªçng s·ªë seed node c·∫©n th·∫≠n (c√¢n b·∫±ng passage node v√† phrase node).
4. Ch·∫°y PPR ƒë·ªÉ th·ª±c hi·ªán multi-hop reasoning trong m·ªôt b∆∞·ªõc, ti·∫øt ki·ªám chi ph√≠ so v·ªõi iterative retrieval.
5. Cu·ªëi c√πng, LLM m·ªõi ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë√£ ch·ªçn ƒë·ªÉ tr·∫£ l·ªùi, t·ªëi ∆∞u ƒë·ªô ch√≠nh x√°c.
T·ª´ng b∆∞·ªõc ƒë·ªÅu c√≥ l√Ω do t·ªìn t·∫°i, v√† k·∫øt qu·∫£ th·ª±c nghi·ªám ch·ª©ng minh m√¥ h√¨nh n√†y tƒÉng hi·ªáu qu·∫£ truy xu·∫•t (retrieval) c≈©ng nh∆∞ ƒë·ªô ch√≠nh x√°c khi tr·∫£ l·ªùi c√¢u h·ªèi (QA).

---
(3) Assigning Seed Node Weights
ƒê·∫∑t c√¢u h·ªèi ph√° nhi·ªÅu h∆°n ·ªü b∆∞·ªõc n√†y 
- Ch·∫≥ng h·∫°n: l√∫c m√† x√¢y Graph c√°c nodes vai tr√≤ nh∆∞ nhau hay l√† Hippo RAG 2 c√≥ ƒë√°nh d·∫•u r√µ l√† phrase node hay passage node ? 
- C√°ch ƒë√°nh Assing Seed Node Weights nh∆∞ n√†o trong b√†i b√°o c√≥ n√≥i ko 
- ... nhi·ªÅu c√¢u n·ªØa nh√©

√Ω l√† ph√° 1, m√¨nh bi·∫øt l√† n√≥ ƒë∆∞·ª£c r√µ r√†ng vai tr√≤, nh∆∞ng l√∫c ƒë∆∞a l√™n ƒë·ªì th·ªã √Ω th√¨ n√≥ c√≥ ƒë∆∞·ª£c r√µ r√†ng v·∫≠y kh√¥ng ? ki·ªÉu h·ªç d√πng c√°ch n√†o ƒë·ªÉ ƒë√°nh d·∫•u 2 lo·∫°i node ƒë√≥ ƒë·ªÉ khi query d√πng

Trong qu√° tr√¨nh x√¢y d·ª±ng ƒë·ªì th·ªã, HippoRAG 2 kh√¥ng ch·ªâ tr√≠ch xu·∫•t c√°c node m√† c√≤n "ƒë√°nh d·∫•u" r√µ r√†ng t·ª´ng lo·∫°i node th√¥ng qua c·∫•u tr√∫c v√† c√°c lo·∫°i c·∫°nh n·ªëi k·∫øt:
- Ph√¢n lo·∫°i qua ngu·ªìn g·ªëc t·∫°o node:
  - Phrase node: ƒê∆∞·ª£c t·∫°o ra t·ª´ vi·ªác tr√≠ch xu·∫•t b·ªô ba (triple) c·ªßa c√°c kh√°i ni·ªám qua OpenIE. Nh·ªØng node n√†y ch·ªâ ch·ª©a th√¥ng tin "concept" r√∫t g·ªçn v√† li√™n quan ƒë·∫øn c√°c m·ªëi quan h·ªá gi·ªØa th·ª±c th·ªÉ.
  - Passage node: ƒê·∫°i di·ªán cho to√†n b·ªô ƒëo·∫°n vƒÉn, ch·ª©a ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh v√† th√¥ng tin chi ti·∫øt.
 Qua ƒë√≥, qu√° tr√¨nh offline indexing c·ªßa HippoRAG 2 ƒë√£ ph√¢n bi·ªát r√µ r√†ng ngu·ªìn g·ªëc c·ªßa c√°c node.
- ƒê√°nh d·∫•u qua lo·∫°i c·∫°nh (edge type):
  - C√°c passage node ƒë∆∞·ª£c li√™n k·∫øt v·ªõi c√°c phrase node th√¥ng qua c√°c c·∫°nh mang nh√£n "contains" (ho·∫∑c "context edge"), nh·∫±m th·ªÉ hi·ªán r·∫±ng ƒëo·∫°n vƒÉn ch·ª©a c√°c c·ª•m t·ª´ (phrase) ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ n√≥.
  - Trong khi ƒë√≥, c√°c phrase node c√≥ th·ªÉ ƒë∆∞·ª£c n·ªëi v·ªõi nhau qua c√°c c·∫°nh kh√°c nh∆∞ "synonym edge" hay "relation edge". Vi·ªác n√†y gi√∫p nh·∫•n m·∫°nh m·ªëi li√™n h·ªá gi·ªØa c√°c kh√°i ni·ªám tr·ª´u t∆∞·ª£ng, t√°ch bi·ªát v·ªõi th√¥ng tin ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß t·ª´ passage.
- T√°c ƒë·ªông khi query:
 Khi m·ªôt truy v·∫•n ƒë∆∞·ª£c ƒë∆∞a v√†o, h·ªá th·ªëng s·ª≠ d·ª•ng c√°c embedding v√† tr·ªçng s·ªë ri√™ng bi·ªát cho t·ª´ng lo·∫°i node. Nh·ªù v·∫≠y, vi·ªác so s√°nh cosine similarity ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n c·∫£ phrase node v√† passage node theo c√°ch ph√π h·ª£p v·ªõi ƒë·∫∑c tr∆∞ng c·ªßa ch√∫ng. C√°ch g√°n tr·ªçng s·ªë (seed node weights) c≈©ng ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ri√™ng cho t·ª´ng lo·∫°i, gi√∫p qu√° tr√¨nh Personalized PageRank lan t·ªèa th√¥ng tin ƒë√∫ng theo c·∫•u tr√∫c ƒë√£ ƒë∆∞·ª£c ƒë√°nh d·∫•u.
D·∫´n ch·ª©ng: Trong ph·∫ßn m√¥ t·∫£ c·ªßa HippoRAG 2, t√°c gi·∫£ tr√¨nh b√†y r·∫±ng m·ªói passage trong corpus ƒë∆∞·ª£c x·ª≠ l√Ω nh∆∞ m·ªôt passage node v√† ƒë∆∞·ª£c n·ªëi v·ªõi t·∫•t c·∫£ c√°c phrase ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ n√≥ th√¥ng qua c√°c "context edge". ƒêi·ªÅu n√†y kh√¥ng ch·ªâ gi√∫p x√¢y d·ª±ng m·ªôt ki·∫øn tr√∫c ƒë·ªì th·ªã r√µ r√†ng m√† c√≤n cho ph√©p h·ªá th·ªëng ph√¢n bi·ªát v√† √°p d·ª•ng tr·ªçng s·ªë kh√°c nhau cho m·ªói lo·∫°i node trong qu√° tr√¨nh truy xu·∫•t.
Nh∆∞ v·∫≠y, b·∫±ng c√°ch ph√¢n lo·∫°i qua ngu·ªìn g·ªëc t·∫°o node v√† ƒë√°nh d·∫•u th√¥ng qua c√°c lo·∫°i c·∫°nh n·ªëi k·∫øt, HippoRAG 2 ƒë·∫£m b·∫£o r·∫±ng trong qu√° tr√¨nh truy v·∫•n, h·ªá th·ªëng c√≥ th·ªÉ nh·∫≠n di·ªán v√† t·∫≠n d·ª•ng ƒë√∫ng vai tr√≤ c·ªßa t·ª´ng lo·∫°i node, t·ª´ ƒë√≥ c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa retrieval v√† QA.

D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• chi ti·∫øt, k√®m theo ƒë·ªëi tho·∫°i ‚Äúph·∫£n bi·ªán ‚Äì tr·∫£ l·ªùi‚Äù minh h·ªça r√µ r√†ng c√°ch Assigning Seed Node Weights ho·∫°t ƒë·ªông trong qu√° tr√¨nh Online Retrieval c·ªßa HippoRAG.

---
V√≠ d·ª• c·ª• th·ªÉ
Truy v·∫•n: "T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?"
Sau khi h·ªá th·ªëng x·ª≠ l√Ω truy v·∫•n, ta thu ƒë∆∞·ª£c c√°c th√†nh ph·∫ßn sau:
1. Tr√≠ch xu·∫•t t·ª´ truy v·∫•n (Query Extraction):
  - C√°c th·ª±c th·ªÉ ƒë∆∞·ª£c tr√≠ch xu·∫•t:
    - ‚ÄúThe Great Novel‚Äù
    - ‚Äút√°c gi·∫£‚Äù
2. Seed Nodes ƒë∆∞·ª£c x√°c ƒë·ªãnh trong Knowledge Graph (KG):
  - Phrase Nodes (ƒë·∫°i di·ªán cho kh√°i ni·ªám, th√¥ng tin tr√≠ch xu·∫•t t·ª´ c√°c triple):
    - Node A: "t√°c gi·∫£"
      - Gi√° tr·ªã similarity (v·ªõi "t√°c gi·∫£" truy v·∫•n): 0.72
    - Node B: "The Great Novel"
      - Gi√° tr·ªã similarity (v·ªõi ti√™u ƒë·ªÅ truy v·∫•n): 0.68
  - Passage Nodes (ƒëo·∫°n vƒÉn cung c·∫•p ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß):
    - Node C: ƒêo·∫°n vƒÉn 1: ‚ÄúThe Great Novel, m·ªôt t√°c ph·∫©m kinh ƒëi·ªÉn, ƒë∆∞·ª£c vi·∫øt b·ªüi John Doe ‚Äì t√°c gi·∫£ n·ªïi ti·∫øng v·ªõi nhi·ªÅu gi·∫£i th∆∞·ªüng vƒÉn h·ªçc."
      - Gi√° tr·ªã similarity (v·ªõi truy v·∫•n, d·ª±a tr√™n c·∫£ ng·ªØ c·∫£nh v√† t·ª´ kh√≥a): 0.80
    - Node D: ƒêo·∫°n vƒÉn 2: M·ªôt ƒëo·∫°n vƒÉn kh√°c n√≥i v·ªÅ John Doe nh∆∞ng kh√¥ng ƒë·ªÅ c·∫≠p r√µ r√†ng ƒë·∫øn cu·ªën s√°ch.
      - Gi√° tr·ªã similarity: 0.55

---
B∆∞·ªõc 1: X√°c ƒë·ªãnh Seed Nodes v√† G√°n Tr·ªçng S·ªë Ban ƒë·∫ßu
Gi·∫£ s·ª≠ h·ªá th·ªëng quy ∆∞·ªõc:
- Phrase Nodes nh·∫≠n tr·ªçng s·ªë m·∫∑c ƒë·ªãnh = 1.0
- Passage Nodes ban ƒë·∫ßu nh·∫≠n tr·ªçng s·ªë th·∫•p h∆°n, v√¨ ch√∫ng mang th√¥ng tin c·ª• th·ªÉ nh∆∞ng c·∫ßn ƒë∆∞·ª£c ‚Äúƒëi·ªÅu ch·ªânh‚Äù qua weight factor.
  - V√≠ d·ª•: h·ªá th·ªëng g√°n weight factor cho Passage Nodes = 0.05.
Do ƒë√≥, ta c√≥:
- Node A ("t√°c gi·∫£"):
  - Similarity score = 0.72 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.72 √ó 1.0 = 0.72
- Node B ("The Great Novel"):
  - Similarity score = 0.68 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.68 √ó 1.0 = 0.68
- Node C (Passage 1):
  - Similarity score = 0.80 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.80 √ó 0.05 = 0.04
- Node D (Passage 2):
  - Similarity score = 0.55 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.55 √ó 0.05 = 0.0275

---
B∆∞·ªõc 2: ƒê·ªëi tho·∫°i ph·∫£n bi·ªán ‚Äì Gi·∫£i th√≠ch chi ti·∫øt
Ph·∫£n bi·ªán 1: T·∫°i sao Passage Nodes l·∫°i ƒë∆∞·ª£c nh√¢n v·ªõi weight factor th·∫•p (0.05) trong khi Phrase Nodes gi·ªØ nguy√™n tr·ªçng s·ªë 1.0?
Q: N·∫øu Passage Nodes mang to√†n b·ªô ng·ªØ c·∫£nh c·ªßa ƒëo·∫°n vƒÉn, t·∫°i sao ch√∫ng l·∫°i ƒë∆∞·ª£c g√°n tr·ªçng s·ªë th·∫•p h∆°n so v·ªõi Phrase Nodes?
A:
- L√Ω do:
  - Phrase Nodes ƒë·∫°i di·ªán cho c√°c kh√°i ni·ªám tr·ª´u t∆∞·ª£ng, th∆∞·ªùng c√≥ t√≠nh kh√°i qu√°t v√† c√≥ th·ªÉ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn truy v·∫•n. Ch√∫ng c·∫ßn ƒë∆∞·ª£c ∆∞u ti√™n cao ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng c√°c kh√°i ni·ªám ch√≠nh ƒë∆∞·ª£c nh·∫≠n di·ªán.
  - Passage Nodes m·∫∑c d√π ch·ª©a ng·ªØ c·∫£nh chi ti·∫øt, nh∆∞ng n·∫øu g√°n tr·ªçng s·ªë qu√° cao, th√¥ng tin qu√° ‚Äúph√¢n t√°n‚Äù c√≥ th·ªÉ l√†m nhi·ªÖu k·∫øt qu·∫£ retrieval. Weight factor th·∫•p gi√∫p ki·ªÉm so√°t s·ª± lan t·ªèa c·ªßa th√¥ng tin c·ª• th·ªÉ, ch·ªâ khi m√† c√°c passage c√≥ nhi·ªÅu li√™n k·∫øt v·ªõi c√°c Phrase Nodes, ch√∫ng m·ªõi ‚Äúƒë∆∞·ª£c b√π ƒë·∫Øp‚Äù th√¥ng qua qu√° tr√¨nh PPR.
- D·∫´n ch·ª©ng th·ª±c nghi·ªám:
  - B√†i b√°o tr√¨nh b√†y c√°c th√≠ nghi·ªám ablation v·ªõi c√°c gi√° tr·ªã weight factor kh√°c nhau (v√≠ d·ª•: 0.01, 0.05, 0.1,...) cho Passage Nodes. K·∫øt qu·∫£ cho th·∫•y, khi ch·ªçn weight factor ph√π h·ª£p (0.05 trong v√≠ d·ª• n√†y), h·ªá th·ªëng ƒë·∫°t Recall@5 t·ªëi ∆∞u. ƒêi·ªÅu n√†y ch·ª©ng t·ªè r·∫±ng vi·ªác c√¢n b·∫±ng gi·ªØa kh√°i ni·ªám v√† ng·ªØ c·∫£nh l√† r·∫•t quan tr·ªçng.

---
Ph·∫£n bi·ªán 2: Vi·ªác nh√¢n weight factor ·∫£nh h∆∞·ªüng nh∆∞ th·∫ø n√†o ƒë·∫øn k·∫øt qu·∫£ ban ƒë·∫ßu c·ªßa PPR?
Q: V·ªõi c√°c tr·ªçng s·ªë ban ƒë·∫ßu nh∆∞ tr√™n, l√†m th·∫ø n√†o ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng Passage Node C ‚Äì m·∫∑c d√π c√≥ similarity score cao (0.80) ‚Äì c√≥ th·ªÉ c·∫°nh tranh ƒë∆∞·ª£c v·ªõi Phrase Nodes khi weight factor th·∫•p?
A:
- Gi·∫£i th√≠ch:
  - Ban ƒë·∫ßu, Passage Node C c√≥ gi√° tr·ªã tr·ªçng s·ªë ch·ªâ l√† 0.04, so v·ªõi 0.72 (Node A) v√† 0.68 (Node B).
  - Tuy nhi√™n, trong qu√° tr√¨nh PPR, tr·ªçng s·ªë n√†y s·∫Ω ƒë∆∞·ª£c ‚Äúlan t·ªèa‚Äù qua c√°c c·∫°nh trong Knowledge Graph. N·∫øu Passage Node C ƒë∆∞·ª£c li√™n k·∫øt m·∫°nh m·∫Ω v·ªõi c√°c Phrase Nodes c√≥ tr·ªçng s·ªë cao, c√°c c·∫°nh li√™n k·∫øt n√†y s·∫Ω gi√∫p ‚Äúk√©o‚Äù th√™m probability mass cho Node C.
  - V√≠ d·ª•: n·∫øu Node C c√≥ nhi·ªÅu li√™n k·∫øt ƒë·∫øn Node A v√† Node B, qu√° tr√¨nh PPR s·∫Ω c·ªông d·ªìn th√™m gi√° tr·ªã cho Node C, gi√∫p n√≥ ƒë∆∞·ª£c ∆∞u ti√™n khi x·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn cu·ªëi c√πng.
- D·∫´n ch·ª©ng:
  - B√†i b√°o cho bi·∫øt r·∫±ng k·∫øt h·ª£p gi·ªØa distribution t·ª´ PPR v√† ma tr·∫≠n xu·∫•t hi·ªán (P matrix) cho ph√©p c√°c passage nh·∫≠n ƒë∆∞·ª£c ƒëi·ªÉm t·ªïng h·ª£p cao h∆°n, n·∫øu ch√∫ng ƒë∆∞·ª£c li√™n k·∫øt ch·∫∑t ch·∫Ω v·ªõi c√°c Phrase Nodes c√≥ tr·ªçng s·ªë ban ƒë·∫ßu cao.

---
Ph·∫£n bi·ªán 3: Gi·∫£ s·ª≠ c√≥ m·ªôt Passage Node D c√≥ similarity score th·∫•p (0.55 ‚Üí 0.0275), li·ªáu n√≥ c√≥ b·ªã lo·∫°i b·ªè ho√†n to√†n hay v·∫´n c√≥ vai tr√≤?
Q: Passage Node D c√≥ ƒëi·ªÉm ban ƒë·∫ßu r·∫•t th·∫•p, nh∆∞ng li·ªáu n√≥ c√≥ ƒë√≥ng g√≥p hay kh√¥ng trong qu√° tr√¨nh retrieval?
A:
- Gi·∫£i th√≠ch:
  - M·∫∑c d√π Passage Node D c√≥ similarity score th·∫•p v√† tr·ªçng s·ªë ban ƒë·∫ßu ch·ªâ l√† 0.0275, n·∫øu n√≥ c√≥ m·ªôt s·ªë li√™n k·∫øt h·ªØu √≠ch trong KG, ƒë·∫∑c bi·ªát l√† v·ªõi c√°c Phrase Nodes c√≥ ƒëi·ªÉm cao, qu√° tr√¨nh PPR c√≥ th·ªÉ ‚Äún√¢ng ƒë·ª°‚Äù gi√° tr·ªã c·ªßa n√≥ qua vi·ªác c·ªông d·ªìn t·ª´ c√°c n√∫t l√¢n c·∫≠n.
  - Tuy nhi√™n, n·∫øu kh√¥ng c√≥ li√™n k·∫øt m·∫°nh m·∫Ω n√†o, Node D c√≥ th·ªÉ kh√¥ng ƒë√≥ng g√≥p nhi·ªÅu v√† s·∫Ω ƒë∆∞·ª£c x·∫øp h·∫°ng th·∫•p. ƒêi·ªÅu n√†y gi√∫p h·ªá th·ªëng l·ªçc ra c√°c passage kh√¥ng ƒë·ªß li√™n quan.
- D·∫´n ch·ª©ng:
  - K·∫øt qu·∫£ th·ª±c nghi·ªám v√† ablation study trong b√†i b√°o ch·ªâ ra r·∫±ng nh·ªØng passage kh√¥ng ƒë∆∞·ª£c li√™n k·∫øt ch·∫∑t ch·∫Ω s·∫Ω nh·∫≠n ƒëi·ªÉm th·∫•p sau PPR, t·ª´ ƒë√≥ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn k·∫øt qu·∫£ cu·ªëi c√πng.

---
B∆∞·ªõc 3: T√≠ch h·ª£p qua PPR
Sau khi c√°c seed node ƒë∆∞·ª£c g√°n tr·ªçng s·ªë, h·ªá th·ªëng ch·∫°y thu·∫≠t to√°n Personalized PageRank tr√™n KG. Qu√° tr√¨nh n√†y s·∫Ω:
- Lan t·ªèa probability mass: T·ª´ c√°c seed node ban ƒë·∫ßu, gi√° tr·ªã s·∫Ω ƒë∆∞·ª£c lan t·ªèa qua c√°c c·∫°nh li√™n k·∫øt.
- K·∫øt h·ª£p v·ªõi P matrix: Sau ƒë√≥, probability mass ƒë∆∞·ª£c nh√¢n v·ªõi ma tr·∫≠n P ch·ª©a th√¥ng tin t·∫ßn s·ªë xu·∫•t hi·ªán c·ªßa c√°c n√∫t trong c√°c ƒëo·∫°n vƒÉn, gi√∫p x√°c ƒë·ªãnh th·ª© h·∫°ng cu·ªëi c√πng cho c√°c passage.
K·∫øt qu·∫£ cu·ªëi c√πng, m·∫∑c d√π ban ƒë·∫ßu Passage Node C ch·ªâ c√≥ tr·ªçng s·ªë 0.04, nh∆∞ng n·∫øu li√™n k·∫øt t·ªët, n√≥ c√≥ th·ªÉ ƒë∆∞·ª£c ‚Äún√¢ng‚Äù l√™n v√† x·∫øp h·∫°ng cao, gi√∫p h·ªá th·ªëng truy xu·∫•t ƒëo·∫°n vƒÉn ch·ª©a th√¥ng tin ‚ÄúJohn Doe l√† t√°c gi·∫£ c·ªßa The Great Novel‚Äù m·ªôt c√°ch ch√≠nh x√°c.

---
T·ªïng k·∫øt v√≠ d·ª•
1. Tr√≠ch xu·∫•t v√† x√°c ƒë·ªãnh Seed Nodes:
  - Phrase Nodes: "t√°c gi·∫£" (0.72), "The Great Novel" (0.68)
  - Passage Nodes: Passage 1 (0.80 ‚Üí 0.04 sau nh√¢n), Passage 2 (0.55 ‚Üí 0.0275)
2. G√°n tr·ªçng s·ªë:
  - Phrase Nodes gi·ªØ tr·ªçng s·ªë ƒë·∫ßy ƒë·ªß (1.0)
  - Passage Nodes nh√¢n v·ªõi weight factor (0.05)
3. Qu√° tr√¨nh PPR:
  - Lan t·ªèa probability mass t·ª´ c√°c seed node qua c√°c c·∫°nh c·ªßa KG.
  - C√°c Passage Nodes, m·∫∑c d√π c√≥ tr·ªçng s·ªë ban ƒë·∫ßu th·∫•p, s·∫Ω nh·∫≠n ƒë∆∞·ª£c th√™m gi√° tr·ªã n·∫øu ƒë∆∞·ª£c li√™n k·∫øt m·∫°nh m·∫Ω v·ªõi c√°c Phrase Nodes.
4. K·∫øt qu·∫£ cu·ªëi c√πng:
  - Passage Node ch·ª©a ƒëo·∫°n vƒÉn ch√≠nh v·ªÅ John Doe s·∫Ω ƒë∆∞·ª£c x·∫øp h·∫°ng cao, ƒë·∫£m b·∫£o h·ªá th·ªëng tr·∫£ l·ªùi ch√≠nh x√°c cho truy v·∫•n "T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?".
Qua v√≠ d·ª• n√†y, ch√∫ng ta th·∫•y r·∫±ng vi·ªác Assigning Seed Node Weights kh√¥ng ch·ªâ l√† m·ªôt thao t√°c ƒë·ªãnh l∆∞·ª£ng ban ƒë·∫ßu, m√† c√≤n l√† b∆∞·ªõc quy·∫øt ƒë·ªãnh ƒë·ªÉ c√¢n b·∫±ng gi·ªØa th√¥ng tin kh√°i ni·ªám v√† ng·ªØ c·∫£nh, t·ª´ ƒë√≥ t·ªëi ∆∞u qu√° tr√¨nh PPR v√† c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa h·ªá th·ªëng truy xu·∫•t.
ƒê√∫ng v·∫≠y, khi n√≥i v·ªÅ tr·ªçng s·ªë m·∫∑c ƒë·ªãnh cho Phrase Nodes, ta c√≥ th·ªÉ ƒë·∫∑t weight factor l√† 1.0, nh∆∞ng ƒëi·ªÅu n√†y kh√¥ng c√≥ nghƒ©a l√† gi√° tr·ªã similarity c·ªßa t·ª´ng node s·∫Ω lu√¥n l√† 1.0.
C·ª• th·ªÉ:
- Tr·ªçng s·ªë (weight factor) c·ªßa Phrase Nodes: Khi g√°n tr·ªçng s·ªë cho c√°c Phrase Nodes, h·ªá th·ªëng s·ª≠ d·ª•ng h·ªá s·ªë 1.0 ƒë·ªÉ gi·ªØ nguy√™n gi√° tr·ªã similarity ban ƒë·∫ßu.
- Gi√° tr·ªã similarity: ƒê√¢y l√† m·ªôt ch·ªâ s·ªë ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa embedding c·ªßa th·ª±c th·ªÉ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ truy v·∫•n v√† embedding c·ªßa node trong Knowledge Graph. Gi√° tr·ªã n√†y th∆∞·ªùng n·∫±m trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1, v√† kh√¥ng ph·∫£i l√∫c n√†o c≈©ng b·∫±ng 1.0. V√≠ d·ª•, Node A c√≥ similarity l√† 0.72 v√† Node B c√≥ similarity l√† 0.68, ƒëi·ªÅu n√†y ph·∫£n √°nh m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng (kh√¥ng ph·∫£i l√† ‚Äútr·ªçng s·ªë‚Äù m√† l√† k·∫øt qu·∫£ c·ªßa ph√©p ƒëo similarity).
T√≥m l·∫°i, m·∫∑c ƒë·ªãnh weight factor c·ªßa Phrase Nodes l√† 1.0, nghƒ©a l√† ch√∫ng ta kh√¥ng ƒëi·ªÅu ch·ªânh th√™m gi√° tr·ªã similarity c·ªßa ch√∫ng, nh∆∞ng gi√° tr·ªã similarity th·ª±c s·ª± (0.72, 0.68, ‚Ä¶) ph·ª• thu·ªôc v√†o qu√° tr√¨nh t√≠nh to√°n t·ª´ c√°c embedding ch·ª© kh√¥ng ph·∫£i l√† 1.0 lu√¥n.

---
D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• chi ti·∫øt k√®m ƒë·ªëi tho·∫°i ph·∫£n bi·ªán ‚Äì ‚Äúng∆∞·ªùi ph√° ‚Äì ng∆∞·ªùi tr·∫£ l·ªùi‚Äù ‚Äì ƒë·ªÉ minh h·ªça c√°ch Seed Node Weights v√† thu·∫≠t to√°n Personalized PageRank (PPR) ho·∫°t ƒë·ªông c√πng nhau trong HippoRAG.

---
B·ªëi c·∫£nh v√≠ d·ª•
Gi·∫£ s·ª≠ truy v·∫•n l√†:
"T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?"
Sau khi tr√≠ch xu·∫•t th√¥ng tin t·ª´ truy v·∫•n v√† x√¢y d·ª±ng Knowledge Graph (KG), h·ªá th·ªëng x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c seed node v·ªõi c√°c gi√° tr·ªã similarity nh∆∞ sau:
- Phrase Node A: "t√°c gi·∫£"
  - Similarity score = 0.72
  - Weight factor (Phrase) = 1.0 ‚Üí Effective weight = 0.72
- Phrase Node B: "The Great Novel"
  - Similarity score = 0.68
  - Weight factor (Phrase) = 1.0 ‚Üí Effective weight = 0.68
- Passage Node C: ƒêo·∫°n vƒÉn 1 ch·ª©a th√¥ng tin:
 ‚ÄúThe Great Novel, m·ªôt t√°c ph·∫©m kinh ƒëi·ªÉn, ƒë∆∞·ª£c vi·∫øt b·ªüi John Doe ‚Äì t√°c gi·∫£ n·ªïi ti·∫øng‚Äù
  - Similarity score = 0.80
  - Weight factor (Passage) = 0.05 ‚Üí Effective weight = 0.80 √ó 0.05 = 0.04
- Passage Node D: ƒêo·∫°n vƒÉn 2 n√≥i chung v·ªÅ John Doe nh∆∞ng kh√¥ng nh·∫Øc tr·ª±c ti·∫øp ti√™u ƒë·ªÅ
  - Similarity score = 0.55
  - Weight factor (Passage) = 0.05 ‚Üí Effective weight = 0.55 √ó 0.05 = 0.0275

---
ƒê·ªëi tho·∫°i ph·∫£n bi·ªán ‚Äì C√°ch PPR ho·∫°t ƒë·ªông v·ªõi Seed Node Weights
Ph·∫£n bi·ªán 1: Kh·ªüi t·∫°o Seed Vector
Q: L√†m th·∫ø n√†o ta kh·ªüi t·∫°o vector seed cho PPR v·ªõi c√°c seed node tr√™n?
A:
- Ta t·∫°o m·ªôt vector ss ch·ª©a c√°c gi√° tr·ªã kh·ªüi t·∫°o cho c√°c node theo effective weight:
  - sA=0.72s_A = 0.72 (Phrase node A)
  - sB=0.68s_B = 0.68 (Phrase node B)
  - sC=0.04s_C = 0.04 (Passage node C)
  - sD=0.0275s_D = 0.0275 (Passage node D)
- Sau ƒë√≥, ta chu·∫©n h√≥a vector ss sao cho t·ªïng c√°c ph·∫ßn t·ª≠ b·∫±ng 1. V√≠ d·ª•:
  - T·ªïng S=0.72+0.68+0.04+0.0275=1.4675S = 0.72 + 0.68 + 0.04 + 0.0275 = 1.4675
  - Vector seed chu·∫©n h√≥a:
    - sA‚Ä≤=0.72/1.4675‚âà0.49s'_A = 0.72 / 1.4675 \approx 0.49
    - sB‚Ä≤=0.68/1.4675‚âà0.46s'_B = 0.68 / 1.4675 \approx 0.46
    - sC‚Ä≤=0.04/1.4675‚âà0.027s'_C = 0.04 / 1.4675 \approx 0.027
    - sD‚Ä≤=0.0275/1.4675‚âà0.019s'_D = 0.0275 / 1.4675 \approx 0.019
Nh∆∞ v·∫≠y, c√°c Phrase Nodes chi·∫øm ∆∞u th·∫ø ngay t·ª´ ƒë·∫ßu so v·ªõi c√°c Passage Nodes do effective weight c·ªßa ch√∫ng cao h∆°n.

---
Ph·∫£n bi·ªán 2: Qu√° tr√¨nh PPR ‚Äì Lan t·ªèa x√°c su·∫•t
Q: PPR s·∫Ω l√†m g√¨ v·ªõi vector seed ƒë√£ chu·∫©n h√≥a n√†y?
A:
- Thu·∫≠t to√°n PPR th·ª±c hi·ªán c·∫≠p nh·∫≠t x√°c su·∫•t theo c√¥ng th·ª©c:
- v(t+1)=(1‚àíd)‚ãÖs‚Ä≤+d‚ãÖT‚ãÖv(t)v^{(t+1)} = (1-d) \cdot s' + d \cdot T \cdot v^{(t)}
- trong ƒë√≥:
  - v(t)v^{(t)} l√† vector x√°c su·∫•t ·ªü v√≤ng l·∫∑p th·ª© tt
  - s‚Ä≤s' l√† vector seed (ƒë√£ chu·∫©n h√≥a)
  - TT l√† ma tr·∫≠n chuy·ªÉn ƒë·ªïi (transition matrix) c·ªßa KG, x√°c ƒë·ªãnh x√°c su·∫•t chuy·ªÉn t·ª´ node n√†y sang node kh√°c d·ª±a tr√™n c√°c c·∫°nh li√™n k·∫øt
  - dd l√† damping factor (th∆∞·ªùng kho·∫£ng 0.5‚Äì0.85)
- Qu√° tr√¨nh n√†y s·∫Ω ‚Äúlan t·ªèa‚Äù c√°c gi√° tr·ªã t·ª´ c√°c seed node ban ƒë·∫ßu theo c√°c li√™n k·∫øt trong ƒë·ªì th·ªã. C√°c Phrase Nodes v·ªõi gi√° tr·ªã cao ban ƒë·∫ßu (0.49 v√† 0.46) s·∫Ω ‚Äúƒë·∫©y‚Äù x√°c su·∫•t sang c√°c n√∫t l√¢n c·∫≠n. N·∫øu Passage Node C (v·ªõi gi√° tr·ªã ban ƒë·∫ßu 0.027) c√≥ c√°c c·∫°nh m·∫°nh li√™n k·∫øt v·ªõi Phrase Nodes, n√≥ c√≥ th·ªÉ nh·∫≠n th√™m x√°c su·∫•t t·ª´ ch√∫ng.

---
Ph·∫£n bi·ªán 3: V√≠ d·ª• s·ªë h·ªçc trong qu√° tr√¨nh PPR
Q: H√£y l·∫•y m·ªôt v√≤ng l·∫∑p PPR minh h·ªça c·ª• th·ªÉ cho v√≠ d·ª• n√†y.
A:
- Gi·∫£ s·ª≠ ma tr·∫≠n chuy·ªÉn ƒë·ªïi TT c·ªßa m·ªôt ph·∫ßn nh·ªè trong KG ƒë∆∞·ª£c ƒë∆°n gi·∫£n ho√° nh∆∞ sau (ch·ªâ x√©t 4 node tr√™n):
  - TABT_{AB} l√† x√°c su·∫•t t·ª´ node A sang B, TACT_{AC} t·ª´ A sang C, v.v‚Ä¶
- V√≠ d·ª•, ta gi·∫£ s·ª≠:
  - Node A (Phrase "t√°c gi·∫£") c√≥ li√™n k·∫øt:
    - 60% ƒë·∫øn Node B ("The Great Novel")
    - 40% ƒë·∫øn Node C (Passage ch·ª©a th√¥ng tin ch√≠nh)
  - Node B ("The Great Novel") li√™n k·∫øt:
    - 70% ƒë·∫øn Node A
    - 30% ƒë·∫øn Node D (Passage ph·ª•)
  - Node C (Passage 1) li√™n k·∫øt:
    - 100% ƒë·∫øn Node A (gi·∫£ s·ª≠ c√≥ m·ªëi li√™n h·ªá m·∫°nh v·ªõi kh√°i ni·ªám "t√°c gi·∫£")
  - Node D (Passage 2) li√™n k·∫øt:
    - 100% ƒë·∫øn Node B
- Ta c√≥ ma tr·∫≠n TT (ƒë∆°n gi·∫£n ho√°) nh∆∞ sau:
- T=[00.600.4000.70000.301.000001.000]T = \begin{bmatrix} 0   & 0.60 & 0.40 & 0 \\ 0.70 & 0   & 0    & 0.30 \\ 1.0  & 0   & 0    & 0 \\ 0   & 1.0 & 0    & 0  \end{bmatrix}
- Gi·∫£ s·ª≠ damping factor d=0.5d = 0.5. V·ªõi vector seed chu·∫©n h√≥a s‚Ä≤=[0.49,0.46,0.027,0.019]‚ä§s' = [0.49, 0.46, 0.027, 0.019]^\top, ta kh·ªüi t·∫°o v(0)=s‚Ä≤v^{(0)} = s'.
- V√≤ng l·∫∑p th·ª© 1:
 T√≠nh v(1)=0.5‚ãÖs‚Ä≤+0.5‚ãÖT‚ãÖv(0)v^{(1)} = 0.5 \cdot s' + 0.5 \cdot T \cdot v^{(0)}
- Tr∆∞·ªõc h·∫øt, t√≠nh T‚ãÖv(0)T \cdot v^{(0)}:
  - (T‚ãÖv(0))A=0‚àó0.49+0.60‚àó0.46+0.40‚àó0.027+0‚àó0.019‚âà0.60√ó0.46+0.40√ó0.027‚âà0.276+0.0108‚âà0.287(T \cdot v^{(0)})_A = 0*0.49 + 0.60*0.46 + 0.40*0.027 + 0*0.019 \approx 0.60 \times 0.46 + 0.40 \times 0.027 \approx 0.276 + 0.0108 \approx 0.287
  - (T‚ãÖv(0))B=0.70‚àó0.49+0‚àó0.46+0‚àó0.027+0.30‚àó0.019‚âà0.343+0.0057‚âà0.3487(T \cdot v^{(0)})_B = 0.70*0.49 + 0*0.46 + 0*0.027 + 0.30*0.019 \approx 0.343 + 0.0057 \approx 0.3487
  - (T‚ãÖv(0))C=1.0‚àó0.49+0‚àó0.46+0‚àó0.027+0‚àó0.019=0.49(T \cdot v^{(0)})_C = 1.0*0.49 + 0*0.46 + 0*0.027 + 0*0.019 = 0.49
  - (T‚ãÖv(0))D=0‚àó0.49+1.0‚àó0.46+0‚àó0.027+0‚àó0.019=0.46(T \cdot v^{(0)})_D = 0*0.49 + 1.0*0.46 + 0*0.027 + 0*0.019 = 0.46
- Sau ƒë√≥,
- v(1)=0.5‚ãÖ[0.49,0.46,0.027,0.019]‚ä§+0.5‚ãÖ[0.287,0.3487,0.49,0.46]‚ä§v^{(1)} = 0.5 \cdot [0.49, 0.46, 0.027, 0.019]^\top + 0.5 \cdot [0.287, 0.3487, 0.49, 0.46]^\top 
- =
- [0.245+0.1435,‚Äâ0.23+0.17435,‚Äâ0.0135+0.245,‚Äâ0.0095+0.23][0.245 + 0.1435,\, 0.23 + 0.17435,\, 0.0135 + 0.245,\, 0.0095 + 0.23]
- =
- [0.3885,‚Äâ0.40435,‚Äâ0.2585,‚Äâ0.2395][0.3885,\, 0.40435,\, 0.2585,\, 0.2395]
- Gi·∫£i th√≠ch:
 Nh√¨n v√†o v(1)v^{(1)}:
  - Node A v√† Node B (Phrase Nodes) c√≥ gi√° tr·ªã v·∫´n cao (0.3885 v√† 0.40435).
  - Node C (Passage 1) tƒÉng t·ª´ 0.027 l√™n 0.2585 nh·ªù nh·∫≠n ƒë∆∞·ª£c th√¥ng tin t·ª´ Node A (v√¨ c√≥ li√™n k·∫øt 100% t·ª´ Node C sang A) v√† t·ª´ m·ªëi li√™n k·∫øt c·ªßa A ƒë·∫øn C.
  - Node D (Passage 2) c√≥ gi√° tr·ªã 0.2395 nh·ªù li√™n k·∫øt v·ªõi Node B.
Qu√° tr√¨nh n√†y s·∫Ω l·∫∑p l·∫°i cho ƒë·∫øn khi h·ªôi t·ª•. K·∫øt qu·∫£ cu·ªëi c√πng s·∫Ω l√† vector x√°c su·∫•t ph√¢n b·ªï qua to√†n b·ªô KG, trong ƒë√≥ c√°c node c√≥ m·ªëi li√™n k·∫øt m·∫°nh v·ªõi seed nodes ban ƒë·∫ßu s·∫Ω nh·∫≠n ƒë∆∞·ª£c x√°c su·∫•t cao h∆°n.

---
Ph·∫£n bi·ªán 4: ·∫¢nh h∆∞·ªüng c·ªßa PPR ƒë·∫øn vi·ªác x·∫øp h·∫°ng c√°c Passage
Q: Sau khi vector x√°c su·∫•t ƒë∆∞·ª£c t√≠nh xong, l√†m sao c√°c gi√° tr·ªã n√†y chuy·ªÉn th√†nh vi·ªác x·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn tr·∫£ l·ªùi truy v·∫•n?
A:
- Sau khi c√≥ vector vv cho c√°c node trong KG, h·ªá th·ªëng s·∫Ω nh√¢n c√°c gi√° tr·ªã n√†y v·ªõi ma tr·∫≠n P (P matrix), m√† trong ƒë√≥ m·ªói ph·∫ßn t·ª≠ bi·ªÉu th·ªã s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªôt n√∫t (phrase ho·∫∑c passage) trong c√°c ƒëo·∫°n vƒÉn g·ªëc.
- T·ªïng h·ª£p gi√° tr·ªã t·ª´ c√°c node li√™n quan s·∫Ω cho ƒëi·ªÉm x·∫øp h·∫°ng cho t·ª´ng ƒëo·∫°n vƒÉn. C√°c ƒëo·∫°n vƒÉn c√≥ t·ªïng x√°c su·∫•t cao (ƒë·∫∑c bi·ªát l√† do nh·∫≠n ƒë∆∞·ª£c ‚Äúƒëi·ªÉm c·ªông‚Äù t·ª´ c√°c Phrase Nodes m·∫°nh v√† Passage Nodes c√≥ li√™n k·∫øt t·ªët) s·∫Ω ƒë∆∞·ª£c x·∫øp h·∫°ng ∆∞u ti√™n.
V√≠ d·ª•:
- Passage Node C, sau PPR, c√≥ gi√° tr·ªã tƒÉng l√™n ƒë√°ng k·ªÉ (v√≠ d·ª• 0.2585), khi nh√¢n v·ªõi s·ªë l·∫ßn xu·∫•t hi·ªán v√† c√°c li√™n k·∫øt li√™n quan, n√≥ c√≥ th·ªÉ ƒë∆∞·ª£c x·∫øp h·∫°ng cao v√† ch·ªçn l√†m c√¢u tr·∫£ l·ªùi cho truy v·∫•n "T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?".

---
T·ªïng k·∫øt
Qua v√≠ d·ª• chi ti·∫øt tr√™n, ta th·∫•y r·∫±ng:
1. Seed Node Weights ƒë∆∞·ª£c x√°c ƒë·ªãnh d·ª±a tr√™n similarity score v√† weight factor ri√™ng bi·ªát cho Phrase Nodes (1.0) v√† Passage Nodes (0.05).
2. Vector seed sau khi chu·∫©n h√≥a cung c·∫•p ƒëi·ªÉm kh·ªüi ƒë·∫ßu cho PPR.
3. PPR lan t·ªèa x√°c su·∫•t t·ª´ c√°c seed node theo c√°c li√™n k·∫øt trong KG, gi√∫p c√°c Passage Nodes nh·∫≠n ƒë∆∞·ª£c ‚Äúƒëi·ªÉm c·ªông‚Äù t·ª´ c√°c Phrase Nodes n·∫øu ch√∫ng c√≥ li√™n k·∫øt ch·∫∑t ch·∫Ω.
4. K·∫øt qu·∫£ cu·ªëi c√πng sau PPR v√† k·∫øt h·ª£p v·ªõi ma tr·∫≠n P s·∫Ω x·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi truy v·∫•n.
Qua ƒë√≥, ta th·∫•y r·∫±ng vi·ªác g√°n tr·ªçng s·ªë seed node v√† ·ª©ng d·ª•ng PPR kh√¥ng ch·ªâ gi√∫p c√¢n b·∫±ng gi·ªØa th√¥ng tin kh√°i ni·ªám v√† ng·ªØ c·∫£nh, m√† c√≤n t·∫°o ƒëi·ªÅu ki·ªán cho qu√° tr√¨nh multi-hop retrieval di·ªÖn ra hi·ªáu qu·∫£ v√† ch√≠nh x√°c.

---
[Image]

ƒê·∫∑t v·∫•n ƒë·ªÅ nh∆∞ n√†o, gi·∫£i quy·∫øt nh∆∞ n√†o? 
- B√†i to√°n suy lu·∫≠n ƒëa b∆∞·ªõc. Dataset lu√¥n? 
- https://www.kaggle.com/datasets/danghuy249/rag-dataset?select=data
-  However, its reliance  on vector retrieval hinders its ability to mimic  the dynamic and interconnected nature of hu man long-term memory. Recent RAG approaches  augment vector embeddings with various struc tures like knowledge graphs to address some of  these gaps, namely sense-making and associativ ity. However, their performance on more basic  factual memory tasks drops considerably below  standard RAG. 
- Tuy nhi√™n, vi·ªác ph·ª• thu·ªôc v√†o truy xu·∫•t vector h·∫°n ch·∫ø kh·∫£ nƒÉng c·ªßa n√≥ trong vi·ªác m√¥ ph·ªèng t√≠nh ch·∫•t ƒë·ªông v√† li√™n k·∫øt c·ªßa tr√≠ nh·ªõ d√†i h·∫°n ·ªü con ng∆∞·ªùi. C√°c ph∆∞∆°ng ph√°p RAG g·∫ßn ƒë√¢y ƒë√£ b·ªï sung c√°c c·∫•u tr√∫c kh√°c nhau nh∆∞ ƒë·ªì th·ªã tri th·ª©c v√†o c√°c vector nh√∫ng ƒë·ªÉ gi·∫£i quy·∫øt m·ªôt s·ªë thi·∫øu s√≥t n√†y, c·ª• th·ªÉ l√† kh·∫£ nƒÉng c·∫£m nh·∫≠n v√† li√™n k·∫øt. Tuy nhi√™n, hi·ªáu su·∫•t c·ªßa ch√∫ng trong c√°c nhi·ªám v·ª• tr√≠ nh·ªõ th·ª±c t·∫ø c∆° b·∫£n l·∫°i gi·∫£m ƒë√°ng k·ªÉ so v·ªõi RAG ti√™u chu·∫©n. => ƒê·ªÅ xu·∫•t HippoRAG. 
+, Long Term Memory? (l∆∞u d·∫°ng fact c·ªßa user) 
- Baseline ng√†y x∆∞a. RAG truy·ªÅn th·ªëng. Dataset ng√†y x∆∞a. 
- Reason LLms -> ƒêo·∫°n Retrieval Reasoning r·ªìi Hippo. 
HippoRAG kh√¥ng ch·ªù ƒë·∫øn khi LLM l√†m reasoning, m√† n√≥ ƒë√£ reasoning ngay t·ª´ b∆∞·ªõc truy h·ªìi.
V·ªõi Personalized PageRank ‚Üí n√≥ t√¨m ra node trung gian (John Smith) tr∆∞·ªõc khi v√†o LLM
Tr·∫£ v·ªÅ ƒëo·∫°n ƒë√£ ƒë·ªß logic ‚Üí LLM ch·ªâ c·∫ßn n√≥i l·∫°i
- xAI cho c√¢u h·ªèi b·∫±ng c√°ch x√¢y d·ª±ng KG. ƒê∆∞a KG v√†o h·ªçc? 
- Dataset ng√†y x∆∞a c·ªßa anh Huy, ... 
-  HotpotQA

============
 ForHippoRAG2,weusetheopen-sourceLlama-3.3-70B Instruct(AI@Meta,2024)asboththeextraction(NERand  OpenIE)andtriplefilteringmodel,andweusenvidia/NV Embed-v2astheretriever.


ƒê·ªÉ ƒë·∫∑t v·∫•n ƒë·ªÅ t·∫°i sao c·∫ßn HippoRAG, ta c√≥ th·ªÉ tr√¨nh b√†y nh∆∞ sau ‚Äì k·∫øt h·ª£p gi·ªØa l·∫≠p lu·∫≠n logic v√† c√°c ph√°t hi·ªán t·ª´ b√†i b√°o:

---
üß† T·∫°i sao c·∫ßn HippoRAG?
1. Gi·ªõi h·∫°n c·ªßa RAG truy·ªÅn th·ªëng
Retrieval-Augmented Generation (RAG) l√† m·ªôt gi·∫£i ph√°p ph·ªï bi·∫øn gi√∫p LLM ti·∫øp c·∫≠n th√¥ng tin m·ªõi m√† kh√¥ng c·∫ßn thay ƒë·ªïi tham s·ªë m√¥ h√¨nh. Tuy nhi√™n, RAG truy·ªÅn th·ªëng ch·ªß y·∫øu d·ª±a v√†o vector retrieval ƒë∆°n gi·∫£n, d·∫´n ƒë·∫øn 2 ƒëi·ªÉm y·∫øu l·ªõn:
- Thi·∫øu kh·∫£ nƒÉng sense-making: Kh√¥ng th·ªÉ hi·ªÉu, di·ªÖn gi·∫£i c√°c t√¨nh hu·ªëng d√†i v√† ph·ª©c t·∫°p m·ªôt c√°ch li·ªÅn m·∫°ch.
- Thi·∫øu t√≠nh li√™n k·∫øt (associativity): Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒëa b∆∞·ªõc gi·ªØa c√°c m·∫£nh ki·∫øn th·ª©c r·ªùi r·∫°c nh∆∞ b·ªô nh·ªõ d√†i h·∫°n c·ªßa con ng∆∞·ªùi.
üìå V√≠ d·ª•: Trong c√°c t√°c v·ª• truy v·∫•n ƒëa b∆∞·ªõc nh∆∞ "Ng∆∞·ªùi ph√°t minh ra thi·∫øt b·ªã X s·ªëng ·ªü ƒë√¢u?", vector retrieval truy·ªÅn th·ªëng th∆∞·ªùng th·∫•t b·∫°i v√¨ kh√¥ng th·ªÉ n·ªëi c√°c m·∫£nh th√¥ng tin l·∫°i (ph√°t minh X ‚Üí ng∆∞·ªùi A ‚Üí n∆°i s·ªëng c·ªßa A).

---
2. C√°c gi·∫£i ph√°p tr∆∞·ªõc ƒë√¢y v·∫´n ch∆∞a ƒë·ªß
C√°c h·ªá th·ªëng RAG c·∫•u tr√∫c m·ªõi nh∆∞ RAPTOR, GraphRAG, LightRAG ƒë√£ th·ª≠ kh·∫Øc ph·ª•c b·∫±ng c√°ch:
- T·∫°o t√≥m t·∫Øt ho·∫∑c knowledge graph ƒë·ªÉ b·ªï sung ng·ªØ c·∫£nh.
- Tuy nhi√™n, l·∫°i g·∫∑p ph·∫£i v·∫•n ƒë·ªÅ gi·∫£m hi·ªáu qu·∫£ trong c√°c t√°c v·ª• ghi nh·ªõ c∆° b·∫£n (factual QA) do nhi·ªÖu t·ª´ LLM ho·∫∑c c·∫•u tr√∫c ph·ª©c t·∫°p g√¢y qu√° t·∫£i.
‚ùå Kh√¥ng c√≥ m√¥ h√¨nh n√†o c√¢n b·∫±ng t·ªët c·∫£ 3 kh√≠a c·∫°nh: factual memory, sense-making v√† associativity.

---
3. HippoRAG ra ƒë·ªùi ƒë·ªÉ m√¥ ph·ªèng b·ªô nh·ªõ d√†i h·∫°n nh∆∞ ng∆∞·ªùi
HippoRAG (v√† b·∫£n c·∫£i ti·∫øn HippoRAG 2) ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n c·∫£m h·ª©ng t·ª´ c·∫•u tr√∫c b·ªô nh·ªõ sinh h·ªçc c·ªßa con ng∆∞·ªùi (neocortex, hippocampus, parahippocampal region), v·ªõi m·ª•c ti√™u:
- Ghi nh·ªõ b·ªÅn v·ªØng ki·∫øn th·ª©c m·ªõi m√† kh√¥ng l√†m qu√™n ki·∫øn th·ª©c c≈©.
- K·∫øt n·ªëi ƒë∆∞·ª£c nhi·ªÅu th√¥ng tin d·∫°ng bi·ªÉu ƒë·ªì, h·ªó tr·ª£ multi-hop reasoning.
- TƒÉng t√≠nh ng·ªØ c·∫£nh v√† t√≠nh li√™n k·∫øt, nh·ªù t√≠ch h·ª£p PageRank c√° nh√¢n h√≥a v√† bi·ªÉu ƒë·ªì tri th·ª©c m·ªü (open KG).

---
4. L·ª£i √≠ch c·ªßa HippoRAG
Theo k·∫øt qu·∫£ th·ª±c nghi·ªám:
- HippoRAG 2 v∆∞·ª£t tr·ªôi 7% trong c√°c b√†i to√°n truy v·∫•n li√™n k·∫øt (associative tasks).
- Gi·ªØ ƒë∆∞·ª£c ho·∫∑c c·∫£i thi·ªán nh·∫π ·ªü c√°c t√°c v·ª• ghi nh·ªõ ƒë∆°n (factual) v√† hi·ªÉu ng·ªØ c·∫£nh (sense-making).
- Kh·∫£ nƒÉng h·ªçc li√™n t·ª•c phi tham s·ªë (non-parametric continual learning) ‚Äì kh√¥ng c·∫ßn fine-tune LLM li√™n t·ª•c.

---
N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ vi·∫øt ph·∫ßn n√†y th√†nh m·ªôt ƒëo·∫°n motivation ng·∫Øn, chu·∫©n phong c√°ch b√†i nghi√™n c·ª©u ho·∫∑c b√†i thuy·∫øt tr√¨nh h·ªçc thu·∫≠t. Qu·ªëc mu·ªën phong c√°ch n√†o: ng·∫Øn g·ªçn ‚Äì g√£y g·ªçn ‚Äì tr√¨nh b√†y v·∫•n ƒë·ªÅ? Hay s√¢u s·∫Øc ‚Äì h·ªçc thu·∫≠t ‚Äì ƒë∆∞a d·∫´n ch·ª©ng nghi√™n c·ª©u?


D∆∞·ªõi ƒë√¢y l√† b·∫£n "ƒê·∫∑t v·∫•n ƒë·ªÅ v√† Gi·∫£i ph√°p" cho HippoRAG, ƒë∆∞·ª£c vi·∫øt theo phong c√°ch h·ªçc thu·∫≠t, s√∫c t√≠ch v√† logic ‚Äì ph√π h·ª£p ƒë·ªÉ ƒë∆∞a v√†o gi·ªõi thi·ªáu m·ªôt b√†i nghi√™n c·ª©u, thuy·∫øt tr√¨nh chuy√™n ƒë·ªÅ ho·∫∑c ƒë·ªì √°n t·ªët nghi·ªáp.

---
üß© ƒê·∫∑t v·∫•n ƒë·ªÅ v√† Gi·∫£i ph√°p
üîç ƒê·∫∑t v·∫•n ƒë·ªÅ
Kh·∫£ nƒÉng ghi nh·ªõ, li√™n k·∫øt v√† s·ª≠ d·ª•ng ki·∫øn th·ª©c l√¢u d√†i l√† m·ªôt ƒë·∫∑c tr∆∞ng c·ªët l√µi c·ªßa tr√≠ th√¥ng minh con ng∆∞·ªùi. Tuy nhi√™n, c√°c h·ªá th·ªëng AI hi·ªán nay ‚Äì ƒë·∫∑c bi·ªát l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs) ‚Äì ch∆∞a th·ªÉ hi·ªán ƒë∆∞·ª£c kh·∫£ nƒÉng h·ªçc li√™n t·ª•c v√† ghi nh·ªõ l√¢u d√†i m·ªôt c√°ch hi·ªáu qu·∫£.
M·∫∑c d√π Retrieval-Augmented Generation (RAG) ƒëang l√† gi·∫£i ph√°p ch√≠nh cho vi·ªác ti·∫øp c·∫≠n th√¥ng tin ngo√†i m√¥ h√¨nh (non-parametric learning), RAG truy·ªÅn th·ªëng v·∫´n c√≤n nhi·ªÅu h·∫°n ch·∫ø:
- Ph·ª• thu·ªôc ho√†n to√†n v√†o truy h·ªìi vector (vector retrieval), d·∫´n ƒë·∫øn:
  - Kh√¥ng hi·ªÉu s√¢u ng·ªØ c·∫£nh d√†i ho·∫∑c ph·ª©c t·∫°p (thi·∫øu sense-making).
  - Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c th√¥ng tin r·ªùi r·∫°c qua nhi·ªÅu b∆∞·ªõc (thi·∫øu associativity).
- C√°c ph∆∞∆°ng ph√°p c·∫£i ti·∫øn nh∆∞ RAPTOR hay GraphRAG d√πng LLM ƒë·ªÉ t·∫°o t√≥m t·∫Øt ho·∫∑c graph, nh∆∞ng l·∫°i g√¢y suy gi·∫£m hi·ªáu nƒÉng ·ªü c√°c b√†i to√°n factual QA, do nhi·ªÖu t·ª´ LLM ho·∫∑c qu√° ph·ª©c t·∫°p trong c·∫•u tr√∫c.
T·ª´ ƒë√≥ ƒë·∫∑t ra c√¢u h·ªèi:
L√†m th·∫ø n√†o ƒë·ªÉ x√¢y d·ª±ng m·ªôt h·ªá th·ªëng ghi nh·ªõ l√¢u d√†i cho LLMs v·ª´a m·∫°nh v·ªÅ factual, v·ª´a s√¢u s·∫Øc v·ªÅ ng·ªØ c·∫£nh, v√† linh ho·∫°t trong li√™n k·∫øt ki·∫øn th·ª©c?

---
üí° Gi·∫£i ph√°p: HippoRAG
HippoRAG ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t nh∆∞ m·ªôt khung ghi nh·ªõ l·∫•y c·∫£m h·ª©ng t·ª´ c·∫•u tr√∫c b·ªô nh·ªõ c·ªßa con ng∆∞·ªùi (g·ªìm t√¢n v·ªè n√£o, hippocampus, v√† v√πng trung gian). Gi·∫£i ph√°p n√†y bao g·ªìm hai giai ƒëo·∫°n:
1. Ch·ªâ m·ª•c h√≥a ngo·∫°i tuy·∫øn (Offline Indexing):
  - D√πng LLM ƒë·ªÉ tr√≠ch xu·∫•t c√°c triple ki·∫øn th·ª©c d·∫°ng KG m·ªü t·ª´ t√†i li·ªáu.
  - T·∫°o bi·ªÉu ƒë·ªì tri th·ª©c g·ªìm c√°c node kh√°i ni·ªám (phrase) v√† node ng·ªØ c·∫£nh (passage).
  - Ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a v√† li√™n k·∫øt c√°c kh√°i ni·ªám t∆∞∆°ng ƒë∆∞∆°ng b·∫±ng embedding.
2. Truy h·ªìi tr·ª±c tuy·∫øn (Online Retrieval):
  - D√πng m√¥ h√¨nh embedding ƒë·ªÉ li√™n k·∫øt c√¢u h·ªèi v·ªõi triple v√† passage.
  - √Åp d·ª•ng thu·∫≠t to√°n Personalized PageRank ƒë·ªÉ t√¨m ƒë∆∞·ªùng d·∫´n li√™n k·∫øt ng·ªØ nghƒ©a v√† truy xu·∫•t c√°c ƒëo·∫°n vƒÉn li√™n quan.
  - D√πng LLM ƒë·ªÉ l·ªçc triple theo ng·ªØ c·∫£nh truy v·∫•n (b·ªô nh·ªõ nh·∫≠n di·ªán ‚Äì recognition memory).
üéØ K·∫øt qu·∫£: HippoRAG 2 v∆∞·ª£t tr·ªôi h∆°n t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p hi·ªán t·∫°i tr√™n c·∫£ ba nƒÉng l·ª±c c·ªët l√µi: ghi nh·ªõ fact, hi·ªÉu ng·ªØ c·∫£nh v√† li√™n k·∫øt th√¥ng tin.

---
N·∫øu Qu·ªëc mu·ªën m√¨nh vi·∫øt l·∫°i b·∫£n n√†y theo b·∫£n slide tr√¨nh b√†y, d·∫°ng b√†i vi·∫øt b√°o c√°o khoa h·ªçc, ho·∫∑c b·∫£n n√≥i thuy·∫øt tr√¨nh mi·ªáng 2-3 ph√∫t, m√¨nh s·∫Ω chuy·ªÉn th·ªÉ ph√π h·ª£p nh√©. Qu·ªëc mu·ªën theo h∆∞·ªõng n√†o ti·∫øp?


ƒê·ªÉ m√¨nh gi·∫£i th√≠ch chi ti·∫øt s∆° ƒë·ªì HippoRAG 2 n√†y m·ªôt c√°ch d·ªÖ hi·ªÉu, chia l√†m 2 giai ƒëo·∫°n: Offline Indexing v√† Online Retrieval & QA, v·ªõi c√°c b∆∞·ªõc r√µ r√†ng nh√©!

---
üß† 1. Offline Indexing ‚Äì Giai ƒëo·∫°n t·∫°o b·ªô nh·ªõ (kh·ªüi t·∫°o bi·ªÉu ƒë·ªì tri th·ª©c)
üéØ M·ª•c ti√™u: Bi·∫øn c√°c ƒëo·∫°n vƒÉn (passages) th√†nh m·ªôt bi·ªÉu ƒë·ªì tri th·ª©c (Knowledge Graph ‚Äì KG) ƒë·ªÉ sau n√†y d·ªÖ truy h·ªìi th√¥ng minh h∆°n.
G·ªìm 3 b∆∞·ªõc:
üîµ (1) OpenIE by LLM
 ‚Äì D√πng LLM (Llama-3.3-70B) ƒë·ªÉ tr√≠ch xu·∫•t c√°c triple ki·∫øn th·ª©c t·ª´ m·ªói ƒëo·∫°n vƒÉn, d·∫°ng:
(subject ‚Äì relation ‚Äì object)
 üëâ V√≠ d·ª•: ("Elon Musk", "founder of", "SpaceX")
üîµ (2) Synonym detection by embedding
 ‚Äì D√πng m√¥ h√¨nh embedding ƒë·ªÉ t√¨m c√°c c·∫∑p t·ª´/c·ª•m t·ª´ gi·ªëng nhau v·ªÅ nghƒ©a, r·ªìi n·ªëi ch√∫ng l·∫°i b·∫±ng synonym edge.
 üëâ V√≠ d·ª•: ‚ÄúNYC‚Äù ‚Üî ‚ÄúNew York City‚Äù
üîµ (3) Dense-sparse integration
 ‚Äì T·∫°o 2 lo·∫°i node trong bi·ªÉu ƒë·ªì:
- Phrase node: c√°c kh√°i ni·ªám r√∫t g·ªçn, nh∆∞ ‚ÄúElon Musk‚Äù ho·∫∑c ‚ÄúCEO‚Äù
- Passage node: ch·ª©a to√†n b·ªô ƒëo·∫°n vƒÉn ‚Äì N·ªëi passage node v·ªõi c√°c phrase node t∆∞∆°ng ·ª©ng b·∫±ng context edge, ƒë·ªÉ bi·ªÉu ƒë·ªì v·ª´a c√≥ th√¥ng tin kh√°i ni·ªám, v·ª´a c√≥ ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß.
üì¶ Output cu·ªëi l√† m·ªôt Knowledge Graph gi√†u ng·ªØ nghƒ©a, v·ªõi c√°c lo·∫°i li√™n k·∫øt:
- Relation Edge (gi·ªØa concept v·ªõi concept)
- Synonym Edge
- Context Edge (gi·ªØa passage v√† concept)

---
üîç 2. Online Retrieval & QA ‚Äì Giai ƒëo·∫°n truy h·ªìi v√† tr·∫£ l·ªùi
üéØ M·ª•c ti√™u: T·ª´ c√¢u h·ªèi (query), t√¨m ƒë√∫ng c√°c ƒëo·∫°n vƒÉn li√™n quan ƒë·ªÉ tr·∫£ l·ªùi.
G·ªìm 5 b∆∞·ªõc:
üü¢ (1) Truy h·ªìi passages v√† triples
 ‚Äì D√πng retriever (NV-Embed-v2) ƒë·ªÉ truy h·ªìi c√°c triple v√† ƒëo·∫°n vƒÉn li√™n quan ƒë·∫øn c√¢u h·ªèi.
üü¢ (2) Recognition Memory ‚Äì L·ªçc triple
 ‚Äì D√πng LLM ƒë·ªÉ ƒë√°nh gi√° c√°c triple, gi·ªØ l·∫°i c√°c triple th·∫≠t s·ª± li√™n quan ƒë·∫øn c√¢u h·ªèi.
üü¢ (3) G√°n tr·ªçng s·ªë cho c√°c seed node
 ‚Äì C√°c triple v√† passage ƒë∆∞·ª£c ch·ªçn s·∫Ω tr·ªü th√†nh c√°c ‚Äúseed node‚Äù (node kh·ªüi ƒë·∫ßu). ‚Äì G√°n tr·ªçng s·ªë kh·ªüi ƒë·ªông (reset probability) cho t·ª´ng seed d·ª±a tr√™n m·ª©c ƒë·ªô li√™n quan.
üü¢ (4) Personalized PageRank (PPR)
 ‚Äì Ch·∫°y thu·∫≠t to√°n PPR tr√™n bi·ªÉu ƒë·ªì tri th·ª©c, ƒë·ªÉ t√¨m c√°c ƒëo·∫°n vƒÉn c√≥ k·∫øt n·ªëi ng·ªØ nghƒ©a m·∫°nh nh·∫•t ƒë·∫øn c√¢u h·ªèi, qua nhi·ªÅu b∆∞·ªõc reasoning.
üü¢ (5) QA Reading
 ‚Äì D√πng LLM ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë∆∞·ª£c ch·ªçn v√† t·∫°o c√¢u tr·∫£ l·ªùi cho truy v·∫•n.

---
üó∫Ô∏è Gi·∫£i th√≠ch c√°c k√Ω hi·ªáu trong s∆° ƒë·ªì
This content is only supported in a Lark Docs

---
N·∫øu Qu·ªëc mu·ªën, m√¨nh c√≥ th·ªÉ di·ªÖn gi·∫£i l·∫°i to√†n b·ªô th√†nh slide ho·∫∑c vi·∫øt b√°o c√°o t√≥m t·∫Øt ph·∫ßn ki·∫øn tr√∫c pipeline HippoRAG 2 theo h√¨nh n√†y nh√©. Qu·ªëc c·∫ßn b·∫£n n√†o?

---
1. MOTIVATION R√ï R√ÄNG => M·ªöI TRI·ªÇN?  
2. DATASET? C√ÅCH H·ªå ƒê√ÅNH GI√Å NH∆Ø N√ÄO?
3. METHOD? K·ª∏ THU·∫¨T ƒê√ì L√Ä G√å? ? K·ª∏ THU·∫¨T ƒê√ì NH∆Ø N√ÄO? => ƒê·ªäNH NGHƒ®A ƒê∆Ø·ª¢C C√ÅC KH√ÅI NI·ªÜM => M·ªöI C√ì K·ª∏ THU·∫¨T.  

---
Prompt 1: T√¥i mu·ªën b·∫°n ph·∫£i hi·ªÉu th·∫≠t s√¢u. m·ªói ph·∫ßn ƒë·∫∑t c√°c c√¢u h·ªèi ph·∫£n bi·ªán s·∫Øc x·∫£o ƒë·ªÉ t·ª± tr·∫£ l·ªùi. Gi·ªëng nh∆∞ 1 ng∆∞·ªùi ph√°, 1 ng∆∞·ªùi tr·∫£ l·ªùi, l·∫•y d·∫´n ch·ª©ng
Prompt 2: Gi·∫£i t√≠ch t·ª´ng kh√°i ni·ªám v√† li√™n h·ªá n√≥ v·ªõi th·ª© th·∫≠t d·ªÖ nh·ªõ ƒë∆°n gi·∫£n cho h·ªçc sinh c·∫•p 2

üß† PH·∫¶N 1: V·∫•n ƒë·ªÅ c·ªßa RAG hi·ªán t·∫°i & S·ª± xu·∫•t hi·ªán c·ªßa HippoRAG
üîç Gi·∫£i th√≠ch l·∫°i:
RAG hi·ªán t·∫°i ho·∫°t ƒë·ªông theo c∆° ch·∫ø:
- T√°ch vƒÉn b·∫£n th√†nh c√°c chunk nh·ªè
- D√πng embedding ‚Üí l∆∞u v√†o VectorDB
- Khi c√≥ query ‚Üí t√¨m vector t∆∞∆°ng ƒë·ªìng ‚Üí l·∫•y context ‚Üí ƒë∆∞a v√†o LLM
Nh∆∞ng v·ªõi c√°c t√°c v·ª• multi-hop reasoning, nhi·ªÅu th√¥ng tin ·ªü r·∫£i r√°c c√°c ƒëo·∫°n kh√°c nhau th√¨ RAG hi·ªán t·∫°i kh√¥ng ƒë·ªß hi·ªáu qu·∫£.
Ph√° 1.1 T·∫°i sao n√≥i v·ªõi c√°c t√°c v·ª• multi-hop reasoning, nhi·ªÅu th√¥ng tin ·ªü r·∫£i r√°c c√°c ƒëo·∫°n kh√°c nhau th√¨ RAG hi·ªán t·∫°i kh√¥ng ƒë·ªß hi·ªáu qu·∫£? 
Gi·∫£ s·ª≠ RAG truy·ªÅn th·ªëng retrieve ƒë∆∞·ª£c c·∫£ hai ƒëo·∫°n r·ªùi r·∫°c, th√¨ n√≥ v·∫´n n√©m h·∫øt v√†o context window c·ªßa LLM m√†? L√∫c ƒë√≥ LLM n·ªëi l·∫°i logic l√† xong ch·ª©?

Ph√° 1: RAG truy·ªÅn th·ªëng, trong l√∫c query ra c√°c ƒëo·∫°n chunking th√¨ n√≥ ch·∫≥ng nh·∫ª kh√¥ng c√≥ t√≠nh m√≥c n·ªëi?
‚ÄúGi·∫£ s·ª≠ RAG truy·ªÅn th·ªëng retrieve ƒë∆∞·ª£c c·∫£ hai ƒëo·∫°n r·ªùi r·∫°c, th√¨ n√≥ v·∫´n n√©m h·∫øt v√†o context window c·ªßa LLM m√†? L√∫c ƒë√≥ LLM n·ªëi l·∫°i logic l√† xong ch·ª©, c·∫ßn g√¨ ph·∫£i HippoRAG r∆∞·ªùm r√†?‚Äù 
üéØ 1. Kh√¥ng c√≥ g√¨ ƒë·∫£m b·∫£o RAG s·∫Ω retrieve ƒë√∫ng 2 ƒëo·∫°n
V·∫•n ƒë·ªÅ l·ªõn nh·∫•t c·ªßa RAG:
- Retrieval = vector similarity ‚Üí ch·ªâ ch·ªçn Top-K
- N·∫øu th√¥ng tin n·∫±m ·ªü 2 ƒëo·∫°n kh√°c nhau, v√† m·ªói c√°i ƒë·ªÅu ‚Äúh∆°i li√™n quan‚Äù, n√≥ c√≥ th·ªÉ b·ªã ƒë·∫©y ra kh·ªèi Top-K
V√≠ d·ª• c·ª• th·ªÉ:
Top-K = 3
 B·∫°n c·∫ßn ƒëo·∫°n A v√† ƒëo·∫°n B
 Nh∆∞ng c√≥ ƒëo·∫°n C, D r·∫•t gi·ªëng c√¢u h·ªèi v·ªÅ m·∫∑t ng·ªØ nghƒ©a ‚Üí l√™n Top
 A ho·∫∑c B b·ªã lo·∫°i v√¨ vector kh√¥ng ƒë·ªß g·∫ßn
üí• ‚Üí L√∫c n√†y LLM kh√¥ng c√≥ nguy√™n li·ªáu ƒë·ªÉ suy lu·∫≠n, d√π n√≥ gi·ªèi c·ª° n√†o. 

TƒÉng k. 
D·ªØ li·ªáu n·∫±m r·∫£i r√°c t·∫°i nhi·ªÅu n∆°i. 

üß† 2. LLM suy lu·∫≠n = t·ªën compute + d·ªÖ l·ªói logic
C√¢u h·ªèi:
"Which Stanford researcher who is not Alice but studies Alzheimer's worked at the same place as Thomas?"
Dataset:
902. Alice is a Stanford professor.
903. Thomas researches Alzheimer‚Äôs at UCSF.
904. Mike works at UCSF.
905. Sarah studies Alzheimer‚Äôs.
906. Sarah and Thomas are labmates at UCSF.

‚ö†Ô∏è V·∫•n ƒë·ªÅ c·ªßa RAG khi retrieve Top-K
Gi·∫£ s·ª≠ retrieve ƒë∆∞·ª£c 3 ƒëo·∫°n:
- (2) Thomas ‚Üí Alzheimer + UCSF
- (4) Sarah ‚Üí Alzheimer
- (5) Sarah ‚Üí labmate of Thomas ‚Üí UCSF
LLM ph·∫£i l√†m:
1267. G·∫Øn Sarah v·ªõi UCSF (t·ª´ labmate)
1268. Tr·ª´ Alice kh·ªèi k·∫øt qu·∫£
1269. Ch·ªçn ng∆∞·ªùi v·ª´a Stanford researcher, v·ª´a Alzheimer, v·ª´a ·ªü UCSF, nh∆∞ng kh√¥ng ph·∫£i Alice

---
‚ú≥Ô∏è ƒê√¢y l√† 3-hop reasoning. Kh√¥ng ph·∫£i v√¨ LLM kh√¥ng th·ªÉ, m√† v√¨:
- N√≥ ph·∫£i gi·ªØ ·ªïn ƒë·ªãnh attention gi·ªØa 4-5 ƒëo·∫°n vƒÉn
- N√≥ kh√¥ng bi·∫øt ch·∫Øc m·ªëi li√™n k·∫øt l√† quan tr·ªçng hay ch·ªâ l√† ‚Äúnhi·ªÖu‚Äù
- Kh√¥ng c√≥ logic structure r√µ r√†ng ƒë·ªÉ l·∫ßn t·ª´ A ‚Üí B ‚Üí C
‚û°Ô∏è Khi n√†y, retrieval d·∫°ng vector (d√π chunk t·ªët) kh√¥ng cung c·∫•p structure reasoning.
 Ch·ªâ n√©m ƒëo·∫°n v√†o context ‚Üí LLM ph·∫£i lo c·∫£ ƒë·ªçc, t√¨m logic, t·ªïng h·ª£p, tr√°nh sai l·ªách.

C√¢u h·ªèi ti·∫øp ƒë·∫∑t ra l√†: ‚ùó‚ÄúB√¢y gi·ªù c√≥ model 4o, Claude Opus, GPT-4-Turbo‚Ä¶ n√≥ c√≥ th·ªÉ context window 128k tokens, b·∫°n n√©m s·∫°ch v√¥ l√† n√≥ l√†m reasoning c·ª±c t·ªët. Th·∫ø th√¨ l√†m retrieval ƒë·ªÉ l√†m g√¨ n·ªØa? HippoRAG c√≥ c√≤n gi√° tr·ªã kh√¥ng?‚Äù
Tr·∫£ l·ªùi: ƒê√∫ng! Nh∆∞ng ch·ªâ khi b·∫°n s·ªëng trong th·∫ø gi·ªõi l√Ω t∆∞·ªüng.

‚ú≥Ô∏è PH·∫¶N I: Th·ª´a nh·∫≠n s·ª± th·∫≠t
‚úÖ ƒê√∫ng, model 4o, Claude Opus, GPT-4 Turbo hi·ªán nay ƒë√£ c√≥ kh·∫£ nƒÉng context-wide reasoning r·∫•t m·∫°nh.
üìå B·∫°n c√≥ th·ªÉ ƒë∆∞a v√†o:
- M·ªôt dataset 80k tokens g·ªìm nhi·ªÅu t√†i li·ªáu
- M·ªôt c√¢u h·ªèi c·∫ßn n·ªëi 3‚Äì4 th√¥ng tin l·∫°i
- V√† model s·∫Ω tr·∫£ l·ªùi ƒë√∫ng ‚Äì th·∫≠m ch√≠ c√≤n gi·∫£i th√≠ch t·∫°i sao.

---
‚ú≥Ô∏è PH·∫¶N II: Nh∆∞ng ƒë√≥ kh√¥ng ph·∫£i l√† c√°ch l√†m AI hi·ªáu qu·∫£ ho·∫∑c quy m√¥
‚ùó L√Ω do 1: Context-wide LLM reasoning = t·ªën compute kh·ªßng khi·∫øp
N√©m v√†o 128k tokens = gi√° infer cao, latency cao
- Claude Opus ho·∫∑c GPT-4 Turbo 128k input ‚Üí th·ªùi gian tr·∫£ l·ªùi l√¢u h∆°n, chi ph√≠ t√≠nh theo token c·ª±c l·ªõn (v√†i $ m·ªói l∆∞·ª£t n·∫øu full)
- B·∫°n kh√¥ng th·ªÉ scale 1 tri·ªáu c√¢u h·ªèi ki·ªÉu ƒë√≥

---
‚ùó L√Ω do 2: LLM = black box reasoning, kh√¥ng th·ªÉ debug, kh√¥ng th·ªÉ ki·ªÉm so√°t
- LLM kh√¥ng cho b·∫°n bi·∫øt reasoning path
- B·∫°n kh√¥ng bi·∫øt model n·ªëi A ‚Üí B ‚Üí C nh∆∞ th·∫ø n√†o
- N·∫øu n√≥ tr·∫£ sai? B·∫°n kh√¥ng bi·∫øt s·ª≠a t·ª´ ƒë√¢u
‚û°Ô∏è Retrieval reasoning = transparent, modular, debuggable
‚Üí HippoRAG = ta l·∫ßn ƒë∆∞·ª£c ƒë∆∞·ªùng ƒëi qua graph
 ‚Üí C√≥ th·ªÉ ghi log reasoning path
 ‚Üí C√≥ th·ªÉ gi·∫£i th√≠ch cho ng∆∞·ªùi d√πng ho·∫∑c ki·ªÉm tra b·∫±ng rule engine

---
‚ùó L√Ω do 3: Kh√¥ng ph·∫£i l√∫c n√†o c≈©ng c√≥ quy·ªÅn ‚Äún√©m h·∫øt v√†o‚Äù
- N·∫øu user h·ªèi li√™n t·ª•c, m·ªói c√¢u ƒë·ªÅu c·∫ßn to√†n b·ªô t√†i li·ªáu ‚Üí c·ª±c k·ª≥ l√£ng ph√≠
- V√≠ d·ª•: app legal, ho·∫∑c chatbot h·ªçc thu·∫≠t, ho·∫∑c QA y khoa
B·∫°n c·∫ßn selective retrieval ‚Üí RAG gi·∫£i quy·∫øt
 B·∫°n c·∫ßn reasoning t·ª´ c√°c concept, entity ‚Üí kh√¥ng c√≥ s·∫µn text chunk ‚Üí HippoRAG gi·∫£i quy·∫øt

üß† 3. HippoRAG cho ph√©p retrieval ch√≠nh l√† reasoning
ƒê√¢y l√† ƒëi·ªÉm ‚Äúƒë·∫≠p n√°t ph·∫£n bi·ªán‚Äù:
üß† HippoRAG kh√¥ng ch·ªù ƒë·∫øn khi LLM l√†m reasoning, m√† n√≥ ƒë√£ reasoning ngay t·ª´ b∆∞·ªõc truy h·ªìi.
- V·ªõi Personalized PageRank ‚Üí n√≥ t√¨m ra node trung gian (John Smith) tr∆∞·ªõc khi v√†o LLM
- Tr·∫£ v·ªÅ ƒëo·∫°n ƒë√£ ƒë·ªß logic ‚Üí LLM ch·ªâ c·∫ßn n√≥i l·∫°i
üí° ƒêi·ªÅu n√†y bi·∫øn retrieval th√†nh m·ªôt ‚Äúm√¥-ƒëun l·∫≠p lu·∫≠n‚Äù (reasoning module), kh√¥ng ch·ªâ l√† ‚Äúscanner ƒëo·∫°n vƒÉn‚Äù.

Ph√° 1.2 ƒê·∫∑t v·∫•n ƒë·ªÅ? 
Tuy·ªát v·ªùi, Qu·ªëc ƒëang d√πng k·ªπ thu·∫≠t ph·∫£n bi·ªán c·ª±c k·ª≥ hay ƒë·ªÉ ‚Äúƒë·ªçc s√¢u‚Äù ‚Äî ki·ªÉu Socratic method trong nghi√™n c·ª©u. M√¨nh s·∫Ω ƒë√≥ng c·∫£ ng∆∞·ªùi h·ªèi s·∫Øc s·∫£o v√† ng∆∞·ªùi tr·∫£ l·ªùi ch·∫∑t ch·∫Ω, b√°m s√°t ph·∫ßn ‚Äú1. Introduction‚Äù c·ªßa b√†i b√°o HippoRAG 2.

---
üß© PH√ÇN T√çCH PH·∫¶N ƒê·∫∂T V·∫§N ƒê·ªÄ (INTRODUCTION - HippoRAG 2)

---
‚öîÔ∏è Ph·∫£n bi·ªán 1:
H·ªèi: T·∫°i sao c·ª© ph·∫£i b·∫Øt AI h·ªçc nh∆∞ con ng∆∞·ªùi? Vi·ªác m√¥ ph·ªèng ‚Äúlong-term memory‚Äù c·ªßa ng∆∞·ªùi c√≥ th·∫≠t s·ª± c·∫ßn thi·∫øt cho LLM kh√¥ng?
Tr·∫£ l·ªùi: V√¨ m·ª•c ti√™u cu·ªëi c√πng c·ªßa LLM l√† tr·ªü th√†nh tr·ª£ l√Ω th√¥ng minh v√† hi·ªáu qu·∫£ nh∆∞ con ng∆∞·ªùi. Trong th·∫ø gi·ªõi th·ª±c, con ng∆∞·ªùi li√™n t·ª•c h·ªçc, t√≠ch h·ª£p th√¥ng tin m·ªõi m√† kh√¥ng qu√™n c√°i c≈© ‚Äì ƒë√¢y l√† ƒëi·ªÅu LLM hi·ªán t·∫°i r·∫•t y·∫øu.
B·∫±ng ch·ª©ng:
- LLM g·∫∑p v·∫•n ƒë·ªÅ catastrophic forgetting n·∫øu fine-tune.
- RAG tuy ti·∫øp c·∫≠n t·ªët h∆°n nh∆∞ng v·∫´n kh√¥ng c√≥ kh·∫£ nƒÉng k·∫øt n·ªëi m·∫°ch l·∫°c, nh∆∞ ng∆∞·ªùi nh·ªõ chuy·ªán ƒë√£ x·∫£y ra t·ª´ l√¢u v√† li√™n k·∫øt n√≥ v·ªõi b·ªëi c·∫£nh hi·ªán t·∫°i.
V√¨ th·∫ø, mu·ªën LLM tr·ªü th√†nh tr·ª£ l√Ω l√¢u d√†i, c·∫≠p nh·∫≠t li√™n t·ª•c, n√≥ c·∫ßn m·ªôt h·ªá th·ªëng b·ªô nh·ªõ d√†i h·∫°n linh ho·∫°t v√† li√™n k·∫øt ‚Äì gi·ªëng nh∆∞ c·ªßa ng∆∞·ªùi.

---
‚öîÔ∏è Ph·∫£n bi·ªán 2:
H·ªèi: RAG ƒëang ch·∫°y t·ªët m√†. T·∫°i sao l·∫°i n√≥i RAG truy·ªÅn th·ªëng kh√¥ng ƒë·ªß? C√≥ b·∫±ng ch·ª©ng g√¨?
Tr·∫£ l·ªùi: RAG truy·ªÅn th·ªëng ch·ªß y·∫øu d·ª±a v√†o vector retrieval ƒë·ªôc l·∫≠p, m·ªói chunk ƒë∆∞·ª£c x·∫øp h·∫°ng d·ª±a v√†o ƒë·ªô t∆∞∆°ng t·ª± embedding v·ªõi query ‚Äì kh√¥ng c√≥ c∆° ch·∫ø ƒë·ªÉ hi·ªÉu m·ªëi quan h·ªá gi·ªØa c√°c chunk, kh√¥ng hi·ªÉu m·∫°ch truy·ªán, kh√¥ng l√†m multi-hop reasoning.
B·∫±ng ch·ª©ng c·ª• th·ªÉ trong b√†i:
‚Äú...inability to capture two vital aspects: sense-making (hi·ªÉu ng·ªØ c·∫£nh ph·ª©c t·∫°p) v√† associativity (k·∫øt n·ªëi ƒëa b∆∞·ªõc gi·ªØa th√¥ng tin r·ªùi r·∫°c)‚Äù ‚Äì (trang 1)
V√¨ v·∫≠y, tuy RAG t·ªët trong vi·ªác ‚Äút√¨m th√¥ng tin g·∫ßn nh·∫•t‚Äù, nh∆∞ng:
- Kh√¥ng li√™n k·∫øt c√°c ƒëo·∫°n ki·∫øn th·ª©c l·∫°i v·ªõi nhau.
- Kh√¥ng bi·∫øt ‚Äúƒëi v√≤ng‚Äù ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi y√™u c·∫ßu reasoning.
=> ƒê√≥ l√† l√Ω do c·∫ßn m·ªôt m√¥ h√¨nh v∆∞·ª£t RAG truy·ªÅn th·ªëng.

---
‚öîÔ∏è Ph·∫£n bi·ªán 3:
H·ªèi: Th·∫ø c√≤n c√°c m√¥ h√¨nh m·ªõi nh∆∞ GraphRAG, RAPTOR? H·ªç ƒë√£ c·ªë kh·∫Øc ph·ª•c r·ªìi m√†?
Tr·∫£ l·ªùi: ƒê√∫ng, nh∆∞ng theo t√°c gi·∫£:
- Nh·ªØng m√¥ h√¨nh ƒë√≥ t·∫≠p trung qu√° h·∫πp v√†o m·ªôt kh√≠a c·∫°nh (sense-making ho·∫∑c associativity) n√™n m·∫•t c√¢n b·∫±ng.
- Khi l√†m t·ªët ph·∫ßn n√†y th√¨ l·∫°i suy gi·∫£m hi·ªáu nƒÉng ·ªü ph·∫ßn kh√°c. V√≠ d·ª•:
  - GraphRAG t·ªët cho discourse ‚Üí k√©m factual QA.
  - RAPTOR t√≥m t·∫Øt t·ªët ‚Üí nh∆∞ng g√¢y nhi·ªÖu khi QA nhi·ªÅu b∆∞·ªõc.
  - HippoRAG 1 t·ªët v·ªõi multi-hop ‚Üí nh∆∞ng thi·∫øu contextualization.
B√†i b√°o vi·∫øt:
‚Äú...each method experiences the largest performance decay in tasks outside its own setup.‚Äù ‚Üí t·ª©c l√† m√¥ h√¨nh t·ªët ch·ªó n√†y th√¨ g√£y ch·ªó kia.

---
‚öîÔ∏è Ph·∫£n bi·ªán 4:
H·ªèi: V·∫≠y m·ª•c ti√™u ch√≠nh c·ªßa HippoRAG l√† g√¨? Kh√°c bi·ªát c·ªët l√µi so v·ªõi RAG v√† c√°c m√¥ h√¨nh m·ªü r·ªông?
Tr·∫£ l·ªùi:
 HippoRAG nh·∫Øm t·ªõi m·ªôt b·ªô nh·ªõ d√†i h·∫°n t·ªïng qu√°t, c√≥ th·ªÉ ho·∫°t ƒë·ªông t·ªët ·ªü:
- Factual QA (ghi nh·ªõ s·ª± ki·ªán c∆° b·∫£n),
- Multi-hop QA (li√™n k·∫øt ki·∫øn th·ª©c),
- Discourse understanding (di·ªÖn gi·∫£i m·∫°ch truy·ªán d√†i).
C·ªët l√µi:
- M√¥ ph·ªèng b·ªô nh·ªõ sinh h·ªçc con ng∆∞·ªùi:
  - Neocortex ‚Üí LLM
  - Parahippocampal ‚Üí encoder g·ª£i nh·ªõ ng·ªØ nghƒ©a
  - Hippocampus ‚Üí KG m·ªü k·∫øt n·ªëi tri th·ª©c
- D√πng Personalized PageRank (PPR) ƒë·ªÉ t√¨m ki·∫øm c√≥ li√™n k·∫øt ‚Äì kh√¥ng ch·ªâ ƒëi·ªÉm, m√† l√† ƒë∆∞·ªùng ƒëi ki·∫øn th·ª©c.
- Th√™m LLM v√†o retrieval ƒë·ªÉ l·ªçc nhi·ªÖu ‚Üí kh√¥ng ch·ªâ t√¨m, m√† c√≤n nh·∫≠n di·ªán ƒë√∫ng ƒëo·∫°n c·∫ßn nh·ªõ.

---
‚öîÔ∏è Ph·∫£n bi·ªán 5:
H·ªèi: V·∫≠y HippoRAG c√≥ b·∫±ng ch·ª©ng n√†o m·∫°nh ƒë·ªÉ ch·ª©ng minh l√† c·∫ßn thi·∫øt kh√¥ng?
Tr·∫£ l·ªùi:
- HippoRAG 2 ƒë·∫°t ƒëi·ªÉm cao nh·∫•t ·ªü c·∫£ 3 lo·∫°i nhi·ªám v·ª• (Factual, Associative, Sense-making) ‚Äì ƒëi·ªÅu m√† kh√¥ng m√¥ h√¨nh n√†o tr∆∞·ªõc ƒë√≥ l√†m ƒë∆∞·ª£c.
- V√≠ d·ª•:
  - +7 ƒëi·ªÉm F1 ·ªü Associativity tasks so v·ªõi embedding t·ªët nh·∫•t (NV-Embed-v2).
  - Kh√¥ng b·ªã suy gi·∫£m ·ªü Factual QA nh∆∞ GraphRAG, RAPTOR.
=> ƒê√¢y l√† b·∫±ng ch·ª©ng cho th·∫•y HippoRAG c√≥ th·ªÉ c√¢n b·∫±ng gi·ªØa s·ª± ch√≠nh x√°c, kh·∫£ nƒÉng di·ªÖn gi·∫£i v√† k·∫øt n·ªëi tri th·ª©c, ƒëi·ªÅu RAG truy·ªÅn th·ªëng kh√¥ng l√†m ƒë∆∞·ª£c.

---
Ch√†o Qu·ªëc,
M√¨nh s·∫Ω gi·∫£i th√≠ch chi ti·∫øt v·ªÅ ba lo·∫°i nhi·ªám v·ª•: Factual, Associative, Sense-making, v√† m·ªëi li√™n h·ªá gi·ªØa c√°c nhi·ªám v·ª• n√†y v·ªõi b·ªô d·ªØ li·ªáu LongMemEval.

---
1. Ba lo·∫°i nhi·ªám v·ª•: Factual, Associative, Sense-making
a. Factual Tasks (Nhi·ªám v·ª• th·ª±c t·∫ø)
- ƒê·ªãnh nghƒ©a: Y√™u c·∫ßu h·ªá th·ªëng truy xu·∫•t v√† cung c·∫•p th√¥ng tin ch√≠nh x√°c v·ªÅ m·ªôt s·ª± ki·ªán ho·∫∑c th·ª±c t·∫ø c·ª• th·ªÉ.ÓàÜ
- V√≠ d·ª•: "Ai l√† t·ªïng th·ªëng ƒë·∫ßu ti√™n c·ªßa Hoa K·ª≥?"ÓàÜ
- M·ª•c ti√™u: Ki·ªÉm tra kh·∫£ nƒÉng ghi nh·ªõ v√† truy xu·∫•t th√¥ng tin ch√≠nh x√°c t·ª´ b·ªô nh·ªõ d√†i h·∫°n.ÓàÜ
b. Associative Tasks (Nhi·ªám v·ª• li√™n k·∫øt)
- ƒê·ªãnh nghƒ©a: Y√™u c·∫ßu h·ªá th·ªëng k·∫øt n·ªëi nhi·ªÅu m·∫£nh th√¥ng tin r·ªùi r·∫°c ƒë·ªÉ suy lu·∫≠n ho·∫∑c tr·∫£ l·ªùi c√¢u h·ªèi.ÓàÜ
- V√≠ d·ª•: "Ng∆∞·ªùi ph√°t minh ra ƒëi·ªán tho·∫°i ƒë√£ s·ªëng ·ªü ƒë√¢u?"ÓàÜ
  - Ph√¢n t√≠ch: C·∫ßn bi·∫øt Alexander Graham Bell l√† ng∆∞·ªùi ph√°t minh ra ƒëi·ªán tho·∫°i v√† √¥ng s·ªëng ·ªü Scotland v√† Canada.ÓàÜ
- M·ª•c ti√™u: ƒê√°nh gi√° kh·∫£ nƒÉng li√™n k·∫øt v√† suy lu·∫≠n t·ª´ nhi·ªÅu ngu·ªìn th√¥ng tin kh√°c nhau.ÓàÜ
c. Sense-making Tasks (Nhi·ªám v·ª• hi·ªÉu ng·ªØ c·∫£nh)
- ƒê·ªãnh nghƒ©a: Y√™u c·∫ßu h·ªá th·ªëng hi·ªÉu v√† di·ªÖn gi·∫£i c√°c t√¨nh hu·ªëng ph·ª©c t·∫°p ho·∫∑c c√¢u chuy·ªán d√†i.ÓàÜ
- V√≠ d·ª•: "T√≥m t·∫Øt n·ªôi dung ch√≠nh c·ªßa cu·ªôc h·ªçp h√¥m qua."ÓàÜ
- M·ª•c ti√™u: Ki·ªÉm tra kh·∫£ nƒÉng t·ªïng h·ª£p, hi·ªÉu ng·ªØ c·∫£nh v√† tr√¨nh b√†y l·∫°i th√¥ng tin m·ªôt c√°ch m·∫°ch l·∫°c.ÓàÜ

---
2. M·ªëi li√™n h·ªá gi·ªØa QA v√† LongMemEval
LongMemEval l√† m·ªôt b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng ghi nh·ªõ d√†i h·∫°n c·ªßa c√°c tr·ª£ l√Ω tr√≤ chuy·ªán. B·ªô d·ªØ li·ªáu n√†y t·∫≠p trung v√†o nƒÉm kh·∫£ nƒÉng c·ªët l√µi:
1. Tr√≠ch xu·∫•t th√¥ng tin (Information Extraction): Kh·∫£ nƒÉng nh·ªõ l·∫°i th√¥ng tin c·ª• th·ªÉ t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c d√†i.ÓàÜ
2. L√Ω lu·∫≠n ƒëa phi√™n (Multi-Session Reasoning): Kh·∫£ nƒÉng t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu phi√™n tr√≤ chuy·ªán.ÓàÜ
3. C·∫≠p nh·∫≠t ki·∫øn th·ª©c (Knowledge Updates): Kh·∫£ nƒÉng nh·∫≠n bi·∫øt v√† c·∫≠p nh·∫≠t th√¥ng tin c√° nh√¢n c·ªßa ng∆∞·ªùi d√πng theo th·ªùi gian.ÓàÜ
4. L√Ω lu·∫≠n th·ªùi gian (Temporal Reasoning): Nh·∫≠n th·ª©c v·ªÅ c√°c kh√≠a c·∫°nh th·ªùi gian c·ªßa th√¥ng tin ng∆∞·ªùi d√πng.ÓàÜ
5. T·ª´ ch·ªëi tr·∫£ l·ªùi (Abstention): Kh·∫£ nƒÉng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn th√¥ng tin kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong l·ªãch s·ª≠ t∆∞∆°ng t√°c.ÓàÜ
M·ªëi li√™n h·ªá:
- Factual Tasks t∆∞∆°ng ·ª©ng v·ªõi Tr√≠ch xu·∫•t th√¥ng tin, n∆°i h·ªá th·ªëng c·∫ßn truy xu·∫•t th√¥ng tin ch√≠nh x√°c t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c.ÓàÜ
- Associative Tasks li√™n quan ƒë·∫øn L√Ω lu·∫≠n ƒëa phi√™n, y√™u c·∫ßu h·ªá th·ªëng k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu phi√™n ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c.ÓàÜ
- Sense-making Tasks g·∫Øn li·ªÅn v·ªõi L√Ω lu·∫≠n th·ªùi gian v√† C·∫≠p nh·∫≠t ki·∫øn th·ª©c, ƒë√≤i h·ªèi h·ªá th·ªëng hi·ªÉu ng·ªØ c·∫£nh t·ªïng th·ªÉ v√† c·∫≠p nh·∫≠t th√¥ng tin m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng.ÓàÜ

---

D∆∞·ªõi ƒë√¢y l√† c√°ch ph√¢n bi·ªát r√µ r√†ng gi·ªØa Factual, Associative, Sense-making, b·∫±ng c√°ch ƒëi t·ª´ ƒë·ªãnh nghƒ©a ‚ûù v√≠ d·ª• c·ª• th·ªÉ ‚ûù nƒÉng l·ª±c c·∫ßn ki·ªÉm tra:

---
üß† 1. Factual Memory ‚Äì Ghi nh·ªõ s·ª± th·∫≠t c·ª• th·ªÉ
üßæ ƒê·ªãnh nghƒ©a:
Kh·∫£ nƒÉng truy xu·∫•t th√¥ng tin ƒë∆°n l·∫ª, ƒë√£ ƒë∆∞·ª£c n√™u r√µ r√†ng trong ng·ªØ li·ªáu ƒë·∫ßu v√†o.
‚ùìV√≠ d·ª• c√¢u h·ªèi:
"Ai l√† th·ªß t∆∞·ªõng ƒê·ª©c nƒÉm 2023?"
- ƒê√¢y l√† c√¢u h·ªèi ƒë√≤i h·ªèi 1 ƒëo·∫°n vƒÉn b·∫£n ch·ª©a th√¥ng tin ch√≠nh x√°c, kh√¥ng c·∫ßn suy lu·∫≠n hay li√™n k·∫øt.
üéØ NƒÉng l·ª±c ƒë∆∞·ª£c ki·ªÉm tra:
- Kh·∫£ nƒÉng l∆∞u tr·ªØ & truy xu·∫•t ch√≠nh x√°c th√¥ng tin ƒë√£ ƒë·ªçc.
- T∆∞∆°ng ƒë∆∞∆°ng v·ªõi nh·ªõ t√™n, ng√†y th√°ng, con s·ªë, ƒë·ªãnh nghƒ©a.

---
üîó 2. Associativity ‚Äì K·∫øt n·ªëi th√¥ng tin r·ªùi r·∫°c
üßæ ƒê·ªãnh nghƒ©a:
Kh·∫£ nƒÉng k·∫øt n·ªëi nhi·ªÅu m·∫£nh ki·∫øn th·ª©c t·ª´ c√°c ph·∫ßn kh√°c nhau trong d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
‚ùìV√≠ d·ª• c√¢u h·ªèi:
"Ng∆∞·ªùi s√°ng l·∫≠p Microsoft c√≥ qu√™ g·ªëc ·ªü ƒë√¢u?"
- C·∫ßn n·ªëi hai m·∫£nh th√¥ng tin:
  1. "Bill Gates l√† ng∆∞·ªùi s√°ng l·∫≠p Microsoft"
  2. "Bill Gates sinh ra ·ªü Seattle"
‚ü∂ C·∫ßn √≠t nh·∫•t 2 ƒëo·∫°n d·ªØ li·ªáu, kh√¥ng c√≥ ƒëo·∫°n n√†o ch·ª©a ƒë·∫ßy ƒë·ªß c√¢u tr·∫£ l·ªùi.
üéØ NƒÉng l·ª±c ƒë∆∞·ª£c ki·ªÉm tra:
- Reasoning nhi·ªÅu b∆∞·ªõc
- Multi-hop retrieval
- Nh∆∞ c√°ch n√£o ng∆∞·ªùi li√™n t∆∞·ªüng v√† x√¢u chu·ªói ki·∫øn th·ª©c l·∫°i v·ªõi nhau.

---
üìñ 3. Sense-making ‚Äì Hi·ªÉu v√† t·ªïng h·ª£p ng·ªØ c·∫£nh ph·ª©c t·∫°p
üßæ ƒê·ªãnh nghƒ©a:
Kh·∫£ nƒÉng hi·ªÉu m·ªôt t√¨nh hu·ªëng d√†i, nhi·ªÅu l·ªõp, nhi·ªÅu ·∫©n √Ω; t√°i c·∫•u tr√∫c v√† r√∫t ra √Ω nghƒ©a to√†n c·ª•c.
‚ùìV√≠ d·ª• c√¢u h·ªèi:
"T·∫°i sao nh√¢n v·∫≠t ch√≠nh l·∫°i quy·∫øt ƒë·ªãnh b·ªè tr·ªën ·ªü cu·ªëi truy·ªán?"
- Tr·∫£ l·ªùi ƒë∆∞·ª£c ƒë√≤i h·ªèi ph·∫£i ƒë·ªçc c·∫£ m·∫°ch truy·ªán, hi·ªÉu ƒë∆∞·ª£c:
  - T√¢m l√Ω nh√¢n v·∫≠t
  - Di·ªÖn bi·∫øn s·ª± ki·ªán
  - C√°c y·∫øu t·ªë x√£ h·ªôi, ng·ªØ c·∫£nh
üéØ NƒÉng l·ª±c ƒë∆∞·ª£c ki·ªÉm tra:
- Discourse understanding
- Cohesion & summarization
- Kh·∫£ nƒÉng c·ªßa LLM gi·ªëng nh∆∞ hi·ªÉu v√† ph√¢n t√≠ch vƒÉn h·ªçc ‚Äì ƒëi·ªÅu m√† vector retrieval ƒë∆°n thu·∫ßn kh√¥ng l√†m ƒë∆∞·ª£c.

---
üìä T√≥m t·∫Øt so s√°nh
This content is only supported in a Lark Docs

---

üß© PH·∫¶N 1: T·∫†I SAO C√ÅC M√î H√åNH RAG HI·ªÜN T·∫†I B·ªä H·∫†N CH·∫æ?

---
‚öîÔ∏è Ph·∫£n bi·ªán 1:
H·ªèi: Vector-based RAG hi·ªán t·∫°i ƒë√£ r·∫•t m·∫°nh v·ªõi embedding hi·ªán ƒë·∫°i. V·∫≠y t·∫°i sao v·∫´n b·ªã ƒë√°nh gi√° l√† "thi·∫øu t√≠nh ng∆∞·ªùi"?
Tr·∫£ l·ªùi: V√¨ vector-based RAG ch·ªâ l√†m ƒë∆∞·ª£c retrieval ƒë·ªôc l·∫≠p: m·ªói ƒëo·∫°n vƒÉn (passage) ƒë∆∞·ª£c nh√∫ng v√† truy xu·∫•t ƒë·ªôc l·∫≠p. N√≥ kh√¥ng c√≥ kh·∫£ nƒÉng:
- Nh·∫≠n bi·∫øt m·ªëi quan h·ªá gi·ªØa c√°c ƒëo·∫°n (multi-hop)
- Kh√¥ng hi·ªÉu ƒë∆∞·ª£c m·∫°ch th√¥ng tin d√†i h·∫°n
- Kh√¥ng h·ªó tr·ª£ truy v·∫•n theo d√≤ng suy nghƒ© (reasoning chain)
Gi·ªëng nh∆∞ b·∫°n nh·ªõ ƒë∆∞·ª£c 1000 m·∫£nh th√¥ng tin, nh∆∞ng kh√¥ng bi·∫øt m·∫£nh n√†o n·ªëi v·ªõi m·∫£nh n√†o ‚Äî b·∫°n kh√¥ng th·ªÉ k·ªÉ l·∫°i m·ªôt c√¢u chuy·ªán, c≈©ng kh√¥ng th·ªÉ suy lu·∫≠n.
üìå H·∫°n ch·∫ø n√†y khi·∫øn vector-based RAG t·ªët trong factual QA, nh∆∞ng fail trong associative v√† sense-making.

---
‚öîÔ∏è Ph·∫£n bi·ªán 2:
H·ªèi: Th·∫ø c√≤n c√°c RAG m·ªõi nh∆∞ RAPTOR, GraphRAG, LightRAG? H·ªç t·∫°o graph, t·∫°o t√≥m t·∫Øt r·ªìi m√†?
Tr·∫£ l·ªùi: ƒê√∫ng l√† h·ªç ƒë√£ th·ª≠:
- RAPTOR: T·∫°o c√°c summary t√≥m g·ªçn, r·ªìi retrieve t·ª´ c√°c ƒëo·∫°n t√≥m t·∫Øt ƒë√≥.
- GraphRAG: T·∫°o c√°c knowledge graph t·ª´ entity, li√™n k·∫øt qua c·ªông ƒë·ªìng (community detection).
- LightRAG: T·∫°o 2 l·ªõp retrieval ‚Äì coarse v√† fine, t√≠ch h·ª£p vector v√† graph.
Nh∆∞ng t·∫°i sao v·∫´n ch∆∞a ƒë·ªß?
- Ch√∫ng thi·∫øu c√¢n b·∫±ng: c·∫£i thi·ªán ch·ªó n√†y, l·∫°i gi·∫£m hi·ªáu qu·∫£ ·ªü ch·ªó kh√°c.
- V√≠ d·ª•:
  - RAPTOR t·ªët khi t√≥m t·∫Øt context ‚Üí nh∆∞ng t·∫°o nhi·ªÖu cho QA v√¨ LLM summary c√≥ th·ªÉ b·ªè s√≥t chi ti·∫øt.
  - GraphRAG d√πng graph m·∫°nh ‚Üí nh∆∞ng kh√¥ng t∆∞∆°ng t√°c t·ªët v·ªõi truy v·∫•n, v√¨ query kh√¥ng li√™n k·∫øt tr·ª±c ti·∫øp v·ªõi edge trong KG.
  - LightRAG th√¨ retrieval t·ªët nh∆∞ng kh√¥ng reasoning ƒë∆∞·ª£c.
üìâ Trong c√°c benchmark t·ªïng h·ª£p (b·∫£ng 2 b√†i b√°o), t·∫•t c·∫£ c√°c RAG c√≥ c·∫•u tr√∫c ƒë·ªÅu t·ª•t ƒëi·ªÉm r√µ r·ªát ·ªü m·ªôt trong 3 lo·∫°i nhi·ªám v·ª•.

---
‚öîÔ∏è Ph·∫£n bi·ªán 3:
H·ªèi: V·∫≠y v·∫•n ƒë·ªÅ s√¢u h∆°n n·∫±m ·ªü ƒë√¢u? V√¨ sao c√°c m√¥ h√¨nh c≈© kh√¥ng ƒë·∫°t ƒë∆∞·ª£c b·ªô nh·ªõ d√†i h·∫°n gi·ªëng con ng∆∞·ªùi?
Tr·∫£ l·ªùi: V√¨ ch√∫ng thi·∫øu 3 th·ª© m√† n√£o ng∆∞·ªùi l√†m r·∫•t t·ªët:
1. Li√™n k·∫øt tri th·ª©c d·∫°ng ƒë·ªì th·ªã ‚Äì n√£o ng∆∞·ªùi l∆∞u th√¥ng tin theo m·∫°ng li√™n k·∫øt.
2. Nh·∫≠n di·ªán ng·ªØ c·∫£nh (recognition memory) ‚Äì bi·∫øt th√¥ng tin n√†o li√™n quan trong l√∫c truy xu·∫•t.
3. K·∫øt h·ª£p gi·ªØa kh√°i ni·ªám (concept) v√† ng·ªØ c·∫£nh (context) ‚Äì ch·ª© kh√¥ng l∆∞u r·ªùi r·∫°c.
C√°c m√¥ h√¨nh tr∆∞·ªõc:
- T·∫≠p trung v√†o embedding ho·∫∑c graph, nh∆∞ng kh√¥ng k·∫øt n·ªëi ch√∫ng l·∫°i.
- Kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c c√°c query d√†i, ph·ª©c t·∫°p, ho·∫∑c c·∫ßn suy lu·∫≠n nhi·ªÅu b∆∞·ªõc.
- Kh√¥ng c√≥ c∆° ch·∫ø ki·ªÉm tra ‚Äúli·ªáu ƒëo·∫°n n√†y c√≥ th·ª±c s·ª± ƒë√∫ng ng·ªØ c·∫£nh truy v·∫•n kh√¥ng?‚Äù (g·ªçi l√† recognition filter trong HippoRAG).

---
üß¨ PH·∫¶N 2: HIPPORAG GI·∫¢I QUY·∫æT TH·∫æ N√ÄO?

---
‚öîÔ∏è Ph·∫£n bi·ªán 4:
H·ªèi: Nghe nh∆∞ ai c≈©ng c√≥ knowledge graph r·ªìi, HippoRAG c√≥ g√¨ kh√°c?
Tr·∫£ l·ªùi: HippoRAG kh√¥ng ch·ªâ c√≥ KG, m√† l√† m·ªôt h·ªá th·ªëng m√¥ ph·ªèng tr√≠ nh·ªõ ng∆∞·ªùi:
- Neocortex ‚Üí LLM: d√πng ƒë·ªÉ suy lu·∫≠n, ƒë·ªçc hi·ªÉu, quy·∫øt ƒë·ªãnh.
- Hippocampus ‚Üí KG m·ªü: m√¥ h√¨nh h√≥a k·∫øt n·ªëi tri th·ª©c.
- Parahippocampal region ‚Üí embedding encoder: d√πng ƒë·ªÉ so kh·ªõp v√† g·ª£i nh·ªõ.
=> HippoRAG d√πng PageRank c√° nh√¢n h√≥a tr√™n KG ƒë·ªÉ t√¨m ra nh·ªØng ƒëo·∫°n li√™n k·∫øt t·ªët nh·∫•t, sau ƒë√≥ k·∫øt h·ª£p l·∫°i ƒë·ªÉ feed v√†o LLM ƒë·ªçc hi·ªÉu.

---
‚öîÔ∏è Ph·∫£n bi·ªán 5:
H·ªèi: V·∫≠y so v·ªõi c√°c m√¥ h√¨nh tr∆∞·ªõc, HippoRAG c√≥ g√¨ v∆∞·ª£t tr·ªôi?
Tr·∫£ l·ªùi:
 üëâ HippoRAG 1 ƒë√£ h∆°n c√°c m√¥ h√¨nh c≈© nh·ªù:
- D√πng Personalized PageRank tr√™n KG ƒë·ªÉ l√†m multi-hop retrieval.
- T√°ch ri√™ng index (offline) v√† truy xu·∫•t (online) ‚Üí ti·ªán l·ª£i cho continual learning.
Nh∆∞ng n√≥ v·∫´n ch∆∞a ƒë·ªß, v√¨:
- D·ª±a qu√° nhi·ªÅu v√†o entity (NER), n√™n b·ªè l·ª° ng·ªØ c·∫£nh.
- Kh√¥ng l·ªçc triple n√†o li√™n quan th·∫≠t s·ª± ƒë·∫øn query.
üëâ HippoRAG 2 ƒë√£ kh·∫Øc ph·ª•c th√™m:
- Query-to-Triple linking thay v√¨ NER ‚Üí k·∫øt n·ªëi t·ªët h∆°n.
- Recognition memory b·∫±ng LLM ‚Üí l·ªçc triple "r√°c".
- Dense-sparse integration ‚Üí k·∫øt h·ª£p phrase node (kh√°i ni·ªám) v√† passage node (ng·ªØ c·∫£nh).
K·∫øt qu·∫£:
+7 ƒëi·ªÉm F1 ·ªü associative tasks, kh√¥ng t·ª•t performance ·ªü factual hay discourse.

---
üîç PH·∫¶N 3: T·ªîNG K·∫æT ‚Äì HIPPORAG C√ì G√å KH√ÅC V√Ä HAY?
This content is only supported in a Lark Docs


---
[Image]
[Image]
R·∫•t t·ªët, Qu·ªëc ƒëang ƒëi ƒë√∫ng h∆∞·ªõng c·ªßa ‚Äúnghi√™n c·ª©u nghi√™m t√∫c‚Äù ‚Äì kh√¥ng ch·ªâ ghi nh·∫≠n k·∫øt qu·∫£, m√† ph·∫£i gi·∫£i th√≠ch s√¢u s·∫Øc v√¨ sao l·∫°i ra k·∫øt qu·∫£ ƒë√≥, b·∫±ng logic, k·ªπ thu·∫≠t v√† ph·∫£n bi·ªán.
M√¨nh s·∫Ω ti·∫øp t·ª•c ph√¢n t√≠ch theo phong c√°ch:
üé≠ M·ªôt ng∆∞·ªùi ph√° (h·ªèi xo√°y) ‚Äì M·ªôt ng∆∞·ªùi tr·∫£ l·ªùi (tr·∫£ ƒë√≤n)
 cho t·ª´ng m√¥ h√¨nh, d·ª±a tr√™n pipeline n·ªôi t·∫°i v√† d·ªØ li·ªáu th·ª±c nghi·ªám t·ª´ Table 2 (QA) + Table 3 (Retrieval).

---
üß† 1. RAG G·ªêC
Retrieval b·∫±ng embedding vector, kh√¥ng c·∫•u tr√∫c.
 QA ƒë∆°n thu·∫ßn d·ª±a v√†o ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√¢u h·ªèi v√† ƒëo·∫°n vƒÉn.

---
‚ùìPh√°: T·∫°i sao RAG g·ªëc l·∫°i t·ªët ·ªü factual QA (NQ, PopQA)?
‚úÖ Tr·∫£ l·ªùi: V√¨:
- C√¢u h·ªèi d·∫°ng factual th∆∞·ªùng ch·ªâ c·∫ßn 1 ƒëo·∫°n ng·∫Øn ch·ª©a ƒë√∫ng entity ‚Üí d·ªÖ t√¨m b·∫±ng embedding similarity.
- C√°c embedding hi·ªán ƒë·∫°i (NV-Embed-v2, GritLM) r·∫•t m·∫°nh ·ªü semantic retrieval.
üìå V√≠ d·ª•:
C√¢u h·ªèi: ‚ÄúWhen was Albert Einstein born?‚Äù ‚Üí ch·ªâ c·∫ßn ƒëo·∫°n c√≥ "Albert Einstein was born in 1879..." ‚Üí l√† ƒë·ªß ‚Üí vector retrieval l√†m t·ªët.

---
‚ùìPh√°: T·∫°i sao l·∫°i r·∫•t y·∫øu ·ªü multi-hop v√† sense-making?
‚úÖ Tr·∫£ l·ªùi: V√¨:
- RAG g·ªëc kh√¥ng li√™n k·∫øt th√¥ng tin gi·ªØa nhi·ªÅu ƒëo·∫°n ‚Üí kh√¥ng th·ªÉ reasoning nhi·ªÅu b∆∞·ªõc.
- Kh√¥ng c√≥ c∆° ch·∫ø hi·ªÉu to√†n truy·ªán, ng·ªØ c·∫£nh r·ªông ‚Üí th·∫•t b·∫°i ·ªü NarrativeQA.
üìå V√≠ d·ª•:
C√¢u h·ªèi: ‚ÄúWhat county is Erik Hort‚Äôs birthplace a part of?‚Äù ‚Üí c·∫ßn bi·∫øt: Erik Hort ‚Üí Montebello ‚Üí thu·ªôc h·∫°t g√¨?
 ‚Üí Vector retrieval ch·ªâ t√¨m ƒë∆∞·ª£c ƒëo·∫°n ƒë·∫ßu ti√™n, kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c sang ƒë·ªãa l√Ω.

---
üß† 2. RAPTOR
RAG v·ªõi t√≥m t·∫Øt (summarization) t·ª´ng c·ª•m t√†i li·ªáu. D·ª±a v√†o Gaussian clustering ƒë·ªÉ nh√≥m r·ªìi summary.

---
‚ùìPh√°: T√≥m t·∫Øt th√¨ ph·∫£i gi√∫p t√¨m nhanh h∆°n ch·ª©? Sao l·∫°i t·ª•t ƒëi·ªÉm ·ªü factual QA?
‚úÖ Tr·∫£ l·ªùi: V√¨:
- LLM t√≥m t·∫Øt th∆∞·ªùng b·ªè qua c√°c chi ti·∫øt nh·ªè nh∆∞ s·ªë li·ªáu, ƒë·ªãa ƒëi·ªÉm, t√™n ng∆∞·ªùi ‚Üí trong khi ƒë√≥ l·∫°i l√† m·∫•u ch·ªët c·ªßa factual QA.
- Sau t√≥m t·∫Øt, ƒëo·∫°n g·ªëc kh√¥ng c√≤n ‚Üí truy h·ªìi m·∫•t ch√≠nh x√°c.
üìå T√°c gi·∫£ n√≥i r√µ:
"LLM summarization mechanism... introduces noise into the retrieval corpora."

---
‚ùìPh√°: V·∫≠y t·∫°i sao v·∫´n t·ªët ·ªü sense-making?
‚úÖ Tr·∫£ l·ªùi:
- V√¨ trong discourse d√†i (nh∆∞ NarrativeQA), ƒë·ªçc t·ª´ng ƒëo·∫°n kh√¥ng ƒë·ªß hi·ªÉu m·∫°ch truy·ªán.
- Vi·ªác gom l·∫°i r·ªìi t√≥m t·∫Øt gi√∫p LLM th·∫•y ƒë∆∞·ª£c t·ªïng th·ªÉ ‚Üí hi·ªÉu c√¢u chuy·ªán nh∆∞ ng∆∞·ªùi ƒë·ªçc m·ªôt l·∫ßn.

---
üß† 3. GraphRAG
T·∫°o graph c·ªông ƒë·ªìng (community graph), gi√∫p hi·ªÉu nh√≥m th·ª±c th·ªÉ ho·∫∑c ƒëo·∫°n vƒÉn li√™n quan.

---
‚ùìPh√°: Sao t·∫°o ƒë∆∞·ª£c graph m√† v·∫´n ‚Äút·ª•t factual‚Äù?
‚úÖ Tr·∫£ l·ªùi:
- V√¨ GraphRAG kh√¥ng gi·ªØ ƒëo·∫°n vƒÉn g·ªëc, ch·ªâ d√πng node graph ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ LLM.
- N√™n chi ti·∫øt c·ª• th·ªÉ d·ªÖ b·ªã m·∫•t khi QA y√™u c·∫ßu entity ho·∫∑c s·ªë li·ªáu ch√≠nh x√°c.

---
‚ùìPh√°: V·∫≠y n√≥ gi√∫p ƒë∆∞·ª£c g√¨?
‚úÖ Tr·∫£ l·ªùi:
- D√πng graph structure ƒë·ªÉ kh√¥i ph·ª•c c√°c m·ªëi li√™n h·ªá gi·ªØa th·ª±c th·ªÉ ‚Äì s·ª± ki·ªán ‚Äì ƒë·ªãa ƒëi·ªÉm ‚Üí h·ªó tr·ª£ multi-hop v√† sense-making.
- Vi·ªác ph√°t hi·ªán c·ªông ƒë·ªìng gi√∫p nh√≥m th√¥ng tin t·ª´ c√°c ngu·ªìn kh√°c nhau ‚Üí tƒÉng associativity.
üìå K·∫øt qu·∫£: NarrativeQA ƒë·∫°t 23.0 (ch·ªâ sau HippoRAG 2)
 ‚Üí nghƒ©a l√† n√≥ hi·ªÉu truy·ªán t·ªët, nh∆∞ng kh√¥ng nh·ªõ chi ti·∫øt.

---
üß† 4. HippoRAG 1
D·ª±a v√†o OpenIE ƒë·ªÉ extract triple ‚Üí d√πng Personalized PageRank (PPR) ƒë·ªÉ reasoning.

---
‚ùìPh√°: V·∫≠y t·∫°i sao HippoRAG 1 r·∫•t t·ªët ·ªü associative QA?
‚úÖ Tr·∫£ l·ªùi:
- PPR + KG cho ph√©p ƒëi qua nhi·ªÅu hop trong graph ‚Üí gi·ªëng nh∆∞ nh·ªõ theo ‚Äúƒë∆∞·ªùng li√™n t∆∞·ªüng‚Äù.
- D·∫°ng QA c·∫ßn li√™n k·∫øt 2-3 ƒëo·∫°n l√† ƒëi·ªÉm m·∫°nh c·ªßa PPR.
üìå 2Wiki ƒë·∫°t 71.8, cao nh·∫•t trong b·∫£ng ‚Üí ch·ª©ng minh multi-hop c·ª±c t·ªët.

---
‚ùìPh√°: Nh∆∞ng v√¨ sao l·∫°i ch∆∞a t·ªët ·ªü sense-making?
‚úÖ Tr·∫£ l·ªùi:
- Do HippoRAG 1 ch·ªâ d·ª±a tr√™n entity-centric extraction (NER) ‚Üí thi·∫øu th√¥ng tin ng·ªØ c·∫£nh (context).
- Query kh√¥ng ƒë∆∞·ª£c embed to√†n b·ªô ƒë·ªÉ match triple, ch·ªâ match theo entity ‚Üí contextualization r·∫•t y·∫øu.
üìå T√°c gi·∫£ g·ªçi ƒë√¢y l√† ‚Äúentity-centric bias‚Äù.

---
üß† 5. HippoRAG 2
B·∫£n n√¢ng c·∫•p to√†n di·ªán:
- KG g·ªìm c·∫£ phrase node & passage node (dense-sparse integration)
- Query-to-triple matching (kh√¥ng NER)
- Triple filtering b·∫±ng LLM
- PPR search d√πng c·∫£ passage & phrase node

---
‚ùìPh√°: T·∫°i sao m√¥ h√¨nh n√†y ‚Äút·ªët ƒë·ªÅu c·∫£ 3‚Äù ‚Äî c√≥ g√¨ ƒë·∫∑c bi·ªát?
‚úÖ Tr·∫£ l·ªùi:
1. Factual QA:
- Gi·ªØ nguy√™n ƒëo·∫°n vƒÉn g·ªëc + phrase-level KG ‚Üí kh√¥ng m·∫•t chi ti·∫øt.
- D√πng embedding m·∫°nh + triple filter ‚Üí gi·ªØ th√¥ng tin chu·∫©n.
1. Associative QA:
- PPR graph search s√¢u, seed node t·ª´ c·∫£ phrase & passage ‚Üí c·ª±c k·ª≥ ph√π h·ª£p v·ªõi multi-hop reasoning.
- Filter gi√∫p lo·∫°i nhi·ªÖu ‚Üí tƒÉng ƒë·ªô ch√≠nh x√°c.
2. Sense-making:
- Dense-sparse integration gi√∫p gi·ªØ ƒë∆∞·ª£c c·∫£ kh√°i ni·ªám l·∫´n ng·ªØ c·∫£nh.
- Query kh√¥ng c√≤n b·ªã chia nh·ªè nh∆∞ NER ‚Üí c·∫£ c√¢u ƒë∆∞·ª£c d√πng ƒë·ªÉ hi·ªÉu context ‚Üí c·∫£i thi·ªán m·∫°nh NarrativeQA.
üìå B·∫±ng ch·ª©ng:
- HippoRAG 2 ƒë·∫°t cao nh·∫•t ·ªü F1 NarrativeQA = 25.9
- Recall@5 = 96.3 ·ªü HotpotQA ‚Üí qu√° v∆∞·ª£t tr·ªôi!

---
üìå T·ªïng K·∫øt Gi·∫£i Th√≠ch (c√≥ th·ªÉ ƒë∆∞a v√†o b√°o c√°o/lu·∫≠n vƒÉn):
This content is only supported in a Lark Docs

---
Gi·ªù c·∫ßn hi·ªÉu c√°c kh√°i ni·ªám chi ti·∫øt c·ªßa Hippo 1 v√† Hippo2 -- Gi·∫£i t√≠ch t·ª´ng kh√°i ni·ªám v√† li√™n h·ªá n√≥ v·ªõi th·ª© th·∫≠t d·ªÖ nh·ªõ ƒë∆°n gi·∫£n cho h·ªçc sinh c·∫•p 2

R·∫•t hay Qu·ªëc ∆°i, gi·ªù m√¨nh s·∫Ω gi·∫£i t√≠ch c√°c kh√°i ni·ªám c·ªët l√µi trong HippoRAG 1 v√† HippoRAG 2, li√™n h·ªá v·ªõi v√≠ d·ª• ƒë·ªùi th·∫≠t th·∫≠t d·ªÖ hi·ªÉu, ki·ªÉu gi·∫£i th√≠ch cho h·ªçc sinh c·∫•p 2.

---
üß† T·ªïng th·ªÉ: HippoRAG l√† g√¨?
HippoRAG l√† m√¥ h√¨nh tr√≠ nh·ªõ d√†i h·∫°n cho AI, l·∫•y c·∫£m h·ª©ng t·ª´ c√°ch con ng∆∞·ªùi ghi nh·ªõ v√† suy lu·∫≠n.
 N√≥ chia l√†m 3 b·ªô ph·∫≠n gi·ªëng nh∆∞ b·ªô n√£o ng∆∞·ªùi:
- Neocortex ‚Üí ch√≠nh l√† LLM (n√£o suy nghƒ©)
- Hippocampus ‚Üí n∆°i l∆∞u gi·ªØ k√Ω ·ª©c (graph)
- Parahippocampal Region ‚Üí k·∫øt n·ªëi √Ω t∆∞·ªüng (t√¨m ƒëi·ªÉm gi·ªëng nhau)

---
ü¶õ HippoRAG 1 ‚Äì GI·∫¢I T√çCH C√ÅC KH√ÅI NI·ªÜM CH√çNH

---
1. OpenIE ‚Üí Tr√≠ch xu·∫•t triple (subject ‚Äì predicate ‚Äì object)
üß© ƒê√¢y l√† b∆∞·ªõc l·∫•y th√¥ng tin t·ª´ vƒÉn b·∫£n v√† bi·∫øn n√≥ th√†nh m·ªôt ‚Äúki·∫øn th·ª©c nh·ªè c√≥ d·∫°ng: A li√™n quan ƒë·∫øn B b·∫±ng c√°ch C‚Äù.
üîç V√≠ d·ª• vƒÉn b·∫£n:
‚ÄúAlbert Einstein was born in Ulm in 1879.‚Äù
‚Üí s·∫Ω th√†nh triple:
- (Albert Einstein, born in, Ulm)
- (Albert Einstein, born in year, 1879)
ü™Ñ Li√™n h·ªá cho h·ªçc sinh:
 Gi·ªëng nh∆∞ khi b·∫°n ƒë·ªçc s√°ch v√† t·ª± ghi v·ªü ‚Äút√≥m t·∫Øt √Ω ch√≠nh‚Äù ki·ªÉu:
 ‚Üí ‚ÄúEinstein ‚Üí sinh ·ªü Ulm‚Äù, ‚ÄúEinstein ‚Üí sinh nƒÉm 1879‚Äù.

---
2. Graph of Knowledge Triples (Bi·ªÉu ƒë·ªì ki·∫øn th·ª©c)
üß† C√°c triple tr√™n ƒë∆∞·ª£c gh√©p th√†nh 1 b·∫£n ƒë·ªì ki·∫øn th·ª©c, gi·ªëng nh∆∞ b·∫£n ƒë·ªì t∆∞ duy (mindmap).
üìå M·ªói v√≤ng tr√≤n l√† 1 t·ª´ kh√≥a, m·ªói m≈©i t√™n l√† m·ªëi li√™n h·ªá.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 B·∫°n v·∫Ω s∆° ƒë·ªì mindmap m√¥n Sinh h·ªçc:
- ‚ÄúL√° c√¢y ‚Üí c√≥ ch·ª©c nƒÉng ‚Üí quang h·ª£p‚Äù
- ‚ÄúQuang h·ª£p ‚Üí t·∫°o ra ‚Üí oxy‚Äù
 ‚Üí L√† b·∫°n ƒëang v·∫Ω KG ƒë√≥!

---
3. Personalized PageRank (PPR)
üîÑ ƒê√¢y l√† c√°ch m√¥ h√¨nh t√¨m ƒë∆∞·ªùng ƒëi qua nhi·ªÅu b∆∞·ªõc trong b·∫£n ƒë·ªì ki·∫øn th·ª©c ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 Gi·ªëng nh∆∞ ch∆°i tr√≤ ‚Äúƒëi t√¨m kho b√°u‚Äù trong m√™ cung. B·∫°n b·∫Øt ƒë·∫ßu t·ª´ t·ª´ kh√≥a trong c√¢u h·ªèi, ƒëi theo c√°c m·ªëi li√™n h·ªá ƒë·ªÉ ƒë·∫øn c√¢u tr·∫£ l·ªùi.
C√¢u h·ªèi: ‚ÄúL√° c√¢y gi√∫p g√¨ cho m√¥i tr∆∞·ªùng?‚Äù
 ‚Üí L√° c√¢y ‚Üí quang h·ª£p ‚Üí t·∫°o oxy ‚Üí gi√∫p m√¥i tr∆∞·ªùng ‚Üí th·ªü

---
4. Entity-Centric Query Linking (Li√™n k·∫øt d·ª±a v√†o t√™n ri√™ng)
HippoRAG 1 d√πng c√°ch ph√°t hi·ªán th·ª±c th·ªÉ (entity) trong c√¢u h·ªèi nh∆∞ t√™n ng∆∞·ªùi, n∆°i ch·ªën, ƒë·ªÉ n·ªëi v√†o graph.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 C√¢u h·ªèi: ‚ÄúBill Gates h·ªçc tr∆∞·ªùng n√†o?‚Äù
 ‚Üí Ph√°t hi·ªán ‚ÄúBill Gates‚Äù l√† th·ª±c th·ªÉ ‚Üí t√¨m trong b·∫£n ƒë·ªì tr√≠ nh·ªõ.
ƒêi·ªÉm y·∫øu:
- Kh√¥ng hi·ªÉu c√¢u h·ªèi r·ªông (v√≠ d·ª•: ‚ÄúNg∆∞·ªùi s√°ng l·∫≠p Microsoft h·ªçc ·ªü ƒë√¢u?‚Äù)
 ‚Üí n·∫øu kh√¥ng c√≥ t·ª´ ‚ÄúBill Gates‚Äù, th√¨ kh√¥ng t√¨m ƒë∆∞·ª£c ‚Üí m·∫•t context

---
üß† HippoRAG 2 ‚Äì C·∫¢I TI·∫æN QUAN TR·ªåNG + V√ç D·ª§ ƒê·ªúI TH∆Ø·ªúNG

---
1. Dense-Sparse Integration: Gh√©p ki·∫øn th·ª©c v√† ng·ªØ c·∫£nh
HippoRAG 2 kh√¥ng ch·ªâ l∆∞u "ki·∫øn th·ª©c ng·∫Øn g·ªçn" m√† c√≤n l∆∞u c·∫£ ƒëo·∫°n vƒÉn ban ƒë·∫ßu (ng·ªØ c·∫£nh).
üìå Graph c√≥ 2 lo·∫°i node:
- Phrase Node (sparse): gi·ªëng ti√™u ƒë·ªÅ/t·ª´ kh√≥a
- Passage Node (dense): nguy√™n ƒëo·∫°n vƒÉn g·ªëc
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 Khi h·ªçc l·ªãch s·ª≠, b·∫°n v·ª´a g·∫°ch ƒë·∫ßu d√≤ng ‚ÄúTr·∫≠n ƒêi·ªán Bi√™n Ph·ªß th·∫Øng nƒÉm 1954‚Äù, v·ª´a gi·ªØ l·∫°i nguy√™n ƒëo·∫°n m√¥ t·∫£ tr·∫≠n ƒë√°nh ‚Üí v·∫≠y b·∫°n v·ª´a nh·ªõ "√Ω ch√≠nh" v·ª´a c√≥ ‚Äúng·ªØ c·∫£nh‚Äù.

---
2. Query-to-Triple Matching
Kh√¥ng ch·ªâ t√¨m entity n·ªØa, m√† d√πng to√†n c√¢u h·ªèi ƒë·ªÉ match v·ªõi c√°c triple.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 C√¢u h·ªèi: ‚ÄúNg∆∞·ªùi s√°ng l·∫≠p Microsoft h·ªçc ·ªü ƒë√¢u?‚Äù
 ‚Üí L√∫c n√†y, d√π kh√¥ng c√≥ t·ª´ ‚ÄúBill Gates‚Äù, m√¥ h√¨nh v·∫´n hi·ªÉu ƒë∆∞·ª£c v√† t√¨m ƒë∆∞·ª£c triple ph√π h·ª£p.

---
3. Recognition Memory ‚Äì l·ªçc triple b·∫±ng LLM
Sau khi t√¨m ra v√†i m·ªëi li√™n h·ªá ph√π h·ª£p, HippoRAG 2 nh·ªù LLM ki·ªÉm tra l·∫°i xem c√°i n√†o th·ª±c s·ª± li√™n quan.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 B·∫°n t√¨m 5 ƒëo·∫°n c√≥ v·∫ª ƒë√∫ng ƒë·ªÉ l√†m b√†i t·∫≠p. Sau ƒë√≥ b·∫°n ƒë·ªçc k·ªπ t·ª´ng ƒëo·∫°n, lo·∫°i b·ªè c√°i kh√¥ng li√™n quan ‚Üí gi·ªØ l·∫°i 2 ƒëo·∫°n t·ªët nh·∫•t ‚Üí ƒë√≥ l√† ‚Äúrecognition memory‚Äù.

---
4. Contextualized PPR Search
Khi ƒë√£ c√≥ c·∫£ t·ª´ kh√≥a v√† ƒëo·∫°n vƒÉn, m√¥ h√¨nh d√πng PageRank ƒë·ªÉ t√¨m con ƒë∆∞·ªùng t·ªët nh·∫•t qua b·∫£n ƒë·ªì tr√≠ nh·ªõ c√≥ c·∫£ ki·∫øn th·ª©c v√† ng·ªØ c·∫£nh.
ü™Ñ V√≠ d·ª• h·ªçc sinh:
 B·∫°n n·ªëi ‚Äút·ª´ kh√≥a‚Äù v·ªõi c√°c ƒëo·∫°n gi·∫£i th√≠ch trong s√°ch, r·ªìi ch·ªçn ra ƒëo·∫°n n√†o gi√∫p b·∫°n hi·ªÉu to√†n c·∫£nh nh·∫•t ƒë·ªÉ l√†m b√†i.

---
üéì T·ªïng k·∫øt: HippoRAG 1 vs 2 ‚Äì D·ªÖ hi·ªÉu cho h·ªçc sinh c·∫•p 2
This content is only supported in a Lark Docs

---
Tuy·ªát v·ªùi Qu·ªëc ∆°i, gi·ªù m√¨nh s·∫Ω gi·∫£i t√≠ch s√¢u t·ª´ng kh√°i ni·ªám trong s∆° ƒë·ªì pipeline c·ªßa HippoRAG 2, nh∆∞ Qu·ªëc y√™u c·∫ßu:
- Hi·ªÉu r√µ c∆° ch·∫ø
- Ph·∫£n bi·ªán ki·ªÉu ph√° ‚Äì ƒë√°p t·ª´ng b∆∞·ªõc
- Li√™n h·ªá v√≠ d·ª• khi c·∫ßn
S∆° ƒë·ªì ƒë∆∞·ª£c chia l√†m 2 ph·∫ßn:

---
üéØ PH·∫¶N 1: OFFLINE INDEXING
M·ª•c ti√™u: x√¢y d·ª±ng m·ªôt b·ªô nh·ªõ ki·∫øn th·ª©c gi√†u li√™n k·∫øt t·ª´ vƒÉn b·∫£n ƒë·∫ßu v√†o.

---
üîµ B∆∞·ªõc 1: OpenIE by LLM ‚Üí Tr√≠ch xu·∫•t Triple
T√°ch ƒëo·∫°n vƒÉn th√†nh c√°c ƒë∆°n v·ªã ki·∫øn th·ª©c d·∫°ng (subject, predicate, object)

---
‚ùìPh√°: T·∫°i sao d√πng OpenIE b·∫±ng LLM? Tr√≠ch xu·∫•t truy·ªÅn th·ªëng c√≥ v·∫•n ƒë·ªÅ g√¨?
‚úÖ ƒê√°p:
- OpenIE truy·ªÅn th·ªëng (Stanford, ClausIE...) ch·ªâ l√†m ƒë∆∞·ª£c tr√™n c√¢u ƒë∆°n, thi·∫øu kh·∫£ nƒÉng x·ª≠ l√Ω ng√¥n ng·ªØ ph·ª©c t·∫°p.
- LLM c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c m·ªánh ƒë·ªÅ d√†i, ng·ªØ nghƒ©a m·ªü r·ªông, ph·ª©c t·∫°p h∆°n ‚Üí t·∫°o triple ‚Äúgi√†u nghƒ©a‚Äù h∆°n.
üìå V√≠ d·ª•
 VƒÉn b·∫£n: "Marie Curie won two Nobel Prizes in different fields."
 ‚Üí Triple truy·ªÅn th·ªëng: ("Marie Curie", "won", "two Nobel Prizes")
 ‚Üí LLM c√≥ th·ªÉ t·∫°o th√™m triple: ("Marie Curie", "won Nobel in", "Physics") + ("Marie Curie", "won Nobel in", "Chemistry")

---
üîµ B∆∞·ªõc 2: Synonym Detection ‚Üí Ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a
N·ªëi c√°c node g·∫ßn nghƒ©a trong KG b·∫±ng Synonym Edge (ƒë∆∞·ªùng ƒë·ª©t n√©t trong s∆° ƒë·ªì)

---
‚ùìPh√°: Ph√°t hi·ªán synonym ki·ªÉu g√¨? C√≥ d√πng LLM kh√¥ng?
‚úÖ ƒê√°p: Kh√¥ng! D√πng cosine similarity trong embedding space
- C√°c phrase (t·ª´/c·ª•m t·ª´) ƒë∆∞·ª£c nh√∫ng (embedding)
- N·∫øu cosine similarity > ng∆∞·ª°ng ‚Üí ƒë∆∞·ª£c n·ªëi b·∫±ng "synonym edge"
üìå T√°c gi·∫£ kh√¥ng d√πng LLM v√¨:
- LLM ch·∫≠m h∆°n r·∫•t nhi·ªÅu
- Embedding-based synonym c√≥ th·ªÉ c·∫≠p nh·∫≠t li√™n t·ª•c, ƒë∆°n gi·∫£n h∆°n

---
‚ùìPh√°: C√≥ r·ªßi ro g√¨ kh√¥ng khi d√πng embedding ƒë·ªÉ n·ªëi synonym?
‚úÖ ƒê√°p:
- C√≥! C√≥ th·ªÉ n·ªëi nh·∫ßm c√°c t·ª´ g·∫ßn nghƒ©a nh∆∞ng kh√°c ng·ªØ c·∫£nh ("doctor" v√† "professor" ch·∫≥ng h·∫°n).
- Nh∆∞ng v√¨ ƒë√¢y l√† l·ªõp b·ªï tr·ª£ (synonym edge), kh√¥ng ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp n·∫øu downstream filter l√†m t·ªët.

---
üîµ B∆∞·ªõc 3: Dense‚ÄìSparse Integration
T√≠ch h·ª£p c·∫£ node "phrase" v√† node "passage" trong KG ‚Üí hybrid graph

---
‚ùìPh√°: T·∫°i sao c·∫ßn v·ª´a sparse (phrase) v·ª´a dense (passage)? Kh√¥ng gi·ªØ m·ªôt lo·∫°i cho g·ªçn?
‚úÖ ƒê√°p:
- Sparse (phrase): D·ªÖ truy h·ªìi, bi·ªÉu di·ªÖn kh√°i ni·ªám ‚Üí t·ªët cho reasoning
- Dense (passage): Gi·ªØ th√¥ng tin chi ti·∫øt ‚Üí t·ªët cho factual QA
 ‚Üí Gh√©p c·∫£ hai t·∫°o ra m·ªôt KG v·ª´a kh√°i qu√°t v·ª´a gi√†u ng·ªØ c·∫£nh.
ü™Ñ Li√™n h·ªá d·ªÖ hi·ªÉu: Gi·ªëng h·ªçc Sinh h·ªçc b·∫±ng s∆° ƒë·ªì + s√°ch gi√°o khoa:
- S∆° ƒë·ªì (phrase node) gi√∫p nh·ªõ nhanh
- SGK (passage node) gi√∫p hi·ªÉu s√¢u ‚Üí c·∫ßn c·∫£ hai ƒë·ªÉ thi ƒëi·ªÉm cao

---
‚úÖ T·ªïng k·∫øt PH·∫¶N 1: Graph c√≥ c√°c lo·∫°i node v√† edge sau:
This content is only supported in a Lark Docs

---
üîç PH·∫¶N 2: ONLINE RETRIEVAL & QA
M·ª•c ti√™u: Khi c√≥ query ‚Üí t√¨m ƒë∆∞·ªùng trong graph ‚Üí tr·∫£ l·ªùi.

---
üü¢ B∆∞·ªõc 1: Retrieving Passages and Triples

---
‚ùìPh√°: Sao ph·∫£i l·∫•y c·∫£ ƒëo·∫°n vƒÉn l·∫´n triple? Kh√¥ng ch·ªçn 1 c√°i cho nhanh?
‚úÖ ƒê√°p:
- Triple ƒë·ªÉ l√†m reasoning, k·∫øt n·ªëi m·∫°ch ki·∫øn th·ª©c
- Passage ƒë·ªÉ c√≥ th√¥ng tin chi ti·∫øt, l√†m QA cu·ªëi c√πng
‚Üí Hai l·ªõp retrieval b·ªï tr·ª£ nhau, kh√¥ng th·ªÉ thi·∫øu c√°i n√†o.

---
üü¢ B∆∞·ªõc 2: Triple Filtering = Recognition Memory
Sau khi l·∫•y triple v·ªÅ, ta l·ªçc xem c√°i n√†o th·ª±c s·ª± li√™n quan ƒë·∫øn query

---
‚ùìPh√°: T·∫°i sao c·∫ßn b∆∞·ªõc n√†y? Triple ƒë√£ ch·ªçn r·ªìi m√†?
‚úÖ ƒê√°p: V√¨:
- Retrieval ban ƒë·∫ßu b·∫±ng embedding ‚Üí c√≥ th·ªÉ nhi·ªÖu
- Recognition l√† b∆∞·ªõc gi·ªëng b·ªô nh·ªõ con ng∆∞·ªùi:
 ‚Üí ‚Äúm√¨nh t·ª´ng th·∫•y c√°i n√†y r·ªìi‚Ä¶ nh∆∞ng c√≥ ƒë√∫ng c√°i m√¨nh c·∫ßn kh√¥ng?‚Äù
üìå D√πng LLM ƒë·ªÉ so s√°nh l·∫°i t·ª´ng triple v·ªõi query ‚Üí gi·ªØ l·∫°i nh·ªØng c√°i t·ªët nh·∫•t
ü™Ñ Li√™n h·ªá h·ªçc sinh: Gi·ªëng ƒë·ªçc l·∫°i 5 ƒëo·∫°n b·∫°n highlight khi h·ªçc ‚Üí lo·∫°i 3 c√°i kh√¥ng li√™n quan, gi·ªØ l·∫°i 2 ƒëo·∫°n th·∫≠t s·ª± ƒë√∫ng ƒë·ªÅ.

---
üî¥ B∆∞·ªõc 3: Assigning Seed Node Weights
G√°n ƒëi·ªÉm cho c√°c node ‚Äúg·ªëc‚Äù trong KG ‚Üí chu·∫©n b·ªã cho b∆∞·ªõc lan truy·ªÅn PageRank

---
‚ùìPh√°: Seed Node l√† g√¨? T·∫°i sao ph·∫£i g√°n tr·ªçng s·ªë?
‚úÖ ƒê√°p:
- Seed Node l√† node trong graph c√≥ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn query
- Tr·ªçng s·ªë cao th√¨ node ƒë√≥ lan truy·ªÅn ·∫£nh h∆∞·ªüng m·∫°nh h∆°n trong PPR
üìå D√πng c·∫£:
- Passage t·ª´ query retrieval
- Phrase t·ª´ triple l·ªçc
 ‚Üí t·∫•t c·∫£ ƒë·ªÅu tr·ªü th√†nh seed node

---
üîÑ B∆∞·ªõc 4: PPR Graph Search
Personalized PageRank t√¨m ƒë∆∞·ªùng trong KG t·ª´ c√°c seed node

---
‚ùìPh√°: Kh√°c g√¨ so v·ªõi search b√¨nh th∆∞·ªùng?
‚úÖ ƒê√°p:
- PageRank lan t·ªèa ·∫£nh h∆∞·ªüng ‚Üí kh√¥ng t√¨m ƒë∆∞·ªùng ng·∫Øn nh·∫•t, m√† t√¨m ƒëo·∫°n c√≥ li√™n quan nh·∫•t trong to√†n b·ªô c·∫•u tr√∫c
- Personalized ‚Üí ∆∞u ti√™n c√°c v√πng g·∫ßn v·ªõi seed node
ü™Ñ Li√™n h·ªá h·ªçc sinh: Gi·ªëng h·ªçc theo s∆° ƒë·ªì t∆∞ duy, b·∫°n ∆∞u ti√™n ƒë·ªçc ph·∫ßn g·∫ßn ch·ªß ƒë·ªÅ ch√≠nh ‚Üí kh√¥ng l·∫≠t s√°ch lung tung

---
‚úÖ B∆∞·ªõc 5: QA Reading with Selected Passages
LLM ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë√£ ch·ªçn ‚Üí sinh ra c√¢u tr·∫£ l·ªùi.

---
‚ùìPh√°: N·∫øu ch·ªâ d√πng ƒëo·∫°n vƒÉn m√† kh√¥ng graph th√¨ sao?
‚úÖ ƒê√°p:
 ‚Üí Gi·ªëng RAG truy·ªÅn th·ªëng ‚Üí b·ªã l·ªách, thi·∫øu reasoning
 ‚Üí Nh·ªù b∆∞·ªõc graph search, m√¥ h√¨nh nh·∫≠n ƒë∆∞·ª£c ƒëo·∫°n vƒÉn ‚Äúgi√†u li√™n k·∫øt nh·∫•t‚Äù ‚Üí tr·∫£ l·ªùi m·∫°ch l·∫°c h∆°n, ch√≠nh x√°c h∆°n.

---
üîö T·ªïng K·∫øt: Pipeline HippoRAG 2 ‚Äì Hi·ªÉu s√¢u + v√≠ d·ª• h√≥a
This content is only supported in a Lark Docs

---
D∆∞·ªõi ƒë√¢y l√† ph·∫ßn gi·∫£i th√≠ch s√¢u t·ª´ng b∆∞·ªõc trong pipeline c·ªßa HippoRAG 2, k√®m theo c√°c c√¢u h·ªèi ‚Äúph√°‚Äù ‚Äì ‚Äúƒë√°p‚Äù ƒë·ªÉ ƒë√†o s√¢u hi·ªÉu bi·∫øt, gi·ªëng nh∆∞ ƒë·ªëi tho·∫°i gi·ªØa ng∆∞·ªùi ƒë·∫∑t c√¢u h·ªèi s·∫Øc s·∫£o v√† ng∆∞·ªùi tr·∫£ l·ªùi ch·∫∑t ch·∫Ω.

---
1. OpenIE by LLM ‚Äì Tr√≠ch xu·∫•t Triple t·ª´ vƒÉn b·∫£n
M·ª•c ti√™u:
 Chuy·ªÉn ƒë·ªïi ƒëo·∫°n vƒÉn b·∫£n th√†nh c√°c ƒë∆°n v·ªã ki·∫øn th·ª©c (triple: subject ‚Äì predicate ‚Äì object) ƒë·ªÉ l∆∞u tr·ªØ trong b·ªô nh·ªõ d∆∞·ªõi d·∫°ng c√°c m·ªëi li√™n h·ªá ƒë∆°n gi·∫£n.

---
‚ùì Ph√° 1:
T·∫°i sao ch√∫ng ta c·∫ßn tr√≠ch xu·∫•t triple thay v√¨ ch·ªâ l∆∞u to√†n b·ªô ƒëo·∫°n vƒÉn?
‚úÖ ƒê√°p:
- Vi·ªác tr√≠ch xu·∫•t triple gi√∫p t√°ch th√¥ng tin quan tr·ªçng ra kh·ªèi m·ªõ vƒÉn b·∫£n d√†i.
- N√≥ cho ph√©p l∆∞u tr·ªØ d∆∞·ªõi d·∫°ng c√°c m·ªëi quan h·ªá r√µ r√†ng, gi·ªëng nh∆∞ ghi ch√∫ ng·∫Øn g·ªçn, d·ªÖ t√¨m l·∫°i v√† k·∫øt n·ªëi v·ªõi c√°c ki·∫øn th·ª©c kh√°c.
V√≠ d·ª• d·ªÖ hi·ªÉu:
 Gi·ªëng nh∆∞ khi h·ªçc, b·∫°n kh√¥ng c·∫ßn l∆∞u to√†n b·ªô b√†i gi·∫£ng m√† ch·ªâ c·∫ßn ghi l·∫°i c√°c √Ω ch√≠nh: ‚ÄúEinstein ‚Äì sinh ‚Äì Ulm‚Äù thay v√¨ ghi to√†n b·ªô ƒëo·∫°n vƒÉn d√†i.

---
‚ùì Ph√° 2:
T·∫°i sao d√πng LLM ƒë·ªÉ th·ª±c hi·ªán OpenIE, thay v√¨ d√πng c√°c c√¥ng c·ª• truy·ªÅn th·ªëng?
‚úÖ ƒê√°p:
- C√°c c√¥ng c·ª• OpenIE truy·ªÅn th·ªëng th∆∞·ªùng gi·ªõi h·∫°n ·ªü c√¢u ƒë∆°n gi·∫£n, kh√¥ng hi·ªÉu ƒë∆∞·ª£c nh·ªØng c√¢u ph·ª©c t·∫°p, m√¢u thu·∫´n hay ·∫©n d·ª•.
- LLM c√≥ kh·∫£ nƒÉng hi·ªÉu ng·ªØ c·∫£nh, x·ª≠ l√Ω c√¢u d√†i v√† ph·ª©c t·∫°p, t·ª´ ƒë√≥ tr√≠ch xu·∫•t triple ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß h∆°n.
V√≠ d·ª• h·ªçc sinh c·∫•p 2:
 N·∫øu ƒë·ªçc m·ªôt ƒëo·∫°n vƒÉn m√¥ t·∫£ "Marie Curie ƒë√£ kh√°m ph√° ra c√°c nguy√™n t·ªë m·ªõi v√† gi√†nh gi·∫£i Nobel trong nhi·ªÅu lƒ©nh v·ª±c", LLM c√≥ th·ªÉ t·∫°o ra nhi·ªÅu triple nh∆∞:
 ‚Üí (Marie Curie, kh√°m ph√°, nguy√™n t·ªë m·ªõi)
 ‚Üí (Marie Curie, gi√†nh, gi·∫£i Nobel)
 Nh∆∞ v·∫≠y, b·∫°n c√≥ th·ªÉ nh·ªõ nhi·ªÅu th√¥ng tin h∆°n ch·ªâ b·∫±ng m·ªôt c√¢u t√≥m t·∫Øt.

---
2. Synonym Detection ‚Äì Ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a
M·ª•c ti√™u:
 K·∫øt n·ªëi c√°c node (kh√°i ni·ªám, c·ª•m t·ª´) c√≥ √Ω nghƒ©a t∆∞∆°ng t·ª± trong KG, gi√∫p tƒÉng kh·∫£ nƒÉng li√™n k·∫øt gi·ªØa c√°c th√¥ng tin.

---
‚ùì Ph√° 1:
Synonym Detection d√πng thu·∫≠t to√°n n√†o ƒë·ªÉ ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a?
‚úÖ ƒê√°p:
- Th√¥ng th∆∞·ªùng, ta s·ª≠ d·ª•ng cosine similarity trong kh√¥ng gian embedding.
- M·ªói t·ª´ hay c·ª•m t·ª´ ƒë∆∞·ª£c chuy·ªÉn th√†nh m·ªôt vector, sau ƒë√≥ t√≠nh cosine similarity gi·ªØa c√°c vector. N·∫øu gi√° tr·ªã similarity v∆∞·ª£t qua m·ªôt ng∆∞·ª°ng ƒë·ªãnh tr∆∞·ªõc (v√≠ d·ª• 0.8), hai node ƒë∆∞·ª£c coi l√† ƒë·ªìng nghƒ©a v√† ƒë∆∞·ª£c n·ªëi v·ªõi nhau b·∫±ng m·ªôt ‚Äúsynonym edge.‚Äù

---
‚ùì Ph√° 2:
T·∫°i sao l·∫°i c·∫ßn n·ªëi c√°c node ƒë·ªìng nghƒ©a l·∫°i v·ªõi nhau?
‚úÖ ƒê√°p:
- ƒêi·ªÅu n√†y gi√∫p li√™n k·∫øt c√°c kh√°i ni·ªám c√≥ th·ªÉ ƒë∆∞·ª£c di·ªÖn ƒë·∫°t kh√°c nhau.
- Nh·ªù ƒë√≥, khi truy v·∫•n, n·∫øu t·ª´ kh√≥a ƒë∆∞·ª£c d√πng kh√°c v·ªõi c√°ch ghi ch√∫ ban ƒë·∫ßu nh∆∞ng c√≥ c√πng √Ω nghƒ©a, h·ªá th·ªëng v·∫´n c√≥ th·ªÉ li√™n k·∫øt ƒë√∫ng ki·∫øn th·ª©c.
V√≠ d·ª• d·ªÖ hi·ªÉu:
 N·∫øu trong vƒÉn b·∫£n c√≥ ‚Äúth·∫ßy gi√°o‚Äù v√† ‚Äúgi√°o vi√™n‚Äù ‚Äì m·∫∑c d√π hai t·ª´ kh√°c nhau, ch√∫ng ƒë·ªÅu n√≥i v·ªÅ ng∆∞·ªùi d·∫°y h·ªçc. K·∫øt n·ªëi ch√∫ng l·∫°i gi√∫p h·ªá th·ªëng hi·ªÉu r·∫±ng khi h·ªèi v·ªÅ ‚Äúgi√°o vi√™n‚Äù c≈©ng c√≥ th·ªÉ li√™n h·ªá ƒë·∫øn ‚Äúth·∫ßy gi√°o‚Äù.

---
‚ùì Ph√° 3:
C√≥ r·ªßi ro g√¨ khi d√πng cosine similarity?
‚úÖ ƒê√°p:
- C√≥ th·ªÉ x·∫£y ra tr∆∞·ªùng h·ª£p n·ªëi nh·∫ßm nh·ªØng t·ª´ c√≥ c√πng h√¨nh th·ª©c nh∆∞ng ng·ªØ nghƒ©a kh√°c (v√≠ d·ª•: ‚Äúbank‚Äù c√≥ th·ªÉ l√† ng√¢n h√†ng ho·∫∑c b·ªù s√¥ng).
- Tuy nhi√™n, v√¨ ƒë√¢y ch·ªâ l√† l·ªõp b·ªï tr·ª£ trong KG, n·∫øu c√°c b∆∞·ªõc sau (nh∆∞ triple filtering) ho·∫°t ƒë·ªông t·ªët, sai s√≥t n√†y s·∫Ω ƒë∆∞·ª£c gi·∫£m thi·ªÉu.

---
3. Dense‚ÄìSparse Integration ‚Äì T√≠ch h·ª£p c·∫£ Node "Phrase" v√† "Passage"
M·ª•c ti√™u:
 K·∫øt h·ª£p th√¥ng tin ng·∫Øn g·ªçn (phrase node) v√† th√¥ng tin ƒë·∫ßy ƒë·ªß (passage node) v√†o c√πng m·ªôt KG ƒë·ªÉ t·∫≠n d·ª•ng ∆∞u ƒëi·ªÉm c·ªßa c·∫£ hai.

---
‚ùì Ph√° 1:
T·∫°i sao kh√¥ng ch·ªâ l∆∞u th√¥ng tin d·∫°ng ‚Äúphrase‚Äù hay ‚Äúpassage‚Äù ri√™ng r·∫Ω?
‚úÖ ƒê√°p:
- Phrase node (sparse): R√∫t g·ªçn th√¥ng tin, d·ªÖ k·∫øt n·ªëi c√°c kh√°i ni·ªám c∆° b·∫£n, nhanh v√† hi·ªáu qu·∫£ cho reasoning.
- Passage node (dense): Gi·ªØ nguy√™n ng·ªØ c·∫£nh v√† chi ti·∫øt, c·∫ßn thi·∫øt cho c√°c t√°c v·ª• y√™u c·∫ßu ƒë·ªô ch√≠nh x√°c cao nh∆∞ factual QA.
V√≠ d·ª•:
 H·ªçc sinh khi √¥n b√†i to√°n: b·∫°n c√≥ th·ªÉ ghi l·∫°i ‚Äúc√¥ng th·ª©c Pythagoras‚Äù (phrase) v√† c√≥ s·ªï tay ch·ª©a b√†i gi·∫£i chi ti·∫øt (passage).
 C·∫£ hai ƒë·ªÅu quan tr·ªçng ƒë·ªÉ b·∫°n v·ª´a nh·ªõ nhanh, v·ª´a hi·ªÉu s√¢u.

---
‚ùì Ph√° 2:
How does integrating both dense and sparse information help in retrieval?
‚úÖ ƒê√°p:
- Khi c√≥ c·∫£ hai, h·ªá th·ªëng c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c phrase node ƒë·ªÉ x√°c ƒë·ªãnh c√°c kh√°i ni·ªám ch·ªß ƒë·∫°o v√† passage node ƒë·ªÉ l·∫•y chi ti·∫øt c·ª• th·ªÉ cho c√¢u tr·∫£ l·ªùi.
- ƒêi·ªÅu n√†y t·∫°o ra m·ªôt KG to√†n di·ªán, v·ª´a t·ªïng qu√°t v·ª´a chi ti·∫øt, gi√∫p tƒÉng hi·ªáu su·∫•t ·ªü c·∫£ factual QA v√† multi-hop reasoning.

---
4. Triple Retrieval ‚Äì L·∫•y ra c√°c Triple c√≥ li√™n quan v·ªõi Query
M·ª•c ti√™u:
 Khi c√≥ m·ªôt query, t√¨m ra c√°c triple t·ª´ KG ph√π h·ª£p ƒë·ªÉ d√πng cho qu√° tr√¨nh reasoning.

---
‚ùì Ph√° 1:
T·∫°i sao kh√¥ng ch·ªâ l·∫•y to√†n b·ªô passage m√† ph·∫£i l·∫•y ra triple?
‚úÖ ƒê√°p:
- Triple ch·ª©a m·ªëi li√™n h·ªá r√µ r√†ng v√† r√∫t g·ªçn, gi√∫p h·ªá th·ªëng d·ªÖ d√†ng ‚Äúƒëi theo‚Äù c√°c m·ªëi li√™n k·∫øt ƒë·ªÉ reasoning.
- Passage c√≥ th·ªÉ qu√° d√†i, ch·ª©a nhi·ªÖu; triple gi√∫p x√°c ƒë·ªãnh ‚Äúƒëi·ªÉm c·ªët l√µi‚Äù c·ªßa ki·∫øn th·ª©c.
V√≠ d·ª•:
 N·∫øu c√¢u h·ªèi l√† ‚ÄúBill Gates h·ªçc tr∆∞·ªùng n√†o?‚Äù, triple nh∆∞ (‚ÄúBill Gates‚Äù, ‚Äúh·ªçc t·∫°i‚Äù, ‚ÄúHarvard‚Äù) s·∫Ω gi√∫p h·ªá th·ªëng nhanh ch√≥ng t√¨m ra th√¥ng tin c·∫ßn thi·∫øt.

---
5. Triple Filtering (Recognition Memory) ‚Äì L·ªçc l·∫°i c√°c Triple
M·ª•c ti√™u:
 Sau khi l·∫•y ra c√°c triple ban ƒë·∫ßu, s·ª≠ d·ª•ng LLM ƒë·ªÉ x√°c nh·∫≠n l·∫°i t√≠nh li√™n quan c·ªßa ch√∫ng v·ªõi query, lo·∫°i b·ªè nh·ªØng triple kh√¥ng ph√π h·ª£p.

---
‚ùì Ph√° 1:
T·∫°i sao l·∫°i c·∫ßn b∆∞·ªõc l·ªçc triple n·∫øu ƒë√£ c√≥ triple retrieval?
‚úÖ ƒê√°p:
- Do b∆∞·ªõc retrieval ban ƒë·∫ßu c√≥ th·ªÉ mang l·∫°i nhi·ªÅu triple nhi·ªÖu ho·∫∑c kh√¥ng li√™n quan, v√¨ kh√¥ng ph·∫£i l√∫c n√†o c√°c vector embedding c≈©ng ho√†n h·∫£o.
- Recognition Memory gi√∫p ‚Äúƒë·ªçc l·∫°i‚Äù v√† x√°c nh·∫≠n, gi·ªëng nh∆∞ khi b·∫°n ki·ªÉm tra l·∫°i ghi ch√∫ c·ªßa m√¨nh ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ gi·ªØ l·∫°i nh·ªØng th√¥ng tin ƒë√∫ng c·∫ßn thi·∫øt.
V√≠ d·ª•:
 N·∫øu c√≥ 5 triple ƒë∆∞·ª£c l·∫•y ra, nh∆∞ng ch·ªâ c√≥ 2 th·ª±c s·ª± n√≥i v·ªÅ c√¢u h·ªèi, b∆∞·ªõc filtering s·∫Ω lo·∫°i b·ªè 3 triple kh√¥ng c·∫ßn thi·∫øt.

---
6. Seed Node Assignment ‚Äì G√°n Tr·ªçng S·ªë cho Seed Node
M·ª•c ti√™u:
 X√°c ƒë·ªãnh c√°c node ‚Äúg·ªëc‚Äù (seed node) t·ª´ query, v√† g√°n tr·ªçng s·ªë cho ch√∫ng nh·∫±m t·∫°o n·ªÅn t·∫£ng cho b∆∞·ªõc lan truy·ªÅn th√¥ng tin trong PPR.

---
‚ùì Ph√° 1:
Seed Node l√† g√¨?
‚úÖ ƒê√°p:
- Seed Node l√† nh·ªØng node trong KG m√† h·ªá th·ªëng x√°c ƒë·ªãnh c√≥ m·ªëi li√™n h·ªá tr·ª±c ti·∫øp v·ªõi query.
- Ch√∫ng l√† ƒëi·ªÉm kh·ªüi ƒë·∫ßu ƒë·ªÉ thu·∫≠t to√°n lan truy·ªÅn (PPR) ‚Äúkh√°m ph√°‚Äù c√°c node kh√°c c√≥ li√™n quan.
V√≠ d·ª•:
 N·∫øu c√¢u h·ªèi l√† ‚ÄúBill Gates h·ªçc ·ªü ƒë√¢u?‚Äù, seed node c√≥ th·ªÉ l√† ‚ÄúBill Gates‚Äù v√† ‚ÄúHarvard‚Äù n·∫øu ƒë√£ c√≥ triple li√™n quan.

---
‚ùì Ph√° 2:
T·∫°i sao c·∫ßn g√°n tr·ªçng s·ªë cho seed node?
‚úÖ ƒê√°p:
- Tr·ªçng s·ªë c√†ng cao, node ƒë√≥ s·∫Ω ·∫£nh h∆∞·ªüng m·∫°nh h∆°n trong qu√° tr√¨nh lan truy·ªÅn th√¥ng tin c·ªßa PageRank.
- Gi√∫p ∆∞u ti√™n c√°c node quan tr·ªçng, t·∫°o ra k·∫øt qu·∫£ retrieval ch√≠nh x√°c h∆°n.
V√≠ d·ª•:
 Gi·ªëng nh∆∞ khi b·∫°n ch·ªçn ƒëi·ªÉm m·∫•u ch·ªët trong s∆° ƒë·ªì t∆∞ duy, nh·ªØng ƒëi·ªÉm quan tr·ªçng ƒë∆∞·ª£c ƒë√°nh d·∫•u r√µ, ƒë·ªÉ khi ‚Äúƒëi theo s∆° ƒë·ªì‚Äù b·∫°n s·∫Ω ∆∞u ti√™n nh·ªØng ƒëi·ªÉm ƒë√≥.

---
7. PPR (Personalized PageRank) Graph Search ‚Äì T√¨m Ki·∫øm Trong Graph
M·ª•c ti√™u:
 S·ª≠ d·ª•ng thu·∫≠t to√°n PPR ƒë·ªÉ lan truy·ªÅn th√¥ng tin t·ª´ c√°c seed node v√† t√¨m ra c√°c node li√™n quan nh·∫•t trong KG.

---
‚ùì Ph√° 1:
Thu·∫≠t to√°n PageRank ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o v√† kh√°c g√¨ so v·ªõi t√¨m ki·∫øm th√¥ng th∆∞·ªùng?
‚úÖ ƒê√°p:
- PageRank ban ƒë·∫ßu ƒë∆∞·ª£c d√πng ƒë·ªÉ x·∫øp h·∫°ng c√°c trang web d·ª±a tr√™n s·ªë l∆∞·ª£ng v√† ch·∫•t l∆∞·ª£ng c·ªßa c√°c li√™n k·∫øt.
- Personalized PageRank (PPR) ƒëi·ªÅu ch·ªânh thu·∫≠t to√°n ƒë·ªÉ ∆∞u ti√™n c√°c node b·∫Øt ƒë·∫ßu t·ª´ seed node (ƒëi·ªÉm kh·ªüi ƒë·∫ßu t·ª´ query).
- Thay v√¨ ch·ªâ t√¨m ‚Äúƒëo·∫°n vƒÉn g·∫ßn nh·∫•t‚Äù, PPR lan truy·ªÅn tr·ªçng s·ªë qua c√°c li√™n k·∫øt trong graph, gi√∫p h·ªá th·ªëng reasoning multi-hop.
V√≠ d·ª•:
 Gi·ªëng nh∆∞ b·∫°n c√≥ m·ªôt b·∫£n ƒë·ªì m√™ cung v·ªõi c√°c l·ªëi ƒëi, PPR gi√∫p b·∫°n kh√¥ng ch·ªâ t√¨m l·ªëi ƒëi ng·∫Øn nh·∫•t m√† t√¨m l·ªëi ƒëi ‚Äúnhi·ªÅu d·∫•u hi·ªáu nh·∫•t‚Äù li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ c·ªßa c√¢u h·ªèi.

---
‚ùì Ph√° 2:
T·∫°i sao l·∫°i c·∫ßn b∆∞·ªõc n√†y khi c√≥ c√°c b∆∞·ªõc retrieval tr∆∞·ªõc ƒë√≥?
‚úÖ ƒê√°p:
- V√¨ PPR gi√∫p k·∫øt n·ªëi c√°c m·ªëi quan h·ªá trong KG, kh√¥ng ch·ªâ d·ª±a v√†o ƒë·ªô t∆∞∆°ng ƒë·ªìng ƒë∆°n thu·∫ßn m√† c√≤n hi·ªÉu ƒë∆∞·ª£c c·∫•u tr√∫c li√™n k·∫øt gi·ªØa c√°c th√¥ng tin.
- N√≥ cho ph√©p truy xu·∫•t c√°c th√¥ng tin li√™n quan t·ª´ xa trong graph, t·∫°o n√™n c√¢u tr·∫£ l·ªùi ch√≠nh x√°c cho c√°c c√¢u h·ªèi multi-hop.

---
8. QA Reader ‚Äì ƒê·ªçc v√† Sinh Ra C√¢u Tr·∫£ L·ªùi
M·ª•c ti√™u:
 Cu·ªëi c√πng, c√°c ƒëo·∫°n vƒÉn ƒë∆∞·ª£c l·∫•y ra (sau khi ƒë√£ qua c√°c b∆∞·ªõc x·ª≠ l√Ω) ƒë∆∞·ª£c ƒë∆∞a cho LLM ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi cu·ªëi c√πng cho query.

---
‚ùì Ph√° 1:
T·∫°i sao kh√¥ng s·ª≠ d·ª•ng tr·ª±c ti·∫øp c√°c triple l√†m c√¢u tr·∫£ l·ªùi?
‚úÖ ƒê√°p:
- Triple ch·ªâ l√† c√°c m·ªëi li√™n h·ªá r√∫t g·ªçn, ch∆∞a ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh c·∫ßn thi·∫øt ƒë·ªÉ tr·∫£ l·ªùi m·ªôt c√°ch m·∫°ch l·∫°c v√† t·ª± nhi√™n.
- Passage cung c·∫•p ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh, chi ti·∫øt gi√∫p LLM sinh ra c√¢u tr·∫£ l·ªùi trung th·ª±c, ƒë·∫ßy ƒë·ªß v√† t·ª± nhi√™n h∆°n.
V√≠ d·ª•:
 N·∫øu b·∫°n h·ªèi ‚ÄúBill Gates h·ªçc ·ªü ƒë√¢u?‚Äù, triple c√≥ th·ªÉ n√≥i ‚ÄúBill Gates ‚Äì h·ªçc t·∫°i ‚Äì Harvard‚Äù, nh∆∞ng passage c√≥ th·ªÉ cung c·∫•p th√™m chi ti·∫øt nh∆∞ ‚ÄúBill Gates t·ª´ng theo h·ªçc t·∫°i tr∆∞·ªùng Harvard, n∆°i √¥ng ƒë√£ ti·∫øp thu nhi·ªÅu ki·∫øn th·ª©c v·ªÅ kinh doanh v√† c√¥ng ngh·ªá‚Äù, gi√∫p c√¢u tr·∫£ l·ªùi phong ph√∫ h∆°n.

---
T·ªïng K·∫øt & Li√™n H·ªá:
M·ªói b∆∞·ªõc trong HippoRAG 2 ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ b√π tr·ª´ nh·ªØng ƒëi·ªÉm y·∫øu c·ªßa c√°c b∆∞·ªõc tr∆∞·ªõc v√† t·∫°o n√™n m·ªôt chu·ªói x·ª≠ l√Ω to√†n di·ªán:
- OpenIE: Tr√≠ch xu·∫•t ki·∫øn th·ª©c d·∫°ng triple gi√∫p ‚Äúghi nh·ªõ‚Äù c√°c m·ªëi quan h·ªá ch√≠nh.
- Synonym Detection: K·∫øt n·ªëi nh·ªØng kh√°i ni·ªám c√≥ √Ω nghƒ©a t∆∞∆°ng t·ª±, nh∆∞ vi·ªác n·ªëi c√°c t·ª´ ƒë·ªìng nghƒ©a trong t·ª´ ƒëi·ªÉn.
- Dense-Sparse Integration: Gi·ªØ ƒë∆∞·ª£c c·∫£ ‚Äúƒë·∫ßu d√≤ng‚Äù (phrase) v√† ‚Äún·ªôi dung chi ti·∫øt‚Äù (passage) ‚Äì gi·ªëng nh∆∞ ghi ch√∫ ng·∫Øn v√† s·ªï tay chi ti·∫øt.
- Triple Retrieval & Filtering: Nh·∫≠n ra c√°c ghi ch√∫ li√™n quan v√† lo·∫°i b·ªè nhi·ªÖu, nh∆∞ vi·ªác b·∫°n xem l·∫°i b√†i h·ªçc v√† ch·ªçn ra nh·ªØng ƒëi·ªÉm quan tr·ªçng.
- Seed Node Assignment & PPR Graph Search: Gi·ªëng nh∆∞ ch·ªçn c√°c ƒëi·ªÉm m·∫•u ch·ªët tr√™n s∆° ƒë·ªì t∆∞ duy r·ªìi ƒëi theo c√°c m·ªëi li√™n h·ªá ƒë·ªÉ t√¨m ra c√¢u tr·∫£ l·ªùi to√†n di·ªán.
- QA Reader: Cu·ªëi c√πng, t·ªïng h·ª£p c√°c th√¥ng tin ƒë√£ ƒë∆∞·ª£c l·ªçc v√† li√™n k·∫øt ƒë·ªÉ sinh ra c√¢u tr·∫£ l·ªùi t·ª± nhi√™n v√† ch√≠nh x√°c.

---
D∆∞·ªõi ƒë√¢y m√¨nh s·∫Ω ƒë√†o s√¢u v√†o kh√°i ni·ªám triple trong b∆∞·ªõc 1 c·ªßa HippoRAG (v√† HippoRAG 2), v·ªõi phong c√°ch ‚Äúm·ªôt ng∆∞·ªùi ph√° ‚Äì m·ªôt ng∆∞·ªùi tr·∫£ l·ªùi‚Äù ƒë·ªÉ l√†m r√µ t·ª´ng ƒëi·ªÉm.

---
1. Triple v√† Relation Edge
‚ùì Ph√°: Trong b∆∞·ªõc tr√≠ch xu·∫•t triple b·∫±ng OpenIE, c√°c triple ƒë∆∞·ª£c t·∫°o ra c√≥ ‚Äúh∆∞·ªõng‚Äù kh√¥ng?
Tr·∫£ l·ªùi:
- C√≥ h∆∞·ªõng:
 C√°c triple ƒë∆∞·ª£c tr√≠ch xu·∫•t theo ƒë·ªãnh d·∫°ng (subject, predicate, object) v·ªën ƒë√£ c√≥ th·ª© t·ª± r√µ r√†ng.
  - V√≠ d·ª•: t·ª´ c√¢u ‚ÄúMarie Curie won the Nobel Prize‚Äù ta c√≥ triple:
 (Marie Curie, won, Nobel Prize)
  - ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† m·ªëi quan h·ªá ‚Äúwon‚Äù ch·ªâ c√≥ √Ω nghƒ©a t·ª´ ‚ÄúMarie Curie‚Äù ƒë·∫øn ‚ÄúNobel Prize‚Äù.
- L√Ω do gi·ªØ h∆∞·ªõng:
  - Gi√∫p ph√¢n bi·ªát ƒë∆∞·ª£c ai l√†m g√¨ v·ªõi ai.
  - B·∫£o to√†n th·ª© t·ª± th√¥ng tin, r·∫•t quan tr·ªçng trong vi·ªác reasoning (v√≠ d·ª•: ‚Äúwon‚Äù kh√¥ng th·ªÉ ƒë·∫£o ng∆∞·ª£c th√†nh ‚ÄúNobel Prize won Marie Curie‚Äù n·∫øu kh√¥ng c√≥ h∆∞·ªõng).

---
‚ùì Ph√°: N·∫øu b·ªè ƒëi h∆∞·ªõng c·ªßa c√°c triple, li·ªáu c√≥ l√†m m·∫•t ƒëi th√¥ng tin quan tr·ªçng kh√¥ng?
Tr·∫£ l·ªùi:
- C√≥ m·∫•t:
  - H∆∞·ªõng c·ªßa edge x√°c ƒë·ªãnh m·ªëi li√™n h·ªá theo th·ª© t·ª± logic.
  - V√≠ d·ª•:
    - Triple: (Marie Curie, won, Nobel Prize) mang nghƒ©a r√µ r√†ng.
    - N·∫øu tr·ªü th√†nh edge kh√¥ng h∆∞·ªõng, ta ch·ªâ bi·∫øt ‚ÄúMarie Curie‚Äù v√† ‚ÄúNobel Prize‚Äù li√™n quan nhau nh∆∞ng kh√¥ng bi·∫øt ai l√† t√°c nh√¢n hay ƒë·ªëi t∆∞·ª£ng c·ªßa h√†nh ƒë·ªông.
- K·∫øt qu·∫£:
  - Vi·ªác gi·ªØ h∆∞·ªõng l√† c·∫ßn thi·∫øt ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c c·ªßa ng·ªØ nghƒ©a ban ƒë·∫ßu t·ª´ vƒÉn b·∫£n.

---
‚ùì Ph√°: Trong ƒë·ªì th·ªã ki·∫øn th·ª©c, vai tr√≤ c·ªßa relation edge c√≥ h∆∞·ªõng l√† g√¨ trong qu√° tr√¨nh reasoning?
Tr·∫£ l·ªùi:
- Gi√∫p ƒë·ªãnh h∆∞·ªõng th√¥ng tin:
  - Khi th·ª±c hi·ªán Personalized PageRank (PPR), h∆∞·ªõng c·ªßa edge gi√∫p x√°c ƒë·ªãnh lu·ªìng th√¥ng tin t·ª´ c√°c node g·ªëc (seed nodes) sang c√°c node li√™n quan.
- H·ªó tr·ª£ multi-hop reasoning:
  - V√≠ d·ª•: t·ª´ node ‚ÄúMarie Curie‚Äù qua edge ‚Äúwon‚Äù ƒë·∫øn ‚ÄúNobel Prize‚Äù, sau ƒë√≥ n·∫øu c√≥ edge kh√°c n·ªëi ‚ÄúNobel Prize‚Äù v·ªõi ‚ÄúPhysics‚Äù (n·∫øu ƒë∆∞·ª£c tr√≠ch xu·∫•t th√™m), h·ªá th·ªëng s·∫Ω hi·ªÉu ƒë∆∞·ª£c m·ªëi li√™n h·ªá c√≥ th·ª© t·ª± gi·ªØa c√°c kh√°i ni·ªám.
- Gi·ªØ nguy√™n ng·ªØ nghƒ©a:
  - B·∫±ng c√°ch kh√¥ng ƒë·∫£o ng∆∞·ª£c m·ªëi quan h·ªá, m√¥ h√¨nh c√≥ th·ªÉ truy xu·∫•t c√°c th√¥ng tin li√™n quan ch√≠nh x√°c h∆°n.

---
2. Synonym Edge
‚ùì Ph√°: Synonym edge ƒë∆∞·ª£c t·∫°o ra d·ª±a tr√™n thu·∫≠t to√°n g√¨?
Tr·∫£ l·ªùi:
- D·ª±a tr√™n cosine similarity trong embedding space:
  - M·ªói phrase (hay c·ª•m t·ª´) ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng vector.
  - N·∫øu cosine similarity gi·ªØa hai vector v∆∞·ª£t qua m·ªôt ng∆∞·ª°ng ƒë·ªãnh tr∆∞·ªõc, th√¨ hai node ƒë√≥ ƒë∆∞·ª£c n·ªëi b·∫±ng m·ªôt synonym edge.
- ƒê·∫∑c ƒëi·ªÉm c·ªßa synonym edge:
  - Kh√¥ng c√≥ h∆∞·ªõng: V√¨ n·∫øu hai t·ª´ l√† ƒë·ªìng nghƒ©a, m·ªëi quan h·ªá c·ªßa ch√∫ng th∆∞·ªùng l√† ƒë·ªëi x·ª©ng (v√≠ d·ª•: ‚Äúteacher‚Äù v√† ‚Äúinstructor‚Äù c√≥ th·ªÉ ƒë∆∞·ª£c n·ªëi v·ªõi nhau m√† kh√¥ng c·∫ßn ph√¢n bi·ªát th·ª© t·ª±).

---
‚ùì Ph√°: T·∫°i sao l·∫°i s·ª≠ d·ª•ng thu·∫≠t to√°n embedding cho synonym thay v√¨ d√πng LLM tr·ª±c ti·∫øp?
Tr·∫£ l·ªùi:
- Hi·ªáu qu·∫£ v√† t·ªëc ƒë·ªô:
  - T√≠nh to√°n cosine similarity gi·ªØa vector nhanh v√† hi·ªáu qu·∫£, kh√¥ng c·∫ßn g·ªçi LLM nhi·ªÅu l·∫ßn.
- ƒê·ªô ch√≠nh x√°c cao:
  - C√°c m√¥ h√¨nh embedding hi·ªán nay c√≥ kh·∫£ nƒÉng b·∫Øt ƒë∆∞·ª£c ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a r·∫•t t·ªët, cho n√™n ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a l√† kh√° ch√≠nh x√°c.
- Gi·∫£m t·∫£i cho LLM:
  - Gi·∫£m chi ph√≠ t√≠nh to√°n, v√¨ kh√¥ng c·∫ßn LLM ph·∫£i x·ª≠ l√Ω t·∫•t c·∫£ c√°c c·∫∑p t·ª´ trong corpus.

---
3. Seed Node
‚ùì Ph√°: Seed node l√† g√¨ trong b·ªëi c·∫£nh c·ªßa HippoRAG?
Tr·∫£ l·ªùi:
- Seed Node:
  - L√† nh·ªØng node trong ki·∫øn th·ª©c ƒë·ªì th·ªã ƒë∆∞·ª£c ch·ªçn l√†m ƒëi·ªÉm kh·ªüi ƒë·∫ßu cho qu√° tr√¨nh t√¨m ki·∫øm v√† lan truy·ªÅn th√¥ng tin (th√¥ng qua PPR) khi c√≥ query.
- Vai tr√≤ c·ªßa Seed Node:
  - Ch√∫ng ƒë·∫°i di·ªán cho c√°c th√†nh ph·∫ßn quan tr·ªçng li√™n quan ƒë·∫øn c√¢u h·ªèi ƒë∆∞·ª£c ƒë·∫∑t ra.
  - V√≠ d·ª•: N·∫øu c√¢u h·ªèi l√† ‚ÄúBill Gates h·ªçc ·ªü ƒë√¢u?‚Äù, c√°c node ch·ª©a ‚ÄúBill Gates‚Äù ho·∫∑c c√°c th√¥ng tin li√™n quan s·∫Ω ƒë∆∞·ª£c ch·ªçn l√†m seed.

---
‚ùì Ph√°: T·∫°i sao ph·∫£i g√°n tr·ªçng s·ªë (seed node weights) cho c√°c Seed Node?
Tr·∫£ l·ªùi:
- G√°n tr·ªçng s·ªë:
  - Gi√∫p h·ªá th·ªëng bi·∫øt ƒë∆∞·ª£c nh·ªØng node n√†o quan tr·ªçng h∆°n khi th·ª±c hi·ªán lan truy·ªÅn th√¥ng tin trong ƒë·ªì th·ªã.
- L·ª£i √≠ch:
  - C√°c Seed Node c√≥ tr·ªçng s·ªë cao s·∫Ω ‚Äúph√°t s√°ng‚Äù h∆°n trong qu√° tr√¨nh PPR, t·ª´ ƒë√≥ gi√∫p t√¨m ra c√°c node li√™n quan t·ªët h∆°n, ƒë·∫£m b·∫£o c√¢u tr·∫£ l·ªùi ƒë√∫ng v√† ƒë·∫ßy ƒë·ªß.
- V√≠ d·ª• ƒë∆°n gi·∫£n:
  - Trong m·ªôt b·∫£n ƒë·ªì t∆∞ duy, n·∫øu b·∫°n ƒë√°nh d·∫•u m·ªôt s·ªë √Ω ch√≠nh v·ªõi b√∫t d·∫°, nh·ªØng √Ω n√†y s·∫Ω ƒë∆∞·ª£c ch√∫ √Ω nhi·ªÅu h∆°n khi b·∫°n t·ªïng h·ª£p l·∫°i ki·∫øn th·ª©c.

---
4. Relation Edge v√† C√°c Kh√°i Ni·ªám Kh√°c
‚ùì Ph√°: Relation edge c√≥ h∆∞·ªõng nh∆∞ th·∫ø n√†o?
Tr·∫£ l·ªùi:
- Relation Edge:
  - L√† c√°c c·∫°nh n·ªëi gi·ªØa c√°c node trong KG, ƒë∆∞·ª£c t·∫°o ra t·ª´ c√°c triple (subject, predicate, object).
  - Ch√∫ng c√≥ h∆∞·ªõng, t·ª´ subject sang object, th√¥ng qua predicate.
- √ù nghƒ©a c·ªßa h∆∞·ªõng:
  - Gi√∫p x√°c ƒë·ªãnh ƒë√∫ng m·ªëi li√™n h·ªá ng·ªØ nghƒ©a (ai l√†m g√¨ v·ªõi g√¨).

---
‚ùì Ph√°: Ngo√†i c√°c kh√°i ni·ªám tr√™n, c√≤n kh√°i ni·ªám n√†o kh√°c c·∫ßn gi·∫£i th√≠ch chi ti·∫øt trong pipeline kh√¥ng?
Tr·∫£ l·ªùi:
- Context Edge:
  - N·ªëi c√°c Passage Node v·ªõi c√°c Phrase Node m√† ch√∫ng ch·ª©a ƒë·ª±ng.
  - √ù nghƒ©a: Gi√∫p gi·ªØ l·∫°i ng·ªØ c·∫£nh chi ti·∫øt c·ªßa vƒÉn b·∫£n, h·ªó tr·ª£ factual QA.
- Dense‚ÄìSparse Integration:
  - S·ª± k·∫øt h·ª£p gi·ªØa sparse coding (phrase node) v√† dense coding (passage node) nh·∫±m ƒë·∫£m b·∫£o v·ª´a c√≥ √Ω ch√≠nh t√≥m t·∫Øt, v·ª´a c√≥ ƒë·∫ßy ƒë·ªß chi ti·∫øt.
  - V√≠ d·ª• h·ªçc sinh: Nh∆∞ ƒë√£ n√≥i, gi·ªëng nh∆∞ v·ª´a ghi nh·ªõ ti√™u ƒë·ªÅ b√†i h·ªçc (sparse) v√† v·ª´a c√≥ to√†n b·ªô b√†i gi·∫£ng (dense).

---
K·∫æT LU·∫¨N
M·ªói kh√°i ni·ªám trong pipeline c·ªßa HippoRAG 2 ƒë·ªÅu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ b·∫£o to√†n v√† khai th√°c th√¥ng tin m·ªôt c√°ch t·ªëi ∆∞u:
- Triple extraction v·ªõi relation edge c√≥ h∆∞·ªõng gi·ªØ tr·∫≠t t·ª± v√† ng·ªØ nghƒ©a ban ƒë·∫ßu.
- Synonym edge (kh√¥ng c√≥ h∆∞·ªõng) n·ªëi c√°c c·ª•m t·ª´ c√≥ nghƒ©a t∆∞∆°ng ƒë·ªìng, gi√∫p li√™n k·∫øt th√¥ng tin gi·ªØa c√°c ƒëo·∫°n vƒÉn kh√°c nhau.
- Seed Node l√† c√°c ƒëi·ªÉm kh·ªüi ƒë·∫ßu quan tr·ªçng ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√° tr√¨nh t√¨m ki·∫øm th√¥ng tin trong KG, v√† vi·ªác g√°n tr·ªçng s·ªë cho ch√∫ng gi√∫p tƒÉng c∆∞·ªùng kh·∫£ nƒÉng lan truy·ªÅn th√¥ng tin trong qu√° tr√¨nh Personalized PageRank.
- Context edge v√† dense-sparse integration gi√∫p m√¥ h√¨nh kh√¥ng ch·ªâ ghi nh·ªõ √Ω ch√≠nh m√† c√≤n gi·ªØ nguy√™n ng·ªØ c·∫£nh chi ti·∫øt c·ªßa vƒÉn b·∫£n g·ªëc.
Qua t·ª´ng c√¢u h·ªèi ph·∫£n bi·ªán, ch√∫ng ta nh·∫≠n th·∫•y r·∫±ng vi·ªác duy tr√¨ h∆∞·ªõng trong c√°c relation edge l√† r·∫•t quan tr·ªçng cho vi·ªác x√¢y d·ª±ng m·ªôt ƒë·ªì th·ªã ki·∫øn th·ª©c ch√≠nh x√°c, t·ª´ ƒë√≥ h·ªó tr·ª£ qu√° tr√¨nh reasoning v√† retrieval trong to√†n h·ªá th·ªëng HippoRAG.
Ch√∫ng ta h√£y c√πng ph√¢n t√≠ch theo ki·ªÉu "m·ªôt ng∆∞·ªùi ph√° ‚Äì m·ªôt ng∆∞·ªùi tr·∫£ l·ªùi" ƒë·ªÉ hi·ªÉu s√¢u v·ªÅ c√°ch th√™m passage node sau khi ƒë√£ t√°ch ra b·ªô ba (triple) theo b√†i b√°o.

---
‚ùì Ph√°: Sau khi ƒë√£ s·ª≠ d·ª•ng OpenIE t√°ch ra c√°c triple (subject, predicate, object) t·ª´ m·ªói ƒëo·∫°n vƒÉn, l√†m th·∫ø n√†o ƒë·ªÉ "append" th√™m ph·∫ßn passage node v√†o ƒë·ªì th·ªã ki·∫øn th·ª©c?
Tr·∫£ l·ªùi:
 Theo b√†i b√°o, sau khi tr√≠ch xu·∫•t c√°c triple t·ª´ m·ªói passage, ta ti·∫øn h√†nh x√¢y d·ª±ng m·ªôt Knowledge Graph (KG) b·∫±ng c√°ch th·ª±c hi·ªán b∆∞·ªõc sau:
1. T·∫°o Phrase Nodes t·ª´ Triple:
  - M·ªói triple (v√≠ d·ª•: (Marie Curie, won, Nobel Prize)) s·∫Ω t·∫°o ra c√°c phrase nodes cho subject, predicate v√† object.
  - ƒê√¢y l√† c√°c n√∫t ƒë·∫°i di·ªán cho c√°c kh√°i ni·ªám tr·ª´u t∆∞·ª£ng c·ªßa th√¥ng tin.
2. Th√™m Passage Node:
  - M·ªói passage g·ªëc trong corpus ƒë∆∞·ª£c xem nh∆∞ m·ªôt passage node ri√™ng bi·ªát.
  - Sau ƒë√≥, append ph·∫ßn passage node n√†y v√†o ƒë·ªì th·ªã b·∫±ng c√°ch n·ªëi n√≥ v·ªõi t·∫•t c·∫£ c√°c phrase node ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ passage ƒë√≥.
  - C√°c k·∫øt n·ªëi n√†y th∆∞·ªùng ƒë∆∞·ª£c g·ªçi l√† context edge (ho·∫∑c edge "contains") v√¨ ch√∫ng bi·ªÉu th·ªã r·∫±ng passage ch·ª©a ƒë·ª±ng c√°c th√¥ng tin hay kh√°i ni·ªám ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi phrase nodes.

---
‚ùì Ph√°: T·∫°i sao c·∫ßn th√™m passage node? Kh√¥ng ƒë·ªß ch·ªâ c√≥ phrase nodes sao?
Tr·∫£ l·ªùi:
- B·∫£o to√†n ng·ªØ c·∫£nh:
  - Phrase nodes cung c·∫•p m·ªôt d·∫°ng tr·ª´u t∆∞·ª£ng, t·ªïng qu√°t c·ªßa th√¥ng tin. Tuy nhi√™n, th√¥ng tin n√†y c√≥ th·ªÉ m·∫•t ƒëi chi ti·∫øt ng·ªØ c·∫£nh quan tr·ªçng.
  - B·∫±ng c√°ch th√™m passage node, h·ªá th·ªëng l∆∞u gi·ªØ ƒë∆∞·ª£c to√†n b·ªô th√¥ng tin chi ti·∫øt c·ªßa ƒëo·∫°n vƒÉn g·ªëc.
- Dense‚ÄìSparse Integration:
  - Passage node ƒë·∫°i di·ªán cho ph·∫ßn dense coding (ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh, chi ti·∫øt), trong khi phrase nodes ƒë·∫°i di·ªán cho sparse coding (kh√°i qu√°t, d·ªÖ d√†ng reasoning).
  - Vi·ªác k·∫øt h·ª£p c·∫£ hai gi√∫p m√¥ h√¨nh c√≥ th·ªÉ th·ª±c hi·ªán t·ªët h∆°n trong vi·ªác tr·∫£ l·ªùi c√¢u h·ªèi y√™u c·∫ßu v·ª´a hi·ªÉu ƒë∆∞·ª£c √Ω ch√≠nh v·ª´a n·∫Øm b·∫Øt ƒë∆∞·ª£c chi ti·∫øt.

---
‚ùì Ph√°: C√°ch th·ª©c n·ªëi gi·ªØa passage node v√† phrase nodes ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o?
Tr·∫£ l·ªùi:
- M·ªói passage node s·∫Ω ƒë∆∞·ª£c n·ªëi v·ªõi t·∫•t c·∫£ c√°c phrase node ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ ch√≠nh passage ƒë√≥.
- C√°c k·∫øt n·ªëi n√†y (context edges) kh√¥ng c·∫ßn c√≥ h∆∞·ªõng r√µ r√†ng nh∆∞ c√°c relation edge c·ªßa triple, m√† ch·ªß y·∫øu nh·∫±m m·ª•c ƒë√≠ch th·ªÉ hi·ªán m·ªëi li√™n h·ªá v·ªÅ ng·ªØ c·∫£nh.
- Khi th·ª±c hi·ªán retrieval ho·∫∑c Personalized PageRank (PPR), passage node gi√∫p tƒÉng tr·ªçng s·ªë cho c√°c ƒëo·∫°n vƒÉn ch·ª©a ƒë·ª±ng th√¥ng tin chi ti·∫øt, t·ª´ ƒë√≥ c·∫£i thi·ªán kh·∫£ nƒÉng truy xu·∫•t v√† hi·ªÉu s√¢u c·ªßa m√¥ h√¨nh.

---
‚ùì Ph√°: T·∫°i sao kh√¥ng ch·ªâ d√πng phrase nodes m√† b·ªè qua passage node?
Tr·∫£ l·ªùi:
- N·∫øu ch·ªâ s·ª≠ d·ª•ng phrase nodes, ta ch·ªâ c√≥ ƒë∆∞·ª£c m·ªôt b·∫£n t√≥m t·∫Øt r·∫•t c√¥ ƒë·ªçng c·ªßa th√¥ng tin.
- Nhi·ªÅu tr∆∞·ªùng h·ª£p, ƒë·∫∑c bi·ªát l√† c√°c c√¢u h·ªèi factual hay y√™u c·∫ßu s·ª± li√™n k·∫øt s√¢u gi·ªØa c√°c √Ω, ƒë√≤i h·ªèi ph·∫£i c√≥ ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß t·ª´ ƒëo·∫°n vƒÉn g·ªëc.
- Passage node gi√∫p m√¥ h√¨nh truy xu·∫•t c√°c ƒëo·∫°n vƒÉn g·ªëc ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng m·∫•t ƒëi c√°c chi ti·∫øt c·∫ßn thi·∫øt cho vi·ªác tr·∫£ l·ªùi ch√≠nh x√°c.

---
T·ªïng K·∫øt
- B∆∞·ªõc 1: T√°ch c√°c triple (subject, predicate, object) t·ª´ m·ªói passage b·∫±ng OpenIE c·ªßa LLM ‚Üí t·∫°o ra c√°c phrase nodes.
- B∆∞·ªõc 2: V·ªõi m·ªói passage g·ªëc, t·∫°o passage node.
- B∆∞·ªõc 3: Append passage node v√†o ƒë·ªì th·ªã b·∫±ng c√°ch n·ªëi n√≥ v·ªõi t·∫•t c·∫£ c√°c phrase node t·ª´ passage ƒë√≥ th√¥ng qua context edges.
- M·ª•c ƒë√≠ch: Gi·ªØ l·∫°i c·∫£ √Ω ch√≠nh (phrase node) v√† th√¥ng tin chi ti·∫øt (passage node) ƒë·ªÉ h·ªó tr·ª£ qu√° tr√¨nh retrieval v√† reasoning, t·ª´ ƒë√≥ c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa to√†n h·ªá th·ªëng.
D∆∞·ªõi ƒë√¢y l√† c√°ch ti·∫øp c·∫≠n chi ti·∫øt theo d·∫°ng h·ªèi ‚Äì ƒë√°p ph·∫£n bi·ªán (m·ªôt ng∆∞·ªùi ‚Äúph√°‚Äù, m·ªôt ng∆∞·ªùi ‚Äúb√™nh‚Äù) ƒë·ªÉ m·ªï x·∫ª th·∫≠t s√¢u 5 b∆∞·ªõc trong Online Retrieval & QA c·ªßa HippoRAG/HippoRAG 2. M·ªói c√¢u tr·∫£ l·ªùi ƒë·ªÅu c·ªë g·∫Øng g·∫Øn v·ªõi l·∫≠p lu·∫≠n t·ª´ b√†i b√°o ho·∫∑c √Ω t∆∞·ªüng ch√≠nh (c√≥ d·∫´n ch·ª©ng).

---
(1) Retrieving Passages and Triples
Ph·∫£n bi·ªán (Q):
T·∫°i sao ph·∫£i l·∫•y c·∫£ c√°c ƒëo·∫°n vƒÉn (passages) v√† c√°c b·ªô ba (triples)? Li·ªáu ch·ªâ c·∫ßn retrieval embedding t·ªët ƒë·ªÉ l·∫•y ƒëo·∫°n vƒÉn ƒë√£ ƒë·ªß ch∆∞a?
Tr·∫£ l·ªùi (A):
- C·∫ßn c·∫£ passages: v√¨ c√°c ƒëo·∫°n vƒÉn mang ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh. N·∫øu ch·ªâ d·ª±a v√†o node (th∆∞·ªùng l√† m·ªôt c√¢u/triple r·ªùi r·∫°c) th√¨ c√≥ th·ªÉ thi·∫øu m·∫•t chi ti·∫øt quan tr·ªçng.
- C·∫ßn c·∫£ triples: v√¨ triple l√† ‚Äúƒë∆°n v·ªã th√¥ng tin‚Äù mang t√≠nh kh√°i qu√°t ‚Äì gi√∫p m√¥ ph·ªèng s·ª± li√™n k·∫øt (associativity). Khi ta k√©o ƒë∆∞·ª£c triple li√™n quan, n√≥ c√≥ th·ªÉ ‚Äúg·ª£i √Ω‚Äù nhi·ªÅu quan h·ªá s√¢u h∆°n m√† m·ªôt ƒëo·∫°n vƒÉn ch∆∞a ch·∫Øc ƒë√£ hi·ªÉn th·ªã ngay.
- D·∫´n ch·ª©ng: Ph·∫ßn ‚ÄúOnline Retrieval‚Äù trong b√†i HippoRAG/HippoRAG 2 cho th·∫•y, h·ªá th·ªëng ch·∫°y query-to-triple ƒë·ªÉ l·∫•y triple tr∆∞·ªõc, sau ƒë√≥ m·ªõi ch·∫°y ti·∫øp c√°c b∆∞·ªõc kh√°c. K·∫øt h·ª£p ‚Äúpassage node‚Äù v√† ‚Äúphrase node‚Äù (triple) trong ƒë·ªì th·ªã l√† c√°ch m√† m√¥ h√¨nh ‚Äútr·ªôn‚Äù gi·ªØa ‚Äúcontext‚Äù v√† ‚Äúconcepts‚Äù.

---
(2) Recognition Memory (Triple Filtering)
Ph·∫£n bi·ªán (Q):
T·∫°i sao l·∫°i c·∫ßn m·ªôt b∆∞·ªõc g·ªçi l√† ‚ÄúRecognition Memory‚Äù ƒë·ªÉ l·ªçc triple? Ch·∫≥ng ph·∫£i retrieval encoder ƒë√£ x·∫øp h·∫°ng ƒë∆∞·ª£c r·ªìi sao?
Tr·∫£ l·ªùi (A):
- Encoder-based ranking ch·ªâ d·ª±a tr√™n vector similarity, c√≥ th·ªÉ nh·∫∑t ph·∫£i nh·ªØng triple ‚Äúna n√°‚Äù v·ªÅ t·ª´ v·ª±ng nh∆∞ng kh√¥ng th·∫≠t s·ª± li√™n quan ng·ªØ nghƒ©a.
- LLM-based filtering (Recognition Memory) gi·ªëng nh∆∞ ‚Äúduy·ªát l·∫°i‚Äù nh·ªØng triple top-k, ƒë·ªÉ x√°c nh·∫≠n: ‚ÄúTriple n√†y th·ª±c s·ª± n√≥i v·ªÅ c√°i g√¨, c√≥ ƒÉn kh·ªõp v·ªõi c√¢u h·ªèi kh√¥ng?‚Äù ‚Äì T·ª´ ƒë√≥ lo·∫°i b·ªõt nhi·ªÖu.
- V√≠ d·ª•: C√≥ nh·ªØng triple m√† entity gi·ªëng nhau v·ªÅ m·∫∑t ng√¥n ng·ªØ (v√≠ d·ª• ‚ÄúParis‚Äù ‚Äì m·ªôt ng∆∞·ªùi t√™n Paris vs. th√†nh ph·ªë Paris). N·∫øu ch·ªâ d√πng embedding, ƒë√¥i khi b·ªã l·∫´n. B∆∞·ªõc filtering n√†y gi√∫p LLM d√πng ng·ªØ c·∫£nh ƒë·ªÉ ph√¢n bi·ªát.
- D·∫´n ch·ª©ng: B√†i b√°o gi·ªõi thi·ªáu c∆° ch·∫ø ‚Äútwo-step retrieval‚Äù: 1) Query-to-triple (embedding) v√† 2) Triple filter (LLM) ‚Äì m√¥ ph·ªèng ‚Äúrecall vs. recognition‚Äù trong tr√≠ nh·ªõ con ng∆∞·ªùi.

---
(3) Assigning Seed Node Weights
Ph·∫£n bi·ªán (Q):
Ph√¢n ph·ªëi tr·ªçng s·ªë (reset probability) cho c√°c ‚Äúseed node‚Äù ‚Äì ƒë·∫∑c bi·ªát l√† gi·ªØa ‚Äúpassage node‚Äù v√† ‚Äúphrase node‚Äù ‚Äì c√≥ th·ª±c s·ª± c·∫ßn thi·∫øt, hay ch·ªâ c·∫ßn g√°n ƒë·ªÅu l√† ƒë∆∞·ª£c?
Tr·∫£ l·ªùi (A):
- Kh√¥ng th·ªÉ g√°n ƒë·ªÅu: B·ªüi v√¨ ‚Äúphrase node‚Äù (c√°c kh√°i ni·ªám) v√† ‚Äúpassage node‚Äù (c√°c vƒÉn b·∫£n) ƒë√≥ng vai tr√≤ kh√°c nhau. N·∫øu g√°n tr·ªçng s·ªë gi·ªëng nhau, m√¥ h√¨nh c√≥ th·ªÉ nghi√™ng qu√° nhi·ªÅu v·ªÅ concept ho·∫∑c qu√° nhi·ªÅu v·ªÅ context.
- C·∫ßn c√¢n b·∫±ng: Ch·∫≥ng h·∫°n, khi c√¢u h·ªèi r·∫•t c·ª• th·ªÉ (v√≠ d·ª•: ‚ÄúT√°c gi·∫£ cu·ªën s√°ch X l√† ai?‚Äù), ‚Äúphrase node‚Äù v·ªÅ ‚Äúbook_author‚Äù c√≥ th·ªÉ quan tr·ªçng h∆°n. Nh∆∞ng n·∫øu c√¢u h·ªèi ph·ª©c t·∫°p, ‚Äúpassage node‚Äù c√≥ ng·ªØ c·∫£nh r·ªông h∆°n s·∫Ω c·∫ßn tr·ªçng s·ªë cao h∆°n ƒë·ªÉ PPR lan t·ªèa nhi·ªÅu h∆°n.
- D·∫´n ch·ª©ng: T√°c gi·∫£ c√≥ l√†m ablation study (v√≠ d·ª• trong ph·∫ßn Implementation & Discussion) ƒë·ªÉ ch·ª©ng minh ph·∫£i ch·ªçn ‚Äúweight factor‚Äù cho passage node h·ª£p l√Ω (0.01, 0.05, v.v.) th√¨ m·ªõi t·ªëi ∆∞u Recall@5.

---
(4) PPR Graph Search
Ph·∫£n bi·ªán (Q):
Personalized PageRank (PPR) th·ª±c s·ª± gi√∫p ƒë∆∞·ª£c g√¨? Li·ªáu m·ªôt v√≤ng l·∫∑p iterative retrieval (nh∆∞ IRCoT) kh√¥ng ph·∫£i ƒë√£ gi·∫£i quy·∫øt ƒë∆∞·ª£c multi-hop sao?
Tr·∫£ l·ªùi (A):
- PPR l√†m multi-hop ngay trong m·ªôt l∆∞·ª£t: Thay v√¨ ph·∫£i g·ªçi LLM li√™n ti·∫øp ƒë·ªÉ ‚Äúg·ª£i √Ω‚Äù truy v·∫•n ph·ª• (iterative retrieval), PPR cho ph√©p khu·∫øch t√°n x√°c su·∫•t t·ª´ c√°c seed node (passage/phrase) ƒë·∫øn l√¢n c·∫≠n trong KG.
- Hi·ªáu qu·∫£ chi ph√≠: Iterative retrieval ki·ªÉu IRCoT th∆∞·ªùng t·ªën nhi·ªÅu l∆∞·ª£t g·ªçi LLM. PPR l√† m·ªôt thu·∫≠t to√°n ƒë·ªì th·ªã ch·∫°y n·ªôi b·ªô, r·∫ª h∆°n v·ªÅ m·∫∑t inference.
- Ph√π h·ª£p v·ªõi ‚Äúassociative memory‚Äù: B√†i b√°o g·ª£i √Ω r·∫±ng PPR m√¥ ph·ªèng c√°ch hippocampus (n√£o b·ªô) k√≠ch ho·∫°t c√°c li√™n k·∫øt gi·ªØa kh√°i ni·ªám. T·ª´ 1 seed node, lan ra nhi·ªÅu n√∫t kh√°c theo x√°c su·∫•t.
- D·∫´n ch·ª©ng: HippoRAG/HippoRAG 2 so s√°nh v·ªõi IRCoT, cho th·∫•y PPR c√≥ th·ªÉ ‚Äúg√≥i‚Äù vi·ªác multi-hop reasoning trong m·ªôt l·∫ßn retrieval, nhanh h∆°n v√† √≠t t·ªën chi ph√≠ h∆°n.

---
(5) QA Reading with Selected Passages
Ph·∫£n bi·ªán (Q):
N·∫øu cu·ªëi c√πng v·∫´n ph·∫£i g·ªçi LLM ƒë·ªÉ ƒë·ªçc l·∫°i c√°c ƒëo·∫°n vƒÉn (QA reading), v·∫≠y vi·ªác x√¢y d·ª±ng KG hay PPR c√≥ th·∫≠t s·ª± quan tr·ªçng? Ch·∫≥ng ph·∫£i LLM gi·ªù ƒë√£ ‚Äús·∫µn s√†ng‚Äù tr·∫£ l·ªùi n·∫øu cho n√≥ ƒë·ªß context?
Tr·∫£ l·ªùi (A):
- LLM c·∫ßn ‚Äúƒë·∫ßu v√†o s·∫°ch‚Äù: D√π LLM m·∫°nh, nh∆∞ng n·∫øu input context b·ªã nhi·ªÖu (ch·ªçn sai passage) ho·∫∑c qu√° d√†n tr·∫£i, LLM d·ªÖ tr·∫£ l·ªùi sai, ƒë·∫∑c bi·ªát v·ªõi c√¢u h·ªèi c·∫ßn multi-hop.
- KG + PPR gi√∫p ‚Äút√≥m g·ªçn‚Äù ƒë√∫ng ƒëo·∫°n vƒÉn li√™n quan, thay v√¨ l√¥i t·∫•t c·∫£. LLM c√†ng c√≥ context ch√≠nh x√°c, tr·∫£ l·ªùi c√†ng t·ªët.
- T√≠nh m·ªü r·ªông: Khi t·∫≠p d·ªØ li·ªáu r·∫•t l·ªõn, ch·ªâ embedding retrieval c√≥ th·ªÉ b·ªè s√≥t ‚Äúchu·ªói logic‚Äù li√™n quan. B·ªï sung KG + PPR => M√¥ h√¨nh ‚Äúli√™n k·∫øt‚Äù t·ªët h∆°n, LLM ƒë·ª° b·ªã qu√° t·∫£i.
- D·∫´n ch·ª©ng: K·∫øt qu·∫£ th·ª±c nghi·ªám (b·∫£ng so s√°nh F1, EM) cho th·∫•y HippoRAG 2 sau khi ch·ªçn top passages qua PPR, r·ªìi cho LLM ƒë·ªçc, cho k·∫øt qu·∫£ cao h∆°n so v·ªõi retrieval th∆∞·ªùng.

---
T·ªïng k·∫øt
5 b∆∞·ªõc trong Online Retrieval & QA ƒë∆∞·ª£c thi·∫øt k·∫ø c√≥ ch·ªß ƒë√≠ch, m√¥ ph·ªèng c∆° ch·∫ø b·ªô nh·ªõ c·ªßa con ng∆∞·ªùi:
1. L·∫•y passage + triple song song (ƒë·∫£m b·∫£o v·ª´a c√≥ context v·ª´a c√≥ concept).
2. L·ªçc triple b·∫±ng LLM (Recognition Memory) ƒë·ªÉ lo·∫°i nhi·ªÖu.
3. G√°n tr·ªçng s·ªë seed node c·∫©n th·∫≠n (c√¢n b·∫±ng passage node v√† phrase node).
4. Ch·∫°y PPR ƒë·ªÉ th·ª±c hi·ªán multi-hop reasoning trong m·ªôt b∆∞·ªõc, ti·∫øt ki·ªám chi ph√≠ so v·ªõi iterative retrieval.
5. Cu·ªëi c√πng, LLM m·ªõi ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë√£ ch·ªçn ƒë·ªÉ tr·∫£ l·ªùi, t·ªëi ∆∞u ƒë·ªô ch√≠nh x√°c.
T·ª´ng b∆∞·ªõc ƒë·ªÅu c√≥ l√Ω do t·ªìn t·∫°i, v√† k·∫øt qu·∫£ th·ª±c nghi·ªám ch·ª©ng minh m√¥ h√¨nh n√†y tƒÉng hi·ªáu qu·∫£ truy xu·∫•t (retrieval) c≈©ng nh∆∞ ƒë·ªô ch√≠nh x√°c khi tr·∫£ l·ªùi c√¢u h·ªèi (QA).

---
(3) Assigning Seed Node Weights
ƒê·∫∑t c√¢u h·ªèi ph√° nhi·ªÅu h∆°n ·ªü b∆∞·ªõc n√†y 
- Ch·∫≥ng h·∫°n: l√∫c m√† x√¢y Graph c√°c nodes vai tr√≤ nh∆∞ nhau hay l√† Hippo RAG 2 c√≥ ƒë√°nh d·∫•u r√µ l√† phrase node hay passage node ? 
- C√°ch ƒë√°nh Assing Seed Node Weights nh∆∞ n√†o trong b√†i b√°o c√≥ n√≥i ko 
- ... nhi·ªÅu c√¢u n·ªØa nh√©

√Ω l√† ph√° 1, m√¨nh bi·∫øt l√† n√≥ ƒë∆∞·ª£c r√µ r√†ng vai tr√≤, nh∆∞ng l√∫c ƒë∆∞a l√™n ƒë·ªì th·ªã √Ω th√¨ n√≥ c√≥ ƒë∆∞·ª£c r√µ r√†ng v·∫≠y kh√¥ng ? ki·ªÉu h·ªç d√πng c√°ch n√†o ƒë·ªÉ ƒë√°nh d·∫•u 2 lo·∫°i node ƒë√≥ ƒë·ªÉ khi query d√πng

Trong qu√° tr√¨nh x√¢y d·ª±ng ƒë·ªì th·ªã, HippoRAG 2 kh√¥ng ch·ªâ tr√≠ch xu·∫•t c√°c node m√† c√≤n "ƒë√°nh d·∫•u" r√µ r√†ng t·ª´ng lo·∫°i node th√¥ng qua c·∫•u tr√∫c v√† c√°c lo·∫°i c·∫°nh n·ªëi k·∫øt:
- Ph√¢n lo·∫°i qua ngu·ªìn g·ªëc t·∫°o node:
  - Phrase node: ƒê∆∞·ª£c t·∫°o ra t·ª´ vi·ªác tr√≠ch xu·∫•t b·ªô ba (triple) c·ªßa c√°c kh√°i ni·ªám qua OpenIE. Nh·ªØng node n√†y ch·ªâ ch·ª©a th√¥ng tin "concept" r√∫t g·ªçn v√† li√™n quan ƒë·∫øn c√°c m·ªëi quan h·ªá gi·ªØa th·ª±c th·ªÉ.
  - Passage node: ƒê·∫°i di·ªán cho to√†n b·ªô ƒëo·∫°n vƒÉn, ch·ª©a ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh v√† th√¥ng tin chi ti·∫øt.
 Qua ƒë√≥, qu√° tr√¨nh offline indexing c·ªßa HippoRAG 2 ƒë√£ ph√¢n bi·ªát r√µ r√†ng ngu·ªìn g·ªëc c·ªßa c√°c node.
- ƒê√°nh d·∫•u qua lo·∫°i c·∫°nh (edge type):
  - C√°c passage node ƒë∆∞·ª£c li√™n k·∫øt v·ªõi c√°c phrase node th√¥ng qua c√°c c·∫°nh mang nh√£n "contains" (ho·∫∑c "context edge"), nh·∫±m th·ªÉ hi·ªán r·∫±ng ƒëo·∫°n vƒÉn ch·ª©a c√°c c·ª•m t·ª´ (phrase) ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ n√≥.
  - Trong khi ƒë√≥, c√°c phrase node c√≥ th·ªÉ ƒë∆∞·ª£c n·ªëi v·ªõi nhau qua c√°c c·∫°nh kh√°c nh∆∞ "synonym edge" hay "relation edge". Vi·ªác n√†y gi√∫p nh·∫•n m·∫°nh m·ªëi li√™n h·ªá gi·ªØa c√°c kh√°i ni·ªám tr·ª´u t∆∞·ª£ng, t√°ch bi·ªát v·ªõi th√¥ng tin ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß t·ª´ passage.
- T√°c ƒë·ªông khi query:
 Khi m·ªôt truy v·∫•n ƒë∆∞·ª£c ƒë∆∞a v√†o, h·ªá th·ªëng s·ª≠ d·ª•ng c√°c embedding v√† tr·ªçng s·ªë ri√™ng bi·ªát cho t·ª´ng lo·∫°i node. Nh·ªù v·∫≠y, vi·ªác so s√°nh cosine similarity ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n c·∫£ phrase node v√† passage node theo c√°ch ph√π h·ª£p v·ªõi ƒë·∫∑c tr∆∞ng c·ªßa ch√∫ng. C√°ch g√°n tr·ªçng s·ªë (seed node weights) c≈©ng ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ri√™ng cho t·ª´ng lo·∫°i, gi√∫p qu√° tr√¨nh Personalized PageRank lan t·ªèa th√¥ng tin ƒë√∫ng theo c·∫•u tr√∫c ƒë√£ ƒë∆∞·ª£c ƒë√°nh d·∫•u.
D·∫´n ch·ª©ng: Trong ph·∫ßn m√¥ t·∫£ c·ªßa HippoRAG 2, t√°c gi·∫£ tr√¨nh b√†y r·∫±ng m·ªói passage trong corpus ƒë∆∞·ª£c x·ª≠ l√Ω nh∆∞ m·ªôt passage node v√† ƒë∆∞·ª£c n·ªëi v·ªõi t·∫•t c·∫£ c√°c phrase ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ n√≥ th√¥ng qua c√°c "context edge". ƒêi·ªÅu n√†y kh√¥ng ch·ªâ gi√∫p x√¢y d·ª±ng m·ªôt ki·∫øn tr√∫c ƒë·ªì th·ªã r√µ r√†ng m√† c√≤n cho ph√©p h·ªá th·ªëng ph√¢n bi·ªát v√† √°p d·ª•ng tr·ªçng s·ªë kh√°c nhau cho m·ªói lo·∫°i node trong qu√° tr√¨nh truy xu·∫•t.
Nh∆∞ v·∫≠y, b·∫±ng c√°ch ph√¢n lo·∫°i qua ngu·ªìn g·ªëc t·∫°o node v√† ƒë√°nh d·∫•u th√¥ng qua c√°c lo·∫°i c·∫°nh n·ªëi k·∫øt, HippoRAG 2 ƒë·∫£m b·∫£o r·∫±ng trong qu√° tr√¨nh truy v·∫•n, h·ªá th·ªëng c√≥ th·ªÉ nh·∫≠n di·ªán v√† t·∫≠n d·ª•ng ƒë√∫ng vai tr√≤ c·ªßa t·ª´ng lo·∫°i node, t·ª´ ƒë√≥ c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa retrieval v√† QA.

D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• chi ti·∫øt, k√®m theo ƒë·ªëi tho·∫°i ‚Äúph·∫£n bi·ªán ‚Äì tr·∫£ l·ªùi‚Äù minh h·ªça r√µ r√†ng c√°ch Assigning Seed Node Weights ho·∫°t ƒë·ªông trong qu√° tr√¨nh Online Retrieval c·ªßa HippoRAG.

---
V√≠ d·ª• c·ª• th·ªÉ
Truy v·∫•n: "T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?"
Sau khi h·ªá th·ªëng x·ª≠ l√Ω truy v·∫•n, ta thu ƒë∆∞·ª£c c√°c th√†nh ph·∫ßn sau:
1. Tr√≠ch xu·∫•t t·ª´ truy v·∫•n (Query Extraction):
  - C√°c th·ª±c th·ªÉ ƒë∆∞·ª£c tr√≠ch xu·∫•t:
    - ‚ÄúThe Great Novel‚Äù
    - ‚Äút√°c gi·∫£‚Äù
2. Seed Nodes ƒë∆∞·ª£c x√°c ƒë·ªãnh trong Knowledge Graph (KG):
  - Phrase Nodes (ƒë·∫°i di·ªán cho kh√°i ni·ªám, th√¥ng tin tr√≠ch xu·∫•t t·ª´ c√°c triple):
    - Node A: "t√°c gi·∫£"
      - Gi√° tr·ªã similarity (v·ªõi "t√°c gi·∫£" truy v·∫•n): 0.72
    - Node B: "The Great Novel"
      - Gi√° tr·ªã similarity (v·ªõi ti√™u ƒë·ªÅ truy v·∫•n): 0.68
  - Passage Nodes (ƒëo·∫°n vƒÉn cung c·∫•p ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß):
    - Node C: ƒêo·∫°n vƒÉn 1: ‚ÄúThe Great Novel, m·ªôt t√°c ph·∫©m kinh ƒëi·ªÉn, ƒë∆∞·ª£c vi·∫øt b·ªüi John Doe ‚Äì t√°c gi·∫£ n·ªïi ti·∫øng v·ªõi nhi·ªÅu gi·∫£i th∆∞·ªüng vƒÉn h·ªçc."
      - Gi√° tr·ªã similarity (v·ªõi truy v·∫•n, d·ª±a tr√™n c·∫£ ng·ªØ c·∫£nh v√† t·ª´ kh√≥a): 0.80
    - Node D: ƒêo·∫°n vƒÉn 2: M·ªôt ƒëo·∫°n vƒÉn kh√°c n√≥i v·ªÅ John Doe nh∆∞ng kh√¥ng ƒë·ªÅ c·∫≠p r√µ r√†ng ƒë·∫øn cu·ªën s√°ch.
      - Gi√° tr·ªã similarity: 0.55

---
B∆∞·ªõc 1: X√°c ƒë·ªãnh Seed Nodes v√† G√°n Tr·ªçng S·ªë Ban ƒë·∫ßu
Gi·∫£ s·ª≠ h·ªá th·ªëng quy ∆∞·ªõc:
- Phrase Nodes nh·∫≠n tr·ªçng s·ªë m·∫∑c ƒë·ªãnh = 1.0
- Passage Nodes ban ƒë·∫ßu nh·∫≠n tr·ªçng s·ªë th·∫•p h∆°n, v√¨ ch√∫ng mang th√¥ng tin c·ª• th·ªÉ nh∆∞ng c·∫ßn ƒë∆∞·ª£c ‚Äúƒëi·ªÅu ch·ªânh‚Äù qua weight factor.
  - V√≠ d·ª•: h·ªá th·ªëng g√°n weight factor cho Passage Nodes = 0.05.
Do ƒë√≥, ta c√≥:
- Node A ("t√°c gi·∫£"):
  - Similarity score = 0.72 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.72 √ó 1.0 = 0.72
- Node B ("The Great Novel"):
  - Similarity score = 0.68 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.68 √ó 1.0 = 0.68
- Node C (Passage 1):
  - Similarity score = 0.80 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.80 √ó 0.05 = 0.04
- Node D (Passage 2):
  - Similarity score = 0.55 ‚Üí tr·ªçng s·ªë ban ƒë·∫ßu = 0.55 √ó 0.05 = 0.0275

---
B∆∞·ªõc 2: ƒê·ªëi tho·∫°i ph·∫£n bi·ªán ‚Äì Gi·∫£i th√≠ch chi ti·∫øt
Ph·∫£n bi·ªán 1: T·∫°i sao Passage Nodes l·∫°i ƒë∆∞·ª£c nh√¢n v·ªõi weight factor th·∫•p (0.05) trong khi Phrase Nodes gi·ªØ nguy√™n tr·ªçng s·ªë 1.0?
Q: N·∫øu Passage Nodes mang to√†n b·ªô ng·ªØ c·∫£nh c·ªßa ƒëo·∫°n vƒÉn, t·∫°i sao ch√∫ng l·∫°i ƒë∆∞·ª£c g√°n tr·ªçng s·ªë th·∫•p h∆°n so v·ªõi Phrase Nodes?
A:
- L√Ω do:
  - Phrase Nodes ƒë·∫°i di·ªán cho c√°c kh√°i ni·ªám tr·ª´u t∆∞·ª£ng, th∆∞·ªùng c√≥ t√≠nh kh√°i qu√°t v√† c√≥ th·ªÉ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn truy v·∫•n. Ch√∫ng c·∫ßn ƒë∆∞·ª£c ∆∞u ti√™n cao ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng c√°c kh√°i ni·ªám ch√≠nh ƒë∆∞·ª£c nh·∫≠n di·ªán.
  - Passage Nodes m·∫∑c d√π ch·ª©a ng·ªØ c·∫£nh chi ti·∫øt, nh∆∞ng n·∫øu g√°n tr·ªçng s·ªë qu√° cao, th√¥ng tin qu√° ‚Äúph√¢n t√°n‚Äù c√≥ th·ªÉ l√†m nhi·ªÖu k·∫øt qu·∫£ retrieval. Weight factor th·∫•p gi√∫p ki·ªÉm so√°t s·ª± lan t·ªèa c·ªßa th√¥ng tin c·ª• th·ªÉ, ch·ªâ khi m√† c√°c passage c√≥ nhi·ªÅu li√™n k·∫øt v·ªõi c√°c Phrase Nodes, ch√∫ng m·ªõi ‚Äúƒë∆∞·ª£c b√π ƒë·∫Øp‚Äù th√¥ng qua qu√° tr√¨nh PPR.
- D·∫´n ch·ª©ng th·ª±c nghi·ªám:
  - B√†i b√°o tr√¨nh b√†y c√°c th√≠ nghi·ªám ablation v·ªõi c√°c gi√° tr·ªã weight factor kh√°c nhau (v√≠ d·ª•: 0.01, 0.05, 0.1,...) cho Passage Nodes. K·∫øt qu·∫£ cho th·∫•y, khi ch·ªçn weight factor ph√π h·ª£p (0.05 trong v√≠ d·ª• n√†y), h·ªá th·ªëng ƒë·∫°t Recall@5 t·ªëi ∆∞u. ƒêi·ªÅu n√†y ch·ª©ng t·ªè r·∫±ng vi·ªác c√¢n b·∫±ng gi·ªØa kh√°i ni·ªám v√† ng·ªØ c·∫£nh l√† r·∫•t quan tr·ªçng.

---
Ph·∫£n bi·ªán 2: Vi·ªác nh√¢n weight factor ·∫£nh h∆∞·ªüng nh∆∞ th·∫ø n√†o ƒë·∫øn k·∫øt qu·∫£ ban ƒë·∫ßu c·ªßa PPR?
Q: V·ªõi c√°c tr·ªçng s·ªë ban ƒë·∫ßu nh∆∞ tr√™n, l√†m th·∫ø n√†o ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng Passage Node C ‚Äì m·∫∑c d√π c√≥ similarity score cao (0.80) ‚Äì c√≥ th·ªÉ c·∫°nh tranh ƒë∆∞·ª£c v·ªõi Phrase Nodes khi weight factor th·∫•p?
A:
- Gi·∫£i th√≠ch:
  - Ban ƒë·∫ßu, Passage Node C c√≥ gi√° tr·ªã tr·ªçng s·ªë ch·ªâ l√† 0.04, so v·ªõi 0.72 (Node A) v√† 0.68 (Node B).
  - Tuy nhi√™n, trong qu√° tr√¨nh PPR, tr·ªçng s·ªë n√†y s·∫Ω ƒë∆∞·ª£c ‚Äúlan t·ªèa‚Äù qua c√°c c·∫°nh trong Knowledge Graph. N·∫øu Passage Node C ƒë∆∞·ª£c li√™n k·∫øt m·∫°nh m·∫Ω v·ªõi c√°c Phrase Nodes c√≥ tr·ªçng s·ªë cao, c√°c c·∫°nh li√™n k·∫øt n√†y s·∫Ω gi√∫p ‚Äúk√©o‚Äù th√™m probability mass cho Node C.
  - V√≠ d·ª•: n·∫øu Node C c√≥ nhi·ªÅu li√™n k·∫øt ƒë·∫øn Node A v√† Node B, qu√° tr√¨nh PPR s·∫Ω c·ªông d·ªìn th√™m gi√° tr·ªã cho Node C, gi√∫p n√≥ ƒë∆∞·ª£c ∆∞u ti√™n khi x·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn cu·ªëi c√πng.
- D·∫´n ch·ª©ng:
  - B√†i b√°o cho bi·∫øt r·∫±ng k·∫øt h·ª£p gi·ªØa distribution t·ª´ PPR v√† ma tr·∫≠n xu·∫•t hi·ªán (P matrix) cho ph√©p c√°c passage nh·∫≠n ƒë∆∞·ª£c ƒëi·ªÉm t·ªïng h·ª£p cao h∆°n, n·∫øu ch√∫ng ƒë∆∞·ª£c li√™n k·∫øt ch·∫∑t ch·∫Ω v·ªõi c√°c Phrase Nodes c√≥ tr·ªçng s·ªë ban ƒë·∫ßu cao.

---
Ph·∫£n bi·ªán 3: Gi·∫£ s·ª≠ c√≥ m·ªôt Passage Node D c√≥ similarity score th·∫•p (0.55 ‚Üí 0.0275), li·ªáu n√≥ c√≥ b·ªã lo·∫°i b·ªè ho√†n to√†n hay v·∫´n c√≥ vai tr√≤?
Q: Passage Node D c√≥ ƒëi·ªÉm ban ƒë·∫ßu r·∫•t th·∫•p, nh∆∞ng li·ªáu n√≥ c√≥ ƒë√≥ng g√≥p hay kh√¥ng trong qu√° tr√¨nh retrieval?
A:
- Gi·∫£i th√≠ch:
  - M·∫∑c d√π Passage Node D c√≥ similarity score th·∫•p v√† tr·ªçng s·ªë ban ƒë·∫ßu ch·ªâ l√† 0.0275, n·∫øu n√≥ c√≥ m·ªôt s·ªë li√™n k·∫øt h·ªØu √≠ch trong KG, ƒë·∫∑c bi·ªát l√† v·ªõi c√°c Phrase Nodes c√≥ ƒëi·ªÉm cao, qu√° tr√¨nh PPR c√≥ th·ªÉ ‚Äún√¢ng ƒë·ª°‚Äù gi√° tr·ªã c·ªßa n√≥ qua vi·ªác c·ªông d·ªìn t·ª´ c√°c n√∫t l√¢n c·∫≠n.
  - Tuy nhi√™n, n·∫øu kh√¥ng c√≥ li√™n k·∫øt m·∫°nh m·∫Ω n√†o, Node D c√≥ th·ªÉ kh√¥ng ƒë√≥ng g√≥p nhi·ªÅu v√† s·∫Ω ƒë∆∞·ª£c x·∫øp h·∫°ng th·∫•p. ƒêi·ªÅu n√†y gi√∫p h·ªá th·ªëng l·ªçc ra c√°c passage kh√¥ng ƒë·ªß li√™n quan.
- D·∫´n ch·ª©ng:
  - K·∫øt qu·∫£ th·ª±c nghi·ªám v√† ablation study trong b√†i b√°o ch·ªâ ra r·∫±ng nh·ªØng passage kh√¥ng ƒë∆∞·ª£c li√™n k·∫øt ch·∫∑t ch·∫Ω s·∫Ω nh·∫≠n ƒëi·ªÉm th·∫•p sau PPR, t·ª´ ƒë√≥ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn k·∫øt qu·∫£ cu·ªëi c√πng.

---
B∆∞·ªõc 3: T√≠ch h·ª£p qua PPR
Sau khi c√°c seed node ƒë∆∞·ª£c g√°n tr·ªçng s·ªë, h·ªá th·ªëng ch·∫°y thu·∫≠t to√°n Personalized PageRank tr√™n KG. Qu√° tr√¨nh n√†y s·∫Ω:
- Lan t·ªèa probability mass: T·ª´ c√°c seed node ban ƒë·∫ßu, gi√° tr·ªã s·∫Ω ƒë∆∞·ª£c lan t·ªèa qua c√°c c·∫°nh li√™n k·∫øt.
- K·∫øt h·ª£p v·ªõi P matrix: Sau ƒë√≥, probability mass ƒë∆∞·ª£c nh√¢n v·ªõi ma tr·∫≠n P ch·ª©a th√¥ng tin t·∫ßn s·ªë xu·∫•t hi·ªán c·ªßa c√°c n√∫t trong c√°c ƒëo·∫°n vƒÉn, gi√∫p x√°c ƒë·ªãnh th·ª© h·∫°ng cu·ªëi c√πng cho c√°c passage.
K·∫øt qu·∫£ cu·ªëi c√πng, m·∫∑c d√π ban ƒë·∫ßu Passage Node C ch·ªâ c√≥ tr·ªçng s·ªë 0.04, nh∆∞ng n·∫øu li√™n k·∫øt t·ªët, n√≥ c√≥ th·ªÉ ƒë∆∞·ª£c ‚Äún√¢ng‚Äù l√™n v√† x·∫øp h·∫°ng cao, gi√∫p h·ªá th·ªëng truy xu·∫•t ƒëo·∫°n vƒÉn ch·ª©a th√¥ng tin ‚ÄúJohn Doe l√† t√°c gi·∫£ c·ªßa The Great Novel‚Äù m·ªôt c√°ch ch√≠nh x√°c.

---
T·ªïng k·∫øt v√≠ d·ª•
1. Tr√≠ch xu·∫•t v√† x√°c ƒë·ªãnh Seed Nodes:
  - Phrase Nodes: "t√°c gi·∫£" (0.72), "The Great Novel" (0.68)
  - Passage Nodes: Passage 1 (0.80 ‚Üí 0.04 sau nh√¢n), Passage 2 (0.55 ‚Üí 0.0275)
2. G√°n tr·ªçng s·ªë:
  - Phrase Nodes gi·ªØ tr·ªçng s·ªë ƒë·∫ßy ƒë·ªß (1.0)
  - Passage Nodes nh√¢n v·ªõi weight factor (0.05)
3. Qu√° tr√¨nh PPR:
  - Lan t·ªèa probability mass t·ª´ c√°c seed node qua c√°c c·∫°nh c·ªßa KG.
  - C√°c Passage Nodes, m·∫∑c d√π c√≥ tr·ªçng s·ªë ban ƒë·∫ßu th·∫•p, s·∫Ω nh·∫≠n ƒë∆∞·ª£c th√™m gi√° tr·ªã n·∫øu ƒë∆∞·ª£c li√™n k·∫øt m·∫°nh m·∫Ω v·ªõi c√°c Phrase Nodes.
4. K·∫øt qu·∫£ cu·ªëi c√πng:
  - Passage Node ch·ª©a ƒëo·∫°n vƒÉn ch√≠nh v·ªÅ John Doe s·∫Ω ƒë∆∞·ª£c x·∫øp h·∫°ng cao, ƒë·∫£m b·∫£o h·ªá th·ªëng tr·∫£ l·ªùi ch√≠nh x√°c cho truy v·∫•n "T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?".
Qua v√≠ d·ª• n√†y, ch√∫ng ta th·∫•y r·∫±ng vi·ªác Assigning Seed Node Weights kh√¥ng ch·ªâ l√† m·ªôt thao t√°c ƒë·ªãnh l∆∞·ª£ng ban ƒë·∫ßu, m√† c√≤n l√† b∆∞·ªõc quy·∫øt ƒë·ªãnh ƒë·ªÉ c√¢n b·∫±ng gi·ªØa th√¥ng tin kh√°i ni·ªám v√† ng·ªØ c·∫£nh, t·ª´ ƒë√≥ t·ªëi ∆∞u qu√° tr√¨nh PPR v√† c·∫£i thi·ªán hi·ªáu qu·∫£ c·ªßa h·ªá th·ªëng truy xu·∫•t.
ƒê√∫ng v·∫≠y, khi n√≥i v·ªÅ tr·ªçng s·ªë m·∫∑c ƒë·ªãnh cho Phrase Nodes, ta c√≥ th·ªÉ ƒë·∫∑t weight factor l√† 1.0, nh∆∞ng ƒëi·ªÅu n√†y kh√¥ng c√≥ nghƒ©a l√† gi√° tr·ªã similarity c·ªßa t·ª´ng node s·∫Ω lu√¥n l√† 1.0.
C·ª• th·ªÉ:
- Tr·ªçng s·ªë (weight factor) c·ªßa Phrase Nodes: Khi g√°n tr·ªçng s·ªë cho c√°c Phrase Nodes, h·ªá th·ªëng s·ª≠ d·ª•ng h·ªá s·ªë 1.0 ƒë·ªÉ gi·ªØ nguy√™n gi√° tr·ªã similarity ban ƒë·∫ßu.
- Gi√° tr·ªã similarity: ƒê√¢y l√† m·ªôt ch·ªâ s·ªë ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa embedding c·ªßa th·ª±c th·ªÉ ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ truy v·∫•n v√† embedding c·ªßa node trong Knowledge Graph. Gi√° tr·ªã n√†y th∆∞·ªùng n·∫±m trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1, v√† kh√¥ng ph·∫£i l√∫c n√†o c≈©ng b·∫±ng 1.0. V√≠ d·ª•, Node A c√≥ similarity l√† 0.72 v√† Node B c√≥ similarity l√† 0.68, ƒëi·ªÅu n√†y ph·∫£n √°nh m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng (kh√¥ng ph·∫£i l√† ‚Äútr·ªçng s·ªë‚Äù m√† l√† k·∫øt qu·∫£ c·ªßa ph√©p ƒëo similarity).
T√≥m l·∫°i, m·∫∑c ƒë·ªãnh weight factor c·ªßa Phrase Nodes l√† 1.0, nghƒ©a l√† ch√∫ng ta kh√¥ng ƒëi·ªÅu ch·ªânh th√™m gi√° tr·ªã similarity c·ªßa ch√∫ng, nh∆∞ng gi√° tr·ªã similarity th·ª±c s·ª± (0.72, 0.68, ‚Ä¶) ph·ª• thu·ªôc v√†o qu√° tr√¨nh t√≠nh to√°n t·ª´ c√°c embedding ch·ª© kh√¥ng ph·∫£i l√† 1.0 lu√¥n.

---
D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• chi ti·∫øt k√®m ƒë·ªëi tho·∫°i ph·∫£n bi·ªán ‚Äì ‚Äúng∆∞·ªùi ph√° ‚Äì ng∆∞·ªùi tr·∫£ l·ªùi‚Äù ‚Äì ƒë·ªÉ minh h·ªça c√°ch Seed Node Weights v√† thu·∫≠t to√°n Personalized PageRank (PPR) ho·∫°t ƒë·ªông c√πng nhau trong HippoRAG.

---
B·ªëi c·∫£nh v√≠ d·ª•
Gi·∫£ s·ª≠ truy v·∫•n l√†:
"T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?"
Sau khi tr√≠ch xu·∫•t th√¥ng tin t·ª´ truy v·∫•n v√† x√¢y d·ª±ng Knowledge Graph (KG), h·ªá th·ªëng x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c seed node v·ªõi c√°c gi√° tr·ªã similarity nh∆∞ sau:
- Phrase Node A: "t√°c gi·∫£"
  - Similarity score = 0.72
  - Weight factor (Phrase) = 1.0 ‚Üí Effective weight = 0.72
- Phrase Node B: "The Great Novel"
  - Similarity score = 0.68
  - Weight factor (Phrase) = 1.0 ‚Üí Effective weight = 0.68
- Passage Node C: ƒêo·∫°n vƒÉn 1 ch·ª©a th√¥ng tin:
 ‚ÄúThe Great Novel, m·ªôt t√°c ph·∫©m kinh ƒëi·ªÉn, ƒë∆∞·ª£c vi·∫øt b·ªüi John Doe ‚Äì t√°c gi·∫£ n·ªïi ti·∫øng‚Äù
  - Similarity score = 0.80
  - Weight factor (Passage) = 0.05 ‚Üí Effective weight = 0.80 √ó 0.05 = 0.04
- Passage Node D: ƒêo·∫°n vƒÉn 2 n√≥i chung v·ªÅ John Doe nh∆∞ng kh√¥ng nh·∫Øc tr·ª±c ti·∫øp ti√™u ƒë·ªÅ
  - Similarity score = 0.55
  - Weight factor (Passage) = 0.05 ‚Üí Effective weight = 0.55 √ó 0.05 = 0.0275

---
ƒê·ªëi tho·∫°i ph·∫£n bi·ªán ‚Äì C√°ch PPR ho·∫°t ƒë·ªông v·ªõi Seed Node Weights
Ph·∫£n bi·ªán 1: Kh·ªüi t·∫°o Seed Vector
Q: L√†m th·∫ø n√†o ta kh·ªüi t·∫°o vector seed cho PPR v·ªõi c√°c seed node tr√™n?
A:
- Ta t·∫°o m·ªôt vector ss ch·ª©a c√°c gi√° tr·ªã kh·ªüi t·∫°o cho c√°c node theo effective weight:
  - sA=0.72s_A = 0.72 (Phrase node A)
  - sB=0.68s_B = 0.68 (Phrase node B)
  - sC=0.04s_C = 0.04 (Passage node C)
  - sD=0.0275s_D = 0.0275 (Passage node D)
- Sau ƒë√≥, ta chu·∫©n h√≥a vector ss sao cho t·ªïng c√°c ph·∫ßn t·ª≠ b·∫±ng 1. V√≠ d·ª•:
  - T·ªïng S=0.72+0.68+0.04+0.0275=1.4675S = 0.72 + 0.68 + 0.04 + 0.0275 = 1.4675
  - Vector seed chu·∫©n h√≥a:
    - sA‚Ä≤=0.72/1.4675‚âà0.49s'_A = 0.72 / 1.4675 \approx 0.49
    - sB‚Ä≤=0.68/1.4675‚âà0.46s'_B = 0.68 / 1.4675 \approx 0.46
    - sC‚Ä≤=0.04/1.4675‚âà0.027s'_C = 0.04 / 1.4675 \approx 0.027
    - sD‚Ä≤=0.0275/1.4675‚âà0.019s'_D = 0.0275 / 1.4675 \approx 0.019
Nh∆∞ v·∫≠y, c√°c Phrase Nodes chi·∫øm ∆∞u th·∫ø ngay t·ª´ ƒë·∫ßu so v·ªõi c√°c Passage Nodes do effective weight c·ªßa ch√∫ng cao h∆°n.

---
Ph·∫£n bi·ªán 2: Qu√° tr√¨nh PPR ‚Äì Lan t·ªèa x√°c su·∫•t
Q: PPR s·∫Ω l√†m g√¨ v·ªõi vector seed ƒë√£ chu·∫©n h√≥a n√†y?
A:
- Thu·∫≠t to√°n PPR th·ª±c hi·ªán c·∫≠p nh·∫≠t x√°c su·∫•t theo c√¥ng th·ª©c:
- v(t+1)=(1‚àíd)‚ãÖs‚Ä≤+d‚ãÖT‚ãÖv(t)v^{(t+1)} = (1-d) \cdot s' + d \cdot T \cdot v^{(t)}
- trong ƒë√≥:
  - v(t)v^{(t)} l√† vector x√°c su·∫•t ·ªü v√≤ng l·∫∑p th·ª© tt
  - s‚Ä≤s' l√† vector seed (ƒë√£ chu·∫©n h√≥a)
  - TT l√† ma tr·∫≠n chuy·ªÉn ƒë·ªïi (transition matrix) c·ªßa KG, x√°c ƒë·ªãnh x√°c su·∫•t chuy·ªÉn t·ª´ node n√†y sang node kh√°c d·ª±a tr√™n c√°c c·∫°nh li√™n k·∫øt
  - dd l√† damping factor (th∆∞·ªùng kho·∫£ng 0.5‚Äì0.85)
- Qu√° tr√¨nh n√†y s·∫Ω ‚Äúlan t·ªèa‚Äù c√°c gi√° tr·ªã t·ª´ c√°c seed node ban ƒë·∫ßu theo c√°c li√™n k·∫øt trong ƒë·ªì th·ªã. C√°c Phrase Nodes v·ªõi gi√° tr·ªã cao ban ƒë·∫ßu (0.49 v√† 0.46) s·∫Ω ‚Äúƒë·∫©y‚Äù x√°c su·∫•t sang c√°c n√∫t l√¢n c·∫≠n. N·∫øu Passage Node C (v·ªõi gi√° tr·ªã ban ƒë·∫ßu 0.027) c√≥ c√°c c·∫°nh m·∫°nh li√™n k·∫øt v·ªõi Phrase Nodes, n√≥ c√≥ th·ªÉ nh·∫≠n th√™m x√°c su·∫•t t·ª´ ch√∫ng.

---
Ph·∫£n bi·ªán 3: V√≠ d·ª• s·ªë h·ªçc trong qu√° tr√¨nh PPR
Q: H√£y l·∫•y m·ªôt v√≤ng l·∫∑p PPR minh h·ªça c·ª• th·ªÉ cho v√≠ d·ª• n√†y.
A:
- Gi·∫£ s·ª≠ ma tr·∫≠n chuy·ªÉn ƒë·ªïi TT c·ªßa m·ªôt ph·∫ßn nh·ªè trong KG ƒë∆∞·ª£c ƒë∆°n gi·∫£n ho√° nh∆∞ sau (ch·ªâ x√©t 4 node tr√™n):
  - TABT_{AB} l√† x√°c su·∫•t t·ª´ node A sang B, TACT_{AC} t·ª´ A sang C, v.v‚Ä¶
- V√≠ d·ª•, ta gi·∫£ s·ª≠:
  - Node A (Phrase "t√°c gi·∫£") c√≥ li√™n k·∫øt:
    - 60% ƒë·∫øn Node B ("The Great Novel")
    - 40% ƒë·∫øn Node C (Passage ch·ª©a th√¥ng tin ch√≠nh)
  - Node B ("The Great Novel") li√™n k·∫øt:
    - 70% ƒë·∫øn Node A
    - 30% ƒë·∫øn Node D (Passage ph·ª•)
  - Node C (Passage 1) li√™n k·∫øt:
    - 100% ƒë·∫øn Node A (gi·∫£ s·ª≠ c√≥ m·ªëi li√™n h·ªá m·∫°nh v·ªõi kh√°i ni·ªám "t√°c gi·∫£")
  - Node D (Passage 2) li√™n k·∫øt:
    - 100% ƒë·∫øn Node B
- Ta c√≥ ma tr·∫≠n TT (ƒë∆°n gi·∫£n ho√°) nh∆∞ sau:
- T=[00.600.4000.70000.301.000001.000]T = \begin{bmatrix} 0   & 0.60 & 0.40 & 0 \\ 0.70 & 0   & 0    & 0.30 \\ 1.0  & 0   & 0    & 0 \\ 0   & 1.0 & 0    & 0  \end{bmatrix}
- Gi·∫£ s·ª≠ damping factor d=0.5d = 0.5. V·ªõi vector seed chu·∫©n h√≥a s‚Ä≤=[0.49,0.46,0.027,0.019]‚ä§s' = [0.49, 0.46, 0.027, 0.019]^\top, ta kh·ªüi t·∫°o v(0)=s‚Ä≤v^{(0)} = s'.
- V√≤ng l·∫∑p th·ª© 1:
 T√≠nh v(1)=0.5‚ãÖs‚Ä≤+0.5‚ãÖT‚ãÖv(0)v^{(1)} = 0.5 \cdot s' + 0.5 \cdot T \cdot v^{(0)}
- Tr∆∞·ªõc h·∫øt, t√≠nh T‚ãÖv(0)T \cdot v^{(0)}:
  - (T‚ãÖv(0))A=0‚àó0.49+0.60‚àó0.46+0.40‚àó0.027+0‚àó0.019‚âà0.60√ó0.46+0.40√ó0.027‚âà0.276+0.0108‚âà0.287(T \cdot v^{(0)})_A = 0*0.49 + 0.60*0.46 + 0.40*0.027 + 0*0.019 \approx 0.60 \times 0.46 + 0.40 \times 0.027 \approx 0.276 + 0.0108 \approx 0.287
  - (T‚ãÖv(0))B=0.70‚àó0.49+0‚àó0.46+0‚àó0.027+0.30‚àó0.019‚âà0.343+0.0057‚âà0.3487(T \cdot v^{(0)})_B = 0.70*0.49 + 0*0.46 + 0*0.027 + 0.30*0.019 \approx 0.343 + 0.0057 \approx 0.3487
  - (T‚ãÖv(0))C=1.0‚àó0.49+0‚àó0.46+0‚àó0.027+0‚àó0.019=0.49(T \cdot v^{(0)})_C = 1.0*0.49 + 0*0.46 + 0*0.027 + 0*0.019 = 0.49
  - (T‚ãÖv(0))D=0‚àó0.49+1.0‚àó0.46+0‚àó0.027+0‚àó0.019=0.46(T \cdot v^{(0)})_D = 0*0.49 + 1.0*0.46 + 0*0.027 + 0*0.019 = 0.46
- Sau ƒë√≥,
- v(1)=0.5‚ãÖ[0.49,0.46,0.027,0.019]‚ä§+0.5‚ãÖ[0.287,0.3487,0.49,0.46]‚ä§v^{(1)} = 0.5 \cdot [0.49, 0.46, 0.027, 0.019]^\top + 0.5 \cdot [0.287, 0.3487, 0.49, 0.46]^\top 
- =
- [0.245+0.1435,‚Äâ0.23+0.17435,‚Äâ0.0135+0.245,‚Äâ0.0095+0.23][0.245 + 0.1435,\, 0.23 + 0.17435,\, 0.0135 + 0.245,\, 0.0095 + 0.23]
- =
- [0.3885,‚Äâ0.40435,‚Äâ0.2585,‚Äâ0.2395][0.3885,\, 0.40435,\, 0.2585,\, 0.2395]
- Gi·∫£i th√≠ch:
 Nh√¨n v√†o v(1)v^{(1)}:
  - Node A v√† Node B (Phrase Nodes) c√≥ gi√° tr·ªã v·∫´n cao (0.3885 v√† 0.40435).
  - Node C (Passage 1) tƒÉng t·ª´ 0.027 l√™n 0.2585 nh·ªù nh·∫≠n ƒë∆∞·ª£c th√¥ng tin t·ª´ Node A (v√¨ c√≥ li√™n k·∫øt 100% t·ª´ Node C sang A) v√† t·ª´ m·ªëi li√™n k·∫øt c·ªßa A ƒë·∫øn C.
  - Node D (Passage 2) c√≥ gi√° tr·ªã 0.2395 nh·ªù li√™n k·∫øt v·ªõi Node B.
Qu√° tr√¨nh n√†y s·∫Ω l·∫∑p l·∫°i cho ƒë·∫øn khi h·ªôi t·ª•. K·∫øt qu·∫£ cu·ªëi c√πng s·∫Ω l√† vector x√°c su·∫•t ph√¢n b·ªï qua to√†n b·ªô KG, trong ƒë√≥ c√°c node c√≥ m·ªëi li√™n k·∫øt m·∫°nh v·ªõi seed nodes ban ƒë·∫ßu s·∫Ω nh·∫≠n ƒë∆∞·ª£c x√°c su·∫•t cao h∆°n.

---
Ph·∫£n bi·ªán 4: ·∫¢nh h∆∞·ªüng c·ªßa PPR ƒë·∫øn vi·ªác x·∫øp h·∫°ng c√°c Passage
Q: Sau khi vector x√°c su·∫•t ƒë∆∞·ª£c t√≠nh xong, l√†m sao c√°c gi√° tr·ªã n√†y chuy·ªÉn th√†nh vi·ªác x·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn tr·∫£ l·ªùi truy v·∫•n?
A:
- Sau khi c√≥ vector vv cho c√°c node trong KG, h·ªá th·ªëng s·∫Ω nh√¢n c√°c gi√° tr·ªã n√†y v·ªõi ma tr·∫≠n P (P matrix), m√† trong ƒë√≥ m·ªói ph·∫ßn t·ª≠ bi·ªÉu th·ªã s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªôt n√∫t (phrase ho·∫∑c passage) trong c√°c ƒëo·∫°n vƒÉn g·ªëc.
- T·ªïng h·ª£p gi√° tr·ªã t·ª´ c√°c node li√™n quan s·∫Ω cho ƒëi·ªÉm x·∫øp h·∫°ng cho t·ª´ng ƒëo·∫°n vƒÉn. C√°c ƒëo·∫°n vƒÉn c√≥ t·ªïng x√°c su·∫•t cao (ƒë·∫∑c bi·ªát l√† do nh·∫≠n ƒë∆∞·ª£c ‚Äúƒëi·ªÉm c·ªông‚Äù t·ª´ c√°c Phrase Nodes m·∫°nh v√† Passage Nodes c√≥ li√™n k·∫øt t·ªët) s·∫Ω ƒë∆∞·ª£c x·∫øp h·∫°ng ∆∞u ti√™n.
V√≠ d·ª•:
- Passage Node C, sau PPR, c√≥ gi√° tr·ªã tƒÉng l√™n ƒë√°ng k·ªÉ (v√≠ d·ª• 0.2585), khi nh√¢n v·ªõi s·ªë l·∫ßn xu·∫•t hi·ªán v√† c√°c li√™n k·∫øt li√™n quan, n√≥ c√≥ th·ªÉ ƒë∆∞·ª£c x·∫øp h·∫°ng cao v√† ch·ªçn l√†m c√¢u tr·∫£ l·ªùi cho truy v·∫•n "T√°c gi·∫£ c·ªßa cu·ªën s√°ch The Great Novel l√† ai?".

---
T·ªïng k·∫øt
Qua v√≠ d·ª• chi ti·∫øt tr√™n, ta th·∫•y r·∫±ng:
1. Seed Node Weights ƒë∆∞·ª£c x√°c ƒë·ªãnh d·ª±a tr√™n similarity score v√† weight factor ri√™ng bi·ªát cho Phrase Nodes (1.0) v√† Passage Nodes (0.05).
2. Vector seed sau khi chu·∫©n h√≥a cung c·∫•p ƒëi·ªÉm kh·ªüi ƒë·∫ßu cho PPR.
3. PPR lan t·ªèa x√°c su·∫•t t·ª´ c√°c seed node theo c√°c li√™n k·∫øt trong KG, gi√∫p c√°c Passage Nodes nh·∫≠n ƒë∆∞·ª£c ‚Äúƒëi·ªÉm c·ªông‚Äù t·ª´ c√°c Phrase Nodes n·∫øu ch√∫ng c√≥ li√™n k·∫øt ch·∫∑t ch·∫Ω.
4. K·∫øt qu·∫£ cu·ªëi c√πng sau PPR v√† k·∫øt h·ª£p v·ªõi ma tr·∫≠n P s·∫Ω x·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi truy v·∫•n.
Qua ƒë√≥, ta th·∫•y r·∫±ng vi·ªác g√°n tr·ªçng s·ªë seed node v√† ·ª©ng d·ª•ng PPR kh√¥ng ch·ªâ gi√∫p c√¢n b·∫±ng gi·ªØa th√¥ng tin kh√°i ni·ªám v√† ng·ªØ c·∫£nh, m√† c√≤n t·∫°o ƒëi·ªÅu ki·ªán cho qu√° tr√¨nh multi-hop retrieval di·ªÖn ra hi·ªáu qu·∫£ v√† ch√≠nh x√°c.

---
ƒê√∫ng v·∫≠y, b∆∞·ªõc PPR ch·ªß y·∫øu th·ª±c hi·ªán vi·ªác re-rank v√† lan t·ªèa c√°c ƒëi·ªÉm li√™n quan t·ª´ c√°c seed node ƒë√£ ƒë∆∞·ª£c ch·ªçn ·ªü b∆∞·ªõc tr∆∞·ªõc. N√≥i c√°ch kh√°c, PPR kh√¥ng ch·ªâ ƒë∆°n gi·∫£n l√† s·∫Øp x·∫øp l·∫°i c√°c node m√† c√≤n ‚Äúreasoning‚Äù qua c·∫•u tr√∫c c·ªßa Knowledge Graph ƒë·ªÉ tƒÉng c∆∞·ªùng m·ªëi li√™n k·∫øt gi·ªØa c√°c node, gi√∫p h·ªá th·ªëng t√¨m ra nh·ªØng ƒëo·∫°n vƒÉn ho·∫∑c triple c√≥ li√™n quan nh·∫•t d·ª±a tr√™n m·ªëi quan h·ªá logic gi·ªØa ch√∫ng.

---
[Image]

ƒê·∫∑t v·∫•n ƒë·ªÅ nh∆∞ n√†o, gi·∫£i quy·∫øt nh∆∞ n√†o? 
- B√†i to√°n suy lu·∫≠n ƒëa b∆∞·ªõc. Dataset lu√¥n? 
- https://www.kaggle.com/datasets/danghuy249/rag-dataset?select=data
-  However, its reliance  on vector retrieval hinders its ability to mimic  the dynamic and interconnected nature of hu man long-term memory. Recent RAG approaches  augment vector embeddings with various struc tures like knowledge graphs to address some of  these gaps, namely sense-making and associativ ity. However, their performance on more basic  factual memory tasks drops considerably below  standard RAG. 
- Tuy nhi√™n, vi·ªác ph·ª• thu·ªôc v√†o truy xu·∫•t vector h·∫°n ch·∫ø kh·∫£ nƒÉng c·ªßa n√≥ trong vi·ªác m√¥ ph·ªèng t√≠nh ch·∫•t ƒë·ªông v√† li√™n k·∫øt c·ªßa tr√≠ nh·ªõ d√†i h·∫°n ·ªü con ng∆∞·ªùi. C√°c ph∆∞∆°ng ph√°p RAG g·∫ßn ƒë√¢y ƒë√£ b·ªï sung c√°c c·∫•u tr√∫c kh√°c nhau nh∆∞ ƒë·ªì th·ªã tri th·ª©c v√†o c√°c vector nh√∫ng ƒë·ªÉ gi·∫£i quy·∫øt m·ªôt s·ªë thi·∫øu s√≥t n√†y, c·ª• th·ªÉ l√† kh·∫£ nƒÉng c·∫£m nh·∫≠n v√† li√™n k·∫øt. Tuy nhi√™n, hi·ªáu su·∫•t c·ªßa ch√∫ng trong c√°c nhi·ªám v·ª• tr√≠ nh·ªõ th·ª±c t·∫ø c∆° b·∫£n l·∫°i gi·∫£m ƒë√°ng k·ªÉ so v·ªõi RAG ti√™u chu·∫©n. => ƒê·ªÅ xu·∫•t HippoRAG. 
+, Long Term Memory? (l∆∞u d·∫°ng fact c·ªßa user) 
- Baseline ng√†y x∆∞a. RAG truy·ªÅn th·ªëng. Dataset ng√†y x∆∞a. 
- Reason LLms -> ƒêo·∫°n Retrieval Reasoning r·ªìi Hippo. 
HippoRAG kh√¥ng ch·ªù ƒë·∫øn khi LLM l√†m reasoning, m√† n√≥ ƒë√£ reasoning ngay t·ª´ b∆∞·ªõc truy h·ªìi.
V·ªõi Personalized PageRank ‚Üí n√≥ t√¨m ra node trung gian (John Smith) tr∆∞·ªõc khi v√†o LLM
Tr·∫£ v·ªÅ ƒëo·∫°n ƒë√£ ƒë·ªß logic ‚Üí LLM ch·ªâ c·∫ßn n√≥i l·∫°i
- xAI cho c√¢u h·ªèi b·∫±ng c√°ch x√¢y d·ª±ng KG. ƒê∆∞a KG v√†o h·ªçc? 
- Dataset ng√†y x∆∞a c·ªßa anh Huy, ... 
-  HotpotQA

============
 ForHippoRAG2,weusetheopen-sourceLlama-3.3-70B Instruct(AI@Meta,2024)asboththeextraction(NERand  OpenIE)andtriplefilteringmodel,andweusenvidia/NV Embed-v2astheretriever.


ƒê·ªÉ ƒë·∫∑t v·∫•n ƒë·ªÅ t·∫°i sao c·∫ßn HippoRAG, ta c√≥ th·ªÉ tr√¨nh b√†y nh∆∞ sau ‚Äì k·∫øt h·ª£p gi·ªØa l·∫≠p lu·∫≠n logic v√† c√°c ph√°t hi·ªán t·ª´ b√†i b√°o:

---
üß† T·∫°i sao c·∫ßn HippoRAG?
1. Gi·ªõi h·∫°n c·ªßa RAG truy·ªÅn th·ªëng
Retrieval-Augmented Generation (RAG) l√† m·ªôt gi·∫£i ph√°p ph·ªï bi·∫øn gi√∫p LLM ti·∫øp c·∫≠n th√¥ng tin m·ªõi m√† kh√¥ng c·∫ßn thay ƒë·ªïi tham s·ªë m√¥ h√¨nh. Tuy nhi√™n, RAG truy·ªÅn th·ªëng ch·ªß y·∫øu d·ª±a v√†o vector retrieval ƒë∆°n gi·∫£n, d·∫´n ƒë·∫øn 2 ƒëi·ªÉm y·∫øu l·ªõn:
- Thi·∫øu kh·∫£ nƒÉng sense-making: Kh√¥ng th·ªÉ hi·ªÉu, di·ªÖn gi·∫£i c√°c t√¨nh hu·ªëng d√†i v√† ph·ª©c t·∫°p m·ªôt c√°ch li·ªÅn m·∫°ch.
- Thi·∫øu t√≠nh li√™n k·∫øt (associativity): Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒëa b∆∞·ªõc gi·ªØa c√°c m·∫£nh ki·∫øn th·ª©c r·ªùi r·∫°c nh∆∞ b·ªô nh·ªõ d√†i h·∫°n c·ªßa con ng∆∞·ªùi.
üìå V√≠ d·ª•: Trong c√°c t√°c v·ª• truy v·∫•n ƒëa b∆∞·ªõc nh∆∞ "Ng∆∞·ªùi ph√°t minh ra thi·∫øt b·ªã X s·ªëng ·ªü ƒë√¢u?", vector retrieval truy·ªÅn th·ªëng th∆∞·ªùng th·∫•t b·∫°i v√¨ kh√¥ng th·ªÉ n·ªëi c√°c m·∫£nh th√¥ng tin l·∫°i (ph√°t minh X ‚Üí ng∆∞·ªùi A ‚Üí n∆°i s·ªëng c·ªßa A).

---
2. C√°c gi·∫£i ph√°p tr∆∞·ªõc ƒë√¢y v·∫´n ch∆∞a ƒë·ªß
C√°c h·ªá th·ªëng RAG c·∫•u tr√∫c m·ªõi nh∆∞ RAPTOR, GraphRAG, LightRAG ƒë√£ th·ª≠ kh·∫Øc ph·ª•c b·∫±ng c√°ch:
- T·∫°o t√≥m t·∫Øt ho·∫∑c knowledge graph ƒë·ªÉ b·ªï sung ng·ªØ c·∫£nh.
- Tuy nhi√™n, l·∫°i g·∫∑p ph·∫£i v·∫•n ƒë·ªÅ gi·∫£m hi·ªáu qu·∫£ trong c√°c t√°c v·ª• ghi nh·ªõ c∆° b·∫£n (factual QA) do nhi·ªÖu t·ª´ LLM ho·∫∑c c·∫•u tr√∫c ph·ª©c t·∫°p g√¢y qu√° t·∫£i.
‚ùå Kh√¥ng c√≥ m√¥ h√¨nh n√†o c√¢n b·∫±ng t·ªët c·∫£ 3 kh√≠a c·∫°nh: factual memory, sense-making v√† associativity.

---
3. HippoRAG ra ƒë·ªùi ƒë·ªÉ m√¥ ph·ªèng b·ªô nh·ªõ d√†i h·∫°n nh∆∞ ng∆∞·ªùi
HippoRAG (v√† b·∫£n c·∫£i ti·∫øn HippoRAG 2) ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n c·∫£m h·ª©ng t·ª´ c·∫•u tr√∫c b·ªô nh·ªõ sinh h·ªçc c·ªßa con ng∆∞·ªùi (neocortex, hippocampus, parahippocampal region), v·ªõi m·ª•c ti√™u:
- Ghi nh·ªõ b·ªÅn v·ªØng ki·∫øn th·ª©c m·ªõi m√† kh√¥ng l√†m qu√™n ki·∫øn th·ª©c c≈©.
- K·∫øt n·ªëi ƒë∆∞·ª£c nhi·ªÅu th√¥ng tin d·∫°ng bi·ªÉu ƒë·ªì, h·ªó tr·ª£ multi-hop reasoning.
- TƒÉng t√≠nh ng·ªØ c·∫£nh v√† t√≠nh li√™n k·∫øt, nh·ªù t√≠ch h·ª£p PageRank c√° nh√¢n h√≥a v√† bi·ªÉu ƒë·ªì tri th·ª©c m·ªü (open KG).

---
4. L·ª£i √≠ch c·ªßa HippoRAG
Theo k·∫øt qu·∫£ th·ª±c nghi·ªám:
- HippoRAG 2 v∆∞·ª£t tr·ªôi 7% trong c√°c b√†i to√°n truy v·∫•n li√™n k·∫øt (associative tasks).
- Gi·ªØ ƒë∆∞·ª£c ho·∫∑c c·∫£i thi·ªán nh·∫π ·ªü c√°c t√°c v·ª• ghi nh·ªõ ƒë∆°n (factual) v√† hi·ªÉu ng·ªØ c·∫£nh (sense-making).
- Kh·∫£ nƒÉng h·ªçc li√™n t·ª•c phi tham s·ªë (non-parametric continual learning) ‚Äì kh√¥ng c·∫ßn fine-tune LLM li√™n t·ª•c.

---
N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ vi·∫øt ph·∫ßn n√†y th√†nh m·ªôt ƒëo·∫°n motivation ng·∫Øn, chu·∫©n phong c√°ch b√†i nghi√™n c·ª©u ho·∫∑c b√†i thuy·∫øt tr√¨nh h·ªçc thu·∫≠t. Qu·ªëc mu·ªën phong c√°ch n√†o: ng·∫Øn g·ªçn ‚Äì g√£y g·ªçn ‚Äì tr√¨nh b√†y v·∫•n ƒë·ªÅ? Hay s√¢u s·∫Øc ‚Äì h·ªçc thu·∫≠t ‚Äì ƒë∆∞a d·∫´n ch·ª©ng nghi√™n c·ª©u?


D∆∞·ªõi ƒë√¢y l√† b·∫£n "ƒê·∫∑t v·∫•n ƒë·ªÅ v√† Gi·∫£i ph√°p" cho HippoRAG, ƒë∆∞·ª£c vi·∫øt theo phong c√°ch h·ªçc thu·∫≠t, s√∫c t√≠ch v√† logic ‚Äì ph√π h·ª£p ƒë·ªÉ ƒë∆∞a v√†o gi·ªõi thi·ªáu m·ªôt b√†i nghi√™n c·ª©u, thuy·∫øt tr√¨nh chuy√™n ƒë·ªÅ ho·∫∑c ƒë·ªì √°n t·ªët nghi·ªáp.

---
üß© ƒê·∫∑t v·∫•n ƒë·ªÅ v√† Gi·∫£i ph√°p
üîç ƒê·∫∑t v·∫•n ƒë·ªÅ
Kh·∫£ nƒÉng ghi nh·ªõ, li√™n k·∫øt v√† s·ª≠ d·ª•ng ki·∫øn th·ª©c l√¢u d√†i l√† m·ªôt ƒë·∫∑c tr∆∞ng c·ªët l√µi c·ªßa tr√≠ th√¥ng minh con ng∆∞·ªùi. Tuy nhi√™n, c√°c h·ªá th·ªëng AI hi·ªán nay ‚Äì ƒë·∫∑c bi·ªát l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs) ‚Äì ch∆∞a th·ªÉ hi·ªán ƒë∆∞·ª£c kh·∫£ nƒÉng h·ªçc li√™n t·ª•c v√† ghi nh·ªõ l√¢u d√†i m·ªôt c√°ch hi·ªáu qu·∫£.
M·∫∑c d√π Retrieval-Augmented Generation (RAG) ƒëang l√† gi·∫£i ph√°p ch√≠nh cho vi·ªác ti·∫øp c·∫≠n th√¥ng tin ngo√†i m√¥ h√¨nh (non-parametric learning), RAG truy·ªÅn th·ªëng v·∫´n c√≤n nhi·ªÅu h·∫°n ch·∫ø:
- Ph·ª• thu·ªôc ho√†n to√†n v√†o truy h·ªìi vector (vector retrieval), d·∫´n ƒë·∫øn:
  - Kh√¥ng hi·ªÉu s√¢u ng·ªØ c·∫£nh d√†i ho·∫∑c ph·ª©c t·∫°p (thi·∫øu sense-making).
  - Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c th√¥ng tin r·ªùi r·∫°c qua nhi·ªÅu b∆∞·ªõc (thi·∫øu associativity).
- C√°c ph∆∞∆°ng ph√°p c·∫£i ti·∫øn nh∆∞ RAPTOR hay GraphRAG d√πng LLM ƒë·ªÉ t·∫°o t√≥m t·∫Øt ho·∫∑c graph, nh∆∞ng l·∫°i g√¢y suy gi·∫£m hi·ªáu nƒÉng ·ªü c√°c b√†i to√°n factual QA, do nhi·ªÖu t·ª´ LLM ho·∫∑c qu√° ph·ª©c t·∫°p trong c·∫•u tr√∫c.
T·ª´ ƒë√≥ ƒë·∫∑t ra c√¢u h·ªèi:
L√†m th·∫ø n√†o ƒë·ªÉ x√¢y d·ª±ng m·ªôt h·ªá th·ªëng ghi nh·ªõ l√¢u d√†i cho LLMs v·ª´a m·∫°nh v·ªÅ factual, v·ª´a s√¢u s·∫Øc v·ªÅ ng·ªØ c·∫£nh, v√† linh ho·∫°t trong li√™n k·∫øt ki·∫øn th·ª©c?

---
üí° Gi·∫£i ph√°p: HippoRAG
HippoRAG ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t nh∆∞ m·ªôt khung ghi nh·ªõ l·∫•y c·∫£m h·ª©ng t·ª´ c·∫•u tr√∫c b·ªô nh·ªõ c·ªßa con ng∆∞·ªùi (g·ªìm t√¢n v·ªè n√£o, hippocampus, v√† v√πng trung gian). Gi·∫£i ph√°p n√†y bao g·ªìm hai giai ƒëo·∫°n:
1. Ch·ªâ m·ª•c h√≥a ngo·∫°i tuy·∫øn (Offline Indexing):
  - D√πng LLM ƒë·ªÉ tr√≠ch xu·∫•t c√°c triple ki·∫øn th·ª©c d·∫°ng KG m·ªü t·ª´ t√†i li·ªáu.
  - T·∫°o bi·ªÉu ƒë·ªì tri th·ª©c g·ªìm c√°c node kh√°i ni·ªám (phrase) v√† node ng·ªØ c·∫£nh (passage).
  - Ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a v√† li√™n k·∫øt c√°c kh√°i ni·ªám t∆∞∆°ng ƒë∆∞∆°ng b·∫±ng embedding.
2. Truy h·ªìi tr·ª±c tuy·∫øn (Online Retrieval):
  - D√πng m√¥ h√¨nh embedding ƒë·ªÉ li√™n k·∫øt c√¢u h·ªèi v·ªõi triple v√† passage.
  - √Åp d·ª•ng thu·∫≠t to√°n Personalized PageRank ƒë·ªÉ t√¨m ƒë∆∞·ªùng d·∫´n li√™n k·∫øt ng·ªØ nghƒ©a v√† truy xu·∫•t c√°c ƒëo·∫°n vƒÉn li√™n quan.
  - D√πng LLM ƒë·ªÉ l·ªçc triple theo ng·ªØ c·∫£nh truy v·∫•n (b·ªô nh·ªõ nh·∫≠n di·ªán ‚Äì recognition memory).
üéØ K·∫øt qu·∫£: HippoRAG 2 v∆∞·ª£t tr·ªôi h∆°n t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p hi·ªán t·∫°i tr√™n c·∫£ ba nƒÉng l·ª±c c·ªët l√µi: ghi nh·ªõ fact, hi·ªÉu ng·ªØ c·∫£nh v√† li√™n k·∫øt th√¥ng tin.

---
N·∫øu Qu·ªëc mu·ªën m√¨nh vi·∫øt l·∫°i b·∫£n n√†y theo b·∫£n slide tr√¨nh b√†y, d·∫°ng b√†i vi·∫øt b√°o c√°o khoa h·ªçc, ho·∫∑c b·∫£n n√≥i thuy·∫øt tr√¨nh mi·ªáng 2-3 ph√∫t, m√¨nh s·∫Ω chuy·ªÉn th·ªÉ ph√π h·ª£p nh√©. Qu·ªëc mu·ªën theo h∆∞·ªõng n√†o ti·∫øp?


ƒê·ªÉ m√¨nh gi·∫£i th√≠ch chi ti·∫øt s∆° ƒë·ªì HippoRAG 2 n√†y m·ªôt c√°ch d·ªÖ hi·ªÉu, chia l√†m 2 giai ƒëo·∫°n: Offline Indexing v√† Online Retrieval & QA, v·ªõi c√°c b∆∞·ªõc r√µ r√†ng nh√©!

---
üß† 1. Offline Indexing ‚Äì Giai ƒëo·∫°n t·∫°o b·ªô nh·ªõ (kh·ªüi t·∫°o bi·ªÉu ƒë·ªì tri th·ª©c)
üéØ M·ª•c ti√™u: Bi·∫øn c√°c ƒëo·∫°n vƒÉn (passages) th√†nh m·ªôt bi·ªÉu ƒë·ªì tri th·ª©c (Knowledge Graph ‚Äì KG) ƒë·ªÉ sau n√†y d·ªÖ truy h·ªìi th√¥ng minh h∆°n.
G·ªìm 3 b∆∞·ªõc:
üîµ (1) OpenIE by LLM
 ‚Äì D√πng LLM (Llama-3.3-70B) ƒë·ªÉ tr√≠ch xu·∫•t c√°c triple ki·∫øn th·ª©c t·ª´ m·ªói ƒëo·∫°n vƒÉn, d·∫°ng:
(subject ‚Äì relation ‚Äì object)
 üëâ V√≠ d·ª•: ("Elon Musk", "founder of", "SpaceX")
üîµ (2) Synonym detection by embedding
 ‚Äì D√πng m√¥ h√¨nh embedding ƒë·ªÉ t√¨m c√°c c·∫∑p t·ª´/c·ª•m t·ª´ gi·ªëng nhau v·ªÅ nghƒ©a, r·ªìi n·ªëi ch√∫ng l·∫°i b·∫±ng synonym edge.
 üëâ V√≠ d·ª•: ‚ÄúNYC‚Äù ‚Üî ‚ÄúNew York City‚Äù
üîµ (3) Dense-sparse integration
 ‚Äì T·∫°o 2 lo·∫°i node trong bi·ªÉu ƒë·ªì:
- Phrase node: c√°c kh√°i ni·ªám r√∫t g·ªçn, nh∆∞ ‚ÄúElon Musk‚Äù ho·∫∑c ‚ÄúCEO‚Äù
- Passage node: ch·ª©a to√†n b·ªô ƒëo·∫°n vƒÉn ‚Äì N·ªëi passage node v·ªõi c√°c phrase node t∆∞∆°ng ·ª©ng b·∫±ng context edge, ƒë·ªÉ bi·ªÉu ƒë·ªì v·ª´a c√≥ th√¥ng tin kh√°i ni·ªám, v·ª´a c√≥ ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß.
üì¶ Output cu·ªëi l√† m·ªôt Knowledge Graph gi√†u ng·ªØ nghƒ©a, v·ªõi c√°c lo·∫°i li√™n k·∫øt:
- Relation Edge (gi·ªØa concept v·ªõi concept)
- Synonym Edge
- Context Edge (gi·ªØa passage v√† concept)

---
üîç 2. Online Retrieval & QA ‚Äì Giai ƒëo·∫°n truy h·ªìi v√† tr·∫£ l·ªùi
üéØ M·ª•c ti√™u: T·ª´ c√¢u h·ªèi (query), t√¨m ƒë√∫ng c√°c ƒëo·∫°n vƒÉn li√™n quan ƒë·ªÉ tr·∫£ l·ªùi.
G·ªìm 5 b∆∞·ªõc:
üü¢ (1) Truy h·ªìi passages v√† triples
 ‚Äì D√πng retriever (NV-Embed-v2) ƒë·ªÉ truy h·ªìi c√°c triple v√† ƒëo·∫°n vƒÉn li√™n quan ƒë·∫øn c√¢u h·ªèi.
üü¢ (2) Recognition Memory ‚Äì L·ªçc triple
 ‚Äì D√πng LLM ƒë·ªÉ ƒë√°nh gi√° c√°c triple, gi·ªØ l·∫°i c√°c triple th·∫≠t s·ª± li√™n quan ƒë·∫øn c√¢u h·ªèi.
üü¢ (3) G√°n tr·ªçng s·ªë cho c√°c seed node
 ‚Äì C√°c triple v√† passage ƒë∆∞·ª£c ch·ªçn s·∫Ω tr·ªü th√†nh c√°c ‚Äúseed node‚Äù (node kh·ªüi ƒë·∫ßu). ‚Äì G√°n tr·ªçng s·ªë kh·ªüi ƒë·ªông (reset probability) cho t·ª´ng seed d·ª±a tr√™n m·ª©c ƒë·ªô li√™n quan.
üü¢ (4) Personalized PageRank (PPR)
 ‚Äì Ch·∫°y thu·∫≠t to√°n PPR tr√™n bi·ªÉu ƒë·ªì tri th·ª©c, ƒë·ªÉ t√¨m c√°c ƒëo·∫°n vƒÉn c√≥ k·∫øt n·ªëi ng·ªØ nghƒ©a m·∫°nh nh·∫•t ƒë·∫øn c√¢u h·ªèi, qua nhi·ªÅu b∆∞·ªõc reasoning.
üü¢ (5) QA Reading
 ‚Äì D√πng LLM ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë∆∞·ª£c ch·ªçn v√† t·∫°o c√¢u tr·∫£ l·ªùi cho truy v·∫•n.

---
üó∫Ô∏è Gi·∫£i th√≠ch c√°c k√Ω hi·ªáu trong s∆° ƒë·ªì
This content is only supported in a Lark Docs

---
N·∫øu Qu·ªëc mu·ªën, m√¨nh c√≥ th·ªÉ di·ªÖn gi·∫£i l·∫°i to√†n b·ªô th√†nh slide ho·∫∑c vi·∫øt b√°o c√°o t√≥m t·∫Øt ph·∫ßn ki·∫øn tr√∫c pipeline HippoRAG 2 theo h√¨nh n√†y nh√©. Qu·ªëc c·∫ßn b·∫£n n√†o?

ƒê·∫∑t v·∫•n ƒë·ªÅ nh∆∞ n√†o, gi·∫£i quy·∫øt nh∆∞ n√†o? 
- B√†i to√°n suy lu·∫≠n ƒëa b∆∞·ªõc. Dataset lu√¥n? 
- https://www.kaggle.com/datasets/danghuy249/rag-dataset?select=data
-  However, its reliance  on vector retrieval hinders its ability to mimic  the dynamic and interconnected nature of hu man long-term memory. Recent RAG approaches  augment vector embeddings with various struc tures like knowledge graphs to address some of  these gaps, namely sense-making and associativ ity. However, their performance on more basic  factual memory tasks drops considerably below  standard RAG. 
- Tuy nhi√™n, vi·ªác ph·ª• thu·ªôc v√†o truy xu·∫•t vector h·∫°n ch·∫ø kh·∫£ nƒÉng c·ªßa n√≥ trong vi·ªác m√¥ ph·ªèng t√≠nh ch·∫•t ƒë·ªông v√† li√™n k·∫øt c·ªßa tr√≠ nh·ªõ d√†i h·∫°n ·ªü con ng∆∞·ªùi. C√°c ph∆∞∆°ng ph√°p RAG g·∫ßn ƒë√¢y ƒë√£ b·ªï sung c√°c c·∫•u tr√∫c kh√°c nhau nh∆∞ ƒë·ªì th·ªã tri th·ª©c v√†o c√°c vector nh√∫ng ƒë·ªÉ gi·∫£i quy·∫øt m·ªôt s·ªë thi·∫øu s√≥t n√†y, c·ª• th·ªÉ l√† kh·∫£ nƒÉng c·∫£m nh·∫≠n v√† li√™n k·∫øt. Tuy nhi√™n, hi·ªáu su·∫•t c·ªßa ch√∫ng trong c√°c nhi·ªám v·ª• tr√≠ nh·ªõ th·ª±c t·∫ø c∆° b·∫£n l·∫°i gi·∫£m ƒë√°ng k·ªÉ so v·ªõi RAG ti√™u chu·∫©n. => ƒê·ªÅ xu·∫•t HippoRAG. 
+, Long Term Memory? (l∆∞u d·∫°ng fact c·ªßa user) 
- Baseline ng√†y x∆∞a. RAG truy·ªÅn th·ªëng. Dataset ng√†y x∆∞a. 
- Reason LLms -> ƒêo·∫°n Retrieval Reasoning r·ªìi Hippo. 
HippoRAG kh√¥ng ch·ªù ƒë·∫øn khi LLM l√†m reasoning, m√† n√≥ ƒë√£ reasoning ngay t·ª´ b∆∞·ªõc truy h·ªìi.
V·ªõi Personalized PageRank ‚Üí n√≥ t√¨m ra node trung gian (John Smith) tr∆∞·ªõc khi v√†o LLM
Tr·∫£ v·ªÅ ƒëo·∫°n ƒë√£ ƒë·ªß logic ‚Üí LLM ch·ªâ c·∫ßn n√≥i l·∫°i
- xAI cho c√¢u h·ªèi b·∫±ng c√°ch x√¢y d·ª±ng KG. ƒê∆∞a KG v√†o h·ªçc? 
- Dataset ng√†y x∆∞a c·ªßa anh Huy, ... 
-  HotpotQA

============
 ForHippoRAG2,weusetheopen-sourceLlama-3.3-70B Instruct(AI@Meta,2024)asboththeextraction(NERand  OpenIE)andtriplefilteringmodel,andweusenvidia/NV Embed-v2astheretriever.


ƒê·ªÉ ƒë·∫∑t v·∫•n ƒë·ªÅ t·∫°i sao c·∫ßn HippoRAG, ta c√≥ th·ªÉ tr√¨nh b√†y nh∆∞ sau ‚Äì k·∫øt h·ª£p gi·ªØa l·∫≠p lu·∫≠n logic v√† c√°c ph√°t hi·ªán t·ª´ b√†i b√°o:

---
üß† T·∫°i sao c·∫ßn HippoRAG?
1. Gi·ªõi h·∫°n c·ªßa RAG truy·ªÅn th·ªëng
Retrieval-Augmented Generation (RAG) l√† m·ªôt gi·∫£i ph√°p ph·ªï bi·∫øn gi√∫p LLM ti·∫øp c·∫≠n th√¥ng tin m·ªõi m√† kh√¥ng c·∫ßn thay ƒë·ªïi tham s·ªë m√¥ h√¨nh. Tuy nhi√™n, RAG truy·ªÅn th·ªëng ch·ªß y·∫øu d·ª±a v√†o vector retrieval ƒë∆°n gi·∫£n, d·∫´n ƒë·∫øn 2 ƒëi·ªÉm y·∫øu l·ªõn:
- Thi·∫øu kh·∫£ nƒÉng sense-making: Kh√¥ng th·ªÉ hi·ªÉu, di·ªÖn gi·∫£i c√°c t√¨nh hu·ªëng d√†i v√† ph·ª©c t·∫°p m·ªôt c√°ch li·ªÅn m·∫°ch.
- Thi·∫øu t√≠nh li√™n k·∫øt (associativity): Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒëa b∆∞·ªõc gi·ªØa c√°c m·∫£nh ki·∫øn th·ª©c r·ªùi r·∫°c nh∆∞ b·ªô nh·ªõ d√†i h·∫°n c·ªßa con ng∆∞·ªùi.
üìå V√≠ d·ª•: Trong c√°c t√°c v·ª• truy v·∫•n ƒëa b∆∞·ªõc nh∆∞ "Ng∆∞·ªùi ph√°t minh ra thi·∫øt b·ªã X s·ªëng ·ªü ƒë√¢u?", vector retrieval truy·ªÅn th·ªëng th∆∞·ªùng th·∫•t b·∫°i v√¨ kh√¥ng th·ªÉ n·ªëi c√°c m·∫£nh th√¥ng tin l·∫°i (ph√°t minh X ‚Üí ng∆∞·ªùi A ‚Üí n∆°i s·ªëng c·ªßa A).

---
2. C√°c gi·∫£i ph√°p tr∆∞·ªõc ƒë√¢y v·∫´n ch∆∞a ƒë·ªß
C√°c h·ªá th·ªëng RAG c·∫•u tr√∫c m·ªõi nh∆∞ RAPTOR, GraphRAG, LightRAG ƒë√£ th·ª≠ kh·∫Øc ph·ª•c b·∫±ng c√°ch:
- T·∫°o t√≥m t·∫Øt ho·∫∑c knowledge graph ƒë·ªÉ b·ªï sung ng·ªØ c·∫£nh.
- Tuy nhi√™n, l·∫°i g·∫∑p ph·∫£i v·∫•n ƒë·ªÅ gi·∫£m hi·ªáu qu·∫£ trong c√°c t√°c v·ª• ghi nh·ªõ c∆° b·∫£n (factual QA) do nhi·ªÖu t·ª´ LLM ho·∫∑c c·∫•u tr√∫c ph·ª©c t·∫°p g√¢y qu√° t·∫£i.
‚ùå Kh√¥ng c√≥ m√¥ h√¨nh n√†o c√¢n b·∫±ng t·ªët c·∫£ 3 kh√≠a c·∫°nh: factual memory, sense-making v√† associativity.

---
3. HippoRAG ra ƒë·ªùi ƒë·ªÉ m√¥ ph·ªèng b·ªô nh·ªõ d√†i h·∫°n nh∆∞ ng∆∞·ªùi
HippoRAG (v√† b·∫£n c·∫£i ti·∫øn HippoRAG 2) ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n c·∫£m h·ª©ng t·ª´ c·∫•u tr√∫c b·ªô nh·ªõ sinh h·ªçc c·ªßa con ng∆∞·ªùi (neocortex, hippocampus, parahippocampal region), v·ªõi m·ª•c ti√™u:
- Ghi nh·ªõ b·ªÅn v·ªØng ki·∫øn th·ª©c m·ªõi m√† kh√¥ng l√†m qu√™n ki·∫øn th·ª©c c≈©.
- K·∫øt n·ªëi ƒë∆∞·ª£c nhi·ªÅu th√¥ng tin d·∫°ng bi·ªÉu ƒë·ªì, h·ªó tr·ª£ multi-hop reasoning.
- TƒÉng t√≠nh ng·ªØ c·∫£nh v√† t√≠nh li√™n k·∫øt, nh·ªù t√≠ch h·ª£p PageRank c√° nh√¢n h√≥a v√† bi·ªÉu ƒë·ªì tri th·ª©c m·ªü (open KG).

---
4. L·ª£i √≠ch c·ªßa HippoRAG
Theo k·∫øt qu·∫£ th·ª±c nghi·ªám:
- HippoRAG 2 v∆∞·ª£t tr·ªôi 7% trong c√°c b√†i to√°n truy v·∫•n li√™n k·∫øt (associative tasks).
- Gi·ªØ ƒë∆∞·ª£c ho·∫∑c c·∫£i thi·ªán nh·∫π ·ªü c√°c t√°c v·ª• ghi nh·ªõ ƒë∆°n (factual) v√† hi·ªÉu ng·ªØ c·∫£nh (sense-making).
- Kh·∫£ nƒÉng h·ªçc li√™n t·ª•c phi tham s·ªë (non-parametric continual learning) ‚Äì kh√¥ng c·∫ßn fine-tune LLM li√™n t·ª•c.

---
N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ vi·∫øt ph·∫ßn n√†y th√†nh m·ªôt ƒëo·∫°n motivation ng·∫Øn, chu·∫©n phong c√°ch b√†i nghi√™n c·ª©u ho·∫∑c b√†i thuy·∫øt tr√¨nh h·ªçc thu·∫≠t. Qu·ªëc mu·ªën phong c√°ch n√†o: ng·∫Øn g·ªçn ‚Äì g√£y g·ªçn ‚Äì tr√¨nh b√†y v·∫•n ƒë·ªÅ? Hay s√¢u s·∫Øc ‚Äì h·ªçc thu·∫≠t ‚Äì ƒë∆∞a d·∫´n ch·ª©ng nghi√™n c·ª©u?


D∆∞·ªõi ƒë√¢y l√† b·∫£n "ƒê·∫∑t v·∫•n ƒë·ªÅ v√† Gi·∫£i ph√°p" cho HippoRAG, ƒë∆∞·ª£c vi·∫øt theo phong c√°ch h·ªçc thu·∫≠t, s√∫c t√≠ch v√† logic ‚Äì ph√π h·ª£p ƒë·ªÉ ƒë∆∞a v√†o gi·ªõi thi·ªáu m·ªôt b√†i nghi√™n c·ª©u, thuy·∫øt tr√¨nh chuy√™n ƒë·ªÅ ho·∫∑c ƒë·ªì √°n t·ªët nghi·ªáp.

---
üß© ƒê·∫∑t v·∫•n ƒë·ªÅ v√† Gi·∫£i ph√°p
üîç ƒê·∫∑t v·∫•n ƒë·ªÅ
Kh·∫£ nƒÉng ghi nh·ªõ, li√™n k·∫øt v√† s·ª≠ d·ª•ng ki·∫øn th·ª©c l√¢u d√†i l√† m·ªôt ƒë·∫∑c tr∆∞ng c·ªët l√µi c·ªßa tr√≠ th√¥ng minh con ng∆∞·ªùi. Tuy nhi√™n, c√°c h·ªá th·ªëng AI hi·ªán nay ‚Äì ƒë·∫∑c bi·ªát l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs) ‚Äì ch∆∞a th·ªÉ hi·ªán ƒë∆∞·ª£c kh·∫£ nƒÉng h·ªçc li√™n t·ª•c v√† ghi nh·ªõ l√¢u d√†i m·ªôt c√°ch hi·ªáu qu·∫£.
M·∫∑c d√π Retrieval-Augmented Generation (RAG) ƒëang l√† gi·∫£i ph√°p ch√≠nh cho vi·ªác ti·∫øp c·∫≠n th√¥ng tin ngo√†i m√¥ h√¨nh (non-parametric learning), RAG truy·ªÅn th·ªëng v·∫´n c√≤n nhi·ªÅu h·∫°n ch·∫ø:
- Ph·ª• thu·ªôc ho√†n to√†n v√†o truy h·ªìi vector (vector retrieval), d·∫´n ƒë·∫øn:
  - Kh√¥ng hi·ªÉu s√¢u ng·ªØ c·∫£nh d√†i ho·∫∑c ph·ª©c t·∫°p (thi·∫øu sense-making).
  - Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c th√¥ng tin r·ªùi r·∫°c qua nhi·ªÅu b∆∞·ªõc (thi·∫øu associativity).
- C√°c ph∆∞∆°ng ph√°p c·∫£i ti·∫øn nh∆∞ RAPTOR hay GraphRAG d√πng LLM ƒë·ªÉ t·∫°o t√≥m t·∫Øt ho·∫∑c graph, nh∆∞ng l·∫°i g√¢y suy gi·∫£m hi·ªáu nƒÉng ·ªü c√°c b√†i to√°n factual QA, do nhi·ªÖu t·ª´ LLM ho·∫∑c qu√° ph·ª©c t·∫°p trong c·∫•u tr√∫c.
T·ª´ ƒë√≥ ƒë·∫∑t ra c√¢u h·ªèi:
L√†m th·∫ø n√†o ƒë·ªÉ x√¢y d·ª±ng m·ªôt h·ªá th·ªëng ghi nh·ªõ l√¢u d√†i cho LLMs v·ª´a m·∫°nh v·ªÅ factual, v·ª´a s√¢u s·∫Øc v·ªÅ ng·ªØ c·∫£nh, v√† linh ho·∫°t trong li√™n k·∫øt ki·∫øn th·ª©c?

---
üí° Gi·∫£i ph√°p: HippoRAG
HippoRAG ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t nh∆∞ m·ªôt khung ghi nh·ªõ l·∫•y c·∫£m h·ª©ng t·ª´ c·∫•u tr√∫c b·ªô nh·ªõ c·ªßa con ng∆∞·ªùi (g·ªìm t√¢n v·ªè n√£o, hippocampus, v√† v√πng trung gian). Gi·∫£i ph√°p n√†y bao g·ªìm hai giai ƒëo·∫°n:
1. Ch·ªâ m·ª•c h√≥a ngo·∫°i tuy·∫øn (Offline Indexing):
  - D√πng LLM ƒë·ªÉ tr√≠ch xu·∫•t c√°c triple ki·∫øn th·ª©c d·∫°ng KG m·ªü t·ª´ t√†i li·ªáu.
  - T·∫°o bi·ªÉu ƒë·ªì tri th·ª©c g·ªìm c√°c node kh√°i ni·ªám (phrase) v√† node ng·ªØ c·∫£nh (passage).
  - Ph√°t hi·ªán t·ª´ ƒë·ªìng nghƒ©a v√† li√™n k·∫øt c√°c kh√°i ni·ªám t∆∞∆°ng ƒë∆∞∆°ng b·∫±ng embedding.
2. Truy h·ªìi tr·ª±c tuy·∫øn (Online Retrieval):
  - D√πng m√¥ h√¨nh embedding ƒë·ªÉ li√™n k·∫øt c√¢u h·ªèi v·ªõi triple v√† passage.
  - √Åp d·ª•ng thu·∫≠t to√°n Personalized PageRank ƒë·ªÉ t√¨m ƒë∆∞·ªùng d·∫´n li√™n k·∫øt ng·ªØ nghƒ©a v√† truy xu·∫•t c√°c ƒëo·∫°n vƒÉn li√™n quan.
  - D√πng LLM ƒë·ªÉ l·ªçc triple theo ng·ªØ c·∫£nh truy v·∫•n (b·ªô nh·ªõ nh·∫≠n di·ªán ‚Äì recognition memory).
üéØ K·∫øt qu·∫£: HippoRAG 2 v∆∞·ª£t tr·ªôi h∆°n t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p hi·ªán t·∫°i tr√™n c·∫£ ba nƒÉng l·ª±c c·ªët l√µi: ghi nh·ªõ fact, hi·ªÉu ng·ªØ c·∫£nh v√† li√™n k·∫øt th√¥ng tin.

---
N·∫øu Qu·ªëc mu·ªën m√¨nh vi·∫øt l·∫°i b·∫£n n√†y theo b·∫£n slide tr√¨nh b√†y, d·∫°ng b√†i vi·∫øt b√°o c√°o khoa h·ªçc, ho·∫∑c b·∫£n n√≥i thuy·∫øt tr√¨nh mi·ªáng 2-3 ph√∫t, m√¨nh s·∫Ω chuy·ªÉn th·ªÉ ph√π h·ª£p nh√©. Qu·ªëc mu·ªën theo h∆∞·ªõng n√†o ti·∫øp?


ƒê·ªÉ m√¨nh gi·∫£i th√≠ch chi ti·∫øt s∆° ƒë·ªì HippoRAG 2 n√†y m·ªôt c√°ch d·ªÖ hi·ªÉu, chia l√†m 2 giai ƒëo·∫°n: Offline Indexing v√† Online Retrieval & QA, v·ªõi c√°c b∆∞·ªõc r√µ r√†ng nh√©!

---
üß† 1. Offline Indexing ‚Äì Giai ƒëo·∫°n t·∫°o b·ªô nh·ªõ (kh·ªüi t·∫°o bi·ªÉu ƒë·ªì tri th·ª©c)
üéØ M·ª•c ti√™u: Bi·∫øn c√°c ƒëo·∫°n vƒÉn (passages) th√†nh m·ªôt bi·ªÉu ƒë·ªì tri th·ª©c (Knowledge Graph ‚Äì KG) ƒë·ªÉ sau n√†y d·ªÖ truy h·ªìi th√¥ng minh h∆°n.
G·ªìm 3 b∆∞·ªõc:
üîµ (1) OpenIE by LLM
 ‚Äì D√πng LLM (Llama-3.3-70B) ƒë·ªÉ tr√≠ch xu·∫•t c√°c triple ki·∫øn th·ª©c t·ª´ m·ªói ƒëo·∫°n vƒÉn, d·∫°ng:
(subject ‚Äì relation ‚Äì object)
 üëâ V√≠ d·ª•: ("Elon Musk", "founder of", "SpaceX")
üîµ (2) Synonym detection by embedding
 ‚Äì D√πng m√¥ h√¨nh embedding ƒë·ªÉ t√¨m c√°c c·∫∑p t·ª´/c·ª•m t·ª´ gi·ªëng nhau v·ªÅ nghƒ©a, r·ªìi n·ªëi ch√∫ng l·∫°i b·∫±ng synonym edge.
 üëâ V√≠ d·ª•: ‚ÄúNYC‚Äù ‚Üî ‚ÄúNew York City‚Äù
üîµ (3) Dense-sparse integration
 ‚Äì T·∫°o 2 lo·∫°i node trong bi·ªÉu ƒë·ªì:
- Phrase node: c√°c kh√°i ni·ªám r√∫t g·ªçn, nh∆∞ ‚ÄúElon Musk‚Äù ho·∫∑c ‚ÄúCEO‚Äù
- Passage node: ch·ª©a to√†n b·ªô ƒëo·∫°n vƒÉn ‚Äì N·ªëi passage node v·ªõi c√°c phrase node t∆∞∆°ng ·ª©ng b·∫±ng context edge, ƒë·ªÉ bi·ªÉu ƒë·ªì v·ª´a c√≥ th√¥ng tin kh√°i ni·ªám, v·ª´a c√≥ ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß.
üì¶ Output cu·ªëi l√† m·ªôt Knowledge Graph gi√†u ng·ªØ nghƒ©a, v·ªõi c√°c lo·∫°i li√™n k·∫øt:
- Relation Edge (gi·ªØa concept v·ªõi concept)
- Synonym Edge
- Context Edge (gi·ªØa passage v√† concept)

---
üîç 2. Online Retrieval & QA ‚Äì Giai ƒëo·∫°n truy h·ªìi v√† tr·∫£ l·ªùi
üéØ M·ª•c ti√™u: T·ª´ c√¢u h·ªèi (query), t√¨m ƒë√∫ng c√°c ƒëo·∫°n vƒÉn li√™n quan ƒë·ªÉ tr·∫£ l·ªùi.
G·ªìm 5 b∆∞·ªõc:
üü¢ (1) Truy h·ªìi passages v√† triples
 ‚Äì D√πng retriever (NV-Embed-v2) ƒë·ªÉ truy h·ªìi c√°c triple v√† ƒëo·∫°n vƒÉn li√™n quan ƒë·∫øn c√¢u h·ªèi.
üü¢ (2) Recognition Memory ‚Äì L·ªçc triple
 ‚Äì D√πng LLM ƒë·ªÉ ƒë√°nh gi√° c√°c triple, gi·ªØ l·∫°i c√°c triple th·∫≠t s·ª± li√™n quan ƒë·∫øn c√¢u h·ªèi.
üü¢ (3) G√°n tr·ªçng s·ªë cho c√°c seed node
 ‚Äì C√°c triple v√† passage ƒë∆∞·ª£c ch·ªçn s·∫Ω tr·ªü th√†nh c√°c ‚Äúseed node‚Äù (node kh·ªüi ƒë·∫ßu). ‚Äì G√°n tr·ªçng s·ªë kh·ªüi ƒë·ªông (reset probability) cho t·ª´ng seed d·ª±a tr√™n m·ª©c ƒë·ªô li√™n quan.
üü¢ (4) Personalized PageRank (PPR)
 ‚Äì Ch·∫°y thu·∫≠t to√°n PPR tr√™n bi·ªÉu ƒë·ªì tri th·ª©c, ƒë·ªÉ t√¨m c√°c ƒëo·∫°n vƒÉn c√≥ k·∫øt n·ªëi ng·ªØ nghƒ©a m·∫°nh nh·∫•t ƒë·∫øn c√¢u h·ªèi, qua nhi·ªÅu b∆∞·ªõc reasoning.
üü¢ (5) QA Reading
 ‚Äì D√πng LLM ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë∆∞·ª£c ch·ªçn v√† t·∫°o c√¢u tr·∫£ l·ªùi cho truy v·∫•n.

---
üó∫Ô∏è Gi·∫£i th√≠ch c√°c k√Ω hi·ªáu trong s∆° ƒë·ªì
This content is only supported in a Lark Docs

---
N·∫øu Qu·ªëc mu·ªën, m√¨nh c√≥ th·ªÉ di·ªÖn gi·∫£i l·∫°i to√†n b·ªô th√†nh slide ho·∫∑c vi·∫øt b√°o c√°o t√≥m t·∫Øt ph·∫ßn ki·∫øn tr√∫c pipeline HippoRAG 2 theo h√¨nh n√†y nh√©. Qu·ªëc c·∫ßn b·∫£n n√†o?