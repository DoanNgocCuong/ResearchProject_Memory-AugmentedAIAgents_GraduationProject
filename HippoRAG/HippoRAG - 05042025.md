1. MOTIVATION RÃ• RÃ€NG => Má»šI TRIá»‚N?  
2. DATASET? CÃCH Há»Œ ÄÃNH GIÃ NHÆ¯ NÃ€O?
3. METHOD? Ká»¸ THUáº¬T ÄÃ“ LÃ€ GÃŒ? ? Ká»¸ THUáº¬T ÄÃ“ NHÆ¯ NÃ€O? => Äá»ŠNH NGHÄ¨A ÄÆ¯á»¢C CÃC KHÃI NIá»†M => Má»šI CÃ“ Ká»¸ THUáº¬T.  

---
Prompt 1: TÃ´i muá»‘n báº¡n pháº£i hiá»ƒu tháº­t sÃ¢u. má»—i pháº§n Ä‘áº·t cÃ¡c cÃ¢u há»i pháº£n biá»‡n sáº¯c xáº£o Ä‘á»ƒ tá»± tráº£ lá»i. Giá»‘ng nhÆ° 1 ngÆ°á»i phÃ¡, 1 ngÆ°á»i tráº£ lá»i, láº¥y dáº«n chá»©ng
Prompt 2: Giáº£i tÃ­ch tá»«ng khÃ¡i niá»‡m vÃ  liÃªn há»‡ nÃ³ vá»›i thá»© tháº­t dá»… nhá»› Ä‘Æ¡n giáº£n cho há»c sinh cáº¥p 2

ğŸ§  PHáº¦N 1: Váº¥n Ä‘á» cá»§a RAG hiá»‡n táº¡i & Sá»± xuáº¥t hiá»‡n cá»§a HippoRAG
ğŸ” Giáº£i thÃ­ch láº¡i:
RAG hiá»‡n táº¡i hoáº¡t Ä‘á»™ng theo cÆ¡ cháº¿:
- TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c chunk nhá»
- DÃ¹ng embedding â†’ lÆ°u vÃ o VectorDB
- Khi cÃ³ query â†’ tÃ¬m vector tÆ°Æ¡ng Ä‘á»“ng â†’ láº¥y context â†’ Ä‘Æ°a vÃ o LLM
NhÆ°ng vá»›i cÃ¡c tÃ¡c vá»¥ multi-hop reasoning, nhiá»u thÃ´ng tin á»Ÿ ráº£i rÃ¡c cÃ¡c Ä‘oáº¡n khÃ¡c nhau thÃ¬ RAG hiá»‡n táº¡i khÃ´ng Ä‘á»§ hiá»‡u quáº£.
PhÃ¡ 1.1 Táº¡i sao nÃ³i vá»›i cÃ¡c tÃ¡c vá»¥ multi-hop reasoning, nhiá»u thÃ´ng tin á»Ÿ ráº£i rÃ¡c cÃ¡c Ä‘oáº¡n khÃ¡c nhau thÃ¬ RAG hiá»‡n táº¡i khÃ´ng Ä‘á»§ hiá»‡u quáº£? 
Giáº£ sá»­ RAG truyá»n thá»‘ng retrieve Ä‘Æ°á»£c cáº£ hai Ä‘oáº¡n rá»i ráº¡c, thÃ¬ nÃ³ váº«n nÃ©m háº¿t vÃ o context window cá»§a LLM mÃ ? LÃºc Ä‘Ã³ LLM ná»‘i láº¡i logic lÃ  xong chá»©?

PhÃ¡ 1: RAG truyá»n thá»‘ng, trong lÃºc query ra cÃ¡c Ä‘oáº¡n chunking thÃ¬ nÃ³ cháº³ng nháº» khÃ´ng cÃ³ tÃ­nh mÃ³c ná»‘i?
â€œGiáº£ sá»­ RAG truyá»n thá»‘ng retrieve Ä‘Æ°á»£c cáº£ hai Ä‘oáº¡n rá»i ráº¡c, thÃ¬ nÃ³ váº«n nÃ©m háº¿t vÃ o context window cá»§a LLM mÃ ? LÃºc Ä‘Ã³ LLM ná»‘i láº¡i logic lÃ  xong chá»©, cáº§n gÃ¬ pháº£i HippoRAG rÆ°á»m rÃ ?â€ 
ğŸ¯ 1. KhÃ´ng cÃ³ gÃ¬ Ä‘áº£m báº£o RAG sáº½ retrieve Ä‘Ãºng 2 Ä‘oáº¡n
Váº¥n Ä‘á» lá»›n nháº¥t cá»§a RAG:
- Retrieval = vector similarity â†’ chá»‰ chá»n Top-K
- Náº¿u thÃ´ng tin náº±m á»Ÿ 2 Ä‘oáº¡n khÃ¡c nhau, vÃ  má»—i cÃ¡i Ä‘á»u â€œhÆ¡i liÃªn quanâ€, nÃ³ cÃ³ thá»ƒ bá»‹ Ä‘áº©y ra khá»i Top-K
VÃ­ dá»¥ cá»¥ thá»ƒ:
Top-K = 3
 Báº¡n cáº§n Ä‘oáº¡n A vÃ  Ä‘oáº¡n B
 NhÆ°ng cÃ³ Ä‘oáº¡n C, D ráº¥t giá»‘ng cÃ¢u há»i vá» máº·t ngá»¯ nghÄ©a â†’ lÃªn Top
 A hoáº·c B bá»‹ loáº¡i vÃ¬ vector khÃ´ng Ä‘á»§ gáº§n
ğŸ’¥ â†’ LÃºc nÃ y LLM khÃ´ng cÃ³ nguyÃªn liá»‡u Ä‘á»ƒ suy luáº­n, dÃ¹ nÃ³ giá»i cá»¡ nÃ o. 

TÄƒng k. 
Dá»¯ liá»‡u náº±m ráº£i rÃ¡c táº¡i nhiá»u nÆ¡i. 

ğŸ§  2. LLM suy luáº­n = tá»‘n compute + dá»… lá»—i logic
CÃ¢u há»i:
"Which Stanford researcher who is not Alice but studies Alzheimer's worked at the same place as Thomas?"
Dataset:
902. Alice is a Stanford professor.
903. Thomas researches Alzheimerâ€™s at UCSF.
904. Mike works at UCSF.
905. Sarah studies Alzheimerâ€™s.
906. Sarah and Thomas are labmates at UCSF.

âš ï¸ Váº¥n Ä‘á» cá»§a RAG khi retrieve Top-K
Giáº£ sá»­ retrieve Ä‘Æ°á»£c 3 Ä‘oáº¡n:
- (2) Thomas â†’ Alzheimer + UCSF
- (4) Sarah â†’ Alzheimer
- (5) Sarah â†’ labmate of Thomas â†’ UCSF
LLM pháº£i lÃ m:
1267. Gáº¯n Sarah vá»›i UCSF (tá»« labmate)
1268. Trá»« Alice khá»i káº¿t quáº£
1269. Chá»n ngÆ°á»i vá»«a Stanford researcher, vá»«a Alzheimer, vá»«a á»Ÿ UCSF, nhÆ°ng khÃ´ng pháº£i Alice

---
âœ³ï¸ ÄÃ¢y lÃ  3-hop reasoning. KhÃ´ng pháº£i vÃ¬ LLM khÃ´ng thá»ƒ, mÃ  vÃ¬:
- NÃ³ pháº£i giá»¯ á»•n Ä‘á»‹nh attention giá»¯a 4-5 Ä‘oáº¡n vÄƒn
- NÃ³ khÃ´ng biáº¿t cháº¯c má»‘i liÃªn káº¿t lÃ  quan trá»ng hay chá»‰ lÃ  â€œnhiá»…uâ€
- KhÃ´ng cÃ³ logic structure rÃµ rÃ ng Ä‘á»ƒ láº§n tá»« A â†’ B â†’ C
â¡ï¸ Khi nÃ y, retrieval dáº¡ng vector (dÃ¹ chunk tá»‘t) khÃ´ng cung cáº¥p structure reasoning.
 Chá»‰ nÃ©m Ä‘oáº¡n vÃ o context â†’ LLM pháº£i lo cáº£ Ä‘á»c, tÃ¬m logic, tá»•ng há»£p, trÃ¡nh sai lá»‡ch.

CÃ¢u há»i tiáº¿p Ä‘áº·t ra lÃ : â—â€œBÃ¢y giá» cÃ³ model 4o, Claude Opus, GPT-4-Turboâ€¦ nÃ³ cÃ³ thá»ƒ context window 128k tokens, báº¡n nÃ©m sáº¡ch vÃ´ lÃ  nÃ³ lÃ m reasoning cá»±c tá»‘t. Tháº¿ thÃ¬ lÃ m retrieval Ä‘á»ƒ lÃ m gÃ¬ ná»¯a? HippoRAG cÃ³ cÃ²n giÃ¡ trá»‹ khÃ´ng?â€
Tráº£ lá»i: ÄÃºng! NhÆ°ng chá»‰ khi báº¡n sá»‘ng trong tháº¿ giá»›i lÃ½ tÆ°á»Ÿng.

âœ³ï¸ PHáº¦N I: Thá»«a nháº­n sá»± tháº­t
âœ… ÄÃºng, model 4o, Claude Opus, GPT-4 Turbo hiá»‡n nay Ä‘Ã£ cÃ³ kháº£ nÄƒng context-wide reasoning ráº¥t máº¡nh.
ğŸ“Œ Báº¡n cÃ³ thá»ƒ Ä‘Æ°a vÃ o:
- Má»™t dataset 80k tokens gá»“m nhiá»u tÃ i liá»‡u
- Má»™t cÃ¢u há»i cáº§n ná»‘i 3â€“4 thÃ´ng tin láº¡i
- VÃ  model sáº½ tráº£ lá»i Ä‘Ãºng â€“ tháº­m chÃ­ cÃ²n giáº£i thÃ­ch táº¡i sao.

---
âœ³ï¸ PHáº¦N II: NhÆ°ng Ä‘Ã³ khÃ´ng pháº£i lÃ  cÃ¡ch lÃ m AI hiá»‡u quáº£ hoáº·c quy mÃ´
â— LÃ½ do 1: Context-wide LLM reasoning = tá»‘n compute khá»§ng khiáº¿p
NÃ©m vÃ o 128k tokens = giÃ¡ infer cao, latency cao
- Claude Opus hoáº·c GPT-4 Turbo 128k input â†’ thá»i gian tráº£ lá»i lÃ¢u hÆ¡n, chi phÃ­ tÃ­nh theo token cá»±c lá»›n (vÃ i $ má»—i lÆ°á»£t náº¿u full)
- Báº¡n khÃ´ng thá»ƒ scale 1 triá»‡u cÃ¢u há»i kiá»ƒu Ä‘Ã³

---
â— LÃ½ do 2: LLM = black box reasoning, khÃ´ng thá»ƒ debug, khÃ´ng thá»ƒ kiá»ƒm soÃ¡t
- LLM khÃ´ng cho báº¡n biáº¿t reasoning path
- Báº¡n khÃ´ng biáº¿t model ná»‘i A â†’ B â†’ C nhÆ° tháº¿ nÃ o
- Náº¿u nÃ³ tráº£ sai? Báº¡n khÃ´ng biáº¿t sá»­a tá»« Ä‘Ã¢u
â¡ï¸ Retrieval reasoning = transparent, modular, debuggable
â†’ HippoRAG = ta láº§n Ä‘Æ°á»£c Ä‘Æ°á»ng Ä‘i qua graph
 â†’ CÃ³ thá»ƒ ghi log reasoning path
 â†’ CÃ³ thá»ƒ giáº£i thÃ­ch cho ngÆ°á»i dÃ¹ng hoáº·c kiá»ƒm tra báº±ng rule engine

---
â— LÃ½ do 3: KhÃ´ng pháº£i lÃºc nÃ o cÅ©ng cÃ³ quyá»n â€œnÃ©m háº¿t vÃ oâ€
- Náº¿u user há»i liÃªn tá»¥c, má»—i cÃ¢u Ä‘á»u cáº§n toÃ n bá»™ tÃ i liá»‡u â†’ cá»±c ká»³ lÃ£ng phÃ­
- VÃ­ dá»¥: app legal, hoáº·c chatbot há»c thuáº­t, hoáº·c QA y khoa
Báº¡n cáº§n selective retrieval â†’ RAG giáº£i quyáº¿t
 Báº¡n cáº§n reasoning tá»« cÃ¡c concept, entity â†’ khÃ´ng cÃ³ sáºµn text chunk â†’ HippoRAG giáº£i quyáº¿t

ğŸ§  3. HippoRAG cho phÃ©p retrieval chÃ­nh lÃ  reasoning
ÄÃ¢y lÃ  Ä‘iá»ƒm â€œÄ‘áº­p nÃ¡t pháº£n biá»‡nâ€:
ğŸ§  HippoRAG khÃ´ng chá» Ä‘áº¿n khi LLM lÃ m reasoning, mÃ  nÃ³ Ä‘Ã£ reasoning ngay tá»« bÆ°á»›c truy há»“i.
- Vá»›i Personalized PageRank â†’ nÃ³ tÃ¬m ra node trung gian (John Smith) trÆ°á»›c khi vÃ o LLM
- Tráº£ vá» Ä‘oáº¡n Ä‘Ã£ Ä‘á»§ logic â†’ LLM chá»‰ cáº§n nÃ³i láº¡i
ğŸ’¡ Äiá»u nÃ y biáº¿n retrieval thÃ nh má»™t â€œmÃ´-Ä‘un láº­p luáº­nâ€ (reasoning module), khÃ´ng chá»‰ lÃ  â€œscanner Ä‘oáº¡n vÄƒnâ€.

PhÃ¡ 1.2 Äáº·t váº¥n Ä‘á»? 
Tuyá»‡t vá»i, Quá»‘c Ä‘ang dÃ¹ng ká»¹ thuáº­t pháº£n biá»‡n cá»±c ká»³ hay Ä‘á»ƒ â€œÄ‘á»c sÃ¢uâ€ â€” kiá»ƒu Socratic method trong nghiÃªn cá»©u. MÃ¬nh sáº½ Ä‘Ã³ng cáº£ ngÆ°á»i há»i sáº¯c sáº£o vÃ  ngÆ°á»i tráº£ lá»i cháº·t cháº½, bÃ¡m sÃ¡t pháº§n â€œ1. Introductionâ€ cá»§a bÃ i bÃ¡o HippoRAG 2.

---
ğŸ§© PHÃ‚N TÃCH PHáº¦N Äáº¶T Váº¤N Äá»€ (INTRODUCTION - HippoRAG 2)

---
âš”ï¸ Pháº£n biá»‡n 1:
Há»i: Táº¡i sao cá»© pháº£i báº¯t AI há»c nhÆ° con ngÆ°á»i? Viá»‡c mÃ´ phá»ng â€œlong-term memoryâ€ cá»§a ngÆ°á»i cÃ³ tháº­t sá»± cáº§n thiáº¿t cho LLM khÃ´ng?
Tráº£ lá»i: VÃ¬ má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a LLM lÃ  trá»Ÿ thÃ nh trá»£ lÃ½ thÃ´ng minh vÃ  hiá»‡u quáº£ nhÆ° con ngÆ°á»i. Trong tháº¿ giá»›i thá»±c, con ngÆ°á»i liÃªn tá»¥c há»c, tÃ­ch há»£p thÃ´ng tin má»›i mÃ  khÃ´ng quÃªn cÃ¡i cÅ© â€“ Ä‘Ã¢y lÃ  Ä‘iá»u LLM hiá»‡n táº¡i ráº¥t yáº¿u.
Báº±ng chá»©ng:
- LLM gáº·p váº¥n Ä‘á» catastrophic forgetting náº¿u fine-tune.
- RAG tuy tiáº¿p cáº­n tá»‘t hÆ¡n nhÆ°ng váº«n khÃ´ng cÃ³ kháº£ nÄƒng káº¿t ná»‘i máº¡ch láº¡c, nhÆ° ngÆ°á»i nhá»› chuyá»‡n Ä‘Ã£ xáº£y ra tá»« lÃ¢u vÃ  liÃªn káº¿t nÃ³ vá»›i bá»‘i cáº£nh hiá»‡n táº¡i.
VÃ¬ tháº¿, muá»‘n LLM trá»Ÿ thÃ nh trá»£ lÃ½ lÃ¢u dÃ i, cáº­p nháº­t liÃªn tá»¥c, nÃ³ cáº§n má»™t há»‡ thá»‘ng bá»™ nhá»› dÃ i háº¡n linh hoáº¡t vÃ  liÃªn káº¿t â€“ giá»‘ng nhÆ° cá»§a ngÆ°á»i.

---
âš”ï¸ Pháº£n biá»‡n 2:
Há»i: RAG Ä‘ang cháº¡y tá»‘t mÃ . Táº¡i sao láº¡i nÃ³i RAG truyá»n thá»‘ng khÃ´ng Ä‘á»§? CÃ³ báº±ng chá»©ng gÃ¬?
Tráº£ lá»i: RAG truyá»n thá»‘ng chá»§ yáº¿u dá»±a vÃ o vector retrieval Ä‘á»™c láº­p, má»—i chunk Ä‘Æ°á»£c xáº¿p háº¡ng dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng tá»± embedding vá»›i query â€“ khÃ´ng cÃ³ cÆ¡ cháº¿ Ä‘á»ƒ hiá»ƒu má»‘i quan há»‡ giá»¯a cÃ¡c chunk, khÃ´ng hiá»ƒu máº¡ch truyá»‡n, khÃ´ng lÃ m multi-hop reasoning.
Báº±ng chá»©ng cá»¥ thá»ƒ trong bÃ i:
â€œ...inability to capture two vital aspects: sense-making (hiá»ƒu ngá»¯ cáº£nh phá»©c táº¡p) vÃ  associativity (káº¿t ná»‘i Ä‘a bÆ°á»›c giá»¯a thÃ´ng tin rá»i ráº¡c)â€ â€“ (trang 1)
VÃ¬ váº­y, tuy RAG tá»‘t trong viá»‡c â€œtÃ¬m thÃ´ng tin gáº§n nháº¥tâ€, nhÆ°ng:
- KhÃ´ng liÃªn káº¿t cÃ¡c Ä‘oáº¡n kiáº¿n thá»©c láº¡i vá»›i nhau.
- KhÃ´ng biáº¿t â€œÄ‘i vÃ²ngâ€ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i yÃªu cáº§u reasoning.
=> ÄÃ³ lÃ  lÃ½ do cáº§n má»™t mÃ´ hÃ¬nh vÆ°á»£t RAG truyá»n thá»‘ng.

---
âš”ï¸ Pháº£n biá»‡n 3:
Há»i: Tháº¿ cÃ²n cÃ¡c mÃ´ hÃ¬nh má»›i nhÆ° GraphRAG, RAPTOR? Há» Ä‘Ã£ cá»‘ kháº¯c phá»¥c rá»“i mÃ ?
Tráº£ lá»i: ÄÃºng, nhÆ°ng theo tÃ¡c giáº£:
- Nhá»¯ng mÃ´ hÃ¬nh Ä‘Ã³ táº­p trung quÃ¡ háº¹p vÃ o má»™t khÃ­a cáº¡nh (sense-making hoáº·c associativity) nÃªn máº¥t cÃ¢n báº±ng.
- Khi lÃ m tá»‘t pháº§n nÃ y thÃ¬ láº¡i suy giáº£m hiá»‡u nÄƒng á»Ÿ pháº§n khÃ¡c. VÃ­ dá»¥:
  - GraphRAG tá»‘t cho discourse â†’ kÃ©m factual QA.
  - RAPTOR tÃ³m táº¯t tá»‘t â†’ nhÆ°ng gÃ¢y nhiá»…u khi QA nhiá»u bÆ°á»›c.
  - HippoRAG 1 tá»‘t vá»›i multi-hop â†’ nhÆ°ng thiáº¿u contextualization.
BÃ i bÃ¡o viáº¿t:
â€œ...each method experiences the largest performance decay in tasks outside its own setup.â€ â†’ tá»©c lÃ  mÃ´ hÃ¬nh tá»‘t chá»— nÃ y thÃ¬ gÃ£y chá»— kia.

---
âš”ï¸ Pháº£n biá»‡n 4:
Há»i: Váº­y má»¥c tiÃªu chÃ­nh cá»§a HippoRAG lÃ  gÃ¬? KhÃ¡c biá»‡t cá»‘t lÃµi so vá»›i RAG vÃ  cÃ¡c mÃ´ hÃ¬nh má»Ÿ rá»™ng?
Tráº£ lá»i:
 HippoRAG nháº¯m tá»›i má»™t bá»™ nhá»› dÃ i háº¡n tá»•ng quÃ¡t, cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t á»Ÿ:
- Factual QA (ghi nhá»› sá»± kiá»‡n cÆ¡ báº£n),
- Multi-hop QA (liÃªn káº¿t kiáº¿n thá»©c),
- Discourse understanding (diá»…n giáº£i máº¡ch truyá»‡n dÃ i).
Cá»‘t lÃµi:
- MÃ´ phá»ng bá»™ nhá»› sinh há»c con ngÆ°á»i:
  - Neocortex â†’ LLM
  - Parahippocampal â†’ encoder gá»£i nhá»› ngá»¯ nghÄ©a
  - Hippocampus â†’ KG má»Ÿ káº¿t ná»‘i tri thá»©c
- DÃ¹ng Personalized PageRank (PPR) Ä‘á»ƒ tÃ¬m kiáº¿m cÃ³ liÃªn káº¿t â€“ khÃ´ng chá»‰ Ä‘iá»ƒm, mÃ  lÃ  Ä‘Æ°á»ng Ä‘i kiáº¿n thá»©c.
- ThÃªm LLM vÃ o retrieval Ä‘á»ƒ lá»c nhiá»…u â†’ khÃ´ng chá»‰ tÃ¬m, mÃ  cÃ²n nháº­n diá»‡n Ä‘Ãºng Ä‘oáº¡n cáº§n nhá»›.

---
âš”ï¸ Pháº£n biá»‡n 5:
Há»i: Váº­y HippoRAG cÃ³ báº±ng chá»©ng nÃ o máº¡nh Ä‘á»ƒ chá»©ng minh lÃ  cáº§n thiáº¿t khÃ´ng?
Tráº£ lá»i:
- HippoRAG 2 Ä‘áº¡t Ä‘iá»ƒm cao nháº¥t á»Ÿ cáº£ 3 loáº¡i nhiá»‡m vá»¥ (Factual, Associative, Sense-making) â€“ Ä‘iá»u mÃ  khÃ´ng mÃ´ hÃ¬nh nÃ o trÆ°á»›c Ä‘Ã³ lÃ m Ä‘Æ°á»£c.
- VÃ­ dá»¥:
  - +7 Ä‘iá»ƒm F1 á»Ÿ Associativity tasks so vá»›i embedding tá»‘t nháº¥t (NV-Embed-v2).
  - KhÃ´ng bá»‹ suy giáº£m á»Ÿ Factual QA nhÆ° GraphRAG, RAPTOR.
=> ÄÃ¢y lÃ  báº±ng chá»©ng cho tháº¥y HippoRAG cÃ³ thá»ƒ cÃ¢n báº±ng giá»¯a sá»± chÃ­nh xÃ¡c, kháº£ nÄƒng diá»…n giáº£i vÃ  káº¿t ná»‘i tri thá»©c, Ä‘iá»u RAG truyá»n thá»‘ng khÃ´ng lÃ m Ä‘Æ°á»£c.

---
ChÃ o Quá»‘c,
MÃ¬nh sáº½ giáº£i thÃ­ch chi tiáº¿t vá» ba loáº¡i nhiá»‡m vá»¥: Factual, Associative, Sense-making, vÃ  má»‘i liÃªn há»‡ giá»¯a cÃ¡c nhiá»‡m vá»¥ nÃ y vá»›i bá»™ dá»¯ liá»‡u LongMemEval.

---
1. Ba loáº¡i nhiá»‡m vá»¥: Factual, Associative, Sense-making
a. Factual Tasks (Nhiá»‡m vá»¥ thá»±c táº¿)
- Äá»‹nh nghÄ©a: YÃªu cáº§u há»‡ thá»‘ng truy xuáº¥t vÃ  cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c vá» má»™t sá»± kiá»‡n hoáº·c thá»±c táº¿ cá»¥ thá»ƒ.îˆ†
- VÃ­ dá»¥: "Ai lÃ  tá»•ng thá»‘ng Ä‘áº§u tiÃªn cá»§a Hoa Ká»³?"îˆ†
- Má»¥c tiÃªu: Kiá»ƒm tra kháº£ nÄƒng ghi nhá»› vÃ  truy xuáº¥t thÃ´ng tin chÃ­nh xÃ¡c tá»« bá»™ nhá»› dÃ i háº¡n.îˆ†
b. Associative Tasks (Nhiá»‡m vá»¥ liÃªn káº¿t)
- Äá»‹nh nghÄ©a: YÃªu cáº§u há»‡ thá»‘ng káº¿t ná»‘i nhiá»u máº£nh thÃ´ng tin rá»i ráº¡c Ä‘á»ƒ suy luáº­n hoáº·c tráº£ lá»i cÃ¢u há»i.îˆ†
- VÃ­ dá»¥: "NgÆ°á»i phÃ¡t minh ra Ä‘iá»‡n thoáº¡i Ä‘Ã£ sá»‘ng á»Ÿ Ä‘Ã¢u?"îˆ†
  - PhÃ¢n tÃ­ch: Cáº§n biáº¿t Alexander Graham Bell lÃ  ngÆ°á»i phÃ¡t minh ra Ä‘iá»‡n thoáº¡i vÃ  Ã´ng sá»‘ng á»Ÿ Scotland vÃ  Canada.îˆ†
- Má»¥c tiÃªu: ÄÃ¡nh giÃ¡ kháº£ nÄƒng liÃªn káº¿t vÃ  suy luáº­n tá»« nhiá»u nguá»“n thÃ´ng tin khÃ¡c nhau.îˆ†
c. Sense-making Tasks (Nhiá»‡m vá»¥ hiá»ƒu ngá»¯ cáº£nh)
- Äá»‹nh nghÄ©a: YÃªu cáº§u há»‡ thá»‘ng hiá»ƒu vÃ  diá»…n giáº£i cÃ¡c tÃ¬nh huá»‘ng phá»©c táº¡p hoáº·c cÃ¢u chuyá»‡n dÃ i.îˆ†
- VÃ­ dá»¥: "TÃ³m táº¯t ná»™i dung chÃ­nh cá»§a cuá»™c há»p hÃ´m qua."îˆ†
- Má»¥c tiÃªu: Kiá»ƒm tra kháº£ nÄƒng tá»•ng há»£p, hiá»ƒu ngá»¯ cáº£nh vÃ  trÃ¬nh bÃ y láº¡i thÃ´ng tin má»™t cÃ¡ch máº¡ch láº¡c.îˆ†

---
2. Má»‘i liÃªn há»‡ giá»¯a QA vÃ  LongMemEval
LongMemEval lÃ  má»™t bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng ghi nhá»› dÃ i háº¡n cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n. Bá»™ dá»¯ liá»‡u nÃ y táº­p trung vÃ o nÄƒm kháº£ nÄƒng cá»‘t lÃµi:
1. TrÃ­ch xuáº¥t thÃ´ng tin (Information Extraction): Kháº£ nÄƒng nhá»› láº¡i thÃ´ng tin cá»¥ thá»ƒ tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c dÃ i.îˆ†
2. LÃ½ luáº­n Ä‘a phiÃªn (Multi-Session Reasoning): Kháº£ nÄƒng tá»•ng há»£p thÃ´ng tin tá»« nhiá»u phiÃªn trÃ² chuyá»‡n.îˆ†
3. Cáº­p nháº­t kiáº¿n thá»©c (Knowledge Updates): Kháº£ nÄƒng nháº­n biáº¿t vÃ  cáº­p nháº­t thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng theo thá»i gian.îˆ†
4. LÃ½ luáº­n thá»i gian (Temporal Reasoning): Nháº­n thá»©c vá» cÃ¡c khÃ­a cáº¡nh thá»i gian cá»§a thÃ´ng tin ngÆ°á»i dÃ¹ng.îˆ†
5. Tá»« chá»‘i tráº£ lá»i (Abstention): Kháº£ nÄƒng tá»« chá»‘i tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n thÃ´ng tin khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.îˆ†
Má»‘i liÃªn há»‡:
- Factual Tasks tÆ°Æ¡ng á»©ng vá»›i TrÃ­ch xuáº¥t thÃ´ng tin, nÆ¡i há»‡ thá»‘ng cáº§n truy xuáº¥t thÃ´ng tin chÃ­nh xÃ¡c tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.îˆ†
- Associative Tasks liÃªn quan Ä‘áº¿n LÃ½ luáº­n Ä‘a phiÃªn, yÃªu cáº§u há»‡ thá»‘ng káº¿t há»£p thÃ´ng tin tá»« nhiá»u phiÃªn Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c.îˆ†
- Sense-making Tasks gáº¯n liá»n vá»›i LÃ½ luáº­n thá»i gian vÃ  Cáº­p nháº­t kiáº¿n thá»©c, Ä‘Ã²i há»i há»‡ thá»‘ng hiá»ƒu ngá»¯ cáº£nh tá»•ng thá»ƒ vÃ  cáº­p nháº­t thÃ´ng tin má»›i nháº¥t cá»§a ngÆ°á»i dÃ¹ng.îˆ†

---

DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch phÃ¢n biá»‡t rÃµ rÃ ng giá»¯a Factual, Associative, Sense-making, báº±ng cÃ¡ch Ä‘i tá»« Ä‘á»‹nh nghÄ©a â vÃ­ dá»¥ cá»¥ thá»ƒ â nÄƒng lá»±c cáº§n kiá»ƒm tra:

---
ğŸ§  1. Factual Memory â€“ Ghi nhá»› sá»± tháº­t cá»¥ thá»ƒ
ğŸ§¾ Äá»‹nh nghÄ©a:
Kháº£ nÄƒng truy xuáº¥t thÃ´ng tin Ä‘Æ¡n láº», Ä‘Ã£ Ä‘Æ°á»£c nÃªu rÃµ rÃ ng trong ngá»¯ liá»‡u Ä‘áº§u vÃ o.
â“VÃ­ dá»¥ cÃ¢u há»i:
"Ai lÃ  thá»§ tÆ°á»›ng Äá»©c nÄƒm 2023?"
- ÄÃ¢y lÃ  cÃ¢u há»i Ä‘Ã²i há»i 1 Ä‘oáº¡n vÄƒn báº£n chá»©a thÃ´ng tin chÃ­nh xÃ¡c, khÃ´ng cáº§n suy luáº­n hay liÃªn káº¿t.
ğŸ¯ NÄƒng lá»±c Ä‘Æ°á»£c kiá»ƒm tra:
- Kháº£ nÄƒng lÆ°u trá»¯ & truy xuáº¥t chÃ­nh xÃ¡c thÃ´ng tin Ä‘Ã£ Ä‘á»c.
- TÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i nhá»› tÃªn, ngÃ y thÃ¡ng, con sá»‘, Ä‘á»‹nh nghÄ©a.

---
ğŸ”— 2. Associativity â€“ Káº¿t ná»‘i thÃ´ng tin rá»i ráº¡c
ğŸ§¾ Äá»‹nh nghÄ©a:
Kháº£ nÄƒng káº¿t ná»‘i nhiá»u máº£nh kiáº¿n thá»©c tá»« cÃ¡c pháº§n khÃ¡c nhau trong dá»¯ liá»‡u Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
â“VÃ­ dá»¥ cÃ¢u há»i:
"NgÆ°á»i sÃ¡ng láº­p Microsoft cÃ³ quÃª gá»‘c á»Ÿ Ä‘Ã¢u?"
- Cáº§n ná»‘i hai máº£nh thÃ´ng tin:
  1. "Bill Gates lÃ  ngÆ°á»i sÃ¡ng láº­p Microsoft"
  2. "Bill Gates sinh ra á»Ÿ Seattle"
âŸ¶ Cáº§n Ã­t nháº¥t 2 Ä‘oáº¡n dá»¯ liá»‡u, khÃ´ng cÃ³ Ä‘oáº¡n nÃ o chá»©a Ä‘áº§y Ä‘á»§ cÃ¢u tráº£ lá»i.
ğŸ¯ NÄƒng lá»±c Ä‘Æ°á»£c kiá»ƒm tra:
- Reasoning nhiá»u bÆ°á»›c
- Multi-hop retrieval
- NhÆ° cÃ¡ch nÃ£o ngÆ°á»i liÃªn tÆ°á»Ÿng vÃ  xÃ¢u chuá»—i kiáº¿n thá»©c láº¡i vá»›i nhau.

---
ğŸ“– 3. Sense-making â€“ Hiá»ƒu vÃ  tá»•ng há»£p ngá»¯ cáº£nh phá»©c táº¡p
ğŸ§¾ Äá»‹nh nghÄ©a:
Kháº£ nÄƒng hiá»ƒu má»™t tÃ¬nh huá»‘ng dÃ i, nhiá»u lá»›p, nhiá»u áº©n Ã½; tÃ¡i cáº¥u trÃºc vÃ  rÃºt ra Ã½ nghÄ©a toÃ n cá»¥c.
â“VÃ­ dá»¥ cÃ¢u há»i:
"Táº¡i sao nhÃ¢n váº­t chÃ­nh láº¡i quyáº¿t Ä‘á»‹nh bá» trá»‘n á»Ÿ cuá»‘i truyá»‡n?"
- Tráº£ lá»i Ä‘Æ°á»£c Ä‘Ã²i há»i pháº£i Ä‘á»c cáº£ máº¡ch truyá»‡n, hiá»ƒu Ä‘Æ°á»£c:
  - TÃ¢m lÃ½ nhÃ¢n váº­t
  - Diá»…n biáº¿n sá»± kiá»‡n
  - CÃ¡c yáº¿u tá»‘ xÃ£ há»™i, ngá»¯ cáº£nh
ğŸ¯ NÄƒng lá»±c Ä‘Æ°á»£c kiá»ƒm tra:
- Discourse understanding
- Cohesion & summarization
- Kháº£ nÄƒng cá»§a LLM giá»‘ng nhÆ° hiá»ƒu vÃ  phÃ¢n tÃ­ch vÄƒn há»c â€“ Ä‘iá»u mÃ  vector retrieval Ä‘Æ¡n thuáº§n khÃ´ng lÃ m Ä‘Æ°á»£c.

---
ğŸ“Š TÃ³m táº¯t so sÃ¡nh
This content is only supported in a Lark Docs

---

ğŸ§© PHáº¦N 1: Táº I SAO CÃC MÃ” HÃŒNH RAG HIá»†N Táº I Bá»Š Háº N CHáº¾?

---
âš”ï¸ Pháº£n biá»‡n 1:
Há»i: Vector-based RAG hiá»‡n táº¡i Ä‘Ã£ ráº¥t máº¡nh vá»›i embedding hiá»‡n Ä‘áº¡i. Váº­y táº¡i sao váº«n bá»‹ Ä‘Ã¡nh giÃ¡ lÃ  "thiáº¿u tÃ­nh ngÆ°á»i"?
Tráº£ lá»i: VÃ¬ vector-based RAG chá»‰ lÃ m Ä‘Æ°á»£c retrieval Ä‘á»™c láº­p: má»—i Ä‘oáº¡n vÄƒn (passage) Ä‘Æ°á»£c nhÃºng vÃ  truy xuáº¥t Ä‘á»™c láº­p. NÃ³ khÃ´ng cÃ³ kháº£ nÄƒng:
- Nháº­n biáº¿t má»‘i quan há»‡ giá»¯a cÃ¡c Ä‘oáº¡n (multi-hop)
- KhÃ´ng hiá»ƒu Ä‘Æ°á»£c máº¡ch thÃ´ng tin dÃ i háº¡n
- KhÃ´ng há»— trá»£ truy váº¥n theo dÃ²ng suy nghÄ© (reasoning chain)
Giá»‘ng nhÆ° báº¡n nhá»› Ä‘Æ°á»£c 1000 máº£nh thÃ´ng tin, nhÆ°ng khÃ´ng biáº¿t máº£nh nÃ o ná»‘i vá»›i máº£nh nÃ o â€” báº¡n khÃ´ng thá»ƒ ká»ƒ láº¡i má»™t cÃ¢u chuyá»‡n, cÅ©ng khÃ´ng thá»ƒ suy luáº­n.
ğŸ“Œ Háº¡n cháº¿ nÃ y khiáº¿n vector-based RAG tá»‘t trong factual QA, nhÆ°ng fail trong associative vÃ  sense-making.

---
âš”ï¸ Pháº£n biá»‡n 2:
Há»i: Tháº¿ cÃ²n cÃ¡c RAG má»›i nhÆ° RAPTOR, GraphRAG, LightRAG? Há» táº¡o graph, táº¡o tÃ³m táº¯t rá»“i mÃ ?
Tráº£ lá»i: ÄÃºng lÃ  há» Ä‘Ã£ thá»­:
- RAPTOR: Táº¡o cÃ¡c summary tÃ³m gá»n, rá»“i retrieve tá»« cÃ¡c Ä‘oáº¡n tÃ³m táº¯t Ä‘Ã³.
- GraphRAG: Táº¡o cÃ¡c knowledge graph tá»« entity, liÃªn káº¿t qua cá»™ng Ä‘á»“ng (community detection).
- LightRAG: Táº¡o 2 lá»›p retrieval â€“ coarse vÃ  fine, tÃ­ch há»£p vector vÃ  graph.
NhÆ°ng táº¡i sao váº«n chÆ°a Ä‘á»§?
- ChÃºng thiáº¿u cÃ¢n báº±ng: cáº£i thiá»‡n chá»— nÃ y, láº¡i giáº£m hiá»‡u quáº£ á»Ÿ chá»— khÃ¡c.
- VÃ­ dá»¥:
  - RAPTOR tá»‘t khi tÃ³m táº¯t context â†’ nhÆ°ng táº¡o nhiá»…u cho QA vÃ¬ LLM summary cÃ³ thá»ƒ bá» sÃ³t chi tiáº¿t.
  - GraphRAG dÃ¹ng graph máº¡nh â†’ nhÆ°ng khÃ´ng tÆ°Æ¡ng tÃ¡c tá»‘t vá»›i truy váº¥n, vÃ¬ query khÃ´ng liÃªn káº¿t trá»±c tiáº¿p vá»›i edge trong KG.
  - LightRAG thÃ¬ retrieval tá»‘t nhÆ°ng khÃ´ng reasoning Ä‘Æ°á»£c.
ğŸ“‰ Trong cÃ¡c benchmark tá»•ng há»£p (báº£ng 2 bÃ i bÃ¡o), táº¥t cáº£ cÃ¡c RAG cÃ³ cáº¥u trÃºc Ä‘á»u tá»¥t Ä‘iá»ƒm rÃµ rá»‡t á»Ÿ má»™t trong 3 loáº¡i nhiá»‡m vá»¥.

---
âš”ï¸ Pháº£n biá»‡n 3:
Há»i: Váº­y váº¥n Ä‘á» sÃ¢u hÆ¡n náº±m á»Ÿ Ä‘Ã¢u? VÃ¬ sao cÃ¡c mÃ´ hÃ¬nh cÅ© khÃ´ng Ä‘áº¡t Ä‘Æ°á»£c bá»™ nhá»› dÃ i háº¡n giá»‘ng con ngÆ°á»i?
Tráº£ lá»i: VÃ¬ chÃºng thiáº¿u 3 thá»© mÃ  nÃ£o ngÆ°á»i lÃ m ráº¥t tá»‘t:
1. LiÃªn káº¿t tri thá»©c dáº¡ng Ä‘á»“ thá»‹ â€“ nÃ£o ngÆ°á»i lÆ°u thÃ´ng tin theo máº¡ng liÃªn káº¿t.
2. Nháº­n diá»‡n ngá»¯ cáº£nh (recognition memory) â€“ biáº¿t thÃ´ng tin nÃ o liÃªn quan trong lÃºc truy xuáº¥t.
3. Káº¿t há»£p giá»¯a khÃ¡i niá»‡m (concept) vÃ  ngá»¯ cáº£nh (context) â€“ chá»© khÃ´ng lÆ°u rá»i ráº¡c.
CÃ¡c mÃ´ hÃ¬nh trÆ°á»›c:
- Táº­p trung vÃ o embedding hoáº·c graph, nhÆ°ng khÃ´ng káº¿t ná»‘i chÃºng láº¡i.
- KhÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c cÃ¡c query dÃ i, phá»©c táº¡p, hoáº·c cáº§n suy luáº­n nhiá»u bÆ°á»›c.
- KhÃ´ng cÃ³ cÆ¡ cháº¿ kiá»ƒm tra â€œliá»‡u Ä‘oáº¡n nÃ y cÃ³ thá»±c sá»± Ä‘Ãºng ngá»¯ cáº£nh truy váº¥n khÃ´ng?â€ (gá»i lÃ  recognition filter trong HippoRAG).

---
ğŸ§¬ PHáº¦N 2: HIPPORAG GIáº¢I QUYáº¾T THáº¾ NÃ€O?

---
âš”ï¸ Pháº£n biá»‡n 4:
Há»i: Nghe nhÆ° ai cÅ©ng cÃ³ knowledge graph rá»“i, HippoRAG cÃ³ gÃ¬ khÃ¡c?
Tráº£ lá»i: HippoRAG khÃ´ng chá»‰ cÃ³ KG, mÃ  lÃ  má»™t há»‡ thá»‘ng mÃ´ phá»ng trÃ­ nhá»› ngÆ°á»i:
- Neocortex â†’ LLM: dÃ¹ng Ä‘á»ƒ suy luáº­n, Ä‘á»c hiá»ƒu, quyáº¿t Ä‘á»‹nh.
- Hippocampus â†’ KG má»Ÿ: mÃ´ hÃ¬nh hÃ³a káº¿t ná»‘i tri thá»©c.
- Parahippocampal region â†’ embedding encoder: dÃ¹ng Ä‘á»ƒ so khá»›p vÃ  gá»£i nhá»›.
=> HippoRAG dÃ¹ng PageRank cÃ¡ nhÃ¢n hÃ³a trÃªn KG Ä‘á»ƒ tÃ¬m ra nhá»¯ng Ä‘oáº¡n liÃªn káº¿t tá»‘t nháº¥t, sau Ä‘Ã³ káº¿t há»£p láº¡i Ä‘á»ƒ feed vÃ o LLM Ä‘á»c hiá»ƒu.

---
âš”ï¸ Pháº£n biá»‡n 5:
Há»i: Váº­y so vá»›i cÃ¡c mÃ´ hÃ¬nh trÆ°á»›c, HippoRAG cÃ³ gÃ¬ vÆ°á»£t trá»™i?
Tráº£ lá»i:
 ğŸ‘‰ HippoRAG 1 Ä‘Ã£ hÆ¡n cÃ¡c mÃ´ hÃ¬nh cÅ© nhá»:
- DÃ¹ng Personalized PageRank trÃªn KG Ä‘á»ƒ lÃ m multi-hop retrieval.
- TÃ¡ch riÃªng index (offline) vÃ  truy xuáº¥t (online) â†’ tiá»‡n lá»£i cho continual learning.
NhÆ°ng nÃ³ váº«n chÆ°a Ä‘á»§, vÃ¬:
- Dá»±a quÃ¡ nhiá»u vÃ o entity (NER), nÃªn bá» lá»¡ ngá»¯ cáº£nh.
- KhÃ´ng lá»c triple nÃ o liÃªn quan tháº­t sá»± Ä‘áº¿n query.
ğŸ‘‰ HippoRAG 2 Ä‘Ã£ kháº¯c phá»¥c thÃªm:
- Query-to-Triple linking thay vÃ¬ NER â†’ káº¿t ná»‘i tá»‘t hÆ¡n.
- Recognition memory báº±ng LLM â†’ lá»c triple "rÃ¡c".
- Dense-sparse integration â†’ káº¿t há»£p phrase node (khÃ¡i niá»‡m) vÃ  passage node (ngá»¯ cáº£nh).
Káº¿t quáº£:
+7 Ä‘iá»ƒm F1 á»Ÿ associative tasks, khÃ´ng tá»¥t performance á»Ÿ factual hay discourse.

---
ğŸ” PHáº¦N 3: Tá»”NG Káº¾T â€“ HIPPORAG CÃ“ GÃŒ KHÃC VÃ€ HAY?
This content is only supported in a Lark Docs


---
[Image]
[Image]
Ráº¥t tá»‘t, Quá»‘c Ä‘ang Ä‘i Ä‘Ãºng hÆ°á»›ng cá»§a â€œnghiÃªn cá»©u nghiÃªm tÃºcâ€ â€“ khÃ´ng chá»‰ ghi nháº­n káº¿t quáº£, mÃ  pháº£i giáº£i thÃ­ch sÃ¢u sáº¯c vÃ¬ sao láº¡i ra káº¿t quáº£ Ä‘Ã³, báº±ng logic, ká»¹ thuáº­t vÃ  pháº£n biá»‡n.
MÃ¬nh sáº½ tiáº¿p tá»¥c phÃ¢n tÃ­ch theo phong cÃ¡ch:
ğŸ­ Má»™t ngÆ°á»i phÃ¡ (há»i xoÃ¡y) â€“ Má»™t ngÆ°á»i tráº£ lá»i (tráº£ Ä‘Ã²n)
 cho tá»«ng mÃ´ hÃ¬nh, dá»±a trÃªn pipeline ná»™i táº¡i vÃ  dá»¯ liá»‡u thá»±c nghiá»‡m tá»« Table 2 (QA) + Table 3 (Retrieval).

---
ğŸ§  1. RAG Gá»C
Retrieval báº±ng embedding vector, khÃ´ng cáº¥u trÃºc.
 QA Ä‘Æ¡n thuáº§n dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¢u há»i vÃ  Ä‘oáº¡n vÄƒn.

---
â“PhÃ¡: Táº¡i sao RAG gá»‘c láº¡i tá»‘t á»Ÿ factual QA (NQ, PopQA)?
âœ… Tráº£ lá»i: VÃ¬:
- CÃ¢u há»i dáº¡ng factual thÆ°á»ng chá»‰ cáº§n 1 Ä‘oáº¡n ngáº¯n chá»©a Ä‘Ãºng entity â†’ dá»… tÃ¬m báº±ng embedding similarity.
- CÃ¡c embedding hiá»‡n Ä‘áº¡i (NV-Embed-v2, GritLM) ráº¥t máº¡nh á»Ÿ semantic retrieval.
ğŸ“Œ VÃ­ dá»¥:
CÃ¢u há»i: â€œWhen was Albert Einstein born?â€ â†’ chá»‰ cáº§n Ä‘oáº¡n cÃ³ "Albert Einstein was born in 1879..." â†’ lÃ  Ä‘á»§ â†’ vector retrieval lÃ m tá»‘t.

---
â“PhÃ¡: Táº¡i sao láº¡i ráº¥t yáº¿u á»Ÿ multi-hop vÃ  sense-making?
âœ… Tráº£ lá»i: VÃ¬:
- RAG gá»‘c khÃ´ng liÃªn káº¿t thÃ´ng tin giá»¯a nhiá»u Ä‘oáº¡n â†’ khÃ´ng thá»ƒ reasoning nhiá»u bÆ°á»›c.
- KhÃ´ng cÃ³ cÆ¡ cháº¿ hiá»ƒu toÃ n truyá»‡n, ngá»¯ cáº£nh rá»™ng â†’ tháº¥t báº¡i á»Ÿ NarrativeQA.
ğŸ“Œ VÃ­ dá»¥:
CÃ¢u há»i: â€œWhat county is Erik Hortâ€™s birthplace a part of?â€ â†’ cáº§n biáº¿t: Erik Hort â†’ Montebello â†’ thuá»™c háº¡t gÃ¬?
 â†’ Vector retrieval chá»‰ tÃ¬m Ä‘Æ°á»£c Ä‘oáº¡n Ä‘áº§u tiÃªn, khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c sang Ä‘á»‹a lÃ½.

---
ğŸ§  2. RAPTOR
RAG vá»›i tÃ³m táº¯t (summarization) tá»«ng cá»¥m tÃ i liá»‡u. Dá»±a vÃ o Gaussian clustering Ä‘á»ƒ nhÃ³m rá»“i summary.

---
â“PhÃ¡: TÃ³m táº¯t thÃ¬ pháº£i giÃºp tÃ¬m nhanh hÆ¡n chá»©? Sao láº¡i tá»¥t Ä‘iá»ƒm á»Ÿ factual QA?
âœ… Tráº£ lá»i: VÃ¬:
- LLM tÃ³m táº¯t thÆ°á»ng bá» qua cÃ¡c chi tiáº¿t nhá» nhÆ° sá»‘ liá»‡u, Ä‘á»‹a Ä‘iá»ƒm, tÃªn ngÆ°á»i â†’ trong khi Ä‘Ã³ láº¡i lÃ  máº¥u chá»‘t cá»§a factual QA.
- Sau tÃ³m táº¯t, Ä‘oáº¡n gá»‘c khÃ´ng cÃ²n â†’ truy há»“i máº¥t chÃ­nh xÃ¡c.
ğŸ“Œ TÃ¡c giáº£ nÃ³i rÃµ:
"LLM summarization mechanism... introduces noise into the retrieval corpora."

---
â“PhÃ¡: Váº­y táº¡i sao váº«n tá»‘t á»Ÿ sense-making?
âœ… Tráº£ lá»i:
- VÃ¬ trong discourse dÃ i (nhÆ° NarrativeQA), Ä‘á»c tá»«ng Ä‘oáº¡n khÃ´ng Ä‘á»§ hiá»ƒu máº¡ch truyá»‡n.
- Viá»‡c gom láº¡i rá»“i tÃ³m táº¯t giÃºp LLM tháº¥y Ä‘Æ°á»£c tá»•ng thá»ƒ â†’ hiá»ƒu cÃ¢u chuyá»‡n nhÆ° ngÆ°á»i Ä‘á»c má»™t láº§n.

---
ğŸ§  3. GraphRAG
Táº¡o graph cá»™ng Ä‘á»“ng (community graph), giÃºp hiá»ƒu nhÃ³m thá»±c thá»ƒ hoáº·c Ä‘oáº¡n vÄƒn liÃªn quan.

---
â“PhÃ¡: Sao táº¡o Ä‘Æ°á»£c graph mÃ  váº«n â€œtá»¥t factualâ€?
âœ… Tráº£ lá»i:
- VÃ¬ GraphRAG khÃ´ng giá»¯ Ä‘oáº¡n vÄƒn gá»‘c, chá»‰ dÃ¹ng node graph Ä‘Æ°á»£c tá»•ng há»£p tá»« LLM.
- NÃªn chi tiáº¿t cá»¥ thá»ƒ dá»… bá»‹ máº¥t khi QA yÃªu cáº§u entity hoáº·c sá»‘ liá»‡u chÃ­nh xÃ¡c.

---
â“PhÃ¡: Váº­y nÃ³ giÃºp Ä‘Æ°á»£c gÃ¬?
âœ… Tráº£ lá»i:
- DÃ¹ng graph structure Ä‘á»ƒ khÃ´i phá»¥c cÃ¡c má»‘i liÃªn há»‡ giá»¯a thá»±c thá»ƒ â€“ sá»± kiá»‡n â€“ Ä‘á»‹a Ä‘iá»ƒm â†’ há»— trá»£ multi-hop vÃ  sense-making.
- Viá»‡c phÃ¡t hiá»‡n cá»™ng Ä‘á»“ng giÃºp nhÃ³m thÃ´ng tin tá»« cÃ¡c nguá»“n khÃ¡c nhau â†’ tÄƒng associativity.
ğŸ“Œ Káº¿t quáº£: NarrativeQA Ä‘áº¡t 23.0 (chá»‰ sau HippoRAG 2)
 â†’ nghÄ©a lÃ  nÃ³ hiá»ƒu truyá»‡n tá»‘t, nhÆ°ng khÃ´ng nhá»› chi tiáº¿t.

---
ğŸ§  4. HippoRAG 1
Dá»±a vÃ o OpenIE Ä‘á»ƒ extract triple â†’ dÃ¹ng Personalized PageRank (PPR) Ä‘á»ƒ reasoning.

---
â“PhÃ¡: Váº­y táº¡i sao HippoRAG 1 ráº¥t tá»‘t á»Ÿ associative QA?
âœ… Tráº£ lá»i:
- PPR + KG cho phÃ©p Ä‘i qua nhiá»u hop trong graph â†’ giá»‘ng nhÆ° nhá»› theo â€œÄ‘Æ°á»ng liÃªn tÆ°á»Ÿngâ€.
- Dáº¡ng QA cáº§n liÃªn káº¿t 2-3 Ä‘oáº¡n lÃ  Ä‘iá»ƒm máº¡nh cá»§a PPR.
ğŸ“Œ 2Wiki Ä‘áº¡t 71.8, cao nháº¥t trong báº£ng â†’ chá»©ng minh multi-hop cá»±c tá»‘t.

---
â“PhÃ¡: NhÆ°ng vÃ¬ sao láº¡i chÆ°a tá»‘t á»Ÿ sense-making?
âœ… Tráº£ lá»i:
- Do HippoRAG 1 chá»‰ dá»±a trÃªn entity-centric extraction (NER) â†’ thiáº¿u thÃ´ng tin ngá»¯ cáº£nh (context).
- Query khÃ´ng Ä‘Æ°á»£c embed toÃ n bá»™ Ä‘á»ƒ match triple, chá»‰ match theo entity â†’ contextualization ráº¥t yáº¿u.
ğŸ“Œ TÃ¡c giáº£ gá»i Ä‘Ã¢y lÃ  â€œentity-centric biasâ€.

---
ğŸ§  5. HippoRAG 2
Báº£n nÃ¢ng cáº¥p toÃ n diá»‡n:
- KG gá»“m cáº£ phrase node & passage node (dense-sparse integration)
- Query-to-triple matching (khÃ´ng NER)
- Triple filtering báº±ng LLM
- PPR search dÃ¹ng cáº£ passage & phrase node

---
â“PhÃ¡: Táº¡i sao mÃ´ hÃ¬nh nÃ y â€œtá»‘t Ä‘á»u cáº£ 3â€ â€” cÃ³ gÃ¬ Ä‘áº·c biá»‡t?
âœ… Tráº£ lá»i:
1. Factual QA:
- Giá»¯ nguyÃªn Ä‘oáº¡n vÄƒn gá»‘c + phrase-level KG â†’ khÃ´ng máº¥t chi tiáº¿t.
- DÃ¹ng embedding máº¡nh + triple filter â†’ giá»¯ thÃ´ng tin chuáº©n.
1. Associative QA:
- PPR graph search sÃ¢u, seed node tá»« cáº£ phrase & passage â†’ cá»±c ká»³ phÃ¹ há»£p vá»›i multi-hop reasoning.
- Filter giÃºp loáº¡i nhiá»…u â†’ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c.
2. Sense-making:
- Dense-sparse integration giÃºp giá»¯ Ä‘Æ°á»£c cáº£ khÃ¡i niá»‡m láº«n ngá»¯ cáº£nh.
- Query khÃ´ng cÃ²n bá»‹ chia nhá» nhÆ° NER â†’ cáº£ cÃ¢u Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ hiá»ƒu context â†’ cáº£i thiá»‡n máº¡nh NarrativeQA.
ğŸ“Œ Báº±ng chá»©ng:
- HippoRAG 2 Ä‘áº¡t cao nháº¥t á»Ÿ F1 NarrativeQA = 25.9
- Recall@5 = 96.3 á»Ÿ HotpotQA â†’ quÃ¡ vÆ°á»£t trá»™i!

---
ğŸ“Œ Tá»•ng Káº¿t Giáº£i ThÃ­ch (cÃ³ thá»ƒ Ä‘Æ°a vÃ o bÃ¡o cÃ¡o/luáº­n vÄƒn):
This content is only supported in a Lark Docs

---
Giá» cáº§n hiá»ƒu cÃ¡c khÃ¡i niá»‡m chi tiáº¿t cá»§a Hippo 1 vÃ  Hippo2 -- Giáº£i tÃ­ch tá»«ng khÃ¡i niá»‡m vÃ  liÃªn há»‡ nÃ³ vá»›i thá»© tháº­t dá»… nhá»› Ä‘Æ¡n giáº£n cho há»c sinh cáº¥p 2

Ráº¥t hay Quá»‘c Æ¡i, giá» mÃ¬nh sáº½ giáº£i tÃ­ch cÃ¡c khÃ¡i niá»‡m cá»‘t lÃµi trong HippoRAG 1 vÃ  HippoRAG 2, liÃªn há»‡ vá»›i vÃ­ dá»¥ Ä‘á»i tháº­t tháº­t dá»… hiá»ƒu, kiá»ƒu giáº£i thÃ­ch cho há»c sinh cáº¥p 2.

---
ğŸ§  Tá»•ng thá»ƒ: HippoRAG lÃ  gÃ¬?
HippoRAG lÃ  mÃ´ hÃ¬nh trÃ­ nhá»› dÃ i háº¡n cho AI, láº¥y cáº£m há»©ng tá»« cÃ¡ch con ngÆ°á»i ghi nhá»› vÃ  suy luáº­n.
 NÃ³ chia lÃ m 3 bá»™ pháº­n giá»‘ng nhÆ° bá»™ nÃ£o ngÆ°á»i:
- Neocortex â†’ chÃ­nh lÃ  LLM (nÃ£o suy nghÄ©)
- Hippocampus â†’ nÆ¡i lÆ°u giá»¯ kÃ½ á»©c (graph)
- Parahippocampal Region â†’ káº¿t ná»‘i Ã½ tÆ°á»Ÿng (tÃ¬m Ä‘iá»ƒm giá»‘ng nhau)

---
ğŸ¦› HippoRAG 1 â€“ GIáº¢I TÃCH CÃC KHÃI NIá»†M CHÃNH

---
1. OpenIE â†’ TrÃ­ch xuáº¥t triple (subject â€“ predicate â€“ object)
ğŸ§© ÄÃ¢y lÃ  bÆ°á»›c láº¥y thÃ´ng tin tá»« vÄƒn báº£n vÃ  biáº¿n nÃ³ thÃ nh má»™t â€œkiáº¿n thá»©c nhá» cÃ³ dáº¡ng: A liÃªn quan Ä‘áº¿n B báº±ng cÃ¡ch Câ€.
ğŸ” VÃ­ dá»¥ vÄƒn báº£n:
â€œAlbert Einstein was born in Ulm in 1879.â€
â†’ sáº½ thÃ nh triple:
- (Albert Einstein, born in, Ulm)
- (Albert Einstein, born in year, 1879)
ğŸª„ LiÃªn há»‡ cho há»c sinh:
 Giá»‘ng nhÆ° khi báº¡n Ä‘á»c sÃ¡ch vÃ  tá»± ghi vá»Ÿ â€œtÃ³m táº¯t Ã½ chÃ­nhâ€ kiá»ƒu:
 â†’ â€œEinstein â†’ sinh á»Ÿ Ulmâ€, â€œEinstein â†’ sinh nÄƒm 1879â€.

---
2. Graph of Knowledge Triples (Biá»ƒu Ä‘á»“ kiáº¿n thá»©c)
ğŸ§  CÃ¡c triple trÃªn Ä‘Æ°á»£c ghÃ©p thÃ nh 1 báº£n Ä‘á»“ kiáº¿n thá»©c, giá»‘ng nhÆ° báº£n Ä‘á»“ tÆ° duy (mindmap).
ğŸ“Œ Má»—i vÃ²ng trÃ²n lÃ  1 tá»« khÃ³a, má»—i mÅ©i tÃªn lÃ  má»‘i liÃªn há»‡.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Báº¡n váº½ sÆ¡ Ä‘á»“ mindmap mÃ´n Sinh há»c:
- â€œLÃ¡ cÃ¢y â†’ cÃ³ chá»©c nÄƒng â†’ quang há»£pâ€
- â€œQuang há»£p â†’ táº¡o ra â†’ oxyâ€
 â†’ LÃ  báº¡n Ä‘ang váº½ KG Ä‘Ã³!

---
3. Personalized PageRank (PPR)
ğŸ”„ ÄÃ¢y lÃ  cÃ¡ch mÃ´ hÃ¬nh tÃ¬m Ä‘Æ°á»ng Ä‘i qua nhiá»u bÆ°á»›c trong báº£n Ä‘á»“ kiáº¿n thá»©c Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Giá»‘ng nhÆ° chÆ¡i trÃ² â€œÄ‘i tÃ¬m kho bÃ¡uâ€ trong mÃª cung. Báº¡n báº¯t Ä‘áº§u tá»« tá»« khÃ³a trong cÃ¢u há»i, Ä‘i theo cÃ¡c má»‘i liÃªn há»‡ Ä‘á»ƒ Ä‘áº¿n cÃ¢u tráº£ lá»i.
CÃ¢u há»i: â€œLÃ¡ cÃ¢y giÃºp gÃ¬ cho mÃ´i trÆ°á»ng?â€
 â†’ LÃ¡ cÃ¢y â†’ quang há»£p â†’ táº¡o oxy â†’ giÃºp mÃ´i trÆ°á»ng â†’ thá»Ÿ

---
4. Entity-Centric Query Linking (LiÃªn káº¿t dá»±a vÃ o tÃªn riÃªng)
HippoRAG 1 dÃ¹ng cÃ¡ch phÃ¡t hiá»‡n thá»±c thá»ƒ (entity) trong cÃ¢u há»i nhÆ° tÃªn ngÆ°á»i, nÆ¡i chá»‘n, Ä‘á»ƒ ná»‘i vÃ o graph.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 CÃ¢u há»i: â€œBill Gates há»c trÆ°á»ng nÃ o?â€
 â†’ PhÃ¡t hiá»‡n â€œBill Gatesâ€ lÃ  thá»±c thá»ƒ â†’ tÃ¬m trong báº£n Ä‘á»“ trÃ­ nhá»›.
Äiá»ƒm yáº¿u:
- KhÃ´ng hiá»ƒu cÃ¢u há»i rá»™ng (vÃ­ dá»¥: â€œNgÆ°á»i sÃ¡ng láº­p Microsoft há»c á»Ÿ Ä‘Ã¢u?â€)
 â†’ náº¿u khÃ´ng cÃ³ tá»« â€œBill Gatesâ€, thÃ¬ khÃ´ng tÃ¬m Ä‘Æ°á»£c â†’ máº¥t context

---
ğŸ§  HippoRAG 2 â€“ Cáº¢I TIáº¾N QUAN TRá»ŒNG + VÃ Dá»¤ Äá»œI THÆ¯á»œNG

---
1. Dense-Sparse Integration: GhÃ©p kiáº¿n thá»©c vÃ  ngá»¯ cáº£nh
HippoRAG 2 khÃ´ng chá»‰ lÆ°u "kiáº¿n thá»©c ngáº¯n gá»n" mÃ  cÃ²n lÆ°u cáº£ Ä‘oáº¡n vÄƒn ban Ä‘áº§u (ngá»¯ cáº£nh).
ğŸ“Œ Graph cÃ³ 2 loáº¡i node:
- Phrase Node (sparse): giá»‘ng tiÃªu Ä‘á»/tá»« khÃ³a
- Passage Node (dense): nguyÃªn Ä‘oáº¡n vÄƒn gá»‘c
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Khi há»c lá»‹ch sá»­, báº¡n vá»«a gáº¡ch Ä‘áº§u dÃ²ng â€œTráº­n Äiá»‡n BiÃªn Phá»§ tháº¯ng nÄƒm 1954â€, vá»«a giá»¯ láº¡i nguyÃªn Ä‘oáº¡n mÃ´ táº£ tráº­n Ä‘Ã¡nh â†’ váº­y báº¡n vá»«a nhá»› "Ã½ chÃ­nh" vá»«a cÃ³ â€œngá»¯ cáº£nhâ€.

---
2. Query-to-Triple Matching
KhÃ´ng chá»‰ tÃ¬m entity ná»¯a, mÃ  dÃ¹ng toÃ n cÃ¢u há»i Ä‘á»ƒ match vá»›i cÃ¡c triple.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 CÃ¢u há»i: â€œNgÆ°á»i sÃ¡ng láº­p Microsoft há»c á»Ÿ Ä‘Ã¢u?â€
 â†’ LÃºc nÃ y, dÃ¹ khÃ´ng cÃ³ tá»« â€œBill Gatesâ€, mÃ´ hÃ¬nh váº«n hiá»ƒu Ä‘Æ°á»£c vÃ  tÃ¬m Ä‘Æ°á»£c triple phÃ¹ há»£p.

---
3. Recognition Memory â€“ lá»c triple báº±ng LLM
Sau khi tÃ¬m ra vÃ i má»‘i liÃªn há»‡ phÃ¹ há»£p, HippoRAG 2 nhá» LLM kiá»ƒm tra láº¡i xem cÃ¡i nÃ o thá»±c sá»± liÃªn quan.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Báº¡n tÃ¬m 5 Ä‘oáº¡n cÃ³ váº» Ä‘Ãºng Ä‘á»ƒ lÃ m bÃ i táº­p. Sau Ä‘Ã³ báº¡n Ä‘á»c ká»¹ tá»«ng Ä‘oáº¡n, loáº¡i bá» cÃ¡i khÃ´ng liÃªn quan â†’ giá»¯ láº¡i 2 Ä‘oáº¡n tá»‘t nháº¥t â†’ Ä‘Ã³ lÃ  â€œrecognition memoryâ€.

---
4. Contextualized PPR Search
Khi Ä‘Ã£ cÃ³ cáº£ tá»« khÃ³a vÃ  Ä‘oáº¡n vÄƒn, mÃ´ hÃ¬nh dÃ¹ng PageRank Ä‘á»ƒ tÃ¬m con Ä‘Æ°á»ng tá»‘t nháº¥t qua báº£n Ä‘á»“ trÃ­ nhá»› cÃ³ cáº£ kiáº¿n thá»©c vÃ  ngá»¯ cáº£nh.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Báº¡n ná»‘i â€œtá»« khÃ³aâ€ vá»›i cÃ¡c Ä‘oáº¡n giáº£i thÃ­ch trong sÃ¡ch, rá»“i chá»n ra Ä‘oáº¡n nÃ o giÃºp báº¡n hiá»ƒu toÃ n cáº£nh nháº¥t Ä‘á»ƒ lÃ m bÃ i.

---
ğŸ“ Tá»•ng káº¿t: HippoRAG 1 vs 2 â€“ Dá»… hiá»ƒu cho há»c sinh cáº¥p 2
This content is only supported in a Lark Docs

---
Tuyá»‡t vá»i Quá»‘c Æ¡i, giá» mÃ¬nh sáº½ giáº£i tÃ­ch sÃ¢u tá»«ng khÃ¡i niá»‡m trong sÆ¡ Ä‘á»“ pipeline cá»§a HippoRAG 2, nhÆ° Quá»‘c yÃªu cáº§u:
- Hiá»ƒu rÃµ cÆ¡ cháº¿
- Pháº£n biá»‡n kiá»ƒu phÃ¡ â€“ Ä‘Ã¡p tá»«ng bÆ°á»›c
- LiÃªn há»‡ vÃ­ dá»¥ khi cáº§n
SÆ¡ Ä‘á»“ Ä‘Æ°á»£c chia lÃ m 2 pháº§n:

---
ğŸ¯ PHáº¦N 1: OFFLINE INDEXING
Má»¥c tiÃªu: xÃ¢y dá»±ng má»™t bá»™ nhá»› kiáº¿n thá»©c giÃ u liÃªn káº¿t tá»« vÄƒn báº£n Ä‘áº§u vÃ o.

---
ğŸ”µ BÆ°á»›c 1: OpenIE by LLM â†’ TrÃ­ch xuáº¥t Triple
TÃ¡ch Ä‘oáº¡n vÄƒn thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ kiáº¿n thá»©c dáº¡ng (subject, predicate, object)

---
â“PhÃ¡: Táº¡i sao dÃ¹ng OpenIE báº±ng LLM? TrÃ­ch xuáº¥t truyá»n thá»‘ng cÃ³ váº¥n Ä‘á» gÃ¬?
âœ… ÄÃ¡p:
- OpenIE truyá»n thá»‘ng (Stanford, ClausIE...) chá»‰ lÃ m Ä‘Æ°á»£c trÃªn cÃ¢u Ä‘Æ¡n, thiáº¿u kháº£ nÄƒng xá»­ lÃ½ ngÃ´n ngá»¯ phá»©c táº¡p.
- LLM cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c má»‡nh Ä‘á» dÃ i, ngá»¯ nghÄ©a má»Ÿ rá»™ng, phá»©c táº¡p hÆ¡n â†’ táº¡o triple â€œgiÃ u nghÄ©aâ€ hÆ¡n.
ğŸ“Œ VÃ­ dá»¥
 VÄƒn báº£n: "Marie Curie won two Nobel Prizes in different fields."
 â†’ Triple truyá»n thá»‘ng: ("Marie Curie", "won", "two Nobel Prizes")
 â†’ LLM cÃ³ thá»ƒ táº¡o thÃªm triple: ("Marie Curie", "won Nobel in", "Physics") + ("Marie Curie", "won Nobel in", "Chemistry")

---
ğŸ”µ BÆ°á»›c 2: Synonym Detection â†’ PhÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a
Ná»‘i cÃ¡c node gáº§n nghÄ©a trong KG báº±ng Synonym Edge (Ä‘Æ°á»ng Ä‘á»©t nÃ©t trong sÆ¡ Ä‘á»“)

---
â“PhÃ¡: PhÃ¡t hiá»‡n synonym kiá»ƒu gÃ¬? CÃ³ dÃ¹ng LLM khÃ´ng?
âœ… ÄÃ¡p: KhÃ´ng! DÃ¹ng cosine similarity trong embedding space
- CÃ¡c phrase (tá»«/cá»¥m tá»«) Ä‘Æ°á»£c nhÃºng (embedding)
- Náº¿u cosine similarity > ngÆ°á»¡ng â†’ Ä‘Æ°á»£c ná»‘i báº±ng "synonym edge"
ğŸ“Œ TÃ¡c giáº£ khÃ´ng dÃ¹ng LLM vÃ¬:
- LLM cháº­m hÆ¡n ráº¥t nhiá»u
- Embedding-based synonym cÃ³ thá»ƒ cáº­p nháº­t liÃªn tá»¥c, Ä‘Æ¡n giáº£n hÆ¡n

---
â“PhÃ¡: CÃ³ rá»§i ro gÃ¬ khÃ´ng khi dÃ¹ng embedding Ä‘á»ƒ ná»‘i synonym?
âœ… ÄÃ¡p:
- CÃ³! CÃ³ thá»ƒ ná»‘i nháº§m cÃ¡c tá»« gáº§n nghÄ©a nhÆ°ng khÃ¡c ngá»¯ cáº£nh ("doctor" vÃ  "professor" cháº³ng háº¡n).
- NhÆ°ng vÃ¬ Ä‘Ã¢y lÃ  lá»›p bá»• trá»£ (synonym edge), khÃ´ng áº£nh hÆ°á»Ÿng trá»±c tiáº¿p náº¿u downstream filter lÃ m tá»‘t.

---
ğŸ”µ BÆ°á»›c 3: Denseâ€“Sparse Integration
TÃ­ch há»£p cáº£ node "phrase" vÃ  node "passage" trong KG â†’ hybrid graph

---
â“PhÃ¡: Táº¡i sao cáº§n vá»«a sparse (phrase) vá»«a dense (passage)? KhÃ´ng giá»¯ má»™t loáº¡i cho gá»n?
âœ… ÄÃ¡p:
- Sparse (phrase): Dá»… truy há»“i, biá»ƒu diá»…n khÃ¡i niá»‡m â†’ tá»‘t cho reasoning
- Dense (passage): Giá»¯ thÃ´ng tin chi tiáº¿t â†’ tá»‘t cho factual QA
 â†’ GhÃ©p cáº£ hai táº¡o ra má»™t KG vá»«a khÃ¡i quÃ¡t vá»«a giÃ u ngá»¯ cáº£nh.
ğŸª„ LiÃªn há»‡ dá»… hiá»ƒu: Giá»‘ng há»c Sinh há»c báº±ng sÆ¡ Ä‘á»“ + sÃ¡ch giÃ¡o khoa:
- SÆ¡ Ä‘á»“ (phrase node) giÃºp nhá»› nhanh
- SGK (passage node) giÃºp hiá»ƒu sÃ¢u â†’ cáº§n cáº£ hai Ä‘á»ƒ thi Ä‘iá»ƒm cao

---
âœ… Tá»•ng káº¿t PHáº¦N 1: Graph cÃ³ cÃ¡c loáº¡i node vÃ  edge sau:
This content is only supported in a Lark Docs

---
ğŸ” PHáº¦N 2: ONLINE RETRIEVAL & QA
Má»¥c tiÃªu: Khi cÃ³ query â†’ tÃ¬m Ä‘Æ°á»ng trong graph â†’ tráº£ lá»i.

---
ğŸŸ¢ BÆ°á»›c 1: Retrieving Passages and Triples

---
â“PhÃ¡: Sao pháº£i láº¥y cáº£ Ä‘oáº¡n vÄƒn láº«n triple? KhÃ´ng chá»n 1 cÃ¡i cho nhanh?
âœ… ÄÃ¡p:
- Triple Ä‘á»ƒ lÃ m reasoning, káº¿t ná»‘i máº¡ch kiáº¿n thá»©c
- Passage Ä‘á»ƒ cÃ³ thÃ´ng tin chi tiáº¿t, lÃ m QA cuá»‘i cÃ¹ng
â†’ Hai lá»›p retrieval bá»• trá»£ nhau, khÃ´ng thá»ƒ thiáº¿u cÃ¡i nÃ o.

---
ğŸŸ¢ BÆ°á»›c 2: Triple Filtering = Recognition Memory
Sau khi láº¥y triple vá», ta lá»c xem cÃ¡i nÃ o thá»±c sá»± liÃªn quan Ä‘áº¿n query

---
â“PhÃ¡: Táº¡i sao cáº§n bÆ°á»›c nÃ y? Triple Ä‘Ã£ chá»n rá»“i mÃ ?
âœ… ÄÃ¡p: VÃ¬:
- Retrieval ban Ä‘áº§u báº±ng embedding â†’ cÃ³ thá»ƒ nhiá»…u
- Recognition lÃ  bÆ°á»›c giá»‘ng bá»™ nhá»› con ngÆ°á»i:
 â†’ â€œmÃ¬nh tá»«ng tháº¥y cÃ¡i nÃ y rá»“iâ€¦ nhÆ°ng cÃ³ Ä‘Ãºng cÃ¡i mÃ¬nh cáº§n khÃ´ng?â€
ğŸ“Œ DÃ¹ng LLM Ä‘á»ƒ so sÃ¡nh láº¡i tá»«ng triple vá»›i query â†’ giá»¯ láº¡i nhá»¯ng cÃ¡i tá»‘t nháº¥t
ğŸª„ LiÃªn há»‡ há»c sinh: Giá»‘ng Ä‘á»c láº¡i 5 Ä‘oáº¡n báº¡n highlight khi há»c â†’ loáº¡i 3 cÃ¡i khÃ´ng liÃªn quan, giá»¯ láº¡i 2 Ä‘oáº¡n tháº­t sá»± Ä‘Ãºng Ä‘á».

---
ğŸ”´ BÆ°á»›c 3: Assigning Seed Node Weights
GÃ¡n Ä‘iá»ƒm cho cÃ¡c node â€œgá»‘câ€ trong KG â†’ chuáº©n bá»‹ cho bÆ°á»›c lan truyá»n PageRank

---
â“PhÃ¡: Seed Node lÃ  gÃ¬? Táº¡i sao pháº£i gÃ¡n trá»ng sá»‘?
âœ… ÄÃ¡p:
- Seed Node lÃ  node trong graph cÃ³ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n query
- Trá»ng sá»‘ cao thÃ¬ node Ä‘Ã³ lan truyá»n áº£nh hÆ°á»Ÿng máº¡nh hÆ¡n trong PPR
ğŸ“Œ DÃ¹ng cáº£:
- Passage tá»« query retrieval
- Phrase tá»« triple lá»c
 â†’ táº¥t cáº£ Ä‘á»u trá»Ÿ thÃ nh seed node

---
ğŸ”„ BÆ°á»›c 4: PPR Graph Search
Personalized PageRank tÃ¬m Ä‘Æ°á»ng trong KG tá»« cÃ¡c seed node

---
â“PhÃ¡: KhÃ¡c gÃ¬ so vá»›i search bÃ¬nh thÆ°á»ng?
âœ… ÄÃ¡p:
- PageRank lan tá»a áº£nh hÆ°á»Ÿng â†’ khÃ´ng tÃ¬m Ä‘Æ°á»ng ngáº¯n nháº¥t, mÃ  tÃ¬m Ä‘oáº¡n cÃ³ liÃªn quan nháº¥t trong toÃ n bá»™ cáº¥u trÃºc
- Personalized â†’ Æ°u tiÃªn cÃ¡c vÃ¹ng gáº§n vá»›i seed node
ğŸª„ LiÃªn há»‡ há»c sinh: Giá»‘ng há»c theo sÆ¡ Ä‘á»“ tÆ° duy, báº¡n Æ°u tiÃªn Ä‘á»c pháº§n gáº§n chá»§ Ä‘á» chÃ­nh â†’ khÃ´ng láº­t sÃ¡ch lung tung

---
âœ… BÆ°á»›c 5: QA Reading with Selected Passages
LLM Ä‘á»c cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Ã£ chá»n â†’ sinh ra cÃ¢u tráº£ lá»i.

---
â“PhÃ¡: Náº¿u chá»‰ dÃ¹ng Ä‘oáº¡n vÄƒn mÃ  khÃ´ng graph thÃ¬ sao?
âœ… ÄÃ¡p:
 â†’ Giá»‘ng RAG truyá»n thá»‘ng â†’ bá»‹ lá»‡ch, thiáº¿u reasoning
 â†’ Nhá» bÆ°á»›c graph search, mÃ´ hÃ¬nh nháº­n Ä‘Æ°á»£c Ä‘oáº¡n vÄƒn â€œgiÃ u liÃªn káº¿t nháº¥tâ€ â†’ tráº£ lá»i máº¡ch láº¡c hÆ¡n, chÃ­nh xÃ¡c hÆ¡n.

---
ğŸ”š Tá»•ng Káº¿t: Pipeline HippoRAG 2 â€“ Hiá»ƒu sÃ¢u + vÃ­ dá»¥ hÃ³a
This content is only supported in a Lark Docs

---
DÆ°á»›i Ä‘Ã¢y lÃ  pháº§n giáº£i thÃ­ch sÃ¢u tá»«ng bÆ°á»›c trong pipeline cá»§a HippoRAG 2, kÃ¨m theo cÃ¡c cÃ¢u há»i â€œphÃ¡â€ â€“ â€œÄ‘Ã¡pâ€ Ä‘á»ƒ Ä‘Ã o sÃ¢u hiá»ƒu biáº¿t, giá»‘ng nhÆ° Ä‘á»‘i thoáº¡i giá»¯a ngÆ°á»i Ä‘áº·t cÃ¢u há»i sáº¯c sáº£o vÃ  ngÆ°á»i tráº£ lá»i cháº·t cháº½.

---
1. OpenIE by LLM â€“ TrÃ­ch xuáº¥t Triple tá»« vÄƒn báº£n
Má»¥c tiÃªu:
 Chuyá»ƒn Ä‘á»•i Ä‘oáº¡n vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ kiáº¿n thá»©c (triple: subject â€“ predicate â€“ object) Ä‘á»ƒ lÆ°u trá»¯ trong bá»™ nhá»› dÆ°á»›i dáº¡ng cÃ¡c má»‘i liÃªn há»‡ Ä‘Æ¡n giáº£n.

---
â“ PhÃ¡ 1:
Táº¡i sao chÃºng ta cáº§n trÃ­ch xuáº¥t triple thay vÃ¬ chá»‰ lÆ°u toÃ n bá»™ Ä‘oáº¡n vÄƒn?
âœ… ÄÃ¡p:
- Viá»‡c trÃ­ch xuáº¥t triple giÃºp tÃ¡ch thÃ´ng tin quan trá»ng ra khá»i má»› vÄƒn báº£n dÃ i.
- NÃ³ cho phÃ©p lÆ°u trá»¯ dÆ°á»›i dáº¡ng cÃ¡c má»‘i quan há»‡ rÃµ rÃ ng, giá»‘ng nhÆ° ghi chÃº ngáº¯n gá»n, dá»… tÃ¬m láº¡i vÃ  káº¿t ná»‘i vá»›i cÃ¡c kiáº¿n thá»©c khÃ¡c.
VÃ­ dá»¥ dá»… hiá»ƒu:
 Giá»‘ng nhÆ° khi há»c, báº¡n khÃ´ng cáº§n lÆ°u toÃ n bá»™ bÃ i giáº£ng mÃ  chá»‰ cáº§n ghi láº¡i cÃ¡c Ã½ chÃ­nh: â€œEinstein â€“ sinh â€“ Ulmâ€ thay vÃ¬ ghi toÃ n bá»™ Ä‘oáº¡n vÄƒn dÃ i.

---
â“ PhÃ¡ 2:
Táº¡i sao dÃ¹ng LLM Ä‘á»ƒ thá»±c hiá»‡n OpenIE, thay vÃ¬ dÃ¹ng cÃ¡c cÃ´ng cá»¥ truyá»n thá»‘ng?
âœ… ÄÃ¡p:
- CÃ¡c cÃ´ng cá»¥ OpenIE truyá»n thá»‘ng thÆ°á»ng giá»›i háº¡n á»Ÿ cÃ¢u Ä‘Æ¡n giáº£n, khÃ´ng hiá»ƒu Ä‘Æ°á»£c nhá»¯ng cÃ¢u phá»©c táº¡p, mÃ¢u thuáº«n hay áº©n dá»¥.
- LLM cÃ³ kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh, xá»­ lÃ½ cÃ¢u dÃ i vÃ  phá»©c táº¡p, tá»« Ä‘Ã³ trÃ­ch xuáº¥t triple chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§ hÆ¡n.
VÃ­ dá»¥ há»c sinh cáº¥p 2:
 Náº¿u Ä‘á»c má»™t Ä‘oáº¡n vÄƒn mÃ´ táº£ "Marie Curie Ä‘Ã£ khÃ¡m phÃ¡ ra cÃ¡c nguyÃªn tá»‘ má»›i vÃ  giÃ nh giáº£i Nobel trong nhiá»u lÄ©nh vá»±c", LLM cÃ³ thá»ƒ táº¡o ra nhiá»u triple nhÆ°:
 â†’ (Marie Curie, khÃ¡m phÃ¡, nguyÃªn tá»‘ má»›i)
 â†’ (Marie Curie, giÃ nh, giáº£i Nobel)
 NhÆ° váº­y, báº¡n cÃ³ thá»ƒ nhá»› nhiá»u thÃ´ng tin hÆ¡n chá»‰ báº±ng má»™t cÃ¢u tÃ³m táº¯t.

---
2. Synonym Detection â€“ PhÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a
Má»¥c tiÃªu:
 Káº¿t ná»‘i cÃ¡c node (khÃ¡i niá»‡m, cá»¥m tá»«) cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»± trong KG, giÃºp tÄƒng kháº£ nÄƒng liÃªn káº¿t giá»¯a cÃ¡c thÃ´ng tin.

---
â“ PhÃ¡ 1:
Synonym Detection dÃ¹ng thuáº­t toÃ¡n nÃ o Ä‘á»ƒ phÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a?
âœ… ÄÃ¡p:
- ThÃ´ng thÆ°á»ng, ta sá»­ dá»¥ng cosine similarity trong khÃ´ng gian embedding.
- Má»—i tá»« hay cá»¥m tá»« Ä‘Æ°á»£c chuyá»ƒn thÃ nh má»™t vector, sau Ä‘Ã³ tÃ­nh cosine similarity giá»¯a cÃ¡c vector. Náº¿u giÃ¡ trá»‹ similarity vÆ°á»£t qua má»™t ngÆ°á»¡ng Ä‘á»‹nh trÆ°á»›c (vÃ­ dá»¥ 0.8), hai node Ä‘Æ°á»£c coi lÃ  Ä‘á»“ng nghÄ©a vÃ  Ä‘Æ°á»£c ná»‘i vá»›i nhau báº±ng má»™t â€œsynonym edge.â€

---
â“ PhÃ¡ 2:
Táº¡i sao láº¡i cáº§n ná»‘i cÃ¡c node Ä‘á»“ng nghÄ©a láº¡i vá»›i nhau?
âœ… ÄÃ¡p:
- Äiá»u nÃ y giÃºp liÃªn káº¿t cÃ¡c khÃ¡i niá»‡m cÃ³ thá»ƒ Ä‘Æ°á»£c diá»…n Ä‘áº¡t khÃ¡c nhau.
- Nhá» Ä‘Ã³, khi truy váº¥n, náº¿u tá»« khÃ³a Ä‘Æ°á»£c dÃ¹ng khÃ¡c vá»›i cÃ¡ch ghi chÃº ban Ä‘áº§u nhÆ°ng cÃ³ cÃ¹ng Ã½ nghÄ©a, há»‡ thá»‘ng váº«n cÃ³ thá»ƒ liÃªn káº¿t Ä‘Ãºng kiáº¿n thá»©c.
VÃ­ dá»¥ dá»… hiá»ƒu:
 Náº¿u trong vÄƒn báº£n cÃ³ â€œtháº§y giÃ¡oâ€ vÃ  â€œgiÃ¡o viÃªnâ€ â€“ máº·c dÃ¹ hai tá»« khÃ¡c nhau, chÃºng Ä‘á»u nÃ³i vá» ngÆ°á»i dáº¡y há»c. Káº¿t ná»‘i chÃºng láº¡i giÃºp há»‡ thá»‘ng hiá»ƒu ráº±ng khi há»i vá» â€œgiÃ¡o viÃªnâ€ cÅ©ng cÃ³ thá»ƒ liÃªn há»‡ Ä‘áº¿n â€œtháº§y giÃ¡oâ€.

---
â“ PhÃ¡ 3:
CÃ³ rá»§i ro gÃ¬ khi dÃ¹ng cosine similarity?
âœ… ÄÃ¡p:
- CÃ³ thá»ƒ xáº£y ra trÆ°á»ng há»£p ná»‘i nháº§m nhá»¯ng tá»« cÃ³ cÃ¹ng hÃ¬nh thá»©c nhÆ°ng ngá»¯ nghÄ©a khÃ¡c (vÃ­ dá»¥: â€œbankâ€ cÃ³ thá»ƒ lÃ  ngÃ¢n hÃ ng hoáº·c bá» sÃ´ng).
- Tuy nhiÃªn, vÃ¬ Ä‘Ã¢y chá»‰ lÃ  lá»›p bá»• trá»£ trong KG, náº¿u cÃ¡c bÆ°á»›c sau (nhÆ° triple filtering) hoáº¡t Ä‘á»™ng tá»‘t, sai sÃ³t nÃ y sáº½ Ä‘Æ°á»£c giáº£m thiá»ƒu.

---
3. Denseâ€“Sparse Integration â€“ TÃ­ch há»£p cáº£ Node "Phrase" vÃ  "Passage"
Má»¥c tiÃªu:
 Káº¿t há»£p thÃ´ng tin ngáº¯n gá»n (phrase node) vÃ  thÃ´ng tin Ä‘áº§y Ä‘á»§ (passage node) vÃ o cÃ¹ng má»™t KG Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cáº£ hai.

---
â“ PhÃ¡ 1:
Táº¡i sao khÃ´ng chá»‰ lÆ°u thÃ´ng tin dáº¡ng â€œphraseâ€ hay â€œpassageâ€ riÃªng ráº½?
âœ… ÄÃ¡p:
- Phrase node (sparse): RÃºt gá»n thÃ´ng tin, dá»… káº¿t ná»‘i cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n, nhanh vÃ  hiá»‡u quáº£ cho reasoning.
- Passage node (dense): Giá»¯ nguyÃªn ngá»¯ cáº£nh vÃ  chi tiáº¿t, cáº§n thiáº¿t cho cÃ¡c tÃ¡c vá»¥ yÃªu cáº§u Ä‘á»™ chÃ­nh xÃ¡c cao nhÆ° factual QA.
VÃ­ dá»¥:
 Há»c sinh khi Ã´n bÃ i toÃ¡n: báº¡n cÃ³ thá»ƒ ghi láº¡i â€œcÃ´ng thá»©c Pythagorasâ€ (phrase) vÃ  cÃ³ sá»• tay chá»©a bÃ i giáº£i chi tiáº¿t (passage).
 Cáº£ hai Ä‘á»u quan trá»ng Ä‘á»ƒ báº¡n vá»«a nhá»› nhanh, vá»«a hiá»ƒu sÃ¢u.

---
â“ PhÃ¡ 2:
How does integrating both dense and sparse information help in retrieval?
âœ… ÄÃ¡p:
- Khi cÃ³ cáº£ hai, há»‡ thá»‘ng cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c phrase node Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c khÃ¡i niá»‡m chá»§ Ä‘áº¡o vÃ  passage node Ä‘á»ƒ láº¥y chi tiáº¿t cá»¥ thá»ƒ cho cÃ¢u tráº£ lá»i.
- Äiá»u nÃ y táº¡o ra má»™t KG toÃ n diá»‡n, vá»«a tá»•ng quÃ¡t vá»«a chi tiáº¿t, giÃºp tÄƒng hiá»‡u suáº¥t á»Ÿ cáº£ factual QA vÃ  multi-hop reasoning.

---
4. Triple Retrieval â€“ Láº¥y ra cÃ¡c Triple cÃ³ liÃªn quan vá»›i Query
Má»¥c tiÃªu:
 Khi cÃ³ má»™t query, tÃ¬m ra cÃ¡c triple tá»« KG phÃ¹ há»£p Ä‘á»ƒ dÃ¹ng cho quÃ¡ trÃ¬nh reasoning.

---
â“ PhÃ¡ 1:
Táº¡i sao khÃ´ng chá»‰ láº¥y toÃ n bá»™ passage mÃ  pháº£i láº¥y ra triple?
âœ… ÄÃ¡p:
- Triple chá»©a má»‘i liÃªn há»‡ rÃµ rÃ ng vÃ  rÃºt gá»n, giÃºp há»‡ thá»‘ng dá»… dÃ ng â€œÄ‘i theoâ€ cÃ¡c má»‘i liÃªn káº¿t Ä‘á»ƒ reasoning.
- Passage cÃ³ thá»ƒ quÃ¡ dÃ i, chá»©a nhiá»…u; triple giÃºp xÃ¡c Ä‘á»‹nh â€œÄ‘iá»ƒm cá»‘t lÃµiâ€ cá»§a kiáº¿n thá»©c.
VÃ­ dá»¥:
 Náº¿u cÃ¢u há»i lÃ  â€œBill Gates há»c trÆ°á»ng nÃ o?â€, triple nhÆ° (â€œBill Gatesâ€, â€œhá»c táº¡iâ€, â€œHarvardâ€) sáº½ giÃºp há»‡ thá»‘ng nhanh chÃ³ng tÃ¬m ra thÃ´ng tin cáº§n thiáº¿t.

---
5. Triple Filtering (Recognition Memory) â€“ Lá»c láº¡i cÃ¡c Triple
Má»¥c tiÃªu:
 Sau khi láº¥y ra cÃ¡c triple ban Ä‘áº§u, sá»­ dá»¥ng LLM Ä‘á»ƒ xÃ¡c nháº­n láº¡i tÃ­nh liÃªn quan cá»§a chÃºng vá»›i query, loáº¡i bá» nhá»¯ng triple khÃ´ng phÃ¹ há»£p.

---
â“ PhÃ¡ 1:
Táº¡i sao láº¡i cáº§n bÆ°á»›c lá»c triple náº¿u Ä‘Ã£ cÃ³ triple retrieval?
âœ… ÄÃ¡p:
- Do bÆ°á»›c retrieval ban Ä‘áº§u cÃ³ thá»ƒ mang láº¡i nhiá»u triple nhiá»…u hoáº·c khÃ´ng liÃªn quan, vÃ¬ khÃ´ng pháº£i lÃºc nÃ o cÃ¡c vector embedding cÅ©ng hoÃ n háº£o.
- Recognition Memory giÃºp â€œÄ‘á»c láº¡iâ€ vÃ  xÃ¡c nháº­n, giá»‘ng nhÆ° khi báº¡n kiá»ƒm tra láº¡i ghi chÃº cá»§a mÃ¬nh Ä‘á»ƒ Ä‘áº£m báº£o chá»‰ giá»¯ láº¡i nhá»¯ng thÃ´ng tin Ä‘Ãºng cáº§n thiáº¿t.
VÃ­ dá»¥:
 Náº¿u cÃ³ 5 triple Ä‘Æ°á»£c láº¥y ra, nhÆ°ng chá»‰ cÃ³ 2 thá»±c sá»± nÃ³i vá» cÃ¢u há»i, bÆ°á»›c filtering sáº½ loáº¡i bá» 3 triple khÃ´ng cáº§n thiáº¿t.

---
6. Seed Node Assignment â€“ GÃ¡n Trá»ng Sá»‘ cho Seed Node
Má»¥c tiÃªu:
 XÃ¡c Ä‘á»‹nh cÃ¡c node â€œgá»‘câ€ (seed node) tá»« query, vÃ  gÃ¡n trá»ng sá»‘ cho chÃºng nháº±m táº¡o ná»n táº£ng cho bÆ°á»›c lan truyá»n thÃ´ng tin trong PPR.

---
â“ PhÃ¡ 1:
Seed Node lÃ  gÃ¬?
âœ… ÄÃ¡p:
- Seed Node lÃ  nhá»¯ng node trong KG mÃ  há»‡ thá»‘ng xÃ¡c Ä‘á»‹nh cÃ³ má»‘i liÃªn há»‡ trá»±c tiáº¿p vá»›i query.
- ChÃºng lÃ  Ä‘iá»ƒm khá»Ÿi Ä‘áº§u Ä‘á»ƒ thuáº­t toÃ¡n lan truyá»n (PPR) â€œkhÃ¡m phÃ¡â€ cÃ¡c node khÃ¡c cÃ³ liÃªn quan.
VÃ­ dá»¥:
 Náº¿u cÃ¢u há»i lÃ  â€œBill Gates há»c á»Ÿ Ä‘Ã¢u?â€, seed node cÃ³ thá»ƒ lÃ  â€œBill Gatesâ€ vÃ  â€œHarvardâ€ náº¿u Ä‘Ã£ cÃ³ triple liÃªn quan.

---
â“ PhÃ¡ 2:
Táº¡i sao cáº§n gÃ¡n trá»ng sá»‘ cho seed node?
âœ… ÄÃ¡p:
- Trá»ng sá»‘ cÃ ng cao, node Ä‘Ã³ sáº½ áº£nh hÆ°á»Ÿng máº¡nh hÆ¡n trong quÃ¡ trÃ¬nh lan truyá»n thÃ´ng tin cá»§a PageRank.
- GiÃºp Æ°u tiÃªn cÃ¡c node quan trá»ng, táº¡o ra káº¿t quáº£ retrieval chÃ­nh xÃ¡c hÆ¡n.
VÃ­ dá»¥:
 Giá»‘ng nhÆ° khi báº¡n chá»n Ä‘iá»ƒm máº¥u chá»‘t trong sÆ¡ Ä‘á»“ tÆ° duy, nhá»¯ng Ä‘iá»ƒm quan trá»ng Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u rÃµ, Ä‘á»ƒ khi â€œÄ‘i theo sÆ¡ Ä‘á»“â€ báº¡n sáº½ Æ°u tiÃªn nhá»¯ng Ä‘iá»ƒm Ä‘Ã³.

---
7. PPR (Personalized PageRank) Graph Search â€“ TÃ¬m Kiáº¿m Trong Graph
Má»¥c tiÃªu:
 Sá»­ dá»¥ng thuáº­t toÃ¡n PPR Ä‘á»ƒ lan truyá»n thÃ´ng tin tá»« cÃ¡c seed node vÃ  tÃ¬m ra cÃ¡c node liÃªn quan nháº¥t trong KG.

---
â“ PhÃ¡ 1:
Thuáº­t toÃ¡n PageRank hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o vÃ  khÃ¡c gÃ¬ so vá»›i tÃ¬m kiáº¿m thÃ´ng thÆ°á»ng?
âœ… ÄÃ¡p:
- PageRank ban Ä‘áº§u Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ xáº¿p háº¡ng cÃ¡c trang web dá»±a trÃªn sá»‘ lÆ°á»£ng vÃ  cháº¥t lÆ°á»£ng cá»§a cÃ¡c liÃªn káº¿t.
- Personalized PageRank (PPR) Ä‘iá»u chá»‰nh thuáº­t toÃ¡n Ä‘á»ƒ Æ°u tiÃªn cÃ¡c node báº¯t Ä‘áº§u tá»« seed node (Ä‘iá»ƒm khá»Ÿi Ä‘áº§u tá»« query).
- Thay vÃ¬ chá»‰ tÃ¬m â€œÄ‘oáº¡n vÄƒn gáº§n nháº¥tâ€, PPR lan truyá»n trá»ng sá»‘ qua cÃ¡c liÃªn káº¿t trong graph, giÃºp há»‡ thá»‘ng reasoning multi-hop.
VÃ­ dá»¥:
 Giá»‘ng nhÆ° báº¡n cÃ³ má»™t báº£n Ä‘á»“ mÃª cung vá»›i cÃ¡c lá»‘i Ä‘i, PPR giÃºp báº¡n khÃ´ng chá»‰ tÃ¬m lá»‘i Ä‘i ngáº¯n nháº¥t mÃ  tÃ¬m lá»‘i Ä‘i â€œnhiá»u dáº¥u hiá»‡u nháº¥tâ€ liÃªn quan Ä‘áº¿n chá»§ Ä‘á» cá»§a cÃ¢u há»i.

---
â“ PhÃ¡ 2:
Táº¡i sao láº¡i cáº§n bÆ°á»›c nÃ y khi cÃ³ cÃ¡c bÆ°á»›c retrieval trÆ°á»›c Ä‘Ã³?
âœ… ÄÃ¡p:
- VÃ¬ PPR giÃºp káº¿t ná»‘i cÃ¡c má»‘i quan há»‡ trong KG, khÃ´ng chá»‰ dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng Ä‘Æ¡n thuáº§n mÃ  cÃ²n hiá»ƒu Ä‘Æ°á»£c cáº¥u trÃºc liÃªn káº¿t giá»¯a cÃ¡c thÃ´ng tin.
- NÃ³ cho phÃ©p truy xuáº¥t cÃ¡c thÃ´ng tin liÃªn quan tá»« xa trong graph, táº¡o nÃªn cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c cho cÃ¡c cÃ¢u há»i multi-hop.

---
8. QA Reader â€“ Äá»c vÃ  Sinh Ra CÃ¢u Tráº£ Lá»i
Má»¥c tiÃªu:
 Cuá»‘i cÃ¹ng, cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Æ°á»£c láº¥y ra (sau khi Ä‘Ã£ qua cÃ¡c bÆ°á»›c xá»­ lÃ½) Ä‘Æ°á»£c Ä‘Æ°a cho LLM Ä‘á»ƒ táº¡o ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng cho query.

---
â“ PhÃ¡ 1:
Táº¡i sao khÃ´ng sá»­ dá»¥ng trá»±c tiáº¿p cÃ¡c triple lÃ m cÃ¢u tráº£ lá»i?
âœ… ÄÃ¡p:
- Triple chá»‰ lÃ  cÃ¡c má»‘i liÃªn há»‡ rÃºt gá»n, chÆ°a Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh cáº§n thiáº¿t Ä‘á»ƒ tráº£ lá»i má»™t cÃ¡ch máº¡ch láº¡c vÃ  tá»± nhiÃªn.
- Passage cung cáº¥p Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh, chi tiáº¿t giÃºp LLM sinh ra cÃ¢u tráº£ lá»i trung thá»±c, Ä‘áº§y Ä‘á»§ vÃ  tá»± nhiÃªn hÆ¡n.
VÃ­ dá»¥:
 Náº¿u báº¡n há»i â€œBill Gates há»c á»Ÿ Ä‘Ã¢u?â€, triple cÃ³ thá»ƒ nÃ³i â€œBill Gates â€“ há»c táº¡i â€“ Harvardâ€, nhÆ°ng passage cÃ³ thá»ƒ cung cáº¥p thÃªm chi tiáº¿t nhÆ° â€œBill Gates tá»«ng theo há»c táº¡i trÆ°á»ng Harvard, nÆ¡i Ã´ng Ä‘Ã£ tiáº¿p thu nhiá»u kiáº¿n thá»©c vá» kinh doanh vÃ  cÃ´ng nghá»‡â€, giÃºp cÃ¢u tráº£ lá»i phong phÃº hÆ¡n.

---
Tá»•ng Káº¿t & LiÃªn Há»‡:
Má»—i bÆ°á»›c trong HippoRAG 2 Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ bÃ¹ trá»« nhá»¯ng Ä‘iá»ƒm yáº¿u cá»§a cÃ¡c bÆ°á»›c trÆ°á»›c vÃ  táº¡o nÃªn má»™t chuá»—i xá»­ lÃ½ toÃ n diá»‡n:
- OpenIE: TrÃ­ch xuáº¥t kiáº¿n thá»©c dáº¡ng triple giÃºp â€œghi nhá»›â€ cÃ¡c má»‘i quan há»‡ chÃ­nh.
- Synonym Detection: Káº¿t ná»‘i nhá»¯ng khÃ¡i niá»‡m cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»±, nhÆ° viá»‡c ná»‘i cÃ¡c tá»« Ä‘á»“ng nghÄ©a trong tá»« Ä‘iá»ƒn.
- Dense-Sparse Integration: Giá»¯ Ä‘Æ°á»£c cáº£ â€œÄ‘áº§u dÃ²ngâ€ (phrase) vÃ  â€œná»™i dung chi tiáº¿tâ€ (passage) â€“ giá»‘ng nhÆ° ghi chÃº ngáº¯n vÃ  sá»• tay chi tiáº¿t.
- Triple Retrieval & Filtering: Nháº­n ra cÃ¡c ghi chÃº liÃªn quan vÃ  loáº¡i bá» nhiá»…u, nhÆ° viá»‡c báº¡n xem láº¡i bÃ i há»c vÃ  chá»n ra nhá»¯ng Ä‘iá»ƒm quan trá»ng.
- Seed Node Assignment & PPR Graph Search: Giá»‘ng nhÆ° chá»n cÃ¡c Ä‘iá»ƒm máº¥u chá»‘t trÃªn sÆ¡ Ä‘á»“ tÆ° duy rá»“i Ä‘i theo cÃ¡c má»‘i liÃªn há»‡ Ä‘á»ƒ tÃ¬m ra cÃ¢u tráº£ lá»i toÃ n diá»‡n.
- QA Reader: Cuá»‘i cÃ¹ng, tá»•ng há»£p cÃ¡c thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c lá»c vÃ  liÃªn káº¿t Ä‘á»ƒ sinh ra cÃ¢u tráº£ lá»i tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c.

---
DÆ°á»›i Ä‘Ã¢y mÃ¬nh sáº½ Ä‘Ã o sÃ¢u vÃ o khÃ¡i niá»‡m triple trong bÆ°á»›c 1 cá»§a HippoRAG (vÃ  HippoRAG 2), vá»›i phong cÃ¡ch â€œmá»™t ngÆ°á»i phÃ¡ â€“ má»™t ngÆ°á»i tráº£ lá»iâ€ Ä‘á»ƒ lÃ m rÃµ tá»«ng Ä‘iá»ƒm.

---
1. Triple vÃ  Relation Edge
â“ PhÃ¡: Trong bÆ°á»›c trÃ­ch xuáº¥t triple báº±ng OpenIE, cÃ¡c triple Ä‘Æ°á»£c táº¡o ra cÃ³ â€œhÆ°á»›ngâ€ khÃ´ng?
Tráº£ lá»i:
- CÃ³ hÆ°á»›ng:
 CÃ¡c triple Ä‘Æ°á»£c trÃ­ch xuáº¥t theo Ä‘á»‹nh dáº¡ng (subject, predicate, object) vá»‘n Ä‘Ã£ cÃ³ thá»© tá»± rÃµ rÃ ng.
  - VÃ­ dá»¥: tá»« cÃ¢u â€œMarie Curie won the Nobel Prizeâ€ ta cÃ³ triple:
 (Marie Curie, won, Nobel Prize)
  - Äiá»u nÃ y cÃ³ nghÄ©a lÃ  má»‘i quan há»‡ â€œwonâ€ chá»‰ cÃ³ Ã½ nghÄ©a tá»« â€œMarie Curieâ€ Ä‘áº¿n â€œNobel Prizeâ€.
- LÃ½ do giá»¯ hÆ°á»›ng:
  - GiÃºp phÃ¢n biá»‡t Ä‘Æ°á»£c ai lÃ m gÃ¬ vá»›i ai.
  - Báº£o toÃ n thá»© tá»± thÃ´ng tin, ráº¥t quan trá»ng trong viá»‡c reasoning (vÃ­ dá»¥: â€œwonâ€ khÃ´ng thá»ƒ Ä‘áº£o ngÆ°á»£c thÃ nh â€œNobel Prize won Marie Curieâ€ náº¿u khÃ´ng cÃ³ hÆ°á»›ng).

---
â“ PhÃ¡: Náº¿u bá» Ä‘i hÆ°á»›ng cá»§a cÃ¡c triple, liá»‡u cÃ³ lÃ m máº¥t Ä‘i thÃ´ng tin quan trá»ng khÃ´ng?
Tráº£ lá»i:
- CÃ³ máº¥t:
  - HÆ°á»›ng cá»§a edge xÃ¡c Ä‘á»‹nh má»‘i liÃªn há»‡ theo thá»© tá»± logic.
  - VÃ­ dá»¥:
    - Triple: (Marie Curie, won, Nobel Prize) mang nghÄ©a rÃµ rÃ ng.
    - Náº¿u trá»Ÿ thÃ nh edge khÃ´ng hÆ°á»›ng, ta chá»‰ biáº¿t â€œMarie Curieâ€ vÃ  â€œNobel Prizeâ€ liÃªn quan nhau nhÆ°ng khÃ´ng biáº¿t ai lÃ  tÃ¡c nhÃ¢n hay Ä‘á»‘i tÆ°á»£ng cá»§a hÃ nh Ä‘á»™ng.
- Káº¿t quáº£:
  - Viá»‡c giá»¯ hÆ°á»›ng lÃ  cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c cá»§a ngá»¯ nghÄ©a ban Ä‘áº§u tá»« vÄƒn báº£n.

---
â“ PhÃ¡: Trong Ä‘á»“ thá»‹ kiáº¿n thá»©c, vai trÃ² cá»§a relation edge cÃ³ hÆ°á»›ng lÃ  gÃ¬ trong quÃ¡ trÃ¬nh reasoning?
Tráº£ lá»i:
- GiÃºp Ä‘á»‹nh hÆ°á»›ng thÃ´ng tin:
  - Khi thá»±c hiá»‡n Personalized PageRank (PPR), hÆ°á»›ng cá»§a edge giÃºp xÃ¡c Ä‘á»‹nh luá»“ng thÃ´ng tin tá»« cÃ¡c node gá»‘c (seed nodes) sang cÃ¡c node liÃªn quan.
- Há»— trá»£ multi-hop reasoning:
  - VÃ­ dá»¥: tá»« node â€œMarie Curieâ€ qua edge â€œwonâ€ Ä‘áº¿n â€œNobel Prizeâ€, sau Ä‘Ã³ náº¿u cÃ³ edge khÃ¡c ná»‘i â€œNobel Prizeâ€ vá»›i â€œPhysicsâ€ (náº¿u Ä‘Æ°á»£c trÃ­ch xuáº¥t thÃªm), há»‡ thá»‘ng sáº½ hiá»ƒu Ä‘Æ°á»£c má»‘i liÃªn há»‡ cÃ³ thá»© tá»± giá»¯a cÃ¡c khÃ¡i niá»‡m.
- Giá»¯ nguyÃªn ngá»¯ nghÄ©a:
  - Báº±ng cÃ¡ch khÃ´ng Ä‘áº£o ngÆ°á»£c má»‘i quan há»‡, mÃ´ hÃ¬nh cÃ³ thá»ƒ truy xuáº¥t cÃ¡c thÃ´ng tin liÃªn quan chÃ­nh xÃ¡c hÆ¡n.

---
2. Synonym Edge
â“ PhÃ¡: Synonym edge Ä‘Æ°á»£c táº¡o ra dá»±a trÃªn thuáº­t toÃ¡n gÃ¬?
Tráº£ lá»i:
- Dá»±a trÃªn cosine similarity trong embedding space:
  - Má»—i phrase (hay cá»¥m tá»«) Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng vector.
  - Náº¿u cosine similarity giá»¯a hai vector vÆ°á»£t qua má»™t ngÆ°á»¡ng Ä‘á»‹nh trÆ°á»›c, thÃ¬ hai node Ä‘Ã³ Ä‘Æ°á»£c ná»‘i báº±ng má»™t synonym edge.
- Äáº·c Ä‘iá»ƒm cá»§a synonym edge:
  - KhÃ´ng cÃ³ hÆ°á»›ng: VÃ¬ náº¿u hai tá»« lÃ  Ä‘á»“ng nghÄ©a, má»‘i quan há»‡ cá»§a chÃºng thÆ°á»ng lÃ  Ä‘á»‘i xá»©ng (vÃ­ dá»¥: â€œteacherâ€ vÃ  â€œinstructorâ€ cÃ³ thá»ƒ Ä‘Æ°á»£c ná»‘i vá»›i nhau mÃ  khÃ´ng cáº§n phÃ¢n biá»‡t thá»© tá»±).

---
â“ PhÃ¡: Táº¡i sao láº¡i sá»­ dá»¥ng thuáº­t toÃ¡n embedding cho synonym thay vÃ¬ dÃ¹ng LLM trá»±c tiáº¿p?
Tráº£ lá»i:
- Hiá»‡u quáº£ vÃ  tá»‘c Ä‘á»™:
  - TÃ­nh toÃ¡n cosine similarity giá»¯a vector nhanh vÃ  hiá»‡u quáº£, khÃ´ng cáº§n gá»i LLM nhiá»u láº§n.
- Äá»™ chÃ­nh xÃ¡c cao:
  - CÃ¡c mÃ´ hÃ¬nh embedding hiá»‡n nay cÃ³ kháº£ nÄƒng báº¯t Ä‘Æ°á»£c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a ráº¥t tá»‘t, cho nÃªn phÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a lÃ  khÃ¡ chÃ­nh xÃ¡c.
- Giáº£m táº£i cho LLM:
  - Giáº£m chi phÃ­ tÃ­nh toÃ¡n, vÃ¬ khÃ´ng cáº§n LLM pháº£i xá»­ lÃ½ táº¥t cáº£ cÃ¡c cáº·p tá»« trong corpus.

---
3. Seed Node
â“ PhÃ¡: Seed node lÃ  gÃ¬ trong bá»‘i cáº£nh cá»§a HippoRAG?
Tráº£ lá»i:
- Seed Node:
  - LÃ  nhá»¯ng node trong kiáº¿n thá»©c Ä‘á»“ thá»‹ Ä‘Æ°á»£c chá»n lÃ m Ä‘iá»ƒm khá»Ÿi Ä‘áº§u cho quÃ¡ trÃ¬nh tÃ¬m kiáº¿m vÃ  lan truyá»n thÃ´ng tin (thÃ´ng qua PPR) khi cÃ³ query.
- Vai trÃ² cá»§a Seed Node:
  - ChÃºng Ä‘áº¡i diá»‡n cho cÃ¡c thÃ nh pháº§n quan trá»ng liÃªn quan Ä‘áº¿n cÃ¢u há»i Ä‘Æ°á»£c Ä‘áº·t ra.
  - VÃ­ dá»¥: Náº¿u cÃ¢u há»i lÃ  â€œBill Gates há»c á»Ÿ Ä‘Ã¢u?â€, cÃ¡c node chá»©a â€œBill Gatesâ€ hoáº·c cÃ¡c thÃ´ng tin liÃªn quan sáº½ Ä‘Æ°á»£c chá»n lÃ m seed.

---
â“ PhÃ¡: Táº¡i sao pháº£i gÃ¡n trá»ng sá»‘ (seed node weights) cho cÃ¡c Seed Node?
Tráº£ lá»i:
- GÃ¡n trá»ng sá»‘:
  - GiÃºp há»‡ thá»‘ng biáº¿t Ä‘Æ°á»£c nhá»¯ng node nÃ o quan trá»ng hÆ¡n khi thá»±c hiá»‡n lan truyá»n thÃ´ng tin trong Ä‘á»“ thá»‹.
- Lá»£i Ã­ch:
  - CÃ¡c Seed Node cÃ³ trá»ng sá»‘ cao sáº½ â€œphÃ¡t sÃ¡ngâ€ hÆ¡n trong quÃ¡ trÃ¬nh PPR, tá»« Ä‘Ã³ giÃºp tÃ¬m ra cÃ¡c node liÃªn quan tá»‘t hÆ¡n, Ä‘áº£m báº£o cÃ¢u tráº£ lá»i Ä‘Ãºng vÃ  Ä‘áº§y Ä‘á»§.
- VÃ­ dá»¥ Ä‘Æ¡n giáº£n:
  - Trong má»™t báº£n Ä‘á»“ tÆ° duy, náº¿u báº¡n Ä‘Ã¡nh dáº¥u má»™t sá»‘ Ã½ chÃ­nh vá»›i bÃºt dáº¡, nhá»¯ng Ã½ nÃ y sáº½ Ä‘Æ°á»£c chÃº Ã½ nhiá»u hÆ¡n khi báº¡n tá»•ng há»£p láº¡i kiáº¿n thá»©c.

---
4. Relation Edge vÃ  CÃ¡c KhÃ¡i Niá»‡m KhÃ¡c
â“ PhÃ¡: Relation edge cÃ³ hÆ°á»›ng nhÆ° tháº¿ nÃ o?
Tráº£ lá»i:
- Relation Edge:
  - LÃ  cÃ¡c cáº¡nh ná»‘i giá»¯a cÃ¡c node trong KG, Ä‘Æ°á»£c táº¡o ra tá»« cÃ¡c triple (subject, predicate, object).
  - ChÃºng cÃ³ hÆ°á»›ng, tá»« subject sang object, thÃ´ng qua predicate.
- Ã nghÄ©a cá»§a hÆ°á»›ng:
  - GiÃºp xÃ¡c Ä‘á»‹nh Ä‘Ãºng má»‘i liÃªn há»‡ ngá»¯ nghÄ©a (ai lÃ m gÃ¬ vá»›i gÃ¬).

---
â“ PhÃ¡: NgoÃ i cÃ¡c khÃ¡i niá»‡m trÃªn, cÃ²n khÃ¡i niá»‡m nÃ o khÃ¡c cáº§n giáº£i thÃ­ch chi tiáº¿t trong pipeline khÃ´ng?
Tráº£ lá»i:
- Context Edge:
  - Ná»‘i cÃ¡c Passage Node vá»›i cÃ¡c Phrase Node mÃ  chÃºng chá»©a Ä‘á»±ng.
  - Ã nghÄ©a: GiÃºp giá»¯ láº¡i ngá»¯ cáº£nh chi tiáº¿t cá»§a vÄƒn báº£n, há»— trá»£ factual QA.
- Denseâ€“Sparse Integration:
  - Sá»± káº¿t há»£p giá»¯a sparse coding (phrase node) vÃ  dense coding (passage node) nháº±m Ä‘áº£m báº£o vá»«a cÃ³ Ã½ chÃ­nh tÃ³m táº¯t, vá»«a cÃ³ Ä‘áº§y Ä‘á»§ chi tiáº¿t.
  - VÃ­ dá»¥ há»c sinh: NhÆ° Ä‘Ã£ nÃ³i, giá»‘ng nhÆ° vá»«a ghi nhá»› tiÃªu Ä‘á» bÃ i há»c (sparse) vÃ  vá»«a cÃ³ toÃ n bá»™ bÃ i giáº£ng (dense).

---
Káº¾T LUáº¬N
Má»—i khÃ¡i niá»‡m trong pipeline cá»§a HippoRAG 2 Ä‘á»u Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ báº£o toÃ n vÃ  khai thÃ¡c thÃ´ng tin má»™t cÃ¡ch tá»‘i Æ°u:
- Triple extraction vá»›i relation edge cÃ³ hÆ°á»›ng giá»¯ tráº­t tá»± vÃ  ngá»¯ nghÄ©a ban Ä‘áº§u.
- Synonym edge (khÃ´ng cÃ³ hÆ°á»›ng) ná»‘i cÃ¡c cá»¥m tá»« cÃ³ nghÄ©a tÆ°Æ¡ng Ä‘á»“ng, giÃºp liÃªn káº¿t thÃ´ng tin giá»¯a cÃ¡c Ä‘oáº¡n vÄƒn khÃ¡c nhau.
- Seed Node lÃ  cÃ¡c Ä‘iá»ƒm khá»Ÿi Ä‘áº§u quan trá»ng Ä‘á»ƒ báº¯t Ä‘áº§u quÃ¡ trÃ¬nh tÃ¬m kiáº¿m thÃ´ng tin trong KG, vÃ  viá»‡c gÃ¡n trá»ng sá»‘ cho chÃºng giÃºp tÄƒng cÆ°á»ng kháº£ nÄƒng lan truyá»n thÃ´ng tin trong quÃ¡ trÃ¬nh Personalized PageRank.
- Context edge vÃ  dense-sparse integration giÃºp mÃ´ hÃ¬nh khÃ´ng chá»‰ ghi nhá»› Ã½ chÃ­nh mÃ  cÃ²n giá»¯ nguyÃªn ngá»¯ cáº£nh chi tiáº¿t cá»§a vÄƒn báº£n gá»‘c.
Qua tá»«ng cÃ¢u há»i pháº£n biá»‡n, chÃºng ta nháº­n tháº¥y ráº±ng viá»‡c duy trÃ¬ hÆ°á»›ng trong cÃ¡c relation edge lÃ  ráº¥t quan trá»ng cho viá»‡c xÃ¢y dá»±ng má»™t Ä‘á»“ thá»‹ kiáº¿n thá»©c chÃ­nh xÃ¡c, tá»« Ä‘Ã³ há»— trá»£ quÃ¡ trÃ¬nh reasoning vÃ  retrieval trong toÃ n há»‡ thá»‘ng HippoRAG.
ChÃºng ta hÃ£y cÃ¹ng phÃ¢n tÃ­ch theo kiá»ƒu "má»™t ngÆ°á»i phÃ¡ â€“ má»™t ngÆ°á»i tráº£ lá»i" Ä‘á»ƒ hiá»ƒu sÃ¢u vá» cÃ¡ch thÃªm passage node sau khi Ä‘Ã£ tÃ¡ch ra bá»™ ba (triple) theo bÃ i bÃ¡o.

---
â“ PhÃ¡: Sau khi Ä‘Ã£ sá»­ dá»¥ng OpenIE tÃ¡ch ra cÃ¡c triple (subject, predicate, object) tá»« má»—i Ä‘oáº¡n vÄƒn, lÃ m tháº¿ nÃ o Ä‘á»ƒ "append" thÃªm pháº§n passage node vÃ o Ä‘á»“ thá»‹ kiáº¿n thá»©c?
Tráº£ lá»i:
 Theo bÃ i bÃ¡o, sau khi trÃ­ch xuáº¥t cÃ¡c triple tá»« má»—i passage, ta tiáº¿n hÃ nh xÃ¢y dá»±ng má»™t Knowledge Graph (KG) báº±ng cÃ¡ch thá»±c hiá»‡n bÆ°á»›c sau:
1. Táº¡o Phrase Nodes tá»« Triple:
  - Má»—i triple (vÃ­ dá»¥: (Marie Curie, won, Nobel Prize)) sáº½ táº¡o ra cÃ¡c phrase nodes cho subject, predicate vÃ  object.
  - ÄÃ¢y lÃ  cÃ¡c nÃºt Ä‘áº¡i diá»‡n cho cÃ¡c khÃ¡i niá»‡m trá»«u tÆ°á»£ng cá»§a thÃ´ng tin.
2. ThÃªm Passage Node:
  - Má»—i passage gá»‘c trong corpus Ä‘Æ°á»£c xem nhÆ° má»™t passage node riÃªng biá»‡t.
  - Sau Ä‘Ã³, append pháº§n passage node nÃ y vÃ o Ä‘á»“ thá»‹ báº±ng cÃ¡ch ná»‘i nÃ³ vá»›i táº¥t cáº£ cÃ¡c phrase node Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« passage Ä‘Ã³.
  - CÃ¡c káº¿t ná»‘i nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  context edge (hoáº·c edge "contains") vÃ¬ chÃºng biá»ƒu thá»‹ ráº±ng passage chá»©a Ä‘á»±ng cÃ¡c thÃ´ng tin hay khÃ¡i niá»‡m Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi phrase nodes.

---
â“ PhÃ¡: Táº¡i sao cáº§n thÃªm passage node? KhÃ´ng Ä‘á»§ chá»‰ cÃ³ phrase nodes sao?
Tráº£ lá»i:
- Báº£o toÃ n ngá»¯ cáº£nh:
  - Phrase nodes cung cáº¥p má»™t dáº¡ng trá»«u tÆ°á»£ng, tá»•ng quÃ¡t cá»§a thÃ´ng tin. Tuy nhiÃªn, thÃ´ng tin nÃ y cÃ³ thá»ƒ máº¥t Ä‘i chi tiáº¿t ngá»¯ cáº£nh quan trá»ng.
  - Báº±ng cÃ¡ch thÃªm passage node, há»‡ thá»‘ng lÆ°u giá»¯ Ä‘Æ°á»£c toÃ n bá»™ thÃ´ng tin chi tiáº¿t cá»§a Ä‘oáº¡n vÄƒn gá»‘c.
- Denseâ€“Sparse Integration:
  - Passage node Ä‘áº¡i diá»‡n cho pháº§n dense coding (Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh, chi tiáº¿t), trong khi phrase nodes Ä‘áº¡i diá»‡n cho sparse coding (khÃ¡i quÃ¡t, dá»… dÃ ng reasoning).
  - Viá»‡c káº¿t há»£p cáº£ hai giÃºp mÃ´ hÃ¬nh cÃ³ thá»ƒ thá»±c hiá»‡n tá»‘t hÆ¡n trong viá»‡c tráº£ lá»i cÃ¢u há»i yÃªu cáº§u vá»«a hiá»ƒu Ä‘Æ°á»£c Ã½ chÃ­nh vá»«a náº¯m báº¯t Ä‘Æ°á»£c chi tiáº¿t.

---
â“ PhÃ¡: CÃ¡ch thá»©c ná»‘i giá»¯a passage node vÃ  phrase nodes Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° tháº¿ nÃ o?
Tráº£ lá»i:
- Má»—i passage node sáº½ Ä‘Æ°á»£c ná»‘i vá»›i táº¥t cáº£ cÃ¡c phrase node Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« chÃ­nh passage Ä‘Ã³.
- CÃ¡c káº¿t ná»‘i nÃ y (context edges) khÃ´ng cáº§n cÃ³ hÆ°á»›ng rÃµ rÃ ng nhÆ° cÃ¡c relation edge cá»§a triple, mÃ  chá»§ yáº¿u nháº±m má»¥c Ä‘Ã­ch thá»ƒ hiá»‡n má»‘i liÃªn há»‡ vá» ngá»¯ cáº£nh.
- Khi thá»±c hiá»‡n retrieval hoáº·c Personalized PageRank (PPR), passage node giÃºp tÄƒng trá»ng sá»‘ cho cÃ¡c Ä‘oáº¡n vÄƒn chá»©a Ä‘á»±ng thÃ´ng tin chi tiáº¿t, tá»« Ä‘Ã³ cáº£i thiá»‡n kháº£ nÄƒng truy xuáº¥t vÃ  hiá»ƒu sÃ¢u cá»§a mÃ´ hÃ¬nh.

---
â“ PhÃ¡: Táº¡i sao khÃ´ng chá»‰ dÃ¹ng phrase nodes mÃ  bá» qua passage node?
Tráº£ lá»i:
- Náº¿u chá»‰ sá»­ dá»¥ng phrase nodes, ta chá»‰ cÃ³ Ä‘Æ°á»£c má»™t báº£n tÃ³m táº¯t ráº¥t cÃ´ Ä‘á»ng cá»§a thÃ´ng tin.
- Nhiá»u trÆ°á»ng há»£p, Ä‘áº·c biá»‡t lÃ  cÃ¡c cÃ¢u há»i factual hay yÃªu cáº§u sá»± liÃªn káº¿t sÃ¢u giá»¯a cÃ¡c Ã½, Ä‘Ã²i há»i pháº£i cÃ³ ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ tá»« Ä‘oáº¡n vÄƒn gá»‘c.
- Passage node giÃºp mÃ´ hÃ¬nh truy xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn gá»‘c Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng máº¥t Ä‘i cÃ¡c chi tiáº¿t cáº§n thiáº¿t cho viá»‡c tráº£ lá»i chÃ­nh xÃ¡c.

---
Tá»•ng Káº¿t
- BÆ°á»›c 1: TÃ¡ch cÃ¡c triple (subject, predicate, object) tá»« má»—i passage báº±ng OpenIE cá»§a LLM â†’ táº¡o ra cÃ¡c phrase nodes.
- BÆ°á»›c 2: Vá»›i má»—i passage gá»‘c, táº¡o passage node.
- BÆ°á»›c 3: Append passage node vÃ o Ä‘á»“ thá»‹ báº±ng cÃ¡ch ná»‘i nÃ³ vá»›i táº¥t cáº£ cÃ¡c phrase node tá»« passage Ä‘Ã³ thÃ´ng qua context edges.
- Má»¥c Ä‘Ã­ch: Giá»¯ láº¡i cáº£ Ã½ chÃ­nh (phrase node) vÃ  thÃ´ng tin chi tiáº¿t (passage node) Ä‘á»ƒ há»— trá»£ quÃ¡ trÃ¬nh retrieval vÃ  reasoning, tá»« Ä‘Ã³ cáº£i thiá»‡n hiá»‡u quáº£ cá»§a toÃ n há»‡ thá»‘ng.
DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch tiáº¿p cáº­n chi tiáº¿t theo dáº¡ng há»i â€“ Ä‘Ã¡p pháº£n biá»‡n (má»™t ngÆ°á»i â€œphÃ¡â€, má»™t ngÆ°á»i â€œbÃªnhâ€) Ä‘á»ƒ má»• xáº» tháº­t sÃ¢u 5 bÆ°á»›c trong Online Retrieval & QA cá»§a HippoRAG/HippoRAG 2. Má»—i cÃ¢u tráº£ lá»i Ä‘á»u cá»‘ gáº¯ng gáº¯n vá»›i láº­p luáº­n tá»« bÃ i bÃ¡o hoáº·c Ã½ tÆ°á»Ÿng chÃ­nh (cÃ³ dáº«n chá»©ng).

---
(1) Retrieving Passages and Triples
Pháº£n biá»‡n (Q):
Táº¡i sao pháº£i láº¥y cáº£ cÃ¡c Ä‘oáº¡n vÄƒn (passages) vÃ  cÃ¡c bá»™ ba (triples)? Liá»‡u chá»‰ cáº§n retrieval embedding tá»‘t Ä‘á»ƒ láº¥y Ä‘oáº¡n vÄƒn Ä‘Ã£ Ä‘á»§ chÆ°a?
Tráº£ lá»i (A):
- Cáº§n cáº£ passages: vÃ¬ cÃ¡c Ä‘oáº¡n vÄƒn mang Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh. Náº¿u chá»‰ dá»±a vÃ o node (thÆ°á»ng lÃ  má»™t cÃ¢u/triple rá»i ráº¡c) thÃ¬ cÃ³ thá»ƒ thiáº¿u máº¥t chi tiáº¿t quan trá»ng.
- Cáº§n cáº£ triples: vÃ¬ triple lÃ  â€œÄ‘Æ¡n vá»‹ thÃ´ng tinâ€ mang tÃ­nh khÃ¡i quÃ¡t â€“ giÃºp mÃ´ phá»ng sá»± liÃªn káº¿t (associativity). Khi ta kÃ©o Ä‘Æ°á»£c triple liÃªn quan, nÃ³ cÃ³ thá»ƒ â€œgá»£i Ã½â€ nhiá»u quan há»‡ sÃ¢u hÆ¡n mÃ  má»™t Ä‘oáº¡n vÄƒn chÆ°a cháº¯c Ä‘Ã£ hiá»ƒn thá»‹ ngay.
- Dáº«n chá»©ng: Pháº§n â€œOnline Retrievalâ€ trong bÃ i HippoRAG/HippoRAG 2 cho tháº¥y, há»‡ thá»‘ng cháº¡y query-to-triple Ä‘á»ƒ láº¥y triple trÆ°á»›c, sau Ä‘Ã³ má»›i cháº¡y tiáº¿p cÃ¡c bÆ°á»›c khÃ¡c. Káº¿t há»£p â€œpassage nodeâ€ vÃ  â€œphrase nodeâ€ (triple) trong Ä‘á»“ thá»‹ lÃ  cÃ¡ch mÃ  mÃ´ hÃ¬nh â€œtrá»™nâ€ giá»¯a â€œcontextâ€ vÃ  â€œconceptsâ€.

---
(2) Recognition Memory (Triple Filtering)
Pháº£n biá»‡n (Q):
Táº¡i sao láº¡i cáº§n má»™t bÆ°á»›c gá»i lÃ  â€œRecognition Memoryâ€ Ä‘á»ƒ lá»c triple? Cháº³ng pháº£i retrieval encoder Ä‘Ã£ xáº¿p háº¡ng Ä‘Æ°á»£c rá»“i sao?
Tráº£ lá»i (A):
- Encoder-based ranking chá»‰ dá»±a trÃªn vector similarity, cÃ³ thá»ƒ nháº·t pháº£i nhá»¯ng triple â€œna nÃ¡â€ vá» tá»« vá»±ng nhÆ°ng khÃ´ng tháº­t sá»± liÃªn quan ngá»¯ nghÄ©a.
- LLM-based filtering (Recognition Memory) giá»‘ng nhÆ° â€œduyá»‡t láº¡iâ€ nhá»¯ng triple top-k, Ä‘á»ƒ xÃ¡c nháº­n: â€œTriple nÃ y thá»±c sá»± nÃ³i vá» cÃ¡i gÃ¬, cÃ³ Äƒn khá»›p vá»›i cÃ¢u há»i khÃ´ng?â€ â€“ Tá»« Ä‘Ã³ loáº¡i bá»›t nhiá»…u.
- VÃ­ dá»¥: CÃ³ nhá»¯ng triple mÃ  entity giá»‘ng nhau vá» máº·t ngÃ´n ngá»¯ (vÃ­ dá»¥ â€œParisâ€ â€“ má»™t ngÆ°á»i tÃªn Paris vs. thÃ nh phá»‘ Paris). Náº¿u chá»‰ dÃ¹ng embedding, Ä‘Ã´i khi bá»‹ láº«n. BÆ°á»›c filtering nÃ y giÃºp LLM dÃ¹ng ngá»¯ cáº£nh Ä‘á»ƒ phÃ¢n biá»‡t.
- Dáº«n chá»©ng: BÃ i bÃ¡o giá»›i thiá»‡u cÆ¡ cháº¿ â€œtwo-step retrievalâ€: 1) Query-to-triple (embedding) vÃ  2) Triple filter (LLM) â€“ mÃ´ phá»ng â€œrecall vs. recognitionâ€ trong trÃ­ nhá»› con ngÆ°á»i.

---
(3) Assigning Seed Node Weights
Pháº£n biá»‡n (Q):
PhÃ¢n phá»‘i trá»ng sá»‘ (reset probability) cho cÃ¡c â€œseed nodeâ€ â€“ Ä‘áº·c biá»‡t lÃ  giá»¯a â€œpassage nodeâ€ vÃ  â€œphrase nodeâ€ â€“ cÃ³ thá»±c sá»± cáº§n thiáº¿t, hay chá»‰ cáº§n gÃ¡n Ä‘á»u lÃ  Ä‘Æ°á»£c?
Tráº£ lá»i (A):
- KhÃ´ng thá»ƒ gÃ¡n Ä‘á»u: Bá»Ÿi vÃ¬ â€œphrase nodeâ€ (cÃ¡c khÃ¡i niá»‡m) vÃ  â€œpassage nodeâ€ (cÃ¡c vÄƒn báº£n) Ä‘Ã³ng vai trÃ² khÃ¡c nhau. Náº¿u gÃ¡n trá»ng sá»‘ giá»‘ng nhau, mÃ´ hÃ¬nh cÃ³ thá»ƒ nghiÃªng quÃ¡ nhiá»u vá» concept hoáº·c quÃ¡ nhiá»u vá» context.
- Cáº§n cÃ¢n báº±ng: Cháº³ng háº¡n, khi cÃ¢u há»i ráº¥t cá»¥ thá»ƒ (vÃ­ dá»¥: â€œTÃ¡c giáº£ cuá»‘n sÃ¡ch X lÃ  ai?â€), â€œphrase nodeâ€ vá» â€œbook_authorâ€ cÃ³ thá»ƒ quan trá»ng hÆ¡n. NhÆ°ng náº¿u cÃ¢u há»i phá»©c táº¡p, â€œpassage nodeâ€ cÃ³ ngá»¯ cáº£nh rá»™ng hÆ¡n sáº½ cáº§n trá»ng sá»‘ cao hÆ¡n Ä‘á»ƒ PPR lan tá»a nhiá»u hÆ¡n.
- Dáº«n chá»©ng: TÃ¡c giáº£ cÃ³ lÃ m ablation study (vÃ­ dá»¥ trong pháº§n Implementation & Discussion) Ä‘á»ƒ chá»©ng minh pháº£i chá»n â€œweight factorâ€ cho passage node há»£p lÃ½ (0.01, 0.05, v.v.) thÃ¬ má»›i tá»‘i Æ°u Recall@5.

---
(4) PPR Graph Search
Pháº£n biá»‡n (Q):
Personalized PageRank (PPR) thá»±c sá»± giÃºp Ä‘Æ°á»£c gÃ¬? Liá»‡u má»™t vÃ²ng láº·p iterative retrieval (nhÆ° IRCoT) khÃ´ng pháº£i Ä‘Ã£ giáº£i quyáº¿t Ä‘Æ°á»£c multi-hop sao?
Tráº£ lá»i (A):
- PPR lÃ m multi-hop ngay trong má»™t lÆ°á»£t: Thay vÃ¬ pháº£i gá»i LLM liÃªn tiáº¿p Ä‘á»ƒ â€œgá»£i Ã½â€ truy váº¥n phá»¥ (iterative retrieval), PPR cho phÃ©p khuáº¿ch tÃ¡n xÃ¡c suáº¥t tá»« cÃ¡c seed node (passage/phrase) Ä‘áº¿n lÃ¢n cáº­n trong KG.
- Hiá»‡u quáº£ chi phÃ­: Iterative retrieval kiá»ƒu IRCoT thÆ°á»ng tá»‘n nhiá»u lÆ°á»£t gá»i LLM. PPR lÃ  má»™t thuáº­t toÃ¡n Ä‘á»“ thá»‹ cháº¡y ná»™i bá»™, ráº» hÆ¡n vá» máº·t inference.
- PhÃ¹ há»£p vá»›i â€œassociative memoryâ€: BÃ i bÃ¡o gá»£i Ã½ ráº±ng PPR mÃ´ phá»ng cÃ¡ch hippocampus (nÃ£o bá»™) kÃ­ch hoáº¡t cÃ¡c liÃªn káº¿t giá»¯a khÃ¡i niá»‡m. Tá»« 1 seed node, lan ra nhiá»u nÃºt khÃ¡c theo xÃ¡c suáº¥t.
- Dáº«n chá»©ng: HippoRAG/HippoRAG 2 so sÃ¡nh vá»›i IRCoT, cho tháº¥y PPR cÃ³ thá»ƒ â€œgÃ³iâ€ viá»‡c multi-hop reasoning trong má»™t láº§n retrieval, nhanh hÆ¡n vÃ  Ã­t tá»‘n chi phÃ­ hÆ¡n.

---
(5) QA Reading with Selected Passages
Pháº£n biá»‡n (Q):
Náº¿u cuá»‘i cÃ¹ng váº«n pháº£i gá»i LLM Ä‘á»ƒ Ä‘á»c láº¡i cÃ¡c Ä‘oáº¡n vÄƒn (QA reading), váº­y viá»‡c xÃ¢y dá»±ng KG hay PPR cÃ³ tháº­t sá»± quan trá»ng? Cháº³ng pháº£i LLM giá» Ä‘Ã£ â€œsáºµn sÃ ngâ€ tráº£ lá»i náº¿u cho nÃ³ Ä‘á»§ context?
Tráº£ lá»i (A):
- LLM cáº§n â€œÄ‘áº§u vÃ o sáº¡châ€: DÃ¹ LLM máº¡nh, nhÆ°ng náº¿u input context bá»‹ nhiá»…u (chá»n sai passage) hoáº·c quÃ¡ dÃ n tráº£i, LLM dá»… tráº£ lá»i sai, Ä‘áº·c biá»‡t vá»›i cÃ¢u há»i cáº§n multi-hop.
- KG + PPR giÃºp â€œtÃ³m gá»nâ€ Ä‘Ãºng Ä‘oáº¡n vÄƒn liÃªn quan, thay vÃ¬ lÃ´i táº¥t cáº£. LLM cÃ ng cÃ³ context chÃ­nh xÃ¡c, tráº£ lá»i cÃ ng tá»‘t.
- TÃ­nh má»Ÿ rá»™ng: Khi táº­p dá»¯ liá»‡u ráº¥t lá»›n, chá»‰ embedding retrieval cÃ³ thá»ƒ bá» sÃ³t â€œchuá»—i logicâ€ liÃªn quan. Bá»• sung KG + PPR => MÃ´ hÃ¬nh â€œliÃªn káº¿tâ€ tá»‘t hÆ¡n, LLM Ä‘á»¡ bá»‹ quÃ¡ táº£i.
- Dáº«n chá»©ng: Káº¿t quáº£ thá»±c nghiá»‡m (báº£ng so sÃ¡nh F1, EM) cho tháº¥y HippoRAG 2 sau khi chá»n top passages qua PPR, rá»“i cho LLM Ä‘á»c, cho káº¿t quáº£ cao hÆ¡n so vá»›i retrieval thÆ°á»ng.

---
Tá»•ng káº¿t
5 bÆ°á»›c trong Online Retrieval & QA Ä‘Æ°á»£c thiáº¿t káº¿ cÃ³ chá»§ Ä‘Ã­ch, mÃ´ phá»ng cÆ¡ cháº¿ bá»™ nhá»› cá»§a con ngÆ°á»i:
1. Láº¥y passage + triple song song (Ä‘áº£m báº£o vá»«a cÃ³ context vá»«a cÃ³ concept).
2. Lá»c triple báº±ng LLM (Recognition Memory) Ä‘á»ƒ loáº¡i nhiá»…u.
3. GÃ¡n trá»ng sá»‘ seed node cáº©n tháº­n (cÃ¢n báº±ng passage node vÃ  phrase node).
4. Cháº¡y PPR Ä‘á»ƒ thá»±c hiá»‡n multi-hop reasoning trong má»™t bÆ°á»›c, tiáº¿t kiá»‡m chi phÃ­ so vá»›i iterative retrieval.
5. Cuá»‘i cÃ¹ng, LLM má»›i Ä‘á»c cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Ã£ chá»n Ä‘á»ƒ tráº£ lá»i, tá»‘i Æ°u Ä‘á»™ chÃ­nh xÃ¡c.
Tá»«ng bÆ°á»›c Ä‘á»u cÃ³ lÃ½ do tá»“n táº¡i, vÃ  káº¿t quáº£ thá»±c nghiá»‡m chá»©ng minh mÃ´ hÃ¬nh nÃ y tÄƒng hiá»‡u quáº£ truy xuáº¥t (retrieval) cÅ©ng nhÆ° Ä‘á»™ chÃ­nh xÃ¡c khi tráº£ lá»i cÃ¢u há»i (QA).

---
(3) Assigning Seed Node Weights
Äáº·t cÃ¢u há»i phÃ¡ nhiá»u hÆ¡n á»Ÿ bÆ°á»›c nÃ y 
- Cháº³ng háº¡n: lÃºc mÃ  xÃ¢y Graph cÃ¡c nodes vai trÃ² nhÆ° nhau hay lÃ  Hippo RAG 2 cÃ³ Ä‘Ã¡nh dáº¥u rÃµ lÃ  phrase node hay passage node ? 
- CÃ¡ch Ä‘Ã¡nh Assing Seed Node Weights nhÆ° nÃ o trong bÃ i bÃ¡o cÃ³ nÃ³i ko 
- ... nhiá»u cÃ¢u ná»¯a nhÃ©

Ã½ lÃ  phÃ¡ 1, mÃ¬nh biáº¿t lÃ  nÃ³ Ä‘Æ°á»£c rÃµ rÃ ng vai trÃ², nhÆ°ng lÃºc Ä‘Æ°a lÃªn Ä‘á»“ thá»‹ Ã½ thÃ¬ nÃ³ cÃ³ Ä‘Æ°á»£c rÃµ rÃ ng váº­y khÃ´ng ? kiá»ƒu há» dÃ¹ng cÃ¡ch nÃ o Ä‘á»ƒ Ä‘Ã¡nh dáº¥u 2 loáº¡i node Ä‘Ã³ Ä‘á»ƒ khi query dÃ¹ng

Trong quÃ¡ trÃ¬nh xÃ¢y dá»±ng Ä‘á»“ thá»‹, HippoRAG 2 khÃ´ng chá»‰ trÃ­ch xuáº¥t cÃ¡c node mÃ  cÃ²n "Ä‘Ã¡nh dáº¥u" rÃµ rÃ ng tá»«ng loáº¡i node thÃ´ng qua cáº¥u trÃºc vÃ  cÃ¡c loáº¡i cáº¡nh ná»‘i káº¿t:
- PhÃ¢n loáº¡i qua nguá»“n gá»‘c táº¡o node:
  - Phrase node: ÄÆ°á»£c táº¡o ra tá»« viá»‡c trÃ­ch xuáº¥t bá»™ ba (triple) cá»§a cÃ¡c khÃ¡i niá»‡m qua OpenIE. Nhá»¯ng node nÃ y chá»‰ chá»©a thÃ´ng tin "concept" rÃºt gá»n vÃ  liÃªn quan Ä‘áº¿n cÃ¡c má»‘i quan há»‡ giá»¯a thá»±c thá»ƒ.
  - Passage node: Äáº¡i diá»‡n cho toÃ n bá»™ Ä‘oáº¡n vÄƒn, chá»©a Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh vÃ  thÃ´ng tin chi tiáº¿t.
 Qua Ä‘Ã³, quÃ¡ trÃ¬nh offline indexing cá»§a HippoRAG 2 Ä‘Ã£ phÃ¢n biá»‡t rÃµ rÃ ng nguá»“n gá»‘c cá»§a cÃ¡c node.
- ÄÃ¡nh dáº¥u qua loáº¡i cáº¡nh (edge type):
  - CÃ¡c passage node Ä‘Æ°á»£c liÃªn káº¿t vá»›i cÃ¡c phrase node thÃ´ng qua cÃ¡c cáº¡nh mang nhÃ£n "contains" (hoáº·c "context edge"), nháº±m thá»ƒ hiá»‡n ráº±ng Ä‘oáº¡n vÄƒn chá»©a cÃ¡c cá»¥m tá»« (phrase) Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« nÃ³.
  - Trong khi Ä‘Ã³, cÃ¡c phrase node cÃ³ thá»ƒ Ä‘Æ°á»£c ná»‘i vá»›i nhau qua cÃ¡c cáº¡nh khÃ¡c nhÆ° "synonym edge" hay "relation edge". Viá»‡c nÃ y giÃºp nháº¥n máº¡nh má»‘i liÃªn há»‡ giá»¯a cÃ¡c khÃ¡i niá»‡m trá»«u tÆ°á»£ng, tÃ¡ch biá»‡t vá»›i thÃ´ng tin ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ tá»« passage.
- TÃ¡c Ä‘á»™ng khi query:
 Khi má»™t truy váº¥n Ä‘Æ°á»£c Ä‘Æ°a vÃ o, há»‡ thá»‘ng sá»­ dá»¥ng cÃ¡c embedding vÃ  trá»ng sá»‘ riÃªng biá»‡t cho tá»«ng loáº¡i node. Nhá» váº­y, viá»‡c so sÃ¡nh cosine similarity Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cáº£ phrase node vÃ  passage node theo cÃ¡ch phÃ¹ há»£p vá»›i Ä‘áº·c trÆ°ng cá»§a chÃºng. CÃ¡ch gÃ¡n trá»ng sá»‘ (seed node weights) cÅ©ng Ä‘Æ°á»£c Ä‘iá»u chá»‰nh riÃªng cho tá»«ng loáº¡i, giÃºp quÃ¡ trÃ¬nh Personalized PageRank lan tá»a thÃ´ng tin Ä‘Ãºng theo cáº¥u trÃºc Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u.
Dáº«n chá»©ng: Trong pháº§n mÃ´ táº£ cá»§a HippoRAG 2, tÃ¡c giáº£ trÃ¬nh bÃ y ráº±ng má»—i passage trong corpus Ä‘Æ°á»£c xá»­ lÃ½ nhÆ° má»™t passage node vÃ  Ä‘Æ°á»£c ná»‘i vá»›i táº¥t cáº£ cÃ¡c phrase Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« nÃ³ thÃ´ng qua cÃ¡c "context edge". Äiá»u nÃ y khÃ´ng chá»‰ giÃºp xÃ¢y dá»±ng má»™t kiáº¿n trÃºc Ä‘á»“ thá»‹ rÃµ rÃ ng mÃ  cÃ²n cho phÃ©p há»‡ thá»‘ng phÃ¢n biá»‡t vÃ  Ã¡p dá»¥ng trá»ng sá»‘ khÃ¡c nhau cho má»—i loáº¡i node trong quÃ¡ trÃ¬nh truy xuáº¥t.
NhÆ° váº­y, báº±ng cÃ¡ch phÃ¢n loáº¡i qua nguá»“n gá»‘c táº¡o node vÃ  Ä‘Ã¡nh dáº¥u thÃ´ng qua cÃ¡c loáº¡i cáº¡nh ná»‘i káº¿t, HippoRAG 2 Ä‘áº£m báº£o ráº±ng trong quÃ¡ trÃ¬nh truy váº¥n, há»‡ thá»‘ng cÃ³ thá»ƒ nháº­n diá»‡n vÃ  táº­n dá»¥ng Ä‘Ãºng vai trÃ² cá»§a tá»«ng loáº¡i node, tá»« Ä‘Ã³ cáº£i thiá»‡n hiá»‡u quáº£ cá»§a retrieval vÃ  QA.

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ chi tiáº¿t, kÃ¨m theo Ä‘á»‘i thoáº¡i â€œpháº£n biá»‡n â€“ tráº£ lá»iâ€ minh há»a rÃµ rÃ ng cÃ¡ch Assigning Seed Node Weights hoáº¡t Ä‘á»™ng trong quÃ¡ trÃ¬nh Online Retrieval cá»§a HippoRAG.

---
VÃ­ dá»¥ cá»¥ thá»ƒ
Truy váº¥n: "TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?"
Sau khi há»‡ thá»‘ng xá»­ lÃ½ truy váº¥n, ta thu Ä‘Æ°á»£c cÃ¡c thÃ nh pháº§n sau:
1. TrÃ­ch xuáº¥t tá»« truy váº¥n (Query Extraction):
  - CÃ¡c thá»±c thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t:
    - â€œThe Great Novelâ€
    - â€œtÃ¡c giáº£â€
2. Seed Nodes Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trong Knowledge Graph (KG):
  - Phrase Nodes (Ä‘áº¡i diá»‡n cho khÃ¡i niá»‡m, thÃ´ng tin trÃ­ch xuáº¥t tá»« cÃ¡c triple):
    - Node A: "tÃ¡c giáº£"
      - GiÃ¡ trá»‹ similarity (vá»›i "tÃ¡c giáº£" truy váº¥n): 0.72
    - Node B: "The Great Novel"
      - GiÃ¡ trá»‹ similarity (vá»›i tiÃªu Ä‘á» truy váº¥n): 0.68
  - Passage Nodes (Ä‘oáº¡n vÄƒn cung cáº¥p ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§):
    - Node C: Äoáº¡n vÄƒn 1: â€œThe Great Novel, má»™t tÃ¡c pháº©m kinh Ä‘iá»ƒn, Ä‘Æ°á»£c viáº¿t bá»Ÿi John Doe â€“ tÃ¡c giáº£ ná»•i tiáº¿ng vá»›i nhiá»u giáº£i thÆ°á»Ÿng vÄƒn há»c."
      - GiÃ¡ trá»‹ similarity (vá»›i truy váº¥n, dá»±a trÃªn cáº£ ngá»¯ cáº£nh vÃ  tá»« khÃ³a): 0.80
    - Node D: Äoáº¡n vÄƒn 2: Má»™t Ä‘oáº¡n vÄƒn khÃ¡c nÃ³i vá» John Doe nhÆ°ng khÃ´ng Ä‘á» cáº­p rÃµ rÃ ng Ä‘áº¿n cuá»‘n sÃ¡ch.
      - GiÃ¡ trá»‹ similarity: 0.55

---
BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh Seed Nodes vÃ  GÃ¡n Trá»ng Sá»‘ Ban Ä‘áº§u
Giáº£ sá»­ há»‡ thá»‘ng quy Æ°á»›c:
- Phrase Nodes nháº­n trá»ng sá»‘ máº·c Ä‘á»‹nh = 1.0
- Passage Nodes ban Ä‘áº§u nháº­n trá»ng sá»‘ tháº¥p hÆ¡n, vÃ¬ chÃºng mang thÃ´ng tin cá»¥ thá»ƒ nhÆ°ng cáº§n Ä‘Æ°á»£c â€œÄ‘iá»u chá»‰nhâ€ qua weight factor.
  - VÃ­ dá»¥: há»‡ thá»‘ng gÃ¡n weight factor cho Passage Nodes = 0.05.
Do Ä‘Ã³, ta cÃ³:
- Node A ("tÃ¡c giáº£"):
  - Similarity score = 0.72 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.72 Ã— 1.0 = 0.72
- Node B ("The Great Novel"):
  - Similarity score = 0.68 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.68 Ã— 1.0 = 0.68
- Node C (Passage 1):
  - Similarity score = 0.80 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.80 Ã— 0.05 = 0.04
- Node D (Passage 2):
  - Similarity score = 0.55 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.55 Ã— 0.05 = 0.0275

---
BÆ°á»›c 2: Äá»‘i thoáº¡i pháº£n biá»‡n â€“ Giáº£i thÃ­ch chi tiáº¿t
Pháº£n biá»‡n 1: Táº¡i sao Passage Nodes láº¡i Ä‘Æ°á»£c nhÃ¢n vá»›i weight factor tháº¥p (0.05) trong khi Phrase Nodes giá»¯ nguyÃªn trá»ng sá»‘ 1.0?
Q: Náº¿u Passage Nodes mang toÃ n bá»™ ngá»¯ cáº£nh cá»§a Ä‘oáº¡n vÄƒn, táº¡i sao chÃºng láº¡i Ä‘Æ°á»£c gÃ¡n trá»ng sá»‘ tháº¥p hÆ¡n so vá»›i Phrase Nodes?
A:
- LÃ½ do:
  - Phrase Nodes Ä‘áº¡i diá»‡n cho cÃ¡c khÃ¡i niá»‡m trá»«u tÆ°á»£ng, thÆ°á»ng cÃ³ tÃ­nh khÃ¡i quÃ¡t vÃ  cÃ³ thá»ƒ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n truy váº¥n. ChÃºng cáº§n Ä‘Æ°á»£c Æ°u tiÃªn cao Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c khÃ¡i niá»‡m chÃ­nh Ä‘Æ°á»£c nháº­n diá»‡n.
  - Passage Nodes máº·c dÃ¹ chá»©a ngá»¯ cáº£nh chi tiáº¿t, nhÆ°ng náº¿u gÃ¡n trá»ng sá»‘ quÃ¡ cao, thÃ´ng tin quÃ¡ â€œphÃ¢n tÃ¡nâ€ cÃ³ thá»ƒ lÃ m nhiá»…u káº¿t quáº£ retrieval. Weight factor tháº¥p giÃºp kiá»ƒm soÃ¡t sá»± lan tá»a cá»§a thÃ´ng tin cá»¥ thá»ƒ, chá»‰ khi mÃ  cÃ¡c passage cÃ³ nhiá»u liÃªn káº¿t vá»›i cÃ¡c Phrase Nodes, chÃºng má»›i â€œÄ‘Æ°á»£c bÃ¹ Ä‘áº¯pâ€ thÃ´ng qua quÃ¡ trÃ¬nh PPR.
- Dáº«n chá»©ng thá»±c nghiá»‡m:
  - BÃ i bÃ¡o trÃ¬nh bÃ y cÃ¡c thÃ­ nghiá»‡m ablation vá»›i cÃ¡c giÃ¡ trá»‹ weight factor khÃ¡c nhau (vÃ­ dá»¥: 0.01, 0.05, 0.1,...) cho Passage Nodes. Káº¿t quáº£ cho tháº¥y, khi chá»n weight factor phÃ¹ há»£p (0.05 trong vÃ­ dá»¥ nÃ y), há»‡ thá»‘ng Ä‘áº¡t Recall@5 tá»‘i Æ°u. Äiá»u nÃ y chá»©ng tá» ráº±ng viá»‡c cÃ¢n báº±ng giá»¯a khÃ¡i niá»‡m vÃ  ngá»¯ cáº£nh lÃ  ráº¥t quan trá»ng.

---
Pháº£n biá»‡n 2: Viá»‡c nhÃ¢n weight factor áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘áº¿n káº¿t quáº£ ban Ä‘áº§u cá»§a PPR?
Q: Vá»›i cÃ¡c trá»ng sá»‘ ban Ä‘áº§u nhÆ° trÃªn, lÃ m tháº¿ nÃ o Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng Passage Node C â€“ máº·c dÃ¹ cÃ³ similarity score cao (0.80) â€“ cÃ³ thá»ƒ cáº¡nh tranh Ä‘Æ°á»£c vá»›i Phrase Nodes khi weight factor tháº¥p?
A:
- Giáº£i thÃ­ch:
  - Ban Ä‘áº§u, Passage Node C cÃ³ giÃ¡ trá»‹ trá»ng sá»‘ chá»‰ lÃ  0.04, so vá»›i 0.72 (Node A) vÃ  0.68 (Node B).
  - Tuy nhiÃªn, trong quÃ¡ trÃ¬nh PPR, trá»ng sá»‘ nÃ y sáº½ Ä‘Æ°á»£c â€œlan tá»aâ€ qua cÃ¡c cáº¡nh trong Knowledge Graph. Náº¿u Passage Node C Ä‘Æ°á»£c liÃªn káº¿t máº¡nh máº½ vá»›i cÃ¡c Phrase Nodes cÃ³ trá»ng sá»‘ cao, cÃ¡c cáº¡nh liÃªn káº¿t nÃ y sáº½ giÃºp â€œkÃ©oâ€ thÃªm probability mass cho Node C.
  - VÃ­ dá»¥: náº¿u Node C cÃ³ nhiá»u liÃªn káº¿t Ä‘áº¿n Node A vÃ  Node B, quÃ¡ trÃ¬nh PPR sáº½ cá»™ng dá»“n thÃªm giÃ¡ trá»‹ cho Node C, giÃºp nÃ³ Ä‘Æ°á»£c Æ°u tiÃªn khi xáº¿p háº¡ng cÃ¡c Ä‘oáº¡n vÄƒn cuá»‘i cÃ¹ng.
- Dáº«n chá»©ng:
  - BÃ i bÃ¡o cho biáº¿t ráº±ng káº¿t há»£p giá»¯a distribution tá»« PPR vÃ  ma tráº­n xuáº¥t hiá»‡n (P matrix) cho phÃ©p cÃ¡c passage nháº­n Ä‘Æ°á»£c Ä‘iá»ƒm tá»•ng há»£p cao hÆ¡n, náº¿u chÃºng Ä‘Æ°á»£c liÃªn káº¿t cháº·t cháº½ vá»›i cÃ¡c Phrase Nodes cÃ³ trá»ng sá»‘ ban Ä‘áº§u cao.

---
Pháº£n biá»‡n 3: Giáº£ sá»­ cÃ³ má»™t Passage Node D cÃ³ similarity score tháº¥p (0.55 â†’ 0.0275), liá»‡u nÃ³ cÃ³ bá»‹ loáº¡i bá» hoÃ n toÃ n hay váº«n cÃ³ vai trÃ²?
Q: Passage Node D cÃ³ Ä‘iá»ƒm ban Ä‘áº§u ráº¥t tháº¥p, nhÆ°ng liá»‡u nÃ³ cÃ³ Ä‘Ã³ng gÃ³p hay khÃ´ng trong quÃ¡ trÃ¬nh retrieval?
A:
- Giáº£i thÃ­ch:
  - Máº·c dÃ¹ Passage Node D cÃ³ similarity score tháº¥p vÃ  trá»ng sá»‘ ban Ä‘áº§u chá»‰ lÃ  0.0275, náº¿u nÃ³ cÃ³ má»™t sá»‘ liÃªn káº¿t há»¯u Ã­ch trong KG, Ä‘áº·c biá»‡t lÃ  vá»›i cÃ¡c Phrase Nodes cÃ³ Ä‘iá»ƒm cao, quÃ¡ trÃ¬nh PPR cÃ³ thá»ƒ â€œnÃ¢ng Ä‘á»¡â€ giÃ¡ trá»‹ cá»§a nÃ³ qua viá»‡c cá»™ng dá»“n tá»« cÃ¡c nÃºt lÃ¢n cáº­n.
  - Tuy nhiÃªn, náº¿u khÃ´ng cÃ³ liÃªn káº¿t máº¡nh máº½ nÃ o, Node D cÃ³ thá»ƒ khÃ´ng Ä‘Ã³ng gÃ³p nhiá»u vÃ  sáº½ Ä‘Æ°á»£c xáº¿p háº¡ng tháº¥p. Äiá»u nÃ y giÃºp há»‡ thá»‘ng lá»c ra cÃ¡c passage khÃ´ng Ä‘á»§ liÃªn quan.
- Dáº«n chá»©ng:
  - Káº¿t quáº£ thá»±c nghiá»‡m vÃ  ablation study trong bÃ i bÃ¡o chá»‰ ra ráº±ng nhá»¯ng passage khÃ´ng Ä‘Æ°á»£c liÃªn káº¿t cháº·t cháº½ sáº½ nháº­n Ä‘iá»ƒm tháº¥p sau PPR, tá»« Ä‘Ã³ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ cuá»‘i cÃ¹ng.

---
BÆ°á»›c 3: TÃ­ch há»£p qua PPR
Sau khi cÃ¡c seed node Ä‘Æ°á»£c gÃ¡n trá»ng sá»‘, há»‡ thá»‘ng cháº¡y thuáº­t toÃ¡n Personalized PageRank trÃªn KG. QuÃ¡ trÃ¬nh nÃ y sáº½:
- Lan tá»a probability mass: Tá»« cÃ¡c seed node ban Ä‘áº§u, giÃ¡ trá»‹ sáº½ Ä‘Æ°á»£c lan tá»a qua cÃ¡c cáº¡nh liÃªn káº¿t.
- Káº¿t há»£p vá»›i P matrix: Sau Ä‘Ã³, probability mass Ä‘Æ°á»£c nhÃ¢n vá»›i ma tráº­n P chá»©a thÃ´ng tin táº§n sá»‘ xuáº¥t hiá»‡n cá»§a cÃ¡c nÃºt trong cÃ¡c Ä‘oáº¡n vÄƒn, giÃºp xÃ¡c Ä‘á»‹nh thá»© háº¡ng cuá»‘i cÃ¹ng cho cÃ¡c passage.
Káº¿t quáº£ cuá»‘i cÃ¹ng, máº·c dÃ¹ ban Ä‘áº§u Passage Node C chá»‰ cÃ³ trá»ng sá»‘ 0.04, nhÆ°ng náº¿u liÃªn káº¿t tá»‘t, nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c â€œnÃ¢ngâ€ lÃªn vÃ  xáº¿p háº¡ng cao, giÃºp há»‡ thá»‘ng truy xuáº¥t Ä‘oáº¡n vÄƒn chá»©a thÃ´ng tin â€œJohn Doe lÃ  tÃ¡c giáº£ cá»§a The Great Novelâ€ má»™t cÃ¡ch chÃ­nh xÃ¡c.

---
Tá»•ng káº¿t vÃ­ dá»¥
1. TrÃ­ch xuáº¥t vÃ  xÃ¡c Ä‘á»‹nh Seed Nodes:
  - Phrase Nodes: "tÃ¡c giáº£" (0.72), "The Great Novel" (0.68)
  - Passage Nodes: Passage 1 (0.80 â†’ 0.04 sau nhÃ¢n), Passage 2 (0.55 â†’ 0.0275)
2. GÃ¡n trá»ng sá»‘:
  - Phrase Nodes giá»¯ trá»ng sá»‘ Ä‘áº§y Ä‘á»§ (1.0)
  - Passage Nodes nhÃ¢n vá»›i weight factor (0.05)
3. QuÃ¡ trÃ¬nh PPR:
  - Lan tá»a probability mass tá»« cÃ¡c seed node qua cÃ¡c cáº¡nh cá»§a KG.
  - CÃ¡c Passage Nodes, máº·c dÃ¹ cÃ³ trá»ng sá»‘ ban Ä‘áº§u tháº¥p, sáº½ nháº­n Ä‘Æ°á»£c thÃªm giÃ¡ trá»‹ náº¿u Ä‘Æ°á»£c liÃªn káº¿t máº¡nh máº½ vá»›i cÃ¡c Phrase Nodes.
4. Káº¿t quáº£ cuá»‘i cÃ¹ng:
  - Passage Node chá»©a Ä‘oáº¡n vÄƒn chÃ­nh vá» John Doe sáº½ Ä‘Æ°á»£c xáº¿p háº¡ng cao, Ä‘áº£m báº£o há»‡ thá»‘ng tráº£ lá»i chÃ­nh xÃ¡c cho truy váº¥n "TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?".
Qua vÃ­ dá»¥ nÃ y, chÃºng ta tháº¥y ráº±ng viá»‡c Assigning Seed Node Weights khÃ´ng chá»‰ lÃ  má»™t thao tÃ¡c Ä‘á»‹nh lÆ°á»£ng ban Ä‘áº§u, mÃ  cÃ²n lÃ  bÆ°á»›c quyáº¿t Ä‘á»‹nh Ä‘á»ƒ cÃ¢n báº±ng giá»¯a thÃ´ng tin khÃ¡i niá»‡m vÃ  ngá»¯ cáº£nh, tá»« Ä‘Ã³ tá»‘i Æ°u quÃ¡ trÃ¬nh PPR vÃ  cáº£i thiá»‡n hiá»‡u quáº£ cá»§a há»‡ thá»‘ng truy xuáº¥t.
ÄÃºng váº­y, khi nÃ³i vá» trá»ng sá»‘ máº·c Ä‘á»‹nh cho Phrase Nodes, ta cÃ³ thá»ƒ Ä‘áº·t weight factor lÃ  1.0, nhÆ°ng Ä‘iá»u nÃ y khÃ´ng cÃ³ nghÄ©a lÃ  giÃ¡ trá»‹ similarity cá»§a tá»«ng node sáº½ luÃ´n lÃ  1.0.
Cá»¥ thá»ƒ:
- Trá»ng sá»‘ (weight factor) cá»§a Phrase Nodes: Khi gÃ¡n trá»ng sá»‘ cho cÃ¡c Phrase Nodes, há»‡ thá»‘ng sá»­ dá»¥ng há»‡ sá»‘ 1.0 Ä‘á»ƒ giá»¯ nguyÃªn giÃ¡ trá»‹ similarity ban Ä‘áº§u.
- GiÃ¡ trá»‹ similarity: ÄÃ¢y lÃ  má»™t chá»‰ sá»‘ Ä‘Æ°á»£c tÃ­nh dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a embedding cá»§a thá»±c thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« truy váº¥n vÃ  embedding cá»§a node trong Knowledge Graph. GiÃ¡ trá»‹ nÃ y thÆ°á»ng náº±m trong khoáº£ng tá»« 0 Ä‘áº¿n 1, vÃ  khÃ´ng pháº£i lÃºc nÃ o cÅ©ng báº±ng 1.0. VÃ­ dá»¥, Node A cÃ³ similarity lÃ  0.72 vÃ  Node B cÃ³ similarity lÃ  0.68, Ä‘iá»u nÃ y pháº£n Ã¡nh má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng (khÃ´ng pháº£i lÃ  â€œtrá»ng sá»‘â€ mÃ  lÃ  káº¿t quáº£ cá»§a phÃ©p Ä‘o similarity).
TÃ³m láº¡i, máº·c Ä‘á»‹nh weight factor cá»§a Phrase Nodes lÃ  1.0, nghÄ©a lÃ  chÃºng ta khÃ´ng Ä‘iá»u chá»‰nh thÃªm giÃ¡ trá»‹ similarity cá»§a chÃºng, nhÆ°ng giÃ¡ trá»‹ similarity thá»±c sá»± (0.72, 0.68, â€¦) phá»¥ thuá»™c vÃ o quÃ¡ trÃ¬nh tÃ­nh toÃ¡n tá»« cÃ¡c embedding chá»© khÃ´ng pháº£i lÃ  1.0 luÃ´n.

---
DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ chi tiáº¿t kÃ¨m Ä‘á»‘i thoáº¡i pháº£n biá»‡n â€“ â€œngÆ°á»i phÃ¡ â€“ ngÆ°á»i tráº£ lá»iâ€ â€“ Ä‘á»ƒ minh há»a cÃ¡ch Seed Node Weights vÃ  thuáº­t toÃ¡n Personalized PageRank (PPR) hoáº¡t Ä‘á»™ng cÃ¹ng nhau trong HippoRAG.

---
Bá»‘i cáº£nh vÃ­ dá»¥
Giáº£ sá»­ truy váº¥n lÃ :
"TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?"
Sau khi trÃ­ch xuáº¥t thÃ´ng tin tá»« truy váº¥n vÃ  xÃ¢y dá»±ng Knowledge Graph (KG), há»‡ thá»‘ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c cÃ¡c seed node vá»›i cÃ¡c giÃ¡ trá»‹ similarity nhÆ° sau:
- Phrase Node A: "tÃ¡c giáº£"
  - Similarity score = 0.72
  - Weight factor (Phrase) = 1.0 â†’ Effective weight = 0.72
- Phrase Node B: "The Great Novel"
  - Similarity score = 0.68
  - Weight factor (Phrase) = 1.0 â†’ Effective weight = 0.68
- Passage Node C: Äoáº¡n vÄƒn 1 chá»©a thÃ´ng tin:
 â€œThe Great Novel, má»™t tÃ¡c pháº©m kinh Ä‘iá»ƒn, Ä‘Æ°á»£c viáº¿t bá»Ÿi John Doe â€“ tÃ¡c giáº£ ná»•i tiáº¿ngâ€
  - Similarity score = 0.80
  - Weight factor (Passage) = 0.05 â†’ Effective weight = 0.80 Ã— 0.05 = 0.04
- Passage Node D: Äoáº¡n vÄƒn 2 nÃ³i chung vá» John Doe nhÆ°ng khÃ´ng nháº¯c trá»±c tiáº¿p tiÃªu Ä‘á»
  - Similarity score = 0.55
  - Weight factor (Passage) = 0.05 â†’ Effective weight = 0.55 Ã— 0.05 = 0.0275

---
Äá»‘i thoáº¡i pháº£n biá»‡n â€“ CÃ¡ch PPR hoáº¡t Ä‘á»™ng vá»›i Seed Node Weights
Pháº£n biá»‡n 1: Khá»Ÿi táº¡o Seed Vector
Q: LÃ m tháº¿ nÃ o ta khá»Ÿi táº¡o vector seed cho PPR vá»›i cÃ¡c seed node trÃªn?
A:
- Ta táº¡o má»™t vector ss chá»©a cÃ¡c giÃ¡ trá»‹ khá»Ÿi táº¡o cho cÃ¡c node theo effective weight:
  - sA=0.72s_A = 0.72 (Phrase node A)
  - sB=0.68s_B = 0.68 (Phrase node B)
  - sC=0.04s_C = 0.04 (Passage node C)
  - sD=0.0275s_D = 0.0275 (Passage node D)
- Sau Ä‘Ã³, ta chuáº©n hÃ³a vector ss sao cho tá»•ng cÃ¡c pháº§n tá»­ báº±ng 1. VÃ­ dá»¥:
  - Tá»•ng S=0.72+0.68+0.04+0.0275=1.4675S = 0.72 + 0.68 + 0.04 + 0.0275 = 1.4675
  - Vector seed chuáº©n hÃ³a:
    - sAâ€²=0.72/1.4675â‰ˆ0.49s'_A = 0.72 / 1.4675 \approx 0.49
    - sBâ€²=0.68/1.4675â‰ˆ0.46s'_B = 0.68 / 1.4675 \approx 0.46
    - sCâ€²=0.04/1.4675â‰ˆ0.027s'_C = 0.04 / 1.4675 \approx 0.027
    - sDâ€²=0.0275/1.4675â‰ˆ0.019s'_D = 0.0275 / 1.4675 \approx 0.019
NhÆ° váº­y, cÃ¡c Phrase Nodes chiáº¿m Æ°u tháº¿ ngay tá»« Ä‘áº§u so vá»›i cÃ¡c Passage Nodes do effective weight cá»§a chÃºng cao hÆ¡n.

---
Pháº£n biá»‡n 2: QuÃ¡ trÃ¬nh PPR â€“ Lan tá»a xÃ¡c suáº¥t
Q: PPR sáº½ lÃ m gÃ¬ vá»›i vector seed Ä‘Ã£ chuáº©n hÃ³a nÃ y?
A:
- Thuáº­t toÃ¡n PPR thá»±c hiá»‡n cáº­p nháº­t xÃ¡c suáº¥t theo cÃ´ng thá»©c:
- v(t+1)=(1âˆ’d)â‹…sâ€²+dâ‹…Tâ‹…v(t)v^{(t+1)} = (1-d) \cdot s' + d \cdot T \cdot v^{(t)}
- trong Ä‘Ã³:
  - v(t)v^{(t)} lÃ  vector xÃ¡c suáº¥t á»Ÿ vÃ²ng láº·p thá»© tt
  - sâ€²s' lÃ  vector seed (Ä‘Ã£ chuáº©n hÃ³a)
  - TT lÃ  ma tráº­n chuyá»ƒn Ä‘á»•i (transition matrix) cá»§a KG, xÃ¡c Ä‘á»‹nh xÃ¡c suáº¥t chuyá»ƒn tá»« node nÃ y sang node khÃ¡c dá»±a trÃªn cÃ¡c cáº¡nh liÃªn káº¿t
  - dd lÃ  damping factor (thÆ°á»ng khoáº£ng 0.5â€“0.85)
- QuÃ¡ trÃ¬nh nÃ y sáº½ â€œlan tá»aâ€ cÃ¡c giÃ¡ trá»‹ tá»« cÃ¡c seed node ban Ä‘áº§u theo cÃ¡c liÃªn káº¿t trong Ä‘á»“ thá»‹. CÃ¡c Phrase Nodes vá»›i giÃ¡ trá»‹ cao ban Ä‘áº§u (0.49 vÃ  0.46) sáº½ â€œÄ‘áº©yâ€ xÃ¡c suáº¥t sang cÃ¡c nÃºt lÃ¢n cáº­n. Náº¿u Passage Node C (vá»›i giÃ¡ trá»‹ ban Ä‘áº§u 0.027) cÃ³ cÃ¡c cáº¡nh máº¡nh liÃªn káº¿t vá»›i Phrase Nodes, nÃ³ cÃ³ thá»ƒ nháº­n thÃªm xÃ¡c suáº¥t tá»« chÃºng.

---
Pháº£n biá»‡n 3: VÃ­ dá»¥ sá»‘ há»c trong quÃ¡ trÃ¬nh PPR
Q: HÃ£y láº¥y má»™t vÃ²ng láº·p PPR minh há»a cá»¥ thá»ƒ cho vÃ­ dá»¥ nÃ y.
A:
- Giáº£ sá»­ ma tráº­n chuyá»ƒn Ä‘á»•i TT cá»§a má»™t pháº§n nhá» trong KG Ä‘Æ°á»£c Ä‘Æ¡n giáº£n hoÃ¡ nhÆ° sau (chá»‰ xÃ©t 4 node trÃªn):
  - TABT_{AB} lÃ  xÃ¡c suáº¥t tá»« node A sang B, TACT_{AC} tá»« A sang C, v.vâ€¦
- VÃ­ dá»¥, ta giáº£ sá»­:
  - Node A (Phrase "tÃ¡c giáº£") cÃ³ liÃªn káº¿t:
    - 60% Ä‘áº¿n Node B ("The Great Novel")
    - 40% Ä‘áº¿n Node C (Passage chá»©a thÃ´ng tin chÃ­nh)
  - Node B ("The Great Novel") liÃªn káº¿t:
    - 70% Ä‘áº¿n Node A
    - 30% Ä‘áº¿n Node D (Passage phá»¥)
  - Node C (Passage 1) liÃªn káº¿t:
    - 100% Ä‘áº¿n Node A (giáº£ sá»­ cÃ³ má»‘i liÃªn há»‡ máº¡nh vá»›i khÃ¡i niá»‡m "tÃ¡c giáº£")
  - Node D (Passage 2) liÃªn káº¿t:
    - 100% Ä‘áº¿n Node B
- Ta cÃ³ ma tráº­n TT (Ä‘Æ¡n giáº£n hoÃ¡) nhÆ° sau:
- T=[00.600.4000.70000.301.000001.000]T = \begin{bmatrix} 0   & 0.60 & 0.40 & 0 \\ 0.70 & 0   & 0    & 0.30 \\ 1.0  & 0   & 0    & 0 \\ 0   & 1.0 & 0    & 0  \end{bmatrix}
- Giáº£ sá»­ damping factor d=0.5d = 0.5. Vá»›i vector seed chuáº©n hÃ³a sâ€²=[0.49,0.46,0.027,0.019]âŠ¤s' = [0.49, 0.46, 0.027, 0.019]^\top, ta khá»Ÿi táº¡o v(0)=sâ€²v^{(0)} = s'.
- VÃ²ng láº·p thá»© 1:
 TÃ­nh v(1)=0.5â‹…sâ€²+0.5â‹…Tâ‹…v(0)v^{(1)} = 0.5 \cdot s' + 0.5 \cdot T \cdot v^{(0)}
- TrÆ°á»›c háº¿t, tÃ­nh Tâ‹…v(0)T \cdot v^{(0)}:
  - (Tâ‹…v(0))A=0âˆ—0.49+0.60âˆ—0.46+0.40âˆ—0.027+0âˆ—0.019â‰ˆ0.60Ã—0.46+0.40Ã—0.027â‰ˆ0.276+0.0108â‰ˆ0.287(T \cdot v^{(0)})_A = 0*0.49 + 0.60*0.46 + 0.40*0.027 + 0*0.019 \approx 0.60 \times 0.46 + 0.40 \times 0.027 \approx 0.276 + 0.0108 \approx 0.287
  - (Tâ‹…v(0))B=0.70âˆ—0.49+0âˆ—0.46+0âˆ—0.027+0.30âˆ—0.019â‰ˆ0.343+0.0057â‰ˆ0.3487(T \cdot v^{(0)})_B = 0.70*0.49 + 0*0.46 + 0*0.027 + 0.30*0.019 \approx 0.343 + 0.0057 \approx 0.3487
  - (Tâ‹…v(0))C=1.0âˆ—0.49+0âˆ—0.46+0âˆ—0.027+0âˆ—0.019=0.49(T \cdot v^{(0)})_C = 1.0*0.49 + 0*0.46 + 0*0.027 + 0*0.019 = 0.49
  - (Tâ‹…v(0))D=0âˆ—0.49+1.0âˆ—0.46+0âˆ—0.027+0âˆ—0.019=0.46(T \cdot v^{(0)})_D = 0*0.49 + 1.0*0.46 + 0*0.027 + 0*0.019 = 0.46
- Sau Ä‘Ã³,
- v(1)=0.5â‹…[0.49,0.46,0.027,0.019]âŠ¤+0.5â‹…[0.287,0.3487,0.49,0.46]âŠ¤v^{(1)} = 0.5 \cdot [0.49, 0.46, 0.027, 0.019]^\top + 0.5 \cdot [0.287, 0.3487, 0.49, 0.46]^\top 
- =
- [0.245+0.1435,â€‰0.23+0.17435,â€‰0.0135+0.245,â€‰0.0095+0.23][0.245 + 0.1435,\, 0.23 + 0.17435,\, 0.0135 + 0.245,\, 0.0095 + 0.23]
- =
- [0.3885,â€‰0.40435,â€‰0.2585,â€‰0.2395][0.3885,\, 0.40435,\, 0.2585,\, 0.2395]
- Giáº£i thÃ­ch:
 NhÃ¬n vÃ o v(1)v^{(1)}:
  - Node A vÃ  Node B (Phrase Nodes) cÃ³ giÃ¡ trá»‹ váº«n cao (0.3885 vÃ  0.40435).
  - Node C (Passage 1) tÄƒng tá»« 0.027 lÃªn 0.2585 nhá» nháº­n Ä‘Æ°á»£c thÃ´ng tin tá»« Node A (vÃ¬ cÃ³ liÃªn káº¿t 100% tá»« Node C sang A) vÃ  tá»« má»‘i liÃªn káº¿t cá»§a A Ä‘áº¿n C.
  - Node D (Passage 2) cÃ³ giÃ¡ trá»‹ 0.2395 nhá» liÃªn káº¿t vá»›i Node B.
QuÃ¡ trÃ¬nh nÃ y sáº½ láº·p láº¡i cho Ä‘áº¿n khi há»™i tá»¥. Káº¿t quáº£ cuá»‘i cÃ¹ng sáº½ lÃ  vector xÃ¡c suáº¥t phÃ¢n bá»• qua toÃ n bá»™ KG, trong Ä‘Ã³ cÃ¡c node cÃ³ má»‘i liÃªn káº¿t máº¡nh vá»›i seed nodes ban Ä‘áº§u sáº½ nháº­n Ä‘Æ°á»£c xÃ¡c suáº¥t cao hÆ¡n.

---
Pháº£n biá»‡n 4: áº¢nh hÆ°á»Ÿng cá»§a PPR Ä‘áº¿n viá»‡c xáº¿p háº¡ng cÃ¡c Passage
Q: Sau khi vector xÃ¡c suáº¥t Ä‘Æ°á»£c tÃ­nh xong, lÃ m sao cÃ¡c giÃ¡ trá»‹ nÃ y chuyá»ƒn thÃ nh viá»‡c xáº¿p háº¡ng cÃ¡c Ä‘oáº¡n vÄƒn tráº£ lá»i truy váº¥n?
A:
- Sau khi cÃ³ vector vv cho cÃ¡c node trong KG, há»‡ thá»‘ng sáº½ nhÃ¢n cÃ¡c giÃ¡ trá»‹ nÃ y vá»›i ma tráº­n P (P matrix), mÃ  trong Ä‘Ã³ má»—i pháº§n tá»­ biá»ƒu thá»‹ sá»‘ láº§n xuáº¥t hiá»‡n cá»§a má»™t nÃºt (phrase hoáº·c passage) trong cÃ¡c Ä‘oáº¡n vÄƒn gá»‘c.
- Tá»•ng há»£p giÃ¡ trá»‹ tá»« cÃ¡c node liÃªn quan sáº½ cho Ä‘iá»ƒm xáº¿p háº¡ng cho tá»«ng Ä‘oáº¡n vÄƒn. CÃ¡c Ä‘oáº¡n vÄƒn cÃ³ tá»•ng xÃ¡c suáº¥t cao (Ä‘áº·c biá»‡t lÃ  do nháº­n Ä‘Æ°á»£c â€œÄ‘iá»ƒm cá»™ngâ€ tá»« cÃ¡c Phrase Nodes máº¡nh vÃ  Passage Nodes cÃ³ liÃªn káº¿t tá»‘t) sáº½ Ä‘Æ°á»£c xáº¿p háº¡ng Æ°u tiÃªn.
VÃ­ dá»¥:
- Passage Node C, sau PPR, cÃ³ giÃ¡ trá»‹ tÄƒng lÃªn Ä‘Ã¡ng ká»ƒ (vÃ­ dá»¥ 0.2585), khi nhÃ¢n vá»›i sá»‘ láº§n xuáº¥t hiá»‡n vÃ  cÃ¡c liÃªn káº¿t liÃªn quan, nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¿p háº¡ng cao vÃ  chá»n lÃ m cÃ¢u tráº£ lá»i cho truy váº¥n "TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?".

---
Tá»•ng káº¿t
Qua vÃ­ dá»¥ chi tiáº¿t trÃªn, ta tháº¥y ráº±ng:
1. Seed Node Weights Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh dá»±a trÃªn similarity score vÃ  weight factor riÃªng biá»‡t cho Phrase Nodes (1.0) vÃ  Passage Nodes (0.05).
2. Vector seed sau khi chuáº©n hÃ³a cung cáº¥p Ä‘iá»ƒm khá»Ÿi Ä‘áº§u cho PPR.
3. PPR lan tá»a xÃ¡c suáº¥t tá»« cÃ¡c seed node theo cÃ¡c liÃªn káº¿t trong KG, giÃºp cÃ¡c Passage Nodes nháº­n Ä‘Æ°á»£c â€œÄ‘iá»ƒm cá»™ngâ€ tá»« cÃ¡c Phrase Nodes náº¿u chÃºng cÃ³ liÃªn káº¿t cháº·t cháº½.
4. Káº¿t quáº£ cuá»‘i cÃ¹ng sau PPR vÃ  káº¿t há»£p vá»›i ma tráº­n P sáº½ xáº¿p háº¡ng cÃ¡c Ä‘oáº¡n vÄƒn phÃ¹ há»£p Ä‘á»ƒ tráº£ lá»i truy váº¥n.
Qua Ä‘Ã³, ta tháº¥y ráº±ng viá»‡c gÃ¡n trá»ng sá»‘ seed node vÃ  á»©ng dá»¥ng PPR khÃ´ng chá»‰ giÃºp cÃ¢n báº±ng giá»¯a thÃ´ng tin khÃ¡i niá»‡m vÃ  ngá»¯ cáº£nh, mÃ  cÃ²n táº¡o Ä‘iá»u kiá»‡n cho quÃ¡ trÃ¬nh multi-hop retrieval diá»…n ra hiá»‡u quáº£ vÃ  chÃ­nh xÃ¡c.

---
1. MOTIVATION RÃ• RÃ€NG => Má»šI TRIá»‚N?  
2. DATASET? CÃCH Há»Œ ÄÃNH GIÃ NHÆ¯ NÃ€O?
3. METHOD? Ká»¸ THUáº¬T ÄÃ“ LÃ€ GÃŒ? ? Ká»¸ THUáº¬T ÄÃ“ NHÆ¯ NÃ€O? => Äá»ŠNH NGHÄ¨A ÄÆ¯á»¢C CÃC KHÃI NIá»†M => Má»šI CÃ“ Ká»¸ THUáº¬T.  

---
Prompt 1: TÃ´i muá»‘n báº¡n pháº£i hiá»ƒu tháº­t sÃ¢u. má»—i pháº§n Ä‘áº·t cÃ¡c cÃ¢u há»i pháº£n biá»‡n sáº¯c xáº£o Ä‘á»ƒ tá»± tráº£ lá»i. Giá»‘ng nhÆ° 1 ngÆ°á»i phÃ¡, 1 ngÆ°á»i tráº£ lá»i, láº¥y dáº«n chá»©ng
Prompt 2: Giáº£i tÃ­ch tá»«ng khÃ¡i niá»‡m vÃ  liÃªn há»‡ nÃ³ vá»›i thá»© tháº­t dá»… nhá»› Ä‘Æ¡n giáº£n cho há»c sinh cáº¥p 2

ğŸ§  PHáº¦N 1: Váº¥n Ä‘á» cá»§a RAG hiá»‡n táº¡i & Sá»± xuáº¥t hiá»‡n cá»§a HippoRAG
ğŸ” Giáº£i thÃ­ch láº¡i:
RAG hiá»‡n táº¡i hoáº¡t Ä‘á»™ng theo cÆ¡ cháº¿:
- TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c chunk nhá»
- DÃ¹ng embedding â†’ lÆ°u vÃ o VectorDB
- Khi cÃ³ query â†’ tÃ¬m vector tÆ°Æ¡ng Ä‘á»“ng â†’ láº¥y context â†’ Ä‘Æ°a vÃ o LLM
NhÆ°ng vá»›i cÃ¡c tÃ¡c vá»¥ multi-hop reasoning, nhiá»u thÃ´ng tin á»Ÿ ráº£i rÃ¡c cÃ¡c Ä‘oáº¡n khÃ¡c nhau thÃ¬ RAG hiá»‡n táº¡i khÃ´ng Ä‘á»§ hiá»‡u quáº£.
PhÃ¡ 1.1 Táº¡i sao nÃ³i vá»›i cÃ¡c tÃ¡c vá»¥ multi-hop reasoning, nhiá»u thÃ´ng tin á»Ÿ ráº£i rÃ¡c cÃ¡c Ä‘oáº¡n khÃ¡c nhau thÃ¬ RAG hiá»‡n táº¡i khÃ´ng Ä‘á»§ hiá»‡u quáº£? 
Giáº£ sá»­ RAG truyá»n thá»‘ng retrieve Ä‘Æ°á»£c cáº£ hai Ä‘oáº¡n rá»i ráº¡c, thÃ¬ nÃ³ váº«n nÃ©m háº¿t vÃ o context window cá»§a LLM mÃ ? LÃºc Ä‘Ã³ LLM ná»‘i láº¡i logic lÃ  xong chá»©?

PhÃ¡ 1: RAG truyá»n thá»‘ng, trong lÃºc query ra cÃ¡c Ä‘oáº¡n chunking thÃ¬ nÃ³ cháº³ng nháº» khÃ´ng cÃ³ tÃ­nh mÃ³c ná»‘i?
â€œGiáº£ sá»­ RAG truyá»n thá»‘ng retrieve Ä‘Æ°á»£c cáº£ hai Ä‘oáº¡n rá»i ráº¡c, thÃ¬ nÃ³ váº«n nÃ©m háº¿t vÃ o context window cá»§a LLM mÃ ? LÃºc Ä‘Ã³ LLM ná»‘i láº¡i logic lÃ  xong chá»©, cáº§n gÃ¬ pháº£i HippoRAG rÆ°á»m rÃ ?â€ 
ğŸ¯ 1. KhÃ´ng cÃ³ gÃ¬ Ä‘áº£m báº£o RAG sáº½ retrieve Ä‘Ãºng 2 Ä‘oáº¡n
Váº¥n Ä‘á» lá»›n nháº¥t cá»§a RAG:
- Retrieval = vector similarity â†’ chá»‰ chá»n Top-K
- Náº¿u thÃ´ng tin náº±m á»Ÿ 2 Ä‘oáº¡n khÃ¡c nhau, vÃ  má»—i cÃ¡i Ä‘á»u â€œhÆ¡i liÃªn quanâ€, nÃ³ cÃ³ thá»ƒ bá»‹ Ä‘áº©y ra khá»i Top-K
VÃ­ dá»¥ cá»¥ thá»ƒ:
Top-K = 3
 Báº¡n cáº§n Ä‘oáº¡n A vÃ  Ä‘oáº¡n B
 NhÆ°ng cÃ³ Ä‘oáº¡n C, D ráº¥t giá»‘ng cÃ¢u há»i vá» máº·t ngá»¯ nghÄ©a â†’ lÃªn Top
 A hoáº·c B bá»‹ loáº¡i vÃ¬ vector khÃ´ng Ä‘á»§ gáº§n
ğŸ’¥ â†’ LÃºc nÃ y LLM khÃ´ng cÃ³ nguyÃªn liá»‡u Ä‘á»ƒ suy luáº­n, dÃ¹ nÃ³ giá»i cá»¡ nÃ o. 

TÄƒng k. 
Dá»¯ liá»‡u náº±m ráº£i rÃ¡c táº¡i nhiá»u nÆ¡i. 

ğŸ§  2. LLM suy luáº­n = tá»‘n compute + dá»… lá»—i logic
CÃ¢u há»i:
"Which Stanford researcher who is not Alice but studies Alzheimer's worked at the same place as Thomas?"
Dataset:
902. Alice is a Stanford professor.
903. Thomas researches Alzheimerâ€™s at UCSF.
904. Mike works at UCSF.
905. Sarah studies Alzheimerâ€™s.
906. Sarah and Thomas are labmates at UCSF.

âš ï¸ Váº¥n Ä‘á» cá»§a RAG khi retrieve Top-K
Giáº£ sá»­ retrieve Ä‘Æ°á»£c 3 Ä‘oáº¡n:
- (2) Thomas â†’ Alzheimer + UCSF
- (4) Sarah â†’ Alzheimer
- (5) Sarah â†’ labmate of Thomas â†’ UCSF
LLM pháº£i lÃ m:
1267. Gáº¯n Sarah vá»›i UCSF (tá»« labmate)
1268. Trá»« Alice khá»i káº¿t quáº£
1269. Chá»n ngÆ°á»i vá»«a Stanford researcher, vá»«a Alzheimer, vá»«a á»Ÿ UCSF, nhÆ°ng khÃ´ng pháº£i Alice

---
âœ³ï¸ ÄÃ¢y lÃ  3-hop reasoning. KhÃ´ng pháº£i vÃ¬ LLM khÃ´ng thá»ƒ, mÃ  vÃ¬:
- NÃ³ pháº£i giá»¯ á»•n Ä‘á»‹nh attention giá»¯a 4-5 Ä‘oáº¡n vÄƒn
- NÃ³ khÃ´ng biáº¿t cháº¯c má»‘i liÃªn káº¿t lÃ  quan trá»ng hay chá»‰ lÃ  â€œnhiá»…uâ€
- KhÃ´ng cÃ³ logic structure rÃµ rÃ ng Ä‘á»ƒ láº§n tá»« A â†’ B â†’ C
â¡ï¸ Khi nÃ y, retrieval dáº¡ng vector (dÃ¹ chunk tá»‘t) khÃ´ng cung cáº¥p structure reasoning.
 Chá»‰ nÃ©m Ä‘oáº¡n vÃ o context â†’ LLM pháº£i lo cáº£ Ä‘á»c, tÃ¬m logic, tá»•ng há»£p, trÃ¡nh sai lá»‡ch.

CÃ¢u há»i tiáº¿p Ä‘áº·t ra lÃ : â—â€œBÃ¢y giá» cÃ³ model 4o, Claude Opus, GPT-4-Turboâ€¦ nÃ³ cÃ³ thá»ƒ context window 128k tokens, báº¡n nÃ©m sáº¡ch vÃ´ lÃ  nÃ³ lÃ m reasoning cá»±c tá»‘t. Tháº¿ thÃ¬ lÃ m retrieval Ä‘á»ƒ lÃ m gÃ¬ ná»¯a? HippoRAG cÃ³ cÃ²n giÃ¡ trá»‹ khÃ´ng?â€
Tráº£ lá»i: ÄÃºng! NhÆ°ng chá»‰ khi báº¡n sá»‘ng trong tháº¿ giá»›i lÃ½ tÆ°á»Ÿng.

âœ³ï¸ PHáº¦N I: Thá»«a nháº­n sá»± tháº­t
âœ… ÄÃºng, model 4o, Claude Opus, GPT-4 Turbo hiá»‡n nay Ä‘Ã£ cÃ³ kháº£ nÄƒng context-wide reasoning ráº¥t máº¡nh.
ğŸ“Œ Báº¡n cÃ³ thá»ƒ Ä‘Æ°a vÃ o:
- Má»™t dataset 80k tokens gá»“m nhiá»u tÃ i liá»‡u
- Má»™t cÃ¢u há»i cáº§n ná»‘i 3â€“4 thÃ´ng tin láº¡i
- VÃ  model sáº½ tráº£ lá»i Ä‘Ãºng â€“ tháº­m chÃ­ cÃ²n giáº£i thÃ­ch táº¡i sao.

---
âœ³ï¸ PHáº¦N II: NhÆ°ng Ä‘Ã³ khÃ´ng pháº£i lÃ  cÃ¡ch lÃ m AI hiá»‡u quáº£ hoáº·c quy mÃ´
â— LÃ½ do 1: Context-wide LLM reasoning = tá»‘n compute khá»§ng khiáº¿p
NÃ©m vÃ o 128k tokens = giÃ¡ infer cao, latency cao
- Claude Opus hoáº·c GPT-4 Turbo 128k input â†’ thá»i gian tráº£ lá»i lÃ¢u hÆ¡n, chi phÃ­ tÃ­nh theo token cá»±c lá»›n (vÃ i $ má»—i lÆ°á»£t náº¿u full)
- Báº¡n khÃ´ng thá»ƒ scale 1 triá»‡u cÃ¢u há»i kiá»ƒu Ä‘Ã³

---
â— LÃ½ do 2: LLM = black box reasoning, khÃ´ng thá»ƒ debug, khÃ´ng thá»ƒ kiá»ƒm soÃ¡t
- LLM khÃ´ng cho báº¡n biáº¿t reasoning path
- Báº¡n khÃ´ng biáº¿t model ná»‘i A â†’ B â†’ C nhÆ° tháº¿ nÃ o
- Náº¿u nÃ³ tráº£ sai? Báº¡n khÃ´ng biáº¿t sá»­a tá»« Ä‘Ã¢u
â¡ï¸ Retrieval reasoning = transparent, modular, debuggable
â†’ HippoRAG = ta láº§n Ä‘Æ°á»£c Ä‘Æ°á»ng Ä‘i qua graph
 â†’ CÃ³ thá»ƒ ghi log reasoning path
 â†’ CÃ³ thá»ƒ giáº£i thÃ­ch cho ngÆ°á»i dÃ¹ng hoáº·c kiá»ƒm tra báº±ng rule engine

---
â— LÃ½ do 3: KhÃ´ng pháº£i lÃºc nÃ o cÅ©ng cÃ³ quyá»n â€œnÃ©m háº¿t vÃ oâ€
- Náº¿u user há»i liÃªn tá»¥c, má»—i cÃ¢u Ä‘á»u cáº§n toÃ n bá»™ tÃ i liá»‡u â†’ cá»±c ká»³ lÃ£ng phÃ­
- VÃ­ dá»¥: app legal, hoáº·c chatbot há»c thuáº­t, hoáº·c QA y khoa
Báº¡n cáº§n selective retrieval â†’ RAG giáº£i quyáº¿t
 Báº¡n cáº§n reasoning tá»« cÃ¡c concept, entity â†’ khÃ´ng cÃ³ sáºµn text chunk â†’ HippoRAG giáº£i quyáº¿t

ğŸ§  3. HippoRAG cho phÃ©p retrieval chÃ­nh lÃ  reasoning
ÄÃ¢y lÃ  Ä‘iá»ƒm â€œÄ‘áº­p nÃ¡t pháº£n biá»‡nâ€:
ğŸ§  HippoRAG khÃ´ng chá» Ä‘áº¿n khi LLM lÃ m reasoning, mÃ  nÃ³ Ä‘Ã£ reasoning ngay tá»« bÆ°á»›c truy há»“i.
- Vá»›i Personalized PageRank â†’ nÃ³ tÃ¬m ra node trung gian (John Smith) trÆ°á»›c khi vÃ o LLM
- Tráº£ vá» Ä‘oáº¡n Ä‘Ã£ Ä‘á»§ logic â†’ LLM chá»‰ cáº§n nÃ³i láº¡i
ğŸ’¡ Äiá»u nÃ y biáº¿n retrieval thÃ nh má»™t â€œmÃ´-Ä‘un láº­p luáº­nâ€ (reasoning module), khÃ´ng chá»‰ lÃ  â€œscanner Ä‘oáº¡n vÄƒnâ€.

PhÃ¡ 1.2 Äáº·t váº¥n Ä‘á»? 
Tuyá»‡t vá»i, Quá»‘c Ä‘ang dÃ¹ng ká»¹ thuáº­t pháº£n biá»‡n cá»±c ká»³ hay Ä‘á»ƒ â€œÄ‘á»c sÃ¢uâ€ â€” kiá»ƒu Socratic method trong nghiÃªn cá»©u. MÃ¬nh sáº½ Ä‘Ã³ng cáº£ ngÆ°á»i há»i sáº¯c sáº£o vÃ  ngÆ°á»i tráº£ lá»i cháº·t cháº½, bÃ¡m sÃ¡t pháº§n â€œ1. Introductionâ€ cá»§a bÃ i bÃ¡o HippoRAG 2.

---
ğŸ§© PHÃ‚N TÃCH PHáº¦N Äáº¶T Váº¤N Äá»€ (INTRODUCTION - HippoRAG 2)

---
âš”ï¸ Pháº£n biá»‡n 1:
Há»i: Táº¡i sao cá»© pháº£i báº¯t AI há»c nhÆ° con ngÆ°á»i? Viá»‡c mÃ´ phá»ng â€œlong-term memoryâ€ cá»§a ngÆ°á»i cÃ³ tháº­t sá»± cáº§n thiáº¿t cho LLM khÃ´ng?
Tráº£ lá»i: VÃ¬ má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a LLM lÃ  trá»Ÿ thÃ nh trá»£ lÃ½ thÃ´ng minh vÃ  hiá»‡u quáº£ nhÆ° con ngÆ°á»i. Trong tháº¿ giá»›i thá»±c, con ngÆ°á»i liÃªn tá»¥c há»c, tÃ­ch há»£p thÃ´ng tin má»›i mÃ  khÃ´ng quÃªn cÃ¡i cÅ© â€“ Ä‘Ã¢y lÃ  Ä‘iá»u LLM hiá»‡n táº¡i ráº¥t yáº¿u.
Báº±ng chá»©ng:
- LLM gáº·p váº¥n Ä‘á» catastrophic forgetting náº¿u fine-tune.
- RAG tuy tiáº¿p cáº­n tá»‘t hÆ¡n nhÆ°ng váº«n khÃ´ng cÃ³ kháº£ nÄƒng káº¿t ná»‘i máº¡ch láº¡c, nhÆ° ngÆ°á»i nhá»› chuyá»‡n Ä‘Ã£ xáº£y ra tá»« lÃ¢u vÃ  liÃªn káº¿t nÃ³ vá»›i bá»‘i cáº£nh hiá»‡n táº¡i.
VÃ¬ tháº¿, muá»‘n LLM trá»Ÿ thÃ nh trá»£ lÃ½ lÃ¢u dÃ i, cáº­p nháº­t liÃªn tá»¥c, nÃ³ cáº§n má»™t há»‡ thá»‘ng bá»™ nhá»› dÃ i háº¡n linh hoáº¡t vÃ  liÃªn káº¿t â€“ giá»‘ng nhÆ° cá»§a ngÆ°á»i.

---
âš”ï¸ Pháº£n biá»‡n 2:
Há»i: RAG Ä‘ang cháº¡y tá»‘t mÃ . Táº¡i sao láº¡i nÃ³i RAG truyá»n thá»‘ng khÃ´ng Ä‘á»§? CÃ³ báº±ng chá»©ng gÃ¬?
Tráº£ lá»i: RAG truyá»n thá»‘ng chá»§ yáº¿u dá»±a vÃ o vector retrieval Ä‘á»™c láº­p, má»—i chunk Ä‘Æ°á»£c xáº¿p háº¡ng dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng tá»± embedding vá»›i query â€“ khÃ´ng cÃ³ cÆ¡ cháº¿ Ä‘á»ƒ hiá»ƒu má»‘i quan há»‡ giá»¯a cÃ¡c chunk, khÃ´ng hiá»ƒu máº¡ch truyá»‡n, khÃ´ng lÃ m multi-hop reasoning.
Báº±ng chá»©ng cá»¥ thá»ƒ trong bÃ i:
â€œ...inability to capture two vital aspects: sense-making (hiá»ƒu ngá»¯ cáº£nh phá»©c táº¡p) vÃ  associativity (káº¿t ná»‘i Ä‘a bÆ°á»›c giá»¯a thÃ´ng tin rá»i ráº¡c)â€ â€“ (trang 1)
VÃ¬ váº­y, tuy RAG tá»‘t trong viá»‡c â€œtÃ¬m thÃ´ng tin gáº§n nháº¥tâ€, nhÆ°ng:
- KhÃ´ng liÃªn káº¿t cÃ¡c Ä‘oáº¡n kiáº¿n thá»©c láº¡i vá»›i nhau.
- KhÃ´ng biáº¿t â€œÄ‘i vÃ²ngâ€ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i yÃªu cáº§u reasoning.
=> ÄÃ³ lÃ  lÃ½ do cáº§n má»™t mÃ´ hÃ¬nh vÆ°á»£t RAG truyá»n thá»‘ng.

---
âš”ï¸ Pháº£n biá»‡n 3:
Há»i: Tháº¿ cÃ²n cÃ¡c mÃ´ hÃ¬nh má»›i nhÆ° GraphRAG, RAPTOR? Há» Ä‘Ã£ cá»‘ kháº¯c phá»¥c rá»“i mÃ ?
Tráº£ lá»i: ÄÃºng, nhÆ°ng theo tÃ¡c giáº£:
- Nhá»¯ng mÃ´ hÃ¬nh Ä‘Ã³ táº­p trung quÃ¡ háº¹p vÃ o má»™t khÃ­a cáº¡nh (sense-making hoáº·c associativity) nÃªn máº¥t cÃ¢n báº±ng.
- Khi lÃ m tá»‘t pháº§n nÃ y thÃ¬ láº¡i suy giáº£m hiá»‡u nÄƒng á»Ÿ pháº§n khÃ¡c. VÃ­ dá»¥:
  - GraphRAG tá»‘t cho discourse â†’ kÃ©m factual QA.
  - RAPTOR tÃ³m táº¯t tá»‘t â†’ nhÆ°ng gÃ¢y nhiá»…u khi QA nhiá»u bÆ°á»›c.
  - HippoRAG 1 tá»‘t vá»›i multi-hop â†’ nhÆ°ng thiáº¿u contextualization.
BÃ i bÃ¡o viáº¿t:
â€œ...each method experiences the largest performance decay in tasks outside its own setup.â€ â†’ tá»©c lÃ  mÃ´ hÃ¬nh tá»‘t chá»— nÃ y thÃ¬ gÃ£y chá»— kia.

---
âš”ï¸ Pháº£n biá»‡n 4:
Há»i: Váº­y má»¥c tiÃªu chÃ­nh cá»§a HippoRAG lÃ  gÃ¬? KhÃ¡c biá»‡t cá»‘t lÃµi so vá»›i RAG vÃ  cÃ¡c mÃ´ hÃ¬nh má»Ÿ rá»™ng?
Tráº£ lá»i:
 HippoRAG nháº¯m tá»›i má»™t bá»™ nhá»› dÃ i háº¡n tá»•ng quÃ¡t, cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t á»Ÿ:
- Factual QA (ghi nhá»› sá»± kiá»‡n cÆ¡ báº£n),
- Multi-hop QA (liÃªn káº¿t kiáº¿n thá»©c),
- Discourse understanding (diá»…n giáº£i máº¡ch truyá»‡n dÃ i).
Cá»‘t lÃµi:
- MÃ´ phá»ng bá»™ nhá»› sinh há»c con ngÆ°á»i:
  - Neocortex â†’ LLM
  - Parahippocampal â†’ encoder gá»£i nhá»› ngá»¯ nghÄ©a
  - Hippocampus â†’ KG má»Ÿ káº¿t ná»‘i tri thá»©c
- DÃ¹ng Personalized PageRank (PPR) Ä‘á»ƒ tÃ¬m kiáº¿m cÃ³ liÃªn káº¿t â€“ khÃ´ng chá»‰ Ä‘iá»ƒm, mÃ  lÃ  Ä‘Æ°á»ng Ä‘i kiáº¿n thá»©c.
- ThÃªm LLM vÃ o retrieval Ä‘á»ƒ lá»c nhiá»…u â†’ khÃ´ng chá»‰ tÃ¬m, mÃ  cÃ²n nháº­n diá»‡n Ä‘Ãºng Ä‘oáº¡n cáº§n nhá»›.

---
âš”ï¸ Pháº£n biá»‡n 5:
Há»i: Váº­y HippoRAG cÃ³ báº±ng chá»©ng nÃ o máº¡nh Ä‘á»ƒ chá»©ng minh lÃ  cáº§n thiáº¿t khÃ´ng?
Tráº£ lá»i:
- HippoRAG 2 Ä‘áº¡t Ä‘iá»ƒm cao nháº¥t á»Ÿ cáº£ 3 loáº¡i nhiá»‡m vá»¥ (Factual, Associative, Sense-making) â€“ Ä‘iá»u mÃ  khÃ´ng mÃ´ hÃ¬nh nÃ o trÆ°á»›c Ä‘Ã³ lÃ m Ä‘Æ°á»£c.
- VÃ­ dá»¥:
  - +7 Ä‘iá»ƒm F1 á»Ÿ Associativity tasks so vá»›i embedding tá»‘t nháº¥t (NV-Embed-v2).
  - KhÃ´ng bá»‹ suy giáº£m á»Ÿ Factual QA nhÆ° GraphRAG, RAPTOR.
=> ÄÃ¢y lÃ  báº±ng chá»©ng cho tháº¥y HippoRAG cÃ³ thá»ƒ cÃ¢n báº±ng giá»¯a sá»± chÃ­nh xÃ¡c, kháº£ nÄƒng diá»…n giáº£i vÃ  káº¿t ná»‘i tri thá»©c, Ä‘iá»u RAG truyá»n thá»‘ng khÃ´ng lÃ m Ä‘Æ°á»£c.

---
ChÃ o Quá»‘c,
MÃ¬nh sáº½ giáº£i thÃ­ch chi tiáº¿t vá» ba loáº¡i nhiá»‡m vá»¥: Factual, Associative, Sense-making, vÃ  má»‘i liÃªn há»‡ giá»¯a cÃ¡c nhiá»‡m vá»¥ nÃ y vá»›i bá»™ dá»¯ liá»‡u LongMemEval.

---
1. Ba loáº¡i nhiá»‡m vá»¥: Factual, Associative, Sense-making
a. Factual Tasks (Nhiá»‡m vá»¥ thá»±c táº¿)
- Äá»‹nh nghÄ©a: YÃªu cáº§u há»‡ thá»‘ng truy xuáº¥t vÃ  cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c vá» má»™t sá»± kiá»‡n hoáº·c thá»±c táº¿ cá»¥ thá»ƒ.îˆ†
- VÃ­ dá»¥: "Ai lÃ  tá»•ng thá»‘ng Ä‘áº§u tiÃªn cá»§a Hoa Ká»³?"îˆ†
- Má»¥c tiÃªu: Kiá»ƒm tra kháº£ nÄƒng ghi nhá»› vÃ  truy xuáº¥t thÃ´ng tin chÃ­nh xÃ¡c tá»« bá»™ nhá»› dÃ i háº¡n.îˆ†
b. Associative Tasks (Nhiá»‡m vá»¥ liÃªn káº¿t)
- Äá»‹nh nghÄ©a: YÃªu cáº§u há»‡ thá»‘ng káº¿t ná»‘i nhiá»u máº£nh thÃ´ng tin rá»i ráº¡c Ä‘á»ƒ suy luáº­n hoáº·c tráº£ lá»i cÃ¢u há»i.îˆ†
- VÃ­ dá»¥: "NgÆ°á»i phÃ¡t minh ra Ä‘iá»‡n thoáº¡i Ä‘Ã£ sá»‘ng á»Ÿ Ä‘Ã¢u?"îˆ†
  - PhÃ¢n tÃ­ch: Cáº§n biáº¿t Alexander Graham Bell lÃ  ngÆ°á»i phÃ¡t minh ra Ä‘iá»‡n thoáº¡i vÃ  Ã´ng sá»‘ng á»Ÿ Scotland vÃ  Canada.îˆ†
- Má»¥c tiÃªu: ÄÃ¡nh giÃ¡ kháº£ nÄƒng liÃªn káº¿t vÃ  suy luáº­n tá»« nhiá»u nguá»“n thÃ´ng tin khÃ¡c nhau.îˆ†
c. Sense-making Tasks (Nhiá»‡m vá»¥ hiá»ƒu ngá»¯ cáº£nh)
- Äá»‹nh nghÄ©a: YÃªu cáº§u há»‡ thá»‘ng hiá»ƒu vÃ  diá»…n giáº£i cÃ¡c tÃ¬nh huá»‘ng phá»©c táº¡p hoáº·c cÃ¢u chuyá»‡n dÃ i.îˆ†
- VÃ­ dá»¥: "TÃ³m táº¯t ná»™i dung chÃ­nh cá»§a cuá»™c há»p hÃ´m qua."îˆ†
- Má»¥c tiÃªu: Kiá»ƒm tra kháº£ nÄƒng tá»•ng há»£p, hiá»ƒu ngá»¯ cáº£nh vÃ  trÃ¬nh bÃ y láº¡i thÃ´ng tin má»™t cÃ¡ch máº¡ch láº¡c.îˆ†

---
2. Má»‘i liÃªn há»‡ giá»¯a QA vÃ  LongMemEval
LongMemEval lÃ  má»™t bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng ghi nhá»› dÃ i háº¡n cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n. Bá»™ dá»¯ liá»‡u nÃ y táº­p trung vÃ o nÄƒm kháº£ nÄƒng cá»‘t lÃµi:
1. TrÃ­ch xuáº¥t thÃ´ng tin (Information Extraction): Kháº£ nÄƒng nhá»› láº¡i thÃ´ng tin cá»¥ thá»ƒ tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c dÃ i.îˆ†
2. LÃ½ luáº­n Ä‘a phiÃªn (Multi-Session Reasoning): Kháº£ nÄƒng tá»•ng há»£p thÃ´ng tin tá»« nhiá»u phiÃªn trÃ² chuyá»‡n.îˆ†
3. Cáº­p nháº­t kiáº¿n thá»©c (Knowledge Updates): Kháº£ nÄƒng nháº­n biáº¿t vÃ  cáº­p nháº­t thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng theo thá»i gian.îˆ†
4. LÃ½ luáº­n thá»i gian (Temporal Reasoning): Nháº­n thá»©c vá» cÃ¡c khÃ­a cáº¡nh thá»i gian cá»§a thÃ´ng tin ngÆ°á»i dÃ¹ng.îˆ†
5. Tá»« chá»‘i tráº£ lá»i (Abstention): Kháº£ nÄƒng tá»« chá»‘i tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n thÃ´ng tin khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.îˆ†
Má»‘i liÃªn há»‡:
- Factual Tasks tÆ°Æ¡ng á»©ng vá»›i TrÃ­ch xuáº¥t thÃ´ng tin, nÆ¡i há»‡ thá»‘ng cáº§n truy xuáº¥t thÃ´ng tin chÃ­nh xÃ¡c tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.îˆ†
- Associative Tasks liÃªn quan Ä‘áº¿n LÃ½ luáº­n Ä‘a phiÃªn, yÃªu cáº§u há»‡ thá»‘ng káº¿t há»£p thÃ´ng tin tá»« nhiá»u phiÃªn Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c.îˆ†
- Sense-making Tasks gáº¯n liá»n vá»›i LÃ½ luáº­n thá»i gian vÃ  Cáº­p nháº­t kiáº¿n thá»©c, Ä‘Ã²i há»i há»‡ thá»‘ng hiá»ƒu ngá»¯ cáº£nh tá»•ng thá»ƒ vÃ  cáº­p nháº­t thÃ´ng tin má»›i nháº¥t cá»§a ngÆ°á»i dÃ¹ng.îˆ†

---

DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch phÃ¢n biá»‡t rÃµ rÃ ng giá»¯a Factual, Associative, Sense-making, báº±ng cÃ¡ch Ä‘i tá»« Ä‘á»‹nh nghÄ©a â vÃ­ dá»¥ cá»¥ thá»ƒ â nÄƒng lá»±c cáº§n kiá»ƒm tra:

---
ğŸ§  1. Factual Memory â€“ Ghi nhá»› sá»± tháº­t cá»¥ thá»ƒ
ğŸ§¾ Äá»‹nh nghÄ©a:
Kháº£ nÄƒng truy xuáº¥t thÃ´ng tin Ä‘Æ¡n láº», Ä‘Ã£ Ä‘Æ°á»£c nÃªu rÃµ rÃ ng trong ngá»¯ liá»‡u Ä‘áº§u vÃ o.
â“VÃ­ dá»¥ cÃ¢u há»i:
"Ai lÃ  thá»§ tÆ°á»›ng Äá»©c nÄƒm 2023?"
- ÄÃ¢y lÃ  cÃ¢u há»i Ä‘Ã²i há»i 1 Ä‘oáº¡n vÄƒn báº£n chá»©a thÃ´ng tin chÃ­nh xÃ¡c, khÃ´ng cáº§n suy luáº­n hay liÃªn káº¿t.
ğŸ¯ NÄƒng lá»±c Ä‘Æ°á»£c kiá»ƒm tra:
- Kháº£ nÄƒng lÆ°u trá»¯ & truy xuáº¥t chÃ­nh xÃ¡c thÃ´ng tin Ä‘Ã£ Ä‘á»c.
- TÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i nhá»› tÃªn, ngÃ y thÃ¡ng, con sá»‘, Ä‘á»‹nh nghÄ©a.

---
ğŸ”— 2. Associativity â€“ Káº¿t ná»‘i thÃ´ng tin rá»i ráº¡c
ğŸ§¾ Äá»‹nh nghÄ©a:
Kháº£ nÄƒng káº¿t ná»‘i nhiá»u máº£nh kiáº¿n thá»©c tá»« cÃ¡c pháº§n khÃ¡c nhau trong dá»¯ liá»‡u Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
â“VÃ­ dá»¥ cÃ¢u há»i:
"NgÆ°á»i sÃ¡ng láº­p Microsoft cÃ³ quÃª gá»‘c á»Ÿ Ä‘Ã¢u?"
- Cáº§n ná»‘i hai máº£nh thÃ´ng tin:
  1. "Bill Gates lÃ  ngÆ°á»i sÃ¡ng láº­p Microsoft"
  2. "Bill Gates sinh ra á»Ÿ Seattle"
âŸ¶ Cáº§n Ã­t nháº¥t 2 Ä‘oáº¡n dá»¯ liá»‡u, khÃ´ng cÃ³ Ä‘oáº¡n nÃ o chá»©a Ä‘áº§y Ä‘á»§ cÃ¢u tráº£ lá»i.
ğŸ¯ NÄƒng lá»±c Ä‘Æ°á»£c kiá»ƒm tra:
- Reasoning nhiá»u bÆ°á»›c
- Multi-hop retrieval
- NhÆ° cÃ¡ch nÃ£o ngÆ°á»i liÃªn tÆ°á»Ÿng vÃ  xÃ¢u chuá»—i kiáº¿n thá»©c láº¡i vá»›i nhau.

---
ğŸ“– 3. Sense-making â€“ Hiá»ƒu vÃ  tá»•ng há»£p ngá»¯ cáº£nh phá»©c táº¡p
ğŸ§¾ Äá»‹nh nghÄ©a:
Kháº£ nÄƒng hiá»ƒu má»™t tÃ¬nh huá»‘ng dÃ i, nhiá»u lá»›p, nhiá»u áº©n Ã½; tÃ¡i cáº¥u trÃºc vÃ  rÃºt ra Ã½ nghÄ©a toÃ n cá»¥c.
â“VÃ­ dá»¥ cÃ¢u há»i:
"Táº¡i sao nhÃ¢n váº­t chÃ­nh láº¡i quyáº¿t Ä‘á»‹nh bá» trá»‘n á»Ÿ cuá»‘i truyá»‡n?"
- Tráº£ lá»i Ä‘Æ°á»£c Ä‘Ã²i há»i pháº£i Ä‘á»c cáº£ máº¡ch truyá»‡n, hiá»ƒu Ä‘Æ°á»£c:
  - TÃ¢m lÃ½ nhÃ¢n váº­t
  - Diá»…n biáº¿n sá»± kiá»‡n
  - CÃ¡c yáº¿u tá»‘ xÃ£ há»™i, ngá»¯ cáº£nh
ğŸ¯ NÄƒng lá»±c Ä‘Æ°á»£c kiá»ƒm tra:
- Discourse understanding
- Cohesion & summarization
- Kháº£ nÄƒng cá»§a LLM giá»‘ng nhÆ° hiá»ƒu vÃ  phÃ¢n tÃ­ch vÄƒn há»c â€“ Ä‘iá»u mÃ  vector retrieval Ä‘Æ¡n thuáº§n khÃ´ng lÃ m Ä‘Æ°á»£c.

---
ğŸ“Š TÃ³m táº¯t so sÃ¡nh
This content is only supported in a Lark Docs

---

ğŸ§© PHáº¦N 1: Táº I SAO CÃC MÃ” HÃŒNH RAG HIá»†N Táº I Bá»Š Háº N CHáº¾?

---
âš”ï¸ Pháº£n biá»‡n 1:
Há»i: Vector-based RAG hiá»‡n táº¡i Ä‘Ã£ ráº¥t máº¡nh vá»›i embedding hiá»‡n Ä‘áº¡i. Váº­y táº¡i sao váº«n bá»‹ Ä‘Ã¡nh giÃ¡ lÃ  "thiáº¿u tÃ­nh ngÆ°á»i"?
Tráº£ lá»i: VÃ¬ vector-based RAG chá»‰ lÃ m Ä‘Æ°á»£c retrieval Ä‘á»™c láº­p: má»—i Ä‘oáº¡n vÄƒn (passage) Ä‘Æ°á»£c nhÃºng vÃ  truy xuáº¥t Ä‘á»™c láº­p. NÃ³ khÃ´ng cÃ³ kháº£ nÄƒng:
- Nháº­n biáº¿t má»‘i quan há»‡ giá»¯a cÃ¡c Ä‘oáº¡n (multi-hop)
- KhÃ´ng hiá»ƒu Ä‘Æ°á»£c máº¡ch thÃ´ng tin dÃ i háº¡n
- KhÃ´ng há»— trá»£ truy váº¥n theo dÃ²ng suy nghÄ© (reasoning chain)
Giá»‘ng nhÆ° báº¡n nhá»› Ä‘Æ°á»£c 1000 máº£nh thÃ´ng tin, nhÆ°ng khÃ´ng biáº¿t máº£nh nÃ o ná»‘i vá»›i máº£nh nÃ o â€” báº¡n khÃ´ng thá»ƒ ká»ƒ láº¡i má»™t cÃ¢u chuyá»‡n, cÅ©ng khÃ´ng thá»ƒ suy luáº­n.
ğŸ“Œ Háº¡n cháº¿ nÃ y khiáº¿n vector-based RAG tá»‘t trong factual QA, nhÆ°ng fail trong associative vÃ  sense-making.

---
âš”ï¸ Pháº£n biá»‡n 2:
Há»i: Tháº¿ cÃ²n cÃ¡c RAG má»›i nhÆ° RAPTOR, GraphRAG, LightRAG? Há» táº¡o graph, táº¡o tÃ³m táº¯t rá»“i mÃ ?
Tráº£ lá»i: ÄÃºng lÃ  há» Ä‘Ã£ thá»­:
- RAPTOR: Táº¡o cÃ¡c summary tÃ³m gá»n, rá»“i retrieve tá»« cÃ¡c Ä‘oáº¡n tÃ³m táº¯t Ä‘Ã³.
- GraphRAG: Táº¡o cÃ¡c knowledge graph tá»« entity, liÃªn káº¿t qua cá»™ng Ä‘á»“ng (community detection).
- LightRAG: Táº¡o 2 lá»›p retrieval â€“ coarse vÃ  fine, tÃ­ch há»£p vector vÃ  graph.
NhÆ°ng táº¡i sao váº«n chÆ°a Ä‘á»§?
- ChÃºng thiáº¿u cÃ¢n báº±ng: cáº£i thiá»‡n chá»— nÃ y, láº¡i giáº£m hiá»‡u quáº£ á»Ÿ chá»— khÃ¡c.
- VÃ­ dá»¥:
  - RAPTOR tá»‘t khi tÃ³m táº¯t context â†’ nhÆ°ng táº¡o nhiá»…u cho QA vÃ¬ LLM summary cÃ³ thá»ƒ bá» sÃ³t chi tiáº¿t.
  - GraphRAG dÃ¹ng graph máº¡nh â†’ nhÆ°ng khÃ´ng tÆ°Æ¡ng tÃ¡c tá»‘t vá»›i truy váº¥n, vÃ¬ query khÃ´ng liÃªn káº¿t trá»±c tiáº¿p vá»›i edge trong KG.
  - LightRAG thÃ¬ retrieval tá»‘t nhÆ°ng khÃ´ng reasoning Ä‘Æ°á»£c.
ğŸ“‰ Trong cÃ¡c benchmark tá»•ng há»£p (báº£ng 2 bÃ i bÃ¡o), táº¥t cáº£ cÃ¡c RAG cÃ³ cáº¥u trÃºc Ä‘á»u tá»¥t Ä‘iá»ƒm rÃµ rá»‡t á»Ÿ má»™t trong 3 loáº¡i nhiá»‡m vá»¥.

---
âš”ï¸ Pháº£n biá»‡n 3:
Há»i: Váº­y váº¥n Ä‘á» sÃ¢u hÆ¡n náº±m á»Ÿ Ä‘Ã¢u? VÃ¬ sao cÃ¡c mÃ´ hÃ¬nh cÅ© khÃ´ng Ä‘áº¡t Ä‘Æ°á»£c bá»™ nhá»› dÃ i háº¡n giá»‘ng con ngÆ°á»i?
Tráº£ lá»i: VÃ¬ chÃºng thiáº¿u 3 thá»© mÃ  nÃ£o ngÆ°á»i lÃ m ráº¥t tá»‘t:
1. LiÃªn káº¿t tri thá»©c dáº¡ng Ä‘á»“ thá»‹ â€“ nÃ£o ngÆ°á»i lÆ°u thÃ´ng tin theo máº¡ng liÃªn káº¿t.
2. Nháº­n diá»‡n ngá»¯ cáº£nh (recognition memory) â€“ biáº¿t thÃ´ng tin nÃ o liÃªn quan trong lÃºc truy xuáº¥t.
3. Káº¿t há»£p giá»¯a khÃ¡i niá»‡m (concept) vÃ  ngá»¯ cáº£nh (context) â€“ chá»© khÃ´ng lÆ°u rá»i ráº¡c.
CÃ¡c mÃ´ hÃ¬nh trÆ°á»›c:
- Táº­p trung vÃ o embedding hoáº·c graph, nhÆ°ng khÃ´ng káº¿t ná»‘i chÃºng láº¡i.
- KhÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c cÃ¡c query dÃ i, phá»©c táº¡p, hoáº·c cáº§n suy luáº­n nhiá»u bÆ°á»›c.
- KhÃ´ng cÃ³ cÆ¡ cháº¿ kiá»ƒm tra â€œliá»‡u Ä‘oáº¡n nÃ y cÃ³ thá»±c sá»± Ä‘Ãºng ngá»¯ cáº£nh truy váº¥n khÃ´ng?â€ (gá»i lÃ  recognition filter trong HippoRAG).

---
ğŸ§¬ PHáº¦N 2: HIPPORAG GIáº¢I QUYáº¾T THáº¾ NÃ€O?

---
âš”ï¸ Pháº£n biá»‡n 4:
Há»i: Nghe nhÆ° ai cÅ©ng cÃ³ knowledge graph rá»“i, HippoRAG cÃ³ gÃ¬ khÃ¡c?
Tráº£ lá»i: HippoRAG khÃ´ng chá»‰ cÃ³ KG, mÃ  lÃ  má»™t há»‡ thá»‘ng mÃ´ phá»ng trÃ­ nhá»› ngÆ°á»i:
- Neocortex â†’ LLM: dÃ¹ng Ä‘á»ƒ suy luáº­n, Ä‘á»c hiá»ƒu, quyáº¿t Ä‘á»‹nh.
- Hippocampus â†’ KG má»Ÿ: mÃ´ hÃ¬nh hÃ³a káº¿t ná»‘i tri thá»©c.
- Parahippocampal region â†’ embedding encoder: dÃ¹ng Ä‘á»ƒ so khá»›p vÃ  gá»£i nhá»›.
=> HippoRAG dÃ¹ng PageRank cÃ¡ nhÃ¢n hÃ³a trÃªn KG Ä‘á»ƒ tÃ¬m ra nhá»¯ng Ä‘oáº¡n liÃªn káº¿t tá»‘t nháº¥t, sau Ä‘Ã³ káº¿t há»£p láº¡i Ä‘á»ƒ feed vÃ o LLM Ä‘á»c hiá»ƒu.

---
âš”ï¸ Pháº£n biá»‡n 5:
Há»i: Váº­y so vá»›i cÃ¡c mÃ´ hÃ¬nh trÆ°á»›c, HippoRAG cÃ³ gÃ¬ vÆ°á»£t trá»™i?
Tráº£ lá»i:
 ğŸ‘‰ HippoRAG 1 Ä‘Ã£ hÆ¡n cÃ¡c mÃ´ hÃ¬nh cÅ© nhá»:
- DÃ¹ng Personalized PageRank trÃªn KG Ä‘á»ƒ lÃ m multi-hop retrieval.
- TÃ¡ch riÃªng index (offline) vÃ  truy xuáº¥t (online) â†’ tiá»‡n lá»£i cho continual learning.
NhÆ°ng nÃ³ váº«n chÆ°a Ä‘á»§, vÃ¬:
- Dá»±a quÃ¡ nhiá»u vÃ o entity (NER), nÃªn bá» lá»¡ ngá»¯ cáº£nh.
- KhÃ´ng lá»c triple nÃ o liÃªn quan tháº­t sá»± Ä‘áº¿n query.
ğŸ‘‰ HippoRAG 2 Ä‘Ã£ kháº¯c phá»¥c thÃªm:
- Query-to-Triple linking thay vÃ¬ NER â†’ káº¿t ná»‘i tá»‘t hÆ¡n.
- Recognition memory báº±ng LLM â†’ lá»c triple "rÃ¡c".
- Dense-sparse integration â†’ káº¿t há»£p phrase node (khÃ¡i niá»‡m) vÃ  passage node (ngá»¯ cáº£nh).
Káº¿t quáº£:
+7 Ä‘iá»ƒm F1 á»Ÿ associative tasks, khÃ´ng tá»¥t performance á»Ÿ factual hay discourse.

---
ğŸ” PHáº¦N 3: Tá»”NG Káº¾T â€“ HIPPORAG CÃ“ GÃŒ KHÃC VÃ€ HAY?
This content is only supported in a Lark Docs


---
[Image]
[Image]
Ráº¥t tá»‘t, Quá»‘c Ä‘ang Ä‘i Ä‘Ãºng hÆ°á»›ng cá»§a â€œnghiÃªn cá»©u nghiÃªm tÃºcâ€ â€“ khÃ´ng chá»‰ ghi nháº­n káº¿t quáº£, mÃ  pháº£i giáº£i thÃ­ch sÃ¢u sáº¯c vÃ¬ sao láº¡i ra káº¿t quáº£ Ä‘Ã³, báº±ng logic, ká»¹ thuáº­t vÃ  pháº£n biá»‡n.
MÃ¬nh sáº½ tiáº¿p tá»¥c phÃ¢n tÃ­ch theo phong cÃ¡ch:
ğŸ­ Má»™t ngÆ°á»i phÃ¡ (há»i xoÃ¡y) â€“ Má»™t ngÆ°á»i tráº£ lá»i (tráº£ Ä‘Ã²n)
 cho tá»«ng mÃ´ hÃ¬nh, dá»±a trÃªn pipeline ná»™i táº¡i vÃ  dá»¯ liá»‡u thá»±c nghiá»‡m tá»« Table 2 (QA) + Table 3 (Retrieval).

---
ğŸ§  1. RAG Gá»C
Retrieval báº±ng embedding vector, khÃ´ng cáº¥u trÃºc.
 QA Ä‘Æ¡n thuáº§n dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¢u há»i vÃ  Ä‘oáº¡n vÄƒn.

---
â“PhÃ¡: Táº¡i sao RAG gá»‘c láº¡i tá»‘t á»Ÿ factual QA (NQ, PopQA)?
âœ… Tráº£ lá»i: VÃ¬:
- CÃ¢u há»i dáº¡ng factual thÆ°á»ng chá»‰ cáº§n 1 Ä‘oáº¡n ngáº¯n chá»©a Ä‘Ãºng entity â†’ dá»… tÃ¬m báº±ng embedding similarity.
- CÃ¡c embedding hiá»‡n Ä‘áº¡i (NV-Embed-v2, GritLM) ráº¥t máº¡nh á»Ÿ semantic retrieval.
ğŸ“Œ VÃ­ dá»¥:
CÃ¢u há»i: â€œWhen was Albert Einstein born?â€ â†’ chá»‰ cáº§n Ä‘oáº¡n cÃ³ "Albert Einstein was born in 1879..." â†’ lÃ  Ä‘á»§ â†’ vector retrieval lÃ m tá»‘t.

---
â“PhÃ¡: Táº¡i sao láº¡i ráº¥t yáº¿u á»Ÿ multi-hop vÃ  sense-making?
âœ… Tráº£ lá»i: VÃ¬:
- RAG gá»‘c khÃ´ng liÃªn káº¿t thÃ´ng tin giá»¯a nhiá»u Ä‘oáº¡n â†’ khÃ´ng thá»ƒ reasoning nhiá»u bÆ°á»›c.
- KhÃ´ng cÃ³ cÆ¡ cháº¿ hiá»ƒu toÃ n truyá»‡n, ngá»¯ cáº£nh rá»™ng â†’ tháº¥t báº¡i á»Ÿ NarrativeQA.
ğŸ“Œ VÃ­ dá»¥:
CÃ¢u há»i: â€œWhat county is Erik Hortâ€™s birthplace a part of?â€ â†’ cáº§n biáº¿t: Erik Hort â†’ Montebello â†’ thuá»™c háº¡t gÃ¬?
 â†’ Vector retrieval chá»‰ tÃ¬m Ä‘Æ°á»£c Ä‘oáº¡n Ä‘áº§u tiÃªn, khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c sang Ä‘á»‹a lÃ½.

---
ğŸ§  2. RAPTOR
RAG vá»›i tÃ³m táº¯t (summarization) tá»«ng cá»¥m tÃ i liá»‡u. Dá»±a vÃ o Gaussian clustering Ä‘á»ƒ nhÃ³m rá»“i summary.

---
â“PhÃ¡: TÃ³m táº¯t thÃ¬ pháº£i giÃºp tÃ¬m nhanh hÆ¡n chá»©? Sao láº¡i tá»¥t Ä‘iá»ƒm á»Ÿ factual QA?
âœ… Tráº£ lá»i: VÃ¬:
- LLM tÃ³m táº¯t thÆ°á»ng bá» qua cÃ¡c chi tiáº¿t nhá» nhÆ° sá»‘ liá»‡u, Ä‘á»‹a Ä‘iá»ƒm, tÃªn ngÆ°á»i â†’ trong khi Ä‘Ã³ láº¡i lÃ  máº¥u chá»‘t cá»§a factual QA.
- Sau tÃ³m táº¯t, Ä‘oáº¡n gá»‘c khÃ´ng cÃ²n â†’ truy há»“i máº¥t chÃ­nh xÃ¡c.
ğŸ“Œ TÃ¡c giáº£ nÃ³i rÃµ:
"LLM summarization mechanism... introduces noise into the retrieval corpora."

---
â“PhÃ¡: Váº­y táº¡i sao váº«n tá»‘t á»Ÿ sense-making?
âœ… Tráº£ lá»i:
- VÃ¬ trong discourse dÃ i (nhÆ° NarrativeQA), Ä‘á»c tá»«ng Ä‘oáº¡n khÃ´ng Ä‘á»§ hiá»ƒu máº¡ch truyá»‡n.
- Viá»‡c gom láº¡i rá»“i tÃ³m táº¯t giÃºp LLM tháº¥y Ä‘Æ°á»£c tá»•ng thá»ƒ â†’ hiá»ƒu cÃ¢u chuyá»‡n nhÆ° ngÆ°á»i Ä‘á»c má»™t láº§n.

---
ğŸ§  3. GraphRAG
Táº¡o graph cá»™ng Ä‘á»“ng (community graph), giÃºp hiá»ƒu nhÃ³m thá»±c thá»ƒ hoáº·c Ä‘oáº¡n vÄƒn liÃªn quan.

---
â“PhÃ¡: Sao táº¡o Ä‘Æ°á»£c graph mÃ  váº«n â€œtá»¥t factualâ€?
âœ… Tráº£ lá»i:
- VÃ¬ GraphRAG khÃ´ng giá»¯ Ä‘oáº¡n vÄƒn gá»‘c, chá»‰ dÃ¹ng node graph Ä‘Æ°á»£c tá»•ng há»£p tá»« LLM.
- NÃªn chi tiáº¿t cá»¥ thá»ƒ dá»… bá»‹ máº¥t khi QA yÃªu cáº§u entity hoáº·c sá»‘ liá»‡u chÃ­nh xÃ¡c.

---
â“PhÃ¡: Váº­y nÃ³ giÃºp Ä‘Æ°á»£c gÃ¬?
âœ… Tráº£ lá»i:
- DÃ¹ng graph structure Ä‘á»ƒ khÃ´i phá»¥c cÃ¡c má»‘i liÃªn há»‡ giá»¯a thá»±c thá»ƒ â€“ sá»± kiá»‡n â€“ Ä‘á»‹a Ä‘iá»ƒm â†’ há»— trá»£ multi-hop vÃ  sense-making.
- Viá»‡c phÃ¡t hiá»‡n cá»™ng Ä‘á»“ng giÃºp nhÃ³m thÃ´ng tin tá»« cÃ¡c nguá»“n khÃ¡c nhau â†’ tÄƒng associativity.
ğŸ“Œ Káº¿t quáº£: NarrativeQA Ä‘áº¡t 23.0 (chá»‰ sau HippoRAG 2)
 â†’ nghÄ©a lÃ  nÃ³ hiá»ƒu truyá»‡n tá»‘t, nhÆ°ng khÃ´ng nhá»› chi tiáº¿t.

---
ğŸ§  4. HippoRAG 1
Dá»±a vÃ o OpenIE Ä‘á»ƒ extract triple â†’ dÃ¹ng Personalized PageRank (PPR) Ä‘á»ƒ reasoning.

---
â“PhÃ¡: Váº­y táº¡i sao HippoRAG 1 ráº¥t tá»‘t á»Ÿ associative QA?
âœ… Tráº£ lá»i:
- PPR + KG cho phÃ©p Ä‘i qua nhiá»u hop trong graph â†’ giá»‘ng nhÆ° nhá»› theo â€œÄ‘Æ°á»ng liÃªn tÆ°á»Ÿngâ€.
- Dáº¡ng QA cáº§n liÃªn káº¿t 2-3 Ä‘oáº¡n lÃ  Ä‘iá»ƒm máº¡nh cá»§a PPR.
ğŸ“Œ 2Wiki Ä‘áº¡t 71.8, cao nháº¥t trong báº£ng â†’ chá»©ng minh multi-hop cá»±c tá»‘t.

---
â“PhÃ¡: NhÆ°ng vÃ¬ sao láº¡i chÆ°a tá»‘t á»Ÿ sense-making?
âœ… Tráº£ lá»i:
- Do HippoRAG 1 chá»‰ dá»±a trÃªn entity-centric extraction (NER) â†’ thiáº¿u thÃ´ng tin ngá»¯ cáº£nh (context).
- Query khÃ´ng Ä‘Æ°á»£c embed toÃ n bá»™ Ä‘á»ƒ match triple, chá»‰ match theo entity â†’ contextualization ráº¥t yáº¿u.
ğŸ“Œ TÃ¡c giáº£ gá»i Ä‘Ã¢y lÃ  â€œentity-centric biasâ€.

---
ğŸ§  5. HippoRAG 2
Báº£n nÃ¢ng cáº¥p toÃ n diá»‡n:
- KG gá»“m cáº£ phrase node & passage node (dense-sparse integration)
- Query-to-triple matching (khÃ´ng NER)
- Triple filtering báº±ng LLM
- PPR search dÃ¹ng cáº£ passage & phrase node

---
â“PhÃ¡: Táº¡i sao mÃ´ hÃ¬nh nÃ y â€œtá»‘t Ä‘á»u cáº£ 3â€ â€” cÃ³ gÃ¬ Ä‘áº·c biá»‡t?
âœ… Tráº£ lá»i:
1. Factual QA:
- Giá»¯ nguyÃªn Ä‘oáº¡n vÄƒn gá»‘c + phrase-level KG â†’ khÃ´ng máº¥t chi tiáº¿t.
- DÃ¹ng embedding máº¡nh + triple filter â†’ giá»¯ thÃ´ng tin chuáº©n.
1. Associative QA:
- PPR graph search sÃ¢u, seed node tá»« cáº£ phrase & passage â†’ cá»±c ká»³ phÃ¹ há»£p vá»›i multi-hop reasoning.
- Filter giÃºp loáº¡i nhiá»…u â†’ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c.
2. Sense-making:
- Dense-sparse integration giÃºp giá»¯ Ä‘Æ°á»£c cáº£ khÃ¡i niá»‡m láº«n ngá»¯ cáº£nh.
- Query khÃ´ng cÃ²n bá»‹ chia nhá» nhÆ° NER â†’ cáº£ cÃ¢u Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ hiá»ƒu context â†’ cáº£i thiá»‡n máº¡nh NarrativeQA.
ğŸ“Œ Báº±ng chá»©ng:
- HippoRAG 2 Ä‘áº¡t cao nháº¥t á»Ÿ F1 NarrativeQA = 25.9
- Recall@5 = 96.3 á»Ÿ HotpotQA â†’ quÃ¡ vÆ°á»£t trá»™i!

---
ğŸ“Œ Tá»•ng Káº¿t Giáº£i ThÃ­ch (cÃ³ thá»ƒ Ä‘Æ°a vÃ o bÃ¡o cÃ¡o/luáº­n vÄƒn):
This content is only supported in a Lark Docs

---
Giá» cáº§n hiá»ƒu cÃ¡c khÃ¡i niá»‡m chi tiáº¿t cá»§a Hippo 1 vÃ  Hippo2 -- Giáº£i tÃ­ch tá»«ng khÃ¡i niá»‡m vÃ  liÃªn há»‡ nÃ³ vá»›i thá»© tháº­t dá»… nhá»› Ä‘Æ¡n giáº£n cho há»c sinh cáº¥p 2

Ráº¥t hay Quá»‘c Æ¡i, giá» mÃ¬nh sáº½ giáº£i tÃ­ch cÃ¡c khÃ¡i niá»‡m cá»‘t lÃµi trong HippoRAG 1 vÃ  HippoRAG 2, liÃªn há»‡ vá»›i vÃ­ dá»¥ Ä‘á»i tháº­t tháº­t dá»… hiá»ƒu, kiá»ƒu giáº£i thÃ­ch cho há»c sinh cáº¥p 2.

---
ğŸ§  Tá»•ng thá»ƒ: HippoRAG lÃ  gÃ¬?
HippoRAG lÃ  mÃ´ hÃ¬nh trÃ­ nhá»› dÃ i háº¡n cho AI, láº¥y cáº£m há»©ng tá»« cÃ¡ch con ngÆ°á»i ghi nhá»› vÃ  suy luáº­n.
 NÃ³ chia lÃ m 3 bá»™ pháº­n giá»‘ng nhÆ° bá»™ nÃ£o ngÆ°á»i:
- Neocortex â†’ chÃ­nh lÃ  LLM (nÃ£o suy nghÄ©)
- Hippocampus â†’ nÆ¡i lÆ°u giá»¯ kÃ½ á»©c (graph)
- Parahippocampal Region â†’ káº¿t ná»‘i Ã½ tÆ°á»Ÿng (tÃ¬m Ä‘iá»ƒm giá»‘ng nhau)

---
ğŸ¦› HippoRAG 1 â€“ GIáº¢I TÃCH CÃC KHÃI NIá»†M CHÃNH

---
1. OpenIE â†’ TrÃ­ch xuáº¥t triple (subject â€“ predicate â€“ object)
ğŸ§© ÄÃ¢y lÃ  bÆ°á»›c láº¥y thÃ´ng tin tá»« vÄƒn báº£n vÃ  biáº¿n nÃ³ thÃ nh má»™t â€œkiáº¿n thá»©c nhá» cÃ³ dáº¡ng: A liÃªn quan Ä‘áº¿n B báº±ng cÃ¡ch Câ€.
ğŸ” VÃ­ dá»¥ vÄƒn báº£n:
â€œAlbert Einstein was born in Ulm in 1879.â€
â†’ sáº½ thÃ nh triple:
- (Albert Einstein, born in, Ulm)
- (Albert Einstein, born in year, 1879)
ğŸª„ LiÃªn há»‡ cho há»c sinh:
 Giá»‘ng nhÆ° khi báº¡n Ä‘á»c sÃ¡ch vÃ  tá»± ghi vá»Ÿ â€œtÃ³m táº¯t Ã½ chÃ­nhâ€ kiá»ƒu:
 â†’ â€œEinstein â†’ sinh á»Ÿ Ulmâ€, â€œEinstein â†’ sinh nÄƒm 1879â€.

---
2. Graph of Knowledge Triples (Biá»ƒu Ä‘á»“ kiáº¿n thá»©c)
ğŸ§  CÃ¡c triple trÃªn Ä‘Æ°á»£c ghÃ©p thÃ nh 1 báº£n Ä‘á»“ kiáº¿n thá»©c, giá»‘ng nhÆ° báº£n Ä‘á»“ tÆ° duy (mindmap).
ğŸ“Œ Má»—i vÃ²ng trÃ²n lÃ  1 tá»« khÃ³a, má»—i mÅ©i tÃªn lÃ  má»‘i liÃªn há»‡.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Báº¡n váº½ sÆ¡ Ä‘á»“ mindmap mÃ´n Sinh há»c:
- â€œLÃ¡ cÃ¢y â†’ cÃ³ chá»©c nÄƒng â†’ quang há»£pâ€
- â€œQuang há»£p â†’ táº¡o ra â†’ oxyâ€
 â†’ LÃ  báº¡n Ä‘ang váº½ KG Ä‘Ã³!

---
3. Personalized PageRank (PPR)
ğŸ”„ ÄÃ¢y lÃ  cÃ¡ch mÃ´ hÃ¬nh tÃ¬m Ä‘Æ°á»ng Ä‘i qua nhiá»u bÆ°á»›c trong báº£n Ä‘á»“ kiáº¿n thá»©c Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Giá»‘ng nhÆ° chÆ¡i trÃ² â€œÄ‘i tÃ¬m kho bÃ¡uâ€ trong mÃª cung. Báº¡n báº¯t Ä‘áº§u tá»« tá»« khÃ³a trong cÃ¢u há»i, Ä‘i theo cÃ¡c má»‘i liÃªn há»‡ Ä‘á»ƒ Ä‘áº¿n cÃ¢u tráº£ lá»i.
CÃ¢u há»i: â€œLÃ¡ cÃ¢y giÃºp gÃ¬ cho mÃ´i trÆ°á»ng?â€
 â†’ LÃ¡ cÃ¢y â†’ quang há»£p â†’ táº¡o oxy â†’ giÃºp mÃ´i trÆ°á»ng â†’ thá»Ÿ

---
4. Entity-Centric Query Linking (LiÃªn káº¿t dá»±a vÃ o tÃªn riÃªng)
HippoRAG 1 dÃ¹ng cÃ¡ch phÃ¡t hiá»‡n thá»±c thá»ƒ (entity) trong cÃ¢u há»i nhÆ° tÃªn ngÆ°á»i, nÆ¡i chá»‘n, Ä‘á»ƒ ná»‘i vÃ o graph.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 CÃ¢u há»i: â€œBill Gates há»c trÆ°á»ng nÃ o?â€
 â†’ PhÃ¡t hiá»‡n â€œBill Gatesâ€ lÃ  thá»±c thá»ƒ â†’ tÃ¬m trong báº£n Ä‘á»“ trÃ­ nhá»›.
Äiá»ƒm yáº¿u:
- KhÃ´ng hiá»ƒu cÃ¢u há»i rá»™ng (vÃ­ dá»¥: â€œNgÆ°á»i sÃ¡ng láº­p Microsoft há»c á»Ÿ Ä‘Ã¢u?â€)
 â†’ náº¿u khÃ´ng cÃ³ tá»« â€œBill Gatesâ€, thÃ¬ khÃ´ng tÃ¬m Ä‘Æ°á»£c â†’ máº¥t context

---
ğŸ§  HippoRAG 2 â€“ Cáº¢I TIáº¾N QUAN TRá»ŒNG + VÃ Dá»¤ Äá»œI THÆ¯á»œNG

---
1. Dense-Sparse Integration: GhÃ©p kiáº¿n thá»©c vÃ  ngá»¯ cáº£nh
HippoRAG 2 khÃ´ng chá»‰ lÆ°u "kiáº¿n thá»©c ngáº¯n gá»n" mÃ  cÃ²n lÆ°u cáº£ Ä‘oáº¡n vÄƒn ban Ä‘áº§u (ngá»¯ cáº£nh).
ğŸ“Œ Graph cÃ³ 2 loáº¡i node:
- Phrase Node (sparse): giá»‘ng tiÃªu Ä‘á»/tá»« khÃ³a
- Passage Node (dense): nguyÃªn Ä‘oáº¡n vÄƒn gá»‘c
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Khi há»c lá»‹ch sá»­, báº¡n vá»«a gáº¡ch Ä‘áº§u dÃ²ng â€œTráº­n Äiá»‡n BiÃªn Phá»§ tháº¯ng nÄƒm 1954â€, vá»«a giá»¯ láº¡i nguyÃªn Ä‘oáº¡n mÃ´ táº£ tráº­n Ä‘Ã¡nh â†’ váº­y báº¡n vá»«a nhá»› "Ã½ chÃ­nh" vá»«a cÃ³ â€œngá»¯ cáº£nhâ€.

---
2. Query-to-Triple Matching
KhÃ´ng chá»‰ tÃ¬m entity ná»¯a, mÃ  dÃ¹ng toÃ n cÃ¢u há»i Ä‘á»ƒ match vá»›i cÃ¡c triple.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 CÃ¢u há»i: â€œNgÆ°á»i sÃ¡ng láº­p Microsoft há»c á»Ÿ Ä‘Ã¢u?â€
 â†’ LÃºc nÃ y, dÃ¹ khÃ´ng cÃ³ tá»« â€œBill Gatesâ€, mÃ´ hÃ¬nh váº«n hiá»ƒu Ä‘Æ°á»£c vÃ  tÃ¬m Ä‘Æ°á»£c triple phÃ¹ há»£p.

---
3. Recognition Memory â€“ lá»c triple báº±ng LLM
Sau khi tÃ¬m ra vÃ i má»‘i liÃªn há»‡ phÃ¹ há»£p, HippoRAG 2 nhá» LLM kiá»ƒm tra láº¡i xem cÃ¡i nÃ o thá»±c sá»± liÃªn quan.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Báº¡n tÃ¬m 5 Ä‘oáº¡n cÃ³ váº» Ä‘Ãºng Ä‘á»ƒ lÃ m bÃ i táº­p. Sau Ä‘Ã³ báº¡n Ä‘á»c ká»¹ tá»«ng Ä‘oáº¡n, loáº¡i bá» cÃ¡i khÃ´ng liÃªn quan â†’ giá»¯ láº¡i 2 Ä‘oáº¡n tá»‘t nháº¥t â†’ Ä‘Ã³ lÃ  â€œrecognition memoryâ€.

---
4. Contextualized PPR Search
Khi Ä‘Ã£ cÃ³ cáº£ tá»« khÃ³a vÃ  Ä‘oáº¡n vÄƒn, mÃ´ hÃ¬nh dÃ¹ng PageRank Ä‘á»ƒ tÃ¬m con Ä‘Æ°á»ng tá»‘t nháº¥t qua báº£n Ä‘á»“ trÃ­ nhá»› cÃ³ cáº£ kiáº¿n thá»©c vÃ  ngá»¯ cáº£nh.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Báº¡n ná»‘i â€œtá»« khÃ³aâ€ vá»›i cÃ¡c Ä‘oáº¡n giáº£i thÃ­ch trong sÃ¡ch, rá»“i chá»n ra Ä‘oáº¡n nÃ o giÃºp báº¡n hiá»ƒu toÃ n cáº£nh nháº¥t Ä‘á»ƒ lÃ m bÃ i.

---
ğŸ“ Tá»•ng káº¿t: HippoRAG 1 vs 2 â€“ Dá»… hiá»ƒu cho há»c sinh cáº¥p 2
This content is only supported in a Lark Docs

---
Tuyá»‡t vá»i Quá»‘c Æ¡i, giá» mÃ¬nh sáº½ giáº£i tÃ­ch sÃ¢u tá»«ng khÃ¡i niá»‡m trong sÆ¡ Ä‘á»“ pipeline cá»§a HippoRAG 2, nhÆ° Quá»‘c yÃªu cáº§u:
- Hiá»ƒu rÃµ cÆ¡ cháº¿
- Pháº£n biá»‡n kiá»ƒu phÃ¡ â€“ Ä‘Ã¡p tá»«ng bÆ°á»›c
- LiÃªn há»‡ vÃ­ dá»¥ khi cáº§n
SÆ¡ Ä‘á»“ Ä‘Æ°á»£c chia lÃ m 2 pháº§n:

---
ğŸ¯ PHáº¦N 1: OFFLINE INDEXING
Má»¥c tiÃªu: xÃ¢y dá»±ng má»™t bá»™ nhá»› kiáº¿n thá»©c giÃ u liÃªn káº¿t tá»« vÄƒn báº£n Ä‘áº§u vÃ o.

---
ğŸ”µ BÆ°á»›c 1: OpenIE by LLM â†’ TrÃ­ch xuáº¥t Triple
TÃ¡ch Ä‘oáº¡n vÄƒn thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ kiáº¿n thá»©c dáº¡ng (subject, predicate, object)

---
â“PhÃ¡: Táº¡i sao dÃ¹ng OpenIE báº±ng LLM? TrÃ­ch xuáº¥t truyá»n thá»‘ng cÃ³ váº¥n Ä‘á» gÃ¬?
âœ… ÄÃ¡p:
- OpenIE truyá»n thá»‘ng (Stanford, ClausIE...) chá»‰ lÃ m Ä‘Æ°á»£c trÃªn cÃ¢u Ä‘Æ¡n, thiáº¿u kháº£ nÄƒng xá»­ lÃ½ ngÃ´n ngá»¯ phá»©c táº¡p.
- LLM cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c má»‡nh Ä‘á» dÃ i, ngá»¯ nghÄ©a má»Ÿ rá»™ng, phá»©c táº¡p hÆ¡n â†’ táº¡o triple â€œgiÃ u nghÄ©aâ€ hÆ¡n.
ğŸ“Œ VÃ­ dá»¥
 VÄƒn báº£n: "Marie Curie won two Nobel Prizes in different fields."
 â†’ Triple truyá»n thá»‘ng: ("Marie Curie", "won", "two Nobel Prizes")
 â†’ LLM cÃ³ thá»ƒ táº¡o thÃªm triple: ("Marie Curie", "won Nobel in", "Physics") + ("Marie Curie", "won Nobel in", "Chemistry")

---
ğŸ”µ BÆ°á»›c 2: Synonym Detection â†’ PhÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a
Ná»‘i cÃ¡c node gáº§n nghÄ©a trong KG báº±ng Synonym Edge (Ä‘Æ°á»ng Ä‘á»©t nÃ©t trong sÆ¡ Ä‘á»“)

---
â“PhÃ¡: PhÃ¡t hiá»‡n synonym kiá»ƒu gÃ¬? CÃ³ dÃ¹ng LLM khÃ´ng?
âœ… ÄÃ¡p: KhÃ´ng! DÃ¹ng cosine similarity trong embedding space
- CÃ¡c phrase (tá»«/cá»¥m tá»«) Ä‘Æ°á»£c nhÃºng (embedding)
- Náº¿u cosine similarity > ngÆ°á»¡ng â†’ Ä‘Æ°á»£c ná»‘i báº±ng "synonym edge"
ğŸ“Œ TÃ¡c giáº£ khÃ´ng dÃ¹ng LLM vÃ¬:
- LLM cháº­m hÆ¡n ráº¥t nhiá»u
- Embedding-based synonym cÃ³ thá»ƒ cáº­p nháº­t liÃªn tá»¥c, Ä‘Æ¡n giáº£n hÆ¡n

---
â“PhÃ¡: CÃ³ rá»§i ro gÃ¬ khÃ´ng khi dÃ¹ng embedding Ä‘á»ƒ ná»‘i synonym?
âœ… ÄÃ¡p:
- CÃ³! CÃ³ thá»ƒ ná»‘i nháº§m cÃ¡c tá»« gáº§n nghÄ©a nhÆ°ng khÃ¡c ngá»¯ cáº£nh ("doctor" vÃ  "professor" cháº³ng háº¡n).
- NhÆ°ng vÃ¬ Ä‘Ã¢y lÃ  lá»›p bá»• trá»£ (synonym edge), khÃ´ng áº£nh hÆ°á»Ÿng trá»±c tiáº¿p náº¿u downstream filter lÃ m tá»‘t.

---
ğŸ”µ BÆ°á»›c 3: Denseâ€“Sparse Integration
TÃ­ch há»£p cáº£ node "phrase" vÃ  node "passage" trong KG â†’ hybrid graph

---
â“PhÃ¡: Táº¡i sao cáº§n vá»«a sparse (phrase) vá»«a dense (passage)? KhÃ´ng giá»¯ má»™t loáº¡i cho gá»n?
âœ… ÄÃ¡p:
- Sparse (phrase): Dá»… truy há»“i, biá»ƒu diá»…n khÃ¡i niá»‡m â†’ tá»‘t cho reasoning
- Dense (passage): Giá»¯ thÃ´ng tin chi tiáº¿t â†’ tá»‘t cho factual QA
 â†’ GhÃ©p cáº£ hai táº¡o ra má»™t KG vá»«a khÃ¡i quÃ¡t vá»«a giÃ u ngá»¯ cáº£nh.
ğŸª„ LiÃªn há»‡ dá»… hiá»ƒu: Giá»‘ng há»c Sinh há»c báº±ng sÆ¡ Ä‘á»“ + sÃ¡ch giÃ¡o khoa:
- SÆ¡ Ä‘á»“ (phrase node) giÃºp nhá»› nhanh
- SGK (passage node) giÃºp hiá»ƒu sÃ¢u â†’ cáº§n cáº£ hai Ä‘á»ƒ thi Ä‘iá»ƒm cao

---
âœ… Tá»•ng káº¿t PHáº¦N 1: Graph cÃ³ cÃ¡c loáº¡i node vÃ  edge sau:
This content is only supported in a Lark Docs

---
ğŸ” PHáº¦N 2: ONLINE RETRIEVAL & QA
Má»¥c tiÃªu: Khi cÃ³ query â†’ tÃ¬m Ä‘Æ°á»ng trong graph â†’ tráº£ lá»i.

---
ğŸŸ¢ BÆ°á»›c 1: Retrieving Passages and Triples

---
â“PhÃ¡: Sao pháº£i láº¥y cáº£ Ä‘oáº¡n vÄƒn láº«n triple? KhÃ´ng chá»n 1 cÃ¡i cho nhanh?
âœ… ÄÃ¡p:
- Triple Ä‘á»ƒ lÃ m reasoning, káº¿t ná»‘i máº¡ch kiáº¿n thá»©c
- Passage Ä‘á»ƒ cÃ³ thÃ´ng tin chi tiáº¿t, lÃ m QA cuá»‘i cÃ¹ng
â†’ Hai lá»›p retrieval bá»• trá»£ nhau, khÃ´ng thá»ƒ thiáº¿u cÃ¡i nÃ o.

---
ğŸŸ¢ BÆ°á»›c 2: Triple Filtering = Recognition Memory
Sau khi láº¥y triple vá», ta lá»c xem cÃ¡i nÃ o thá»±c sá»± liÃªn quan Ä‘áº¿n query

---
â“PhÃ¡: Táº¡i sao cáº§n bÆ°á»›c nÃ y? Triple Ä‘Ã£ chá»n rá»“i mÃ ?
âœ… ÄÃ¡p: VÃ¬:
- Retrieval ban Ä‘áº§u báº±ng embedding â†’ cÃ³ thá»ƒ nhiá»…u
- Recognition lÃ  bÆ°á»›c giá»‘ng bá»™ nhá»› con ngÆ°á»i:
 â†’ â€œmÃ¬nh tá»«ng tháº¥y cÃ¡i nÃ y rá»“iâ€¦ nhÆ°ng cÃ³ Ä‘Ãºng cÃ¡i mÃ¬nh cáº§n khÃ´ng?â€
ğŸ“Œ DÃ¹ng LLM Ä‘á»ƒ so sÃ¡nh láº¡i tá»«ng triple vá»›i query â†’ giá»¯ láº¡i nhá»¯ng cÃ¡i tá»‘t nháº¥t
ğŸª„ LiÃªn há»‡ há»c sinh: Giá»‘ng Ä‘á»c láº¡i 5 Ä‘oáº¡n báº¡n highlight khi há»c â†’ loáº¡i 3 cÃ¡i khÃ´ng liÃªn quan, giá»¯ láº¡i 2 Ä‘oáº¡n tháº­t sá»± Ä‘Ãºng Ä‘á».

---
ğŸ”´ BÆ°á»›c 3: Assigning Seed Node Weights
GÃ¡n Ä‘iá»ƒm cho cÃ¡c node â€œgá»‘câ€ trong KG â†’ chuáº©n bá»‹ cho bÆ°á»›c lan truyá»n PageRank

---
â“PhÃ¡: Seed Node lÃ  gÃ¬? Táº¡i sao pháº£i gÃ¡n trá»ng sá»‘?
âœ… ÄÃ¡p:
- Seed Node lÃ  node trong graph cÃ³ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n query
- Trá»ng sá»‘ cao thÃ¬ node Ä‘Ã³ lan truyá»n áº£nh hÆ°á»Ÿng máº¡nh hÆ¡n trong PPR
ğŸ“Œ DÃ¹ng cáº£:
- Passage tá»« query retrieval
- Phrase tá»« triple lá»c
 â†’ táº¥t cáº£ Ä‘á»u trá»Ÿ thÃ nh seed node

---
ğŸ”„ BÆ°á»›c 4: PPR Graph Search
Personalized PageRank tÃ¬m Ä‘Æ°á»ng trong KG tá»« cÃ¡c seed node

---
â“PhÃ¡: KhÃ¡c gÃ¬ so vá»›i search bÃ¬nh thÆ°á»ng?
âœ… ÄÃ¡p:
- PageRank lan tá»a áº£nh hÆ°á»Ÿng â†’ khÃ´ng tÃ¬m Ä‘Æ°á»ng ngáº¯n nháº¥t, mÃ  tÃ¬m Ä‘oáº¡n cÃ³ liÃªn quan nháº¥t trong toÃ n bá»™ cáº¥u trÃºc
- Personalized â†’ Æ°u tiÃªn cÃ¡c vÃ¹ng gáº§n vá»›i seed node
ğŸª„ LiÃªn há»‡ há»c sinh: Giá»‘ng há»c theo sÆ¡ Ä‘á»“ tÆ° duy, báº¡n Æ°u tiÃªn Ä‘á»c pháº§n gáº§n chá»§ Ä‘á» chÃ­nh â†’ khÃ´ng láº­t sÃ¡ch lung tung

---
âœ… BÆ°á»›c 5: QA Reading with Selected Passages
LLM Ä‘á»c cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Ã£ chá»n â†’ sinh ra cÃ¢u tráº£ lá»i.

---
â“PhÃ¡: Náº¿u chá»‰ dÃ¹ng Ä‘oáº¡n vÄƒn mÃ  khÃ´ng graph thÃ¬ sao?
âœ… ÄÃ¡p:
 â†’ Giá»‘ng RAG truyá»n thá»‘ng â†’ bá»‹ lá»‡ch, thiáº¿u reasoning
 â†’ Nhá» bÆ°á»›c graph search, mÃ´ hÃ¬nh nháº­n Ä‘Æ°á»£c Ä‘oáº¡n vÄƒn â€œgiÃ u liÃªn káº¿t nháº¥tâ€ â†’ tráº£ lá»i máº¡ch láº¡c hÆ¡n, chÃ­nh xÃ¡c hÆ¡n.

---
ğŸ”š Tá»•ng Káº¿t: Pipeline HippoRAG 2 â€“ Hiá»ƒu sÃ¢u + vÃ­ dá»¥ hÃ³a
This content is only supported in a Lark Docs

---
DÆ°á»›i Ä‘Ã¢y lÃ  pháº§n giáº£i thÃ­ch sÃ¢u tá»«ng bÆ°á»›c trong pipeline cá»§a HippoRAG 2, kÃ¨m theo cÃ¡c cÃ¢u há»i â€œphÃ¡â€ â€“ â€œÄ‘Ã¡pâ€ Ä‘á»ƒ Ä‘Ã o sÃ¢u hiá»ƒu biáº¿t, giá»‘ng nhÆ° Ä‘á»‘i thoáº¡i giá»¯a ngÆ°á»i Ä‘áº·t cÃ¢u há»i sáº¯c sáº£o vÃ  ngÆ°á»i tráº£ lá»i cháº·t cháº½.

---
1. OpenIE by LLM â€“ TrÃ­ch xuáº¥t Triple tá»« vÄƒn báº£n
Má»¥c tiÃªu:
 Chuyá»ƒn Ä‘á»•i Ä‘oáº¡n vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ kiáº¿n thá»©c (triple: subject â€“ predicate â€“ object) Ä‘á»ƒ lÆ°u trá»¯ trong bá»™ nhá»› dÆ°á»›i dáº¡ng cÃ¡c má»‘i liÃªn há»‡ Ä‘Æ¡n giáº£n.

---
â“ PhÃ¡ 1:
Táº¡i sao chÃºng ta cáº§n trÃ­ch xuáº¥t triple thay vÃ¬ chá»‰ lÆ°u toÃ n bá»™ Ä‘oáº¡n vÄƒn?
âœ… ÄÃ¡p:
- Viá»‡c trÃ­ch xuáº¥t triple giÃºp tÃ¡ch thÃ´ng tin quan trá»ng ra khá»i má»› vÄƒn báº£n dÃ i.
- NÃ³ cho phÃ©p lÆ°u trá»¯ dÆ°á»›i dáº¡ng cÃ¡c má»‘i quan há»‡ rÃµ rÃ ng, giá»‘ng nhÆ° ghi chÃº ngáº¯n gá»n, dá»… tÃ¬m láº¡i vÃ  káº¿t ná»‘i vá»›i cÃ¡c kiáº¿n thá»©c khÃ¡c.
VÃ­ dá»¥ dá»… hiá»ƒu:
 Giá»‘ng nhÆ° khi há»c, báº¡n khÃ´ng cáº§n lÆ°u toÃ n bá»™ bÃ i giáº£ng mÃ  chá»‰ cáº§n ghi láº¡i cÃ¡c Ã½ chÃ­nh: â€œEinstein â€“ sinh â€“ Ulmâ€ thay vÃ¬ ghi toÃ n bá»™ Ä‘oáº¡n vÄƒn dÃ i.

---
â“ PhÃ¡ 2:
Táº¡i sao dÃ¹ng LLM Ä‘á»ƒ thá»±c hiá»‡n OpenIE, thay vÃ¬ dÃ¹ng cÃ¡c cÃ´ng cá»¥ truyá»n thá»‘ng?
âœ… ÄÃ¡p:
- CÃ¡c cÃ´ng cá»¥ OpenIE truyá»n thá»‘ng thÆ°á»ng giá»›i háº¡n á»Ÿ cÃ¢u Ä‘Æ¡n giáº£n, khÃ´ng hiá»ƒu Ä‘Æ°á»£c nhá»¯ng cÃ¢u phá»©c táº¡p, mÃ¢u thuáº«n hay áº©n dá»¥.
- LLM cÃ³ kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh, xá»­ lÃ½ cÃ¢u dÃ i vÃ  phá»©c táº¡p, tá»« Ä‘Ã³ trÃ­ch xuáº¥t triple chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§ hÆ¡n.
VÃ­ dá»¥ há»c sinh cáº¥p 2:
 Náº¿u Ä‘á»c má»™t Ä‘oáº¡n vÄƒn mÃ´ táº£ "Marie Curie Ä‘Ã£ khÃ¡m phÃ¡ ra cÃ¡c nguyÃªn tá»‘ má»›i vÃ  giÃ nh giáº£i Nobel trong nhiá»u lÄ©nh vá»±c", LLM cÃ³ thá»ƒ táº¡o ra nhiá»u triple nhÆ°:
 â†’ (Marie Curie, khÃ¡m phÃ¡, nguyÃªn tá»‘ má»›i)
 â†’ (Marie Curie, giÃ nh, giáº£i Nobel)
 NhÆ° váº­y, báº¡n cÃ³ thá»ƒ nhá»› nhiá»u thÃ´ng tin hÆ¡n chá»‰ báº±ng má»™t cÃ¢u tÃ³m táº¯t.

---
2. Synonym Detection â€“ PhÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a
Má»¥c tiÃªu:
 Káº¿t ná»‘i cÃ¡c node (khÃ¡i niá»‡m, cá»¥m tá»«) cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»± trong KG, giÃºp tÄƒng kháº£ nÄƒng liÃªn káº¿t giá»¯a cÃ¡c thÃ´ng tin.

---
â“ PhÃ¡ 1:
Synonym Detection dÃ¹ng thuáº­t toÃ¡n nÃ o Ä‘á»ƒ phÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a?
âœ… ÄÃ¡p:
- ThÃ´ng thÆ°á»ng, ta sá»­ dá»¥ng cosine similarity trong khÃ´ng gian embedding.
- Má»—i tá»« hay cá»¥m tá»« Ä‘Æ°á»£c chuyá»ƒn thÃ nh má»™t vector, sau Ä‘Ã³ tÃ­nh cosine similarity giá»¯a cÃ¡c vector. Náº¿u giÃ¡ trá»‹ similarity vÆ°á»£t qua má»™t ngÆ°á»¡ng Ä‘á»‹nh trÆ°á»›c (vÃ­ dá»¥ 0.8), hai node Ä‘Æ°á»£c coi lÃ  Ä‘á»“ng nghÄ©a vÃ  Ä‘Æ°á»£c ná»‘i vá»›i nhau báº±ng má»™t â€œsynonym edge.â€

---
â“ PhÃ¡ 2:
Táº¡i sao láº¡i cáº§n ná»‘i cÃ¡c node Ä‘á»“ng nghÄ©a láº¡i vá»›i nhau?
âœ… ÄÃ¡p:
- Äiá»u nÃ y giÃºp liÃªn káº¿t cÃ¡c khÃ¡i niá»‡m cÃ³ thá»ƒ Ä‘Æ°á»£c diá»…n Ä‘áº¡t khÃ¡c nhau.
- Nhá» Ä‘Ã³, khi truy váº¥n, náº¿u tá»« khÃ³a Ä‘Æ°á»£c dÃ¹ng khÃ¡c vá»›i cÃ¡ch ghi chÃº ban Ä‘áº§u nhÆ°ng cÃ³ cÃ¹ng Ã½ nghÄ©a, há»‡ thá»‘ng váº«n cÃ³ thá»ƒ liÃªn káº¿t Ä‘Ãºng kiáº¿n thá»©c.
VÃ­ dá»¥ dá»… hiá»ƒu:
 Náº¿u trong vÄƒn báº£n cÃ³ â€œtháº§y giÃ¡oâ€ vÃ  â€œgiÃ¡o viÃªnâ€ â€“ máº·c dÃ¹ hai tá»« khÃ¡c nhau, chÃºng Ä‘á»u nÃ³i vá» ngÆ°á»i dáº¡y há»c. Káº¿t ná»‘i chÃºng láº¡i giÃºp há»‡ thá»‘ng hiá»ƒu ráº±ng khi há»i vá» â€œgiÃ¡o viÃªnâ€ cÅ©ng cÃ³ thá»ƒ liÃªn há»‡ Ä‘áº¿n â€œtháº§y giÃ¡oâ€.

---
â“ PhÃ¡ 3:
CÃ³ rá»§i ro gÃ¬ khi dÃ¹ng cosine similarity?
âœ… ÄÃ¡p:
- CÃ³ thá»ƒ xáº£y ra trÆ°á»ng há»£p ná»‘i nháº§m nhá»¯ng tá»« cÃ³ cÃ¹ng hÃ¬nh thá»©c nhÆ°ng ngá»¯ nghÄ©a khÃ¡c (vÃ­ dá»¥: â€œbankâ€ cÃ³ thá»ƒ lÃ  ngÃ¢n hÃ ng hoáº·c bá» sÃ´ng).
- Tuy nhiÃªn, vÃ¬ Ä‘Ã¢y chá»‰ lÃ  lá»›p bá»• trá»£ trong KG, náº¿u cÃ¡c bÆ°á»›c sau (nhÆ° triple filtering) hoáº¡t Ä‘á»™ng tá»‘t, sai sÃ³t nÃ y sáº½ Ä‘Æ°á»£c giáº£m thiá»ƒu.

---
3. Denseâ€“Sparse Integration â€“ TÃ­ch há»£p cáº£ Node "Phrase" vÃ  "Passage"
Má»¥c tiÃªu:
 Káº¿t há»£p thÃ´ng tin ngáº¯n gá»n (phrase node) vÃ  thÃ´ng tin Ä‘áº§y Ä‘á»§ (passage node) vÃ o cÃ¹ng má»™t KG Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cáº£ hai.

---
â“ PhÃ¡ 1:
Táº¡i sao khÃ´ng chá»‰ lÆ°u thÃ´ng tin dáº¡ng â€œphraseâ€ hay â€œpassageâ€ riÃªng ráº½?
âœ… ÄÃ¡p:
- Phrase node (sparse): RÃºt gá»n thÃ´ng tin, dá»… káº¿t ná»‘i cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n, nhanh vÃ  hiá»‡u quáº£ cho reasoning.
- Passage node (dense): Giá»¯ nguyÃªn ngá»¯ cáº£nh vÃ  chi tiáº¿t, cáº§n thiáº¿t cho cÃ¡c tÃ¡c vá»¥ yÃªu cáº§u Ä‘á»™ chÃ­nh xÃ¡c cao nhÆ° factual QA.
VÃ­ dá»¥:
 Há»c sinh khi Ã´n bÃ i toÃ¡n: báº¡n cÃ³ thá»ƒ ghi láº¡i â€œcÃ´ng thá»©c Pythagorasâ€ (phrase) vÃ  cÃ³ sá»• tay chá»©a bÃ i giáº£i chi tiáº¿t (passage).
 Cáº£ hai Ä‘á»u quan trá»ng Ä‘á»ƒ báº¡n vá»«a nhá»› nhanh, vá»«a hiá»ƒu sÃ¢u.

---
â“ PhÃ¡ 2:
How does integrating both dense and sparse information help in retrieval?
âœ… ÄÃ¡p:
- Khi cÃ³ cáº£ hai, há»‡ thá»‘ng cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c phrase node Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c khÃ¡i niá»‡m chá»§ Ä‘áº¡o vÃ  passage node Ä‘á»ƒ láº¥y chi tiáº¿t cá»¥ thá»ƒ cho cÃ¢u tráº£ lá»i.
- Äiá»u nÃ y táº¡o ra má»™t KG toÃ n diá»‡n, vá»«a tá»•ng quÃ¡t vá»«a chi tiáº¿t, giÃºp tÄƒng hiá»‡u suáº¥t á»Ÿ cáº£ factual QA vÃ  multi-hop reasoning.

---
4. Triple Retrieval â€“ Láº¥y ra cÃ¡c Triple cÃ³ liÃªn quan vá»›i Query
Má»¥c tiÃªu:
 Khi cÃ³ má»™t query, tÃ¬m ra cÃ¡c triple tá»« KG phÃ¹ há»£p Ä‘á»ƒ dÃ¹ng cho quÃ¡ trÃ¬nh reasoning.

---
â“ PhÃ¡ 1:
Táº¡i sao khÃ´ng chá»‰ láº¥y toÃ n bá»™ passage mÃ  pháº£i láº¥y ra triple?
âœ… ÄÃ¡p:
- Triple chá»©a má»‘i liÃªn há»‡ rÃµ rÃ ng vÃ  rÃºt gá»n, giÃºp há»‡ thá»‘ng dá»… dÃ ng â€œÄ‘i theoâ€ cÃ¡c má»‘i liÃªn káº¿t Ä‘á»ƒ reasoning.
- Passage cÃ³ thá»ƒ quÃ¡ dÃ i, chá»©a nhiá»…u; triple giÃºp xÃ¡c Ä‘á»‹nh â€œÄ‘iá»ƒm cá»‘t lÃµiâ€ cá»§a kiáº¿n thá»©c.
VÃ­ dá»¥:
 Náº¿u cÃ¢u há»i lÃ  â€œBill Gates há»c trÆ°á»ng nÃ o?â€, triple nhÆ° (â€œBill Gatesâ€, â€œhá»c táº¡iâ€, â€œHarvardâ€) sáº½ giÃºp há»‡ thá»‘ng nhanh chÃ³ng tÃ¬m ra thÃ´ng tin cáº§n thiáº¿t.

---
5. Triple Filtering (Recognition Memory) â€“ Lá»c láº¡i cÃ¡c Triple
Má»¥c tiÃªu:
 Sau khi láº¥y ra cÃ¡c triple ban Ä‘áº§u, sá»­ dá»¥ng LLM Ä‘á»ƒ xÃ¡c nháº­n láº¡i tÃ­nh liÃªn quan cá»§a chÃºng vá»›i query, loáº¡i bá» nhá»¯ng triple khÃ´ng phÃ¹ há»£p.

---
â“ PhÃ¡ 1:
Táº¡i sao láº¡i cáº§n bÆ°á»›c lá»c triple náº¿u Ä‘Ã£ cÃ³ triple retrieval?
âœ… ÄÃ¡p:
- Do bÆ°á»›c retrieval ban Ä‘áº§u cÃ³ thá»ƒ mang láº¡i nhiá»u triple nhiá»…u hoáº·c khÃ´ng liÃªn quan, vÃ¬ khÃ´ng pháº£i lÃºc nÃ o cÃ¡c vector embedding cÅ©ng hoÃ n háº£o.
- Recognition Memory giÃºp â€œÄ‘á»c láº¡iâ€ vÃ  xÃ¡c nháº­n, giá»‘ng nhÆ° khi báº¡n kiá»ƒm tra láº¡i ghi chÃº cá»§a mÃ¬nh Ä‘á»ƒ Ä‘áº£m báº£o chá»‰ giá»¯ láº¡i nhá»¯ng thÃ´ng tin Ä‘Ãºng cáº§n thiáº¿t.
VÃ­ dá»¥:
 Náº¿u cÃ³ 5 triple Ä‘Æ°á»£c láº¥y ra, nhÆ°ng chá»‰ cÃ³ 2 thá»±c sá»± nÃ³i vá» cÃ¢u há»i, bÆ°á»›c filtering sáº½ loáº¡i bá» 3 triple khÃ´ng cáº§n thiáº¿t.

---
6. Seed Node Assignment â€“ GÃ¡n Trá»ng Sá»‘ cho Seed Node
Má»¥c tiÃªu:
 XÃ¡c Ä‘á»‹nh cÃ¡c node â€œgá»‘câ€ (seed node) tá»« query, vÃ  gÃ¡n trá»ng sá»‘ cho chÃºng nháº±m táº¡o ná»n táº£ng cho bÆ°á»›c lan truyá»n thÃ´ng tin trong PPR.

---
â“ PhÃ¡ 1:
Seed Node lÃ  gÃ¬?
âœ… ÄÃ¡p:
- Seed Node lÃ  nhá»¯ng node trong KG mÃ  há»‡ thá»‘ng xÃ¡c Ä‘á»‹nh cÃ³ má»‘i liÃªn há»‡ trá»±c tiáº¿p vá»›i query.
- ChÃºng lÃ  Ä‘iá»ƒm khá»Ÿi Ä‘áº§u Ä‘á»ƒ thuáº­t toÃ¡n lan truyá»n (PPR) â€œkhÃ¡m phÃ¡â€ cÃ¡c node khÃ¡c cÃ³ liÃªn quan.
VÃ­ dá»¥:
 Náº¿u cÃ¢u há»i lÃ  â€œBill Gates há»c á»Ÿ Ä‘Ã¢u?â€, seed node cÃ³ thá»ƒ lÃ  â€œBill Gatesâ€ vÃ  â€œHarvardâ€ náº¿u Ä‘Ã£ cÃ³ triple liÃªn quan.

---
â“ PhÃ¡ 2:
Táº¡i sao cáº§n gÃ¡n trá»ng sá»‘ cho seed node?
âœ… ÄÃ¡p:
- Trá»ng sá»‘ cÃ ng cao, node Ä‘Ã³ sáº½ áº£nh hÆ°á»Ÿng máº¡nh hÆ¡n trong quÃ¡ trÃ¬nh lan truyá»n thÃ´ng tin cá»§a PageRank.
- GiÃºp Æ°u tiÃªn cÃ¡c node quan trá»ng, táº¡o ra káº¿t quáº£ retrieval chÃ­nh xÃ¡c hÆ¡n.
VÃ­ dá»¥:
 Giá»‘ng nhÆ° khi báº¡n chá»n Ä‘iá»ƒm máº¥u chá»‘t trong sÆ¡ Ä‘á»“ tÆ° duy, nhá»¯ng Ä‘iá»ƒm quan trá»ng Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u rÃµ, Ä‘á»ƒ khi â€œÄ‘i theo sÆ¡ Ä‘á»“â€ báº¡n sáº½ Æ°u tiÃªn nhá»¯ng Ä‘iá»ƒm Ä‘Ã³.

---
7. PPR (Personalized PageRank) Graph Search â€“ TÃ¬m Kiáº¿m Trong Graph
Má»¥c tiÃªu:
 Sá»­ dá»¥ng thuáº­t toÃ¡n PPR Ä‘á»ƒ lan truyá»n thÃ´ng tin tá»« cÃ¡c seed node vÃ  tÃ¬m ra cÃ¡c node liÃªn quan nháº¥t trong KG.

---
â“ PhÃ¡ 1:
Thuáº­t toÃ¡n PageRank hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o vÃ  khÃ¡c gÃ¬ so vá»›i tÃ¬m kiáº¿m thÃ´ng thÆ°á»ng?
âœ… ÄÃ¡p:
- PageRank ban Ä‘áº§u Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ xáº¿p háº¡ng cÃ¡c trang web dá»±a trÃªn sá»‘ lÆ°á»£ng vÃ  cháº¥t lÆ°á»£ng cá»§a cÃ¡c liÃªn káº¿t.
- Personalized PageRank (PPR) Ä‘iá»u chá»‰nh thuáº­t toÃ¡n Ä‘á»ƒ Æ°u tiÃªn cÃ¡c node báº¯t Ä‘áº§u tá»« seed node (Ä‘iá»ƒm khá»Ÿi Ä‘áº§u tá»« query).
- Thay vÃ¬ chá»‰ tÃ¬m â€œÄ‘oáº¡n vÄƒn gáº§n nháº¥tâ€, PPR lan truyá»n trá»ng sá»‘ qua cÃ¡c liÃªn káº¿t trong graph, giÃºp há»‡ thá»‘ng reasoning multi-hop.
VÃ­ dá»¥:
 Giá»‘ng nhÆ° báº¡n cÃ³ má»™t báº£n Ä‘á»“ mÃª cung vá»›i cÃ¡c lá»‘i Ä‘i, PPR giÃºp báº¡n khÃ´ng chá»‰ tÃ¬m lá»‘i Ä‘i ngáº¯n nháº¥t mÃ  tÃ¬m lá»‘i Ä‘i â€œnhiá»u dáº¥u hiá»‡u nháº¥tâ€ liÃªn quan Ä‘áº¿n chá»§ Ä‘á» cá»§a cÃ¢u há»i.

---
â“ PhÃ¡ 2:
Táº¡i sao láº¡i cáº§n bÆ°á»›c nÃ y khi cÃ³ cÃ¡c bÆ°á»›c retrieval trÆ°á»›c Ä‘Ã³?
âœ… ÄÃ¡p:
- VÃ¬ PPR giÃºp káº¿t ná»‘i cÃ¡c má»‘i quan há»‡ trong KG, khÃ´ng chá»‰ dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng Ä‘Æ¡n thuáº§n mÃ  cÃ²n hiá»ƒu Ä‘Æ°á»£c cáº¥u trÃºc liÃªn káº¿t giá»¯a cÃ¡c thÃ´ng tin.
- NÃ³ cho phÃ©p truy xuáº¥t cÃ¡c thÃ´ng tin liÃªn quan tá»« xa trong graph, táº¡o nÃªn cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c cho cÃ¡c cÃ¢u há»i multi-hop.

---
8. QA Reader â€“ Äá»c vÃ  Sinh Ra CÃ¢u Tráº£ Lá»i
Má»¥c tiÃªu:
 Cuá»‘i cÃ¹ng, cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Æ°á»£c láº¥y ra (sau khi Ä‘Ã£ qua cÃ¡c bÆ°á»›c xá»­ lÃ½) Ä‘Æ°á»£c Ä‘Æ°a cho LLM Ä‘á»ƒ táº¡o ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng cho query.

---
â“ PhÃ¡ 1:
Táº¡i sao khÃ´ng sá»­ dá»¥ng trá»±c tiáº¿p cÃ¡c triple lÃ m cÃ¢u tráº£ lá»i?
âœ… ÄÃ¡p:
- Triple chá»‰ lÃ  cÃ¡c má»‘i liÃªn há»‡ rÃºt gá»n, chÆ°a Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh cáº§n thiáº¿t Ä‘á»ƒ tráº£ lá»i má»™t cÃ¡ch máº¡ch láº¡c vÃ  tá»± nhiÃªn.
- Passage cung cáº¥p Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh, chi tiáº¿t giÃºp LLM sinh ra cÃ¢u tráº£ lá»i trung thá»±c, Ä‘áº§y Ä‘á»§ vÃ  tá»± nhiÃªn hÆ¡n.
VÃ­ dá»¥:
 Náº¿u báº¡n há»i â€œBill Gates há»c á»Ÿ Ä‘Ã¢u?â€, triple cÃ³ thá»ƒ nÃ³i â€œBill Gates â€“ há»c táº¡i â€“ Harvardâ€, nhÆ°ng passage cÃ³ thá»ƒ cung cáº¥p thÃªm chi tiáº¿t nhÆ° â€œBill Gates tá»«ng theo há»c táº¡i trÆ°á»ng Harvard, nÆ¡i Ã´ng Ä‘Ã£ tiáº¿p thu nhiá»u kiáº¿n thá»©c vá» kinh doanh vÃ  cÃ´ng nghá»‡â€, giÃºp cÃ¢u tráº£ lá»i phong phÃº hÆ¡n.

---
Tá»•ng Káº¿t & LiÃªn Há»‡:
Má»—i bÆ°á»›c trong HippoRAG 2 Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ bÃ¹ trá»« nhá»¯ng Ä‘iá»ƒm yáº¿u cá»§a cÃ¡c bÆ°á»›c trÆ°á»›c vÃ  táº¡o nÃªn má»™t chuá»—i xá»­ lÃ½ toÃ n diá»‡n:
- OpenIE: TrÃ­ch xuáº¥t kiáº¿n thá»©c dáº¡ng triple giÃºp â€œghi nhá»›â€ cÃ¡c má»‘i quan há»‡ chÃ­nh.
- Synonym Detection: Káº¿t ná»‘i nhá»¯ng khÃ¡i niá»‡m cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»±, nhÆ° viá»‡c ná»‘i cÃ¡c tá»« Ä‘á»“ng nghÄ©a trong tá»« Ä‘iá»ƒn.
- Dense-Sparse Integration: Giá»¯ Ä‘Æ°á»£c cáº£ â€œÄ‘áº§u dÃ²ngâ€ (phrase) vÃ  â€œná»™i dung chi tiáº¿tâ€ (passage) â€“ giá»‘ng nhÆ° ghi chÃº ngáº¯n vÃ  sá»• tay chi tiáº¿t.
- Triple Retrieval & Filtering: Nháº­n ra cÃ¡c ghi chÃº liÃªn quan vÃ  loáº¡i bá» nhiá»…u, nhÆ° viá»‡c báº¡n xem láº¡i bÃ i há»c vÃ  chá»n ra nhá»¯ng Ä‘iá»ƒm quan trá»ng.
- Seed Node Assignment & PPR Graph Search: Giá»‘ng nhÆ° chá»n cÃ¡c Ä‘iá»ƒm máº¥u chá»‘t trÃªn sÆ¡ Ä‘á»“ tÆ° duy rá»“i Ä‘i theo cÃ¡c má»‘i liÃªn há»‡ Ä‘á»ƒ tÃ¬m ra cÃ¢u tráº£ lá»i toÃ n diá»‡n.
- QA Reader: Cuá»‘i cÃ¹ng, tá»•ng há»£p cÃ¡c thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c lá»c vÃ  liÃªn káº¿t Ä‘á»ƒ sinh ra cÃ¢u tráº£ lá»i tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c.

---
DÆ°á»›i Ä‘Ã¢y mÃ¬nh sáº½ Ä‘Ã o sÃ¢u vÃ o khÃ¡i niá»‡m triple trong bÆ°á»›c 1 cá»§a HippoRAG (vÃ  HippoRAG 2), vá»›i phong cÃ¡ch â€œmá»™t ngÆ°á»i phÃ¡ â€“ má»™t ngÆ°á»i tráº£ lá»iâ€ Ä‘á»ƒ lÃ m rÃµ tá»«ng Ä‘iá»ƒm.

---
1. Triple vÃ  Relation Edge
â“ PhÃ¡: Trong bÆ°á»›c trÃ­ch xuáº¥t triple báº±ng OpenIE, cÃ¡c triple Ä‘Æ°á»£c táº¡o ra cÃ³ â€œhÆ°á»›ngâ€ khÃ´ng?
Tráº£ lá»i:
- CÃ³ hÆ°á»›ng:
 CÃ¡c triple Ä‘Æ°á»£c trÃ­ch xuáº¥t theo Ä‘á»‹nh dáº¡ng (subject, predicate, object) vá»‘n Ä‘Ã£ cÃ³ thá»© tá»± rÃµ rÃ ng.
  - VÃ­ dá»¥: tá»« cÃ¢u â€œMarie Curie won the Nobel Prizeâ€ ta cÃ³ triple:
 (Marie Curie, won, Nobel Prize)
  - Äiá»u nÃ y cÃ³ nghÄ©a lÃ  má»‘i quan há»‡ â€œwonâ€ chá»‰ cÃ³ Ã½ nghÄ©a tá»« â€œMarie Curieâ€ Ä‘áº¿n â€œNobel Prizeâ€.
- LÃ½ do giá»¯ hÆ°á»›ng:
  - GiÃºp phÃ¢n biá»‡t Ä‘Æ°á»£c ai lÃ m gÃ¬ vá»›i ai.
  - Báº£o toÃ n thá»© tá»± thÃ´ng tin, ráº¥t quan trá»ng trong viá»‡c reasoning (vÃ­ dá»¥: â€œwonâ€ khÃ´ng thá»ƒ Ä‘áº£o ngÆ°á»£c thÃ nh â€œNobel Prize won Marie Curieâ€ náº¿u khÃ´ng cÃ³ hÆ°á»›ng).

---
â“ PhÃ¡: Náº¿u bá» Ä‘i hÆ°á»›ng cá»§a cÃ¡c triple, liá»‡u cÃ³ lÃ m máº¥t Ä‘i thÃ´ng tin quan trá»ng khÃ´ng?
Tráº£ lá»i:
- CÃ³ máº¥t:
  - HÆ°á»›ng cá»§a edge xÃ¡c Ä‘á»‹nh má»‘i liÃªn há»‡ theo thá»© tá»± logic.
  - VÃ­ dá»¥:
    - Triple: (Marie Curie, won, Nobel Prize) mang nghÄ©a rÃµ rÃ ng.
    - Náº¿u trá»Ÿ thÃ nh edge khÃ´ng hÆ°á»›ng, ta chá»‰ biáº¿t â€œMarie Curieâ€ vÃ  â€œNobel Prizeâ€ liÃªn quan nhau nhÆ°ng khÃ´ng biáº¿t ai lÃ  tÃ¡c nhÃ¢n hay Ä‘á»‘i tÆ°á»£ng cá»§a hÃ nh Ä‘á»™ng.
- Káº¿t quáº£:
  - Viá»‡c giá»¯ hÆ°á»›ng lÃ  cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c cá»§a ngá»¯ nghÄ©a ban Ä‘áº§u tá»« vÄƒn báº£n.

---
â“ PhÃ¡: Trong Ä‘á»“ thá»‹ kiáº¿n thá»©c, vai trÃ² cá»§a relation edge cÃ³ hÆ°á»›ng lÃ  gÃ¬ trong quÃ¡ trÃ¬nh reasoning?
Tráº£ lá»i:
- GiÃºp Ä‘á»‹nh hÆ°á»›ng thÃ´ng tin:
  - Khi thá»±c hiá»‡n Personalized PageRank (PPR), hÆ°á»›ng cá»§a edge giÃºp xÃ¡c Ä‘á»‹nh luá»“ng thÃ´ng tin tá»« cÃ¡c node gá»‘c (seed nodes) sang cÃ¡c node liÃªn quan.
- Há»— trá»£ multi-hop reasoning:
  - VÃ­ dá»¥: tá»« node â€œMarie Curieâ€ qua edge â€œwonâ€ Ä‘áº¿n â€œNobel Prizeâ€, sau Ä‘Ã³ náº¿u cÃ³ edge khÃ¡c ná»‘i â€œNobel Prizeâ€ vá»›i â€œPhysicsâ€ (náº¿u Ä‘Æ°á»£c trÃ­ch xuáº¥t thÃªm), há»‡ thá»‘ng sáº½ hiá»ƒu Ä‘Æ°á»£c má»‘i liÃªn há»‡ cÃ³ thá»© tá»± giá»¯a cÃ¡c khÃ¡i niá»‡m.
- Giá»¯ nguyÃªn ngá»¯ nghÄ©a:
  - Báº±ng cÃ¡ch khÃ´ng Ä‘áº£o ngÆ°á»£c má»‘i quan há»‡, mÃ´ hÃ¬nh cÃ³ thá»ƒ truy xuáº¥t cÃ¡c thÃ´ng tin liÃªn quan chÃ­nh xÃ¡c hÆ¡n.

---
2. Synonym Edge
â“ PhÃ¡: Synonym edge Ä‘Æ°á»£c táº¡o ra dá»±a trÃªn thuáº­t toÃ¡n gÃ¬?
Tráº£ lá»i:
- Dá»±a trÃªn cosine similarity trong embedding space:
  - Má»—i phrase (hay cá»¥m tá»«) Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng vector.
  - Náº¿u cosine similarity giá»¯a hai vector vÆ°á»£t qua má»™t ngÆ°á»¡ng Ä‘á»‹nh trÆ°á»›c, thÃ¬ hai node Ä‘Ã³ Ä‘Æ°á»£c ná»‘i báº±ng má»™t synonym edge.
- Äáº·c Ä‘iá»ƒm cá»§a synonym edge:
  - KhÃ´ng cÃ³ hÆ°á»›ng: VÃ¬ náº¿u hai tá»« lÃ  Ä‘á»“ng nghÄ©a, má»‘i quan há»‡ cá»§a chÃºng thÆ°á»ng lÃ  Ä‘á»‘i xá»©ng (vÃ­ dá»¥: â€œteacherâ€ vÃ  â€œinstructorâ€ cÃ³ thá»ƒ Ä‘Æ°á»£c ná»‘i vá»›i nhau mÃ  khÃ´ng cáº§n phÃ¢n biá»‡t thá»© tá»±).

---
â“ PhÃ¡: Táº¡i sao láº¡i sá»­ dá»¥ng thuáº­t toÃ¡n embedding cho synonym thay vÃ¬ dÃ¹ng LLM trá»±c tiáº¿p?
Tráº£ lá»i:
- Hiá»‡u quáº£ vÃ  tá»‘c Ä‘á»™:
  - TÃ­nh toÃ¡n cosine similarity giá»¯a vector nhanh vÃ  hiá»‡u quáº£, khÃ´ng cáº§n gá»i LLM nhiá»u láº§n.
- Äá»™ chÃ­nh xÃ¡c cao:
  - CÃ¡c mÃ´ hÃ¬nh embedding hiá»‡n nay cÃ³ kháº£ nÄƒng báº¯t Ä‘Æ°á»£c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a ráº¥t tá»‘t, cho nÃªn phÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a lÃ  khÃ¡ chÃ­nh xÃ¡c.
- Giáº£m táº£i cho LLM:
  - Giáº£m chi phÃ­ tÃ­nh toÃ¡n, vÃ¬ khÃ´ng cáº§n LLM pháº£i xá»­ lÃ½ táº¥t cáº£ cÃ¡c cáº·p tá»« trong corpus.

---
3. Seed Node
â“ PhÃ¡: Seed node lÃ  gÃ¬ trong bá»‘i cáº£nh cá»§a HippoRAG?
Tráº£ lá»i:
- Seed Node:
  - LÃ  nhá»¯ng node trong kiáº¿n thá»©c Ä‘á»“ thá»‹ Ä‘Æ°á»£c chá»n lÃ m Ä‘iá»ƒm khá»Ÿi Ä‘áº§u cho quÃ¡ trÃ¬nh tÃ¬m kiáº¿m vÃ  lan truyá»n thÃ´ng tin (thÃ´ng qua PPR) khi cÃ³ query.
- Vai trÃ² cá»§a Seed Node:
  - ChÃºng Ä‘áº¡i diá»‡n cho cÃ¡c thÃ nh pháº§n quan trá»ng liÃªn quan Ä‘áº¿n cÃ¢u há»i Ä‘Æ°á»£c Ä‘áº·t ra.
  - VÃ­ dá»¥: Náº¿u cÃ¢u há»i lÃ  â€œBill Gates há»c á»Ÿ Ä‘Ã¢u?â€, cÃ¡c node chá»©a â€œBill Gatesâ€ hoáº·c cÃ¡c thÃ´ng tin liÃªn quan sáº½ Ä‘Æ°á»£c chá»n lÃ m seed.

---
â“ PhÃ¡: Táº¡i sao pháº£i gÃ¡n trá»ng sá»‘ (seed node weights) cho cÃ¡c Seed Node?
Tráº£ lá»i:
- GÃ¡n trá»ng sá»‘:
  - GiÃºp há»‡ thá»‘ng biáº¿t Ä‘Æ°á»£c nhá»¯ng node nÃ o quan trá»ng hÆ¡n khi thá»±c hiá»‡n lan truyá»n thÃ´ng tin trong Ä‘á»“ thá»‹.
- Lá»£i Ã­ch:
  - CÃ¡c Seed Node cÃ³ trá»ng sá»‘ cao sáº½ â€œphÃ¡t sÃ¡ngâ€ hÆ¡n trong quÃ¡ trÃ¬nh PPR, tá»« Ä‘Ã³ giÃºp tÃ¬m ra cÃ¡c node liÃªn quan tá»‘t hÆ¡n, Ä‘áº£m báº£o cÃ¢u tráº£ lá»i Ä‘Ãºng vÃ  Ä‘áº§y Ä‘á»§.
- VÃ­ dá»¥ Ä‘Æ¡n giáº£n:
  - Trong má»™t báº£n Ä‘á»“ tÆ° duy, náº¿u báº¡n Ä‘Ã¡nh dáº¥u má»™t sá»‘ Ã½ chÃ­nh vá»›i bÃºt dáº¡, nhá»¯ng Ã½ nÃ y sáº½ Ä‘Æ°á»£c chÃº Ã½ nhiá»u hÆ¡n khi báº¡n tá»•ng há»£p láº¡i kiáº¿n thá»©c.

---
4. Relation Edge vÃ  CÃ¡c KhÃ¡i Niá»‡m KhÃ¡c
â“ PhÃ¡: Relation edge cÃ³ hÆ°á»›ng nhÆ° tháº¿ nÃ o?
Tráº£ lá»i:
- Relation Edge:
  - LÃ  cÃ¡c cáº¡nh ná»‘i giá»¯a cÃ¡c node trong KG, Ä‘Æ°á»£c táº¡o ra tá»« cÃ¡c triple (subject, predicate, object).
  - ChÃºng cÃ³ hÆ°á»›ng, tá»« subject sang object, thÃ´ng qua predicate.
- Ã nghÄ©a cá»§a hÆ°á»›ng:
  - GiÃºp xÃ¡c Ä‘á»‹nh Ä‘Ãºng má»‘i liÃªn há»‡ ngá»¯ nghÄ©a (ai lÃ m gÃ¬ vá»›i gÃ¬).

---
â“ PhÃ¡: NgoÃ i cÃ¡c khÃ¡i niá»‡m trÃªn, cÃ²n khÃ¡i niá»‡m nÃ o khÃ¡c cáº§n giáº£i thÃ­ch chi tiáº¿t trong pipeline khÃ´ng?
Tráº£ lá»i:
- Context Edge:
  - Ná»‘i cÃ¡c Passage Node vá»›i cÃ¡c Phrase Node mÃ  chÃºng chá»©a Ä‘á»±ng.
  - Ã nghÄ©a: GiÃºp giá»¯ láº¡i ngá»¯ cáº£nh chi tiáº¿t cá»§a vÄƒn báº£n, há»— trá»£ factual QA.
- Denseâ€“Sparse Integration:
  - Sá»± káº¿t há»£p giá»¯a sparse coding (phrase node) vÃ  dense coding (passage node) nháº±m Ä‘áº£m báº£o vá»«a cÃ³ Ã½ chÃ­nh tÃ³m táº¯t, vá»«a cÃ³ Ä‘áº§y Ä‘á»§ chi tiáº¿t.
  - VÃ­ dá»¥ há»c sinh: NhÆ° Ä‘Ã£ nÃ³i, giá»‘ng nhÆ° vá»«a ghi nhá»› tiÃªu Ä‘á» bÃ i há»c (sparse) vÃ  vá»«a cÃ³ toÃ n bá»™ bÃ i giáº£ng (dense).

---
Káº¾T LUáº¬N
Má»—i khÃ¡i niá»‡m trong pipeline cá»§a HippoRAG 2 Ä‘á»u Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ báº£o toÃ n vÃ  khai thÃ¡c thÃ´ng tin má»™t cÃ¡ch tá»‘i Æ°u:
- Triple extraction vá»›i relation edge cÃ³ hÆ°á»›ng giá»¯ tráº­t tá»± vÃ  ngá»¯ nghÄ©a ban Ä‘áº§u.
- Synonym edge (khÃ´ng cÃ³ hÆ°á»›ng) ná»‘i cÃ¡c cá»¥m tá»« cÃ³ nghÄ©a tÆ°Æ¡ng Ä‘á»“ng, giÃºp liÃªn káº¿t thÃ´ng tin giá»¯a cÃ¡c Ä‘oáº¡n vÄƒn khÃ¡c nhau.
- Seed Node lÃ  cÃ¡c Ä‘iá»ƒm khá»Ÿi Ä‘áº§u quan trá»ng Ä‘á»ƒ báº¯t Ä‘áº§u quÃ¡ trÃ¬nh tÃ¬m kiáº¿m thÃ´ng tin trong KG, vÃ  viá»‡c gÃ¡n trá»ng sá»‘ cho chÃºng giÃºp tÄƒng cÆ°á»ng kháº£ nÄƒng lan truyá»n thÃ´ng tin trong quÃ¡ trÃ¬nh Personalized PageRank.
- Context edge vÃ  dense-sparse integration giÃºp mÃ´ hÃ¬nh khÃ´ng chá»‰ ghi nhá»› Ã½ chÃ­nh mÃ  cÃ²n giá»¯ nguyÃªn ngá»¯ cáº£nh chi tiáº¿t cá»§a vÄƒn báº£n gá»‘c.
Qua tá»«ng cÃ¢u há»i pháº£n biá»‡n, chÃºng ta nháº­n tháº¥y ráº±ng viá»‡c duy trÃ¬ hÆ°á»›ng trong cÃ¡c relation edge lÃ  ráº¥t quan trá»ng cho viá»‡c xÃ¢y dá»±ng má»™t Ä‘á»“ thá»‹ kiáº¿n thá»©c chÃ­nh xÃ¡c, tá»« Ä‘Ã³ há»— trá»£ quÃ¡ trÃ¬nh reasoning vÃ  retrieval trong toÃ n há»‡ thá»‘ng HippoRAG.
ChÃºng ta hÃ£y cÃ¹ng phÃ¢n tÃ­ch theo kiá»ƒu "má»™t ngÆ°á»i phÃ¡ â€“ má»™t ngÆ°á»i tráº£ lá»i" Ä‘á»ƒ hiá»ƒu sÃ¢u vá» cÃ¡ch thÃªm passage node sau khi Ä‘Ã£ tÃ¡ch ra bá»™ ba (triple) theo bÃ i bÃ¡o.

---
â“ PhÃ¡: Sau khi Ä‘Ã£ sá»­ dá»¥ng OpenIE tÃ¡ch ra cÃ¡c triple (subject, predicate, object) tá»« má»—i Ä‘oáº¡n vÄƒn, lÃ m tháº¿ nÃ o Ä‘á»ƒ "append" thÃªm pháº§n passage node vÃ o Ä‘á»“ thá»‹ kiáº¿n thá»©c?
Tráº£ lá»i:
 Theo bÃ i bÃ¡o, sau khi trÃ­ch xuáº¥t cÃ¡c triple tá»« má»—i passage, ta tiáº¿n hÃ nh xÃ¢y dá»±ng má»™t Knowledge Graph (KG) báº±ng cÃ¡ch thá»±c hiá»‡n bÆ°á»›c sau:
1. Táº¡o Phrase Nodes tá»« Triple:
  - Má»—i triple (vÃ­ dá»¥: (Marie Curie, won, Nobel Prize)) sáº½ táº¡o ra cÃ¡c phrase nodes cho subject, predicate vÃ  object.
  - ÄÃ¢y lÃ  cÃ¡c nÃºt Ä‘áº¡i diá»‡n cho cÃ¡c khÃ¡i niá»‡m trá»«u tÆ°á»£ng cá»§a thÃ´ng tin.
2. ThÃªm Passage Node:
  - Má»—i passage gá»‘c trong corpus Ä‘Æ°á»£c xem nhÆ° má»™t passage node riÃªng biá»‡t.
  - Sau Ä‘Ã³, append pháº§n passage node nÃ y vÃ o Ä‘á»“ thá»‹ báº±ng cÃ¡ch ná»‘i nÃ³ vá»›i táº¥t cáº£ cÃ¡c phrase node Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« passage Ä‘Ã³.
  - CÃ¡c káº¿t ná»‘i nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  context edge (hoáº·c edge "contains") vÃ¬ chÃºng biá»ƒu thá»‹ ráº±ng passage chá»©a Ä‘á»±ng cÃ¡c thÃ´ng tin hay khÃ¡i niá»‡m Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi phrase nodes.

---
â“ PhÃ¡: Táº¡i sao cáº§n thÃªm passage node? KhÃ´ng Ä‘á»§ chá»‰ cÃ³ phrase nodes sao?
Tráº£ lá»i:
- Báº£o toÃ n ngá»¯ cáº£nh:
  - Phrase nodes cung cáº¥p má»™t dáº¡ng trá»«u tÆ°á»£ng, tá»•ng quÃ¡t cá»§a thÃ´ng tin. Tuy nhiÃªn, thÃ´ng tin nÃ y cÃ³ thá»ƒ máº¥t Ä‘i chi tiáº¿t ngá»¯ cáº£nh quan trá»ng.
  - Báº±ng cÃ¡ch thÃªm passage node, há»‡ thá»‘ng lÆ°u giá»¯ Ä‘Æ°á»£c toÃ n bá»™ thÃ´ng tin chi tiáº¿t cá»§a Ä‘oáº¡n vÄƒn gá»‘c.
- Denseâ€“Sparse Integration:
  - Passage node Ä‘áº¡i diá»‡n cho pháº§n dense coding (Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh, chi tiáº¿t), trong khi phrase nodes Ä‘áº¡i diá»‡n cho sparse coding (khÃ¡i quÃ¡t, dá»… dÃ ng reasoning).
  - Viá»‡c káº¿t há»£p cáº£ hai giÃºp mÃ´ hÃ¬nh cÃ³ thá»ƒ thá»±c hiá»‡n tá»‘t hÆ¡n trong viá»‡c tráº£ lá»i cÃ¢u há»i yÃªu cáº§u vá»«a hiá»ƒu Ä‘Æ°á»£c Ã½ chÃ­nh vá»«a náº¯m báº¯t Ä‘Æ°á»£c chi tiáº¿t.

---
â“ PhÃ¡: CÃ¡ch thá»©c ná»‘i giá»¯a passage node vÃ  phrase nodes Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° tháº¿ nÃ o?
Tráº£ lá»i:
- Má»—i passage node sáº½ Ä‘Æ°á»£c ná»‘i vá»›i táº¥t cáº£ cÃ¡c phrase node Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« chÃ­nh passage Ä‘Ã³.
- CÃ¡c káº¿t ná»‘i nÃ y (context edges) khÃ´ng cáº§n cÃ³ hÆ°á»›ng rÃµ rÃ ng nhÆ° cÃ¡c relation edge cá»§a triple, mÃ  chá»§ yáº¿u nháº±m má»¥c Ä‘Ã­ch thá»ƒ hiá»‡n má»‘i liÃªn há»‡ vá» ngá»¯ cáº£nh.
- Khi thá»±c hiá»‡n retrieval hoáº·c Personalized PageRank (PPR), passage node giÃºp tÄƒng trá»ng sá»‘ cho cÃ¡c Ä‘oáº¡n vÄƒn chá»©a Ä‘á»±ng thÃ´ng tin chi tiáº¿t, tá»« Ä‘Ã³ cáº£i thiá»‡n kháº£ nÄƒng truy xuáº¥t vÃ  hiá»ƒu sÃ¢u cá»§a mÃ´ hÃ¬nh.

---
â“ PhÃ¡: Táº¡i sao khÃ´ng chá»‰ dÃ¹ng phrase nodes mÃ  bá» qua passage node?
Tráº£ lá»i:
- Náº¿u chá»‰ sá»­ dá»¥ng phrase nodes, ta chá»‰ cÃ³ Ä‘Æ°á»£c má»™t báº£n tÃ³m táº¯t ráº¥t cÃ´ Ä‘á»ng cá»§a thÃ´ng tin.
- Nhiá»u trÆ°á»ng há»£p, Ä‘áº·c biá»‡t lÃ  cÃ¡c cÃ¢u há»i factual hay yÃªu cáº§u sá»± liÃªn káº¿t sÃ¢u giá»¯a cÃ¡c Ã½, Ä‘Ã²i há»i pháº£i cÃ³ ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ tá»« Ä‘oáº¡n vÄƒn gá»‘c.
- Passage node giÃºp mÃ´ hÃ¬nh truy xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn gá»‘c Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng máº¥t Ä‘i cÃ¡c chi tiáº¿t cáº§n thiáº¿t cho viá»‡c tráº£ lá»i chÃ­nh xÃ¡c.

---
Tá»•ng Káº¿t
- BÆ°á»›c 1: TÃ¡ch cÃ¡c triple (subject, predicate, object) tá»« má»—i passage báº±ng OpenIE cá»§a LLM â†’ táº¡o ra cÃ¡c phrase nodes.
- BÆ°á»›c 2: Vá»›i má»—i passage gá»‘c, táº¡o passage node.
- BÆ°á»›c 3: Append passage node vÃ o Ä‘á»“ thá»‹ báº±ng cÃ¡ch ná»‘i nÃ³ vá»›i táº¥t cáº£ cÃ¡c phrase node tá»« passage Ä‘Ã³ thÃ´ng qua context edges.
- Má»¥c Ä‘Ã­ch: Giá»¯ láº¡i cáº£ Ã½ chÃ­nh (phrase node) vÃ  thÃ´ng tin chi tiáº¿t (passage node) Ä‘á»ƒ há»— trá»£ quÃ¡ trÃ¬nh retrieval vÃ  reasoning, tá»« Ä‘Ã³ cáº£i thiá»‡n hiá»‡u quáº£ cá»§a toÃ n há»‡ thá»‘ng.
DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch tiáº¿p cáº­n chi tiáº¿t theo dáº¡ng há»i â€“ Ä‘Ã¡p pháº£n biá»‡n (má»™t ngÆ°á»i â€œphÃ¡â€, má»™t ngÆ°á»i â€œbÃªnhâ€) Ä‘á»ƒ má»• xáº» tháº­t sÃ¢u 5 bÆ°á»›c trong Online Retrieval & QA cá»§a HippoRAG/HippoRAG 2. Má»—i cÃ¢u tráº£ lá»i Ä‘á»u cá»‘ gáº¯ng gáº¯n vá»›i láº­p luáº­n tá»« bÃ i bÃ¡o hoáº·c Ã½ tÆ°á»Ÿng chÃ­nh (cÃ³ dáº«n chá»©ng).

---
(1) Retrieving Passages and Triples
Pháº£n biá»‡n (Q):
Táº¡i sao pháº£i láº¥y cáº£ cÃ¡c Ä‘oáº¡n vÄƒn (passages) vÃ  cÃ¡c bá»™ ba (triples)? Liá»‡u chá»‰ cáº§n retrieval embedding tá»‘t Ä‘á»ƒ láº¥y Ä‘oáº¡n vÄƒn Ä‘Ã£ Ä‘á»§ chÆ°a?
Tráº£ lá»i (A):
- Cáº§n cáº£ passages: vÃ¬ cÃ¡c Ä‘oáº¡n vÄƒn mang Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh. Náº¿u chá»‰ dá»±a vÃ o node (thÆ°á»ng lÃ  má»™t cÃ¢u/triple rá»i ráº¡c) thÃ¬ cÃ³ thá»ƒ thiáº¿u máº¥t chi tiáº¿t quan trá»ng.
- Cáº§n cáº£ triples: vÃ¬ triple lÃ  â€œÄ‘Æ¡n vá»‹ thÃ´ng tinâ€ mang tÃ­nh khÃ¡i quÃ¡t â€“ giÃºp mÃ´ phá»ng sá»± liÃªn káº¿t (associativity). Khi ta kÃ©o Ä‘Æ°á»£c triple liÃªn quan, nÃ³ cÃ³ thá»ƒ â€œgá»£i Ã½â€ nhiá»u quan há»‡ sÃ¢u hÆ¡n mÃ  má»™t Ä‘oáº¡n vÄƒn chÆ°a cháº¯c Ä‘Ã£ hiá»ƒn thá»‹ ngay.
- Dáº«n chá»©ng: Pháº§n â€œOnline Retrievalâ€ trong bÃ i HippoRAG/HippoRAG 2 cho tháº¥y, há»‡ thá»‘ng cháº¡y query-to-triple Ä‘á»ƒ láº¥y triple trÆ°á»›c, sau Ä‘Ã³ má»›i cháº¡y tiáº¿p cÃ¡c bÆ°á»›c khÃ¡c. Káº¿t há»£p â€œpassage nodeâ€ vÃ  â€œphrase nodeâ€ (triple) trong Ä‘á»“ thá»‹ lÃ  cÃ¡ch mÃ  mÃ´ hÃ¬nh â€œtrá»™nâ€ giá»¯a â€œcontextâ€ vÃ  â€œconceptsâ€.

---
(2) Recognition Memory (Triple Filtering)
Pháº£n biá»‡n (Q):
Táº¡i sao láº¡i cáº§n má»™t bÆ°á»›c gá»i lÃ  â€œRecognition Memoryâ€ Ä‘á»ƒ lá»c triple? Cháº³ng pháº£i retrieval encoder Ä‘Ã£ xáº¿p háº¡ng Ä‘Æ°á»£c rá»“i sao?
Tráº£ lá»i (A):
- Encoder-based ranking chá»‰ dá»±a trÃªn vector similarity, cÃ³ thá»ƒ nháº·t pháº£i nhá»¯ng triple â€œna nÃ¡â€ vá» tá»« vá»±ng nhÆ°ng khÃ´ng tháº­t sá»± liÃªn quan ngá»¯ nghÄ©a.
- LLM-based filtering (Recognition Memory) giá»‘ng nhÆ° â€œduyá»‡t láº¡iâ€ nhá»¯ng triple top-k, Ä‘á»ƒ xÃ¡c nháº­n: â€œTriple nÃ y thá»±c sá»± nÃ³i vá» cÃ¡i gÃ¬, cÃ³ Äƒn khá»›p vá»›i cÃ¢u há»i khÃ´ng?â€ â€“ Tá»« Ä‘Ã³ loáº¡i bá»›t nhiá»…u.
- VÃ­ dá»¥: CÃ³ nhá»¯ng triple mÃ  entity giá»‘ng nhau vá» máº·t ngÃ´n ngá»¯ (vÃ­ dá»¥ â€œParisâ€ â€“ má»™t ngÆ°á»i tÃªn Paris vs. thÃ nh phá»‘ Paris). Náº¿u chá»‰ dÃ¹ng embedding, Ä‘Ã´i khi bá»‹ láº«n. BÆ°á»›c filtering nÃ y giÃºp LLM dÃ¹ng ngá»¯ cáº£nh Ä‘á»ƒ phÃ¢n biá»‡t.
- Dáº«n chá»©ng: BÃ i bÃ¡o giá»›i thiá»‡u cÆ¡ cháº¿ â€œtwo-step retrievalâ€: 1) Query-to-triple (embedding) vÃ  2) Triple filter (LLM) â€“ mÃ´ phá»ng â€œrecall vs. recognitionâ€ trong trÃ­ nhá»› con ngÆ°á»i.

---
(3) Assigning Seed Node Weights
Pháº£n biá»‡n (Q):
PhÃ¢n phá»‘i trá»ng sá»‘ (reset probability) cho cÃ¡c â€œseed nodeâ€ â€“ Ä‘áº·c biá»‡t lÃ  giá»¯a â€œpassage nodeâ€ vÃ  â€œphrase nodeâ€ â€“ cÃ³ thá»±c sá»± cáº§n thiáº¿t, hay chá»‰ cáº§n gÃ¡n Ä‘á»u lÃ  Ä‘Æ°á»£c?
Tráº£ lá»i (A):
- KhÃ´ng thá»ƒ gÃ¡n Ä‘á»u: Bá»Ÿi vÃ¬ â€œphrase nodeâ€ (cÃ¡c khÃ¡i niá»‡m) vÃ  â€œpassage nodeâ€ (cÃ¡c vÄƒn báº£n) Ä‘Ã³ng vai trÃ² khÃ¡c nhau. Náº¿u gÃ¡n trá»ng sá»‘ giá»‘ng nhau, mÃ´ hÃ¬nh cÃ³ thá»ƒ nghiÃªng quÃ¡ nhiá»u vá» concept hoáº·c quÃ¡ nhiá»u vá» context.
- Cáº§n cÃ¢n báº±ng: Cháº³ng háº¡n, khi cÃ¢u há»i ráº¥t cá»¥ thá»ƒ (vÃ­ dá»¥: â€œTÃ¡c giáº£ cuá»‘n sÃ¡ch X lÃ  ai?â€), â€œphrase nodeâ€ vá» â€œbook_authorâ€ cÃ³ thá»ƒ quan trá»ng hÆ¡n. NhÆ°ng náº¿u cÃ¢u há»i phá»©c táº¡p, â€œpassage nodeâ€ cÃ³ ngá»¯ cáº£nh rá»™ng hÆ¡n sáº½ cáº§n trá»ng sá»‘ cao hÆ¡n Ä‘á»ƒ PPR lan tá»a nhiá»u hÆ¡n.
- Dáº«n chá»©ng: TÃ¡c giáº£ cÃ³ lÃ m ablation study (vÃ­ dá»¥ trong pháº§n Implementation & Discussion) Ä‘á»ƒ chá»©ng minh pháº£i chá»n â€œweight factorâ€ cho passage node há»£p lÃ½ (0.01, 0.05, v.v.) thÃ¬ má»›i tá»‘i Æ°u Recall@5.

---
(4) PPR Graph Search
Pháº£n biá»‡n (Q):
Personalized PageRank (PPR) thá»±c sá»± giÃºp Ä‘Æ°á»£c gÃ¬? Liá»‡u má»™t vÃ²ng láº·p iterative retrieval (nhÆ° IRCoT) khÃ´ng pháº£i Ä‘Ã£ giáº£i quyáº¿t Ä‘Æ°á»£c multi-hop sao?
Tráº£ lá»i (A):
- PPR lÃ m multi-hop ngay trong má»™t lÆ°á»£t: Thay vÃ¬ pháº£i gá»i LLM liÃªn tiáº¿p Ä‘á»ƒ â€œgá»£i Ã½â€ truy váº¥n phá»¥ (iterative retrieval), PPR cho phÃ©p khuáº¿ch tÃ¡n xÃ¡c suáº¥t tá»« cÃ¡c seed node (passage/phrase) Ä‘áº¿n lÃ¢n cáº­n trong KG.
- Hiá»‡u quáº£ chi phÃ­: Iterative retrieval kiá»ƒu IRCoT thÆ°á»ng tá»‘n nhiá»u lÆ°á»£t gá»i LLM. PPR lÃ  má»™t thuáº­t toÃ¡n Ä‘á»“ thá»‹ cháº¡y ná»™i bá»™, ráº» hÆ¡n vá» máº·t inference.
- PhÃ¹ há»£p vá»›i â€œassociative memoryâ€: BÃ i bÃ¡o gá»£i Ã½ ráº±ng PPR mÃ´ phá»ng cÃ¡ch hippocampus (nÃ£o bá»™) kÃ­ch hoáº¡t cÃ¡c liÃªn káº¿t giá»¯a khÃ¡i niá»‡m. Tá»« 1 seed node, lan ra nhiá»u nÃºt khÃ¡c theo xÃ¡c suáº¥t.
- Dáº«n chá»©ng: HippoRAG/HippoRAG 2 so sÃ¡nh vá»›i IRCoT, cho tháº¥y PPR cÃ³ thá»ƒ â€œgÃ³iâ€ viá»‡c multi-hop reasoning trong má»™t láº§n retrieval, nhanh hÆ¡n vÃ  Ã­t tá»‘n chi phÃ­ hÆ¡n.

---
(5) QA Reading with Selected Passages
Pháº£n biá»‡n (Q):
Náº¿u cuá»‘i cÃ¹ng váº«n pháº£i gá»i LLM Ä‘á»ƒ Ä‘á»c láº¡i cÃ¡c Ä‘oáº¡n vÄƒn (QA reading), váº­y viá»‡c xÃ¢y dá»±ng KG hay PPR cÃ³ tháº­t sá»± quan trá»ng? Cháº³ng pháº£i LLM giá» Ä‘Ã£ â€œsáºµn sÃ ngâ€ tráº£ lá»i náº¿u cho nÃ³ Ä‘á»§ context?
Tráº£ lá»i (A):
- LLM cáº§n â€œÄ‘áº§u vÃ o sáº¡châ€: DÃ¹ LLM máº¡nh, nhÆ°ng náº¿u input context bá»‹ nhiá»…u (chá»n sai passage) hoáº·c quÃ¡ dÃ n tráº£i, LLM dá»… tráº£ lá»i sai, Ä‘áº·c biá»‡t vá»›i cÃ¢u há»i cáº§n multi-hop.
- KG + PPR giÃºp â€œtÃ³m gá»nâ€ Ä‘Ãºng Ä‘oáº¡n vÄƒn liÃªn quan, thay vÃ¬ lÃ´i táº¥t cáº£. LLM cÃ ng cÃ³ context chÃ­nh xÃ¡c, tráº£ lá»i cÃ ng tá»‘t.
- TÃ­nh má»Ÿ rá»™ng: Khi táº­p dá»¯ liá»‡u ráº¥t lá»›n, chá»‰ embedding retrieval cÃ³ thá»ƒ bá» sÃ³t â€œchuá»—i logicâ€ liÃªn quan. Bá»• sung KG + PPR => MÃ´ hÃ¬nh â€œliÃªn káº¿tâ€ tá»‘t hÆ¡n, LLM Ä‘á»¡ bá»‹ quÃ¡ táº£i.
- Dáº«n chá»©ng: Káº¿t quáº£ thá»±c nghiá»‡m (báº£ng so sÃ¡nh F1, EM) cho tháº¥y HippoRAG 2 sau khi chá»n top passages qua PPR, rá»“i cho LLM Ä‘á»c, cho káº¿t quáº£ cao hÆ¡n so vá»›i retrieval thÆ°á»ng.

---
Tá»•ng káº¿t
5 bÆ°á»›c trong Online Retrieval & QA Ä‘Æ°á»£c thiáº¿t káº¿ cÃ³ chá»§ Ä‘Ã­ch, mÃ´ phá»ng cÆ¡ cháº¿ bá»™ nhá»› cá»§a con ngÆ°á»i:
1. Láº¥y passage + triple song song (Ä‘áº£m báº£o vá»«a cÃ³ context vá»«a cÃ³ concept).
2. Lá»c triple báº±ng LLM (Recognition Memory) Ä‘á»ƒ loáº¡i nhiá»…u.
3. GÃ¡n trá»ng sá»‘ seed node cáº©n tháº­n (cÃ¢n báº±ng passage node vÃ  phrase node).
4. Cháº¡y PPR Ä‘á»ƒ thá»±c hiá»‡n multi-hop reasoning trong má»™t bÆ°á»›c, tiáº¿t kiá»‡m chi phÃ­ so vá»›i iterative retrieval.
5. Cuá»‘i cÃ¹ng, LLM má»›i Ä‘á»c cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Ã£ chá»n Ä‘á»ƒ tráº£ lá»i, tá»‘i Æ°u Ä‘á»™ chÃ­nh xÃ¡c.
Tá»«ng bÆ°á»›c Ä‘á»u cÃ³ lÃ½ do tá»“n táº¡i, vÃ  káº¿t quáº£ thá»±c nghiá»‡m chá»©ng minh mÃ´ hÃ¬nh nÃ y tÄƒng hiá»‡u quáº£ truy xuáº¥t (retrieval) cÅ©ng nhÆ° Ä‘á»™ chÃ­nh xÃ¡c khi tráº£ lá»i cÃ¢u há»i (QA).

---
(3) Assigning Seed Node Weights
Äáº·t cÃ¢u há»i phÃ¡ nhiá»u hÆ¡n á»Ÿ bÆ°á»›c nÃ y 
- Cháº³ng háº¡n: lÃºc mÃ  xÃ¢y Graph cÃ¡c nodes vai trÃ² nhÆ° nhau hay lÃ  Hippo RAG 2 cÃ³ Ä‘Ã¡nh dáº¥u rÃµ lÃ  phrase node hay passage node ? 
- CÃ¡ch Ä‘Ã¡nh Assing Seed Node Weights nhÆ° nÃ o trong bÃ i bÃ¡o cÃ³ nÃ³i ko 
- ... nhiá»u cÃ¢u ná»¯a nhÃ©

Ã½ lÃ  phÃ¡ 1, mÃ¬nh biáº¿t lÃ  nÃ³ Ä‘Æ°á»£c rÃµ rÃ ng vai trÃ², nhÆ°ng lÃºc Ä‘Æ°a lÃªn Ä‘á»“ thá»‹ Ã½ thÃ¬ nÃ³ cÃ³ Ä‘Æ°á»£c rÃµ rÃ ng váº­y khÃ´ng ? kiá»ƒu há» dÃ¹ng cÃ¡ch nÃ o Ä‘á»ƒ Ä‘Ã¡nh dáº¥u 2 loáº¡i node Ä‘Ã³ Ä‘á»ƒ khi query dÃ¹ng

Trong quÃ¡ trÃ¬nh xÃ¢y dá»±ng Ä‘á»“ thá»‹, HippoRAG 2 khÃ´ng chá»‰ trÃ­ch xuáº¥t cÃ¡c node mÃ  cÃ²n "Ä‘Ã¡nh dáº¥u" rÃµ rÃ ng tá»«ng loáº¡i node thÃ´ng qua cáº¥u trÃºc vÃ  cÃ¡c loáº¡i cáº¡nh ná»‘i káº¿t:
- PhÃ¢n loáº¡i qua nguá»“n gá»‘c táº¡o node:
  - Phrase node: ÄÆ°á»£c táº¡o ra tá»« viá»‡c trÃ­ch xuáº¥t bá»™ ba (triple) cá»§a cÃ¡c khÃ¡i niá»‡m qua OpenIE. Nhá»¯ng node nÃ y chá»‰ chá»©a thÃ´ng tin "concept" rÃºt gá»n vÃ  liÃªn quan Ä‘áº¿n cÃ¡c má»‘i quan há»‡ giá»¯a thá»±c thá»ƒ.
  - Passage node: Äáº¡i diá»‡n cho toÃ n bá»™ Ä‘oáº¡n vÄƒn, chá»©a Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh vÃ  thÃ´ng tin chi tiáº¿t.
 Qua Ä‘Ã³, quÃ¡ trÃ¬nh offline indexing cá»§a HippoRAG 2 Ä‘Ã£ phÃ¢n biá»‡t rÃµ rÃ ng nguá»“n gá»‘c cá»§a cÃ¡c node.
- ÄÃ¡nh dáº¥u qua loáº¡i cáº¡nh (edge type):
  - CÃ¡c passage node Ä‘Æ°á»£c liÃªn káº¿t vá»›i cÃ¡c phrase node thÃ´ng qua cÃ¡c cáº¡nh mang nhÃ£n "contains" (hoáº·c "context edge"), nháº±m thá»ƒ hiá»‡n ráº±ng Ä‘oáº¡n vÄƒn chá»©a cÃ¡c cá»¥m tá»« (phrase) Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« nÃ³.
  - Trong khi Ä‘Ã³, cÃ¡c phrase node cÃ³ thá»ƒ Ä‘Æ°á»£c ná»‘i vá»›i nhau qua cÃ¡c cáº¡nh khÃ¡c nhÆ° "synonym edge" hay "relation edge". Viá»‡c nÃ y giÃºp nháº¥n máº¡nh má»‘i liÃªn há»‡ giá»¯a cÃ¡c khÃ¡i niá»‡m trá»«u tÆ°á»£ng, tÃ¡ch biá»‡t vá»›i thÃ´ng tin ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ tá»« passage.
- TÃ¡c Ä‘á»™ng khi query:
 Khi má»™t truy váº¥n Ä‘Æ°á»£c Ä‘Æ°a vÃ o, há»‡ thá»‘ng sá»­ dá»¥ng cÃ¡c embedding vÃ  trá»ng sá»‘ riÃªng biá»‡t cho tá»«ng loáº¡i node. Nhá» váº­y, viá»‡c so sÃ¡nh cosine similarity Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cáº£ phrase node vÃ  passage node theo cÃ¡ch phÃ¹ há»£p vá»›i Ä‘áº·c trÆ°ng cá»§a chÃºng. CÃ¡ch gÃ¡n trá»ng sá»‘ (seed node weights) cÅ©ng Ä‘Æ°á»£c Ä‘iá»u chá»‰nh riÃªng cho tá»«ng loáº¡i, giÃºp quÃ¡ trÃ¬nh Personalized PageRank lan tá»a thÃ´ng tin Ä‘Ãºng theo cáº¥u trÃºc Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u.
Dáº«n chá»©ng: Trong pháº§n mÃ´ táº£ cá»§a HippoRAG 2, tÃ¡c giáº£ trÃ¬nh bÃ y ráº±ng má»—i passage trong corpus Ä‘Æ°á»£c xá»­ lÃ½ nhÆ° má»™t passage node vÃ  Ä‘Æ°á»£c ná»‘i vá»›i táº¥t cáº£ cÃ¡c phrase Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« nÃ³ thÃ´ng qua cÃ¡c "context edge". Äiá»u nÃ y khÃ´ng chá»‰ giÃºp xÃ¢y dá»±ng má»™t kiáº¿n trÃºc Ä‘á»“ thá»‹ rÃµ rÃ ng mÃ  cÃ²n cho phÃ©p há»‡ thá»‘ng phÃ¢n biá»‡t vÃ  Ã¡p dá»¥ng trá»ng sá»‘ khÃ¡c nhau cho má»—i loáº¡i node trong quÃ¡ trÃ¬nh truy xuáº¥t.
NhÆ° váº­y, báº±ng cÃ¡ch phÃ¢n loáº¡i qua nguá»“n gá»‘c táº¡o node vÃ  Ä‘Ã¡nh dáº¥u thÃ´ng qua cÃ¡c loáº¡i cáº¡nh ná»‘i káº¿t, HippoRAG 2 Ä‘áº£m báº£o ráº±ng trong quÃ¡ trÃ¬nh truy váº¥n, há»‡ thá»‘ng cÃ³ thá»ƒ nháº­n diá»‡n vÃ  táº­n dá»¥ng Ä‘Ãºng vai trÃ² cá»§a tá»«ng loáº¡i node, tá»« Ä‘Ã³ cáº£i thiá»‡n hiá»‡u quáº£ cá»§a retrieval vÃ  QA.

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ chi tiáº¿t, kÃ¨m theo Ä‘á»‘i thoáº¡i â€œpháº£n biá»‡n â€“ tráº£ lá»iâ€ minh há»a rÃµ rÃ ng cÃ¡ch Assigning Seed Node Weights hoáº¡t Ä‘á»™ng trong quÃ¡ trÃ¬nh Online Retrieval cá»§a HippoRAG.

---
VÃ­ dá»¥ cá»¥ thá»ƒ
Truy váº¥n: "TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?"
Sau khi há»‡ thá»‘ng xá»­ lÃ½ truy váº¥n, ta thu Ä‘Æ°á»£c cÃ¡c thÃ nh pháº§n sau:
1. TrÃ­ch xuáº¥t tá»« truy váº¥n (Query Extraction):
  - CÃ¡c thá»±c thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t:
    - â€œThe Great Novelâ€
    - â€œtÃ¡c giáº£â€
2. Seed Nodes Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trong Knowledge Graph (KG):
  - Phrase Nodes (Ä‘áº¡i diá»‡n cho khÃ¡i niá»‡m, thÃ´ng tin trÃ­ch xuáº¥t tá»« cÃ¡c triple):
    - Node A: "tÃ¡c giáº£"
      - GiÃ¡ trá»‹ similarity (vá»›i "tÃ¡c giáº£" truy váº¥n): 0.72
    - Node B: "The Great Novel"
      - GiÃ¡ trá»‹ similarity (vá»›i tiÃªu Ä‘á» truy váº¥n): 0.68
  - Passage Nodes (Ä‘oáº¡n vÄƒn cung cáº¥p ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§):
    - Node C: Äoáº¡n vÄƒn 1: â€œThe Great Novel, má»™t tÃ¡c pháº©m kinh Ä‘iá»ƒn, Ä‘Æ°á»£c viáº¿t bá»Ÿi John Doe â€“ tÃ¡c giáº£ ná»•i tiáº¿ng vá»›i nhiá»u giáº£i thÆ°á»Ÿng vÄƒn há»c."
      - GiÃ¡ trá»‹ similarity (vá»›i truy váº¥n, dá»±a trÃªn cáº£ ngá»¯ cáº£nh vÃ  tá»« khÃ³a): 0.80
    - Node D: Äoáº¡n vÄƒn 2: Má»™t Ä‘oáº¡n vÄƒn khÃ¡c nÃ³i vá» John Doe nhÆ°ng khÃ´ng Ä‘á» cáº­p rÃµ rÃ ng Ä‘áº¿n cuá»‘n sÃ¡ch.
      - GiÃ¡ trá»‹ similarity: 0.55

---
BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh Seed Nodes vÃ  GÃ¡n Trá»ng Sá»‘ Ban Ä‘áº§u
Giáº£ sá»­ há»‡ thá»‘ng quy Æ°á»›c:
- Phrase Nodes nháº­n trá»ng sá»‘ máº·c Ä‘á»‹nh = 1.0
- Passage Nodes ban Ä‘áº§u nháº­n trá»ng sá»‘ tháº¥p hÆ¡n, vÃ¬ chÃºng mang thÃ´ng tin cá»¥ thá»ƒ nhÆ°ng cáº§n Ä‘Æ°á»£c â€œÄ‘iá»u chá»‰nhâ€ qua weight factor.
  - VÃ­ dá»¥: há»‡ thá»‘ng gÃ¡n weight factor cho Passage Nodes = 0.05.
Do Ä‘Ã³, ta cÃ³:
- Node A ("tÃ¡c giáº£"):
  - Similarity score = 0.72 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.72 Ã— 1.0 = 0.72
- Node B ("The Great Novel"):
  - Similarity score = 0.68 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.68 Ã— 1.0 = 0.68
- Node C (Passage 1):
  - Similarity score = 0.80 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.80 Ã— 0.05 = 0.04
- Node D (Passage 2):
  - Similarity score = 0.55 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.55 Ã— 0.05 = 0.0275

---
BÆ°á»›c 2: Äá»‘i thoáº¡i pháº£n biá»‡n â€“ Giáº£i thÃ­ch chi tiáº¿t
Pháº£n biá»‡n 1: Táº¡i sao Passage Nodes láº¡i Ä‘Æ°á»£c nhÃ¢n vá»›i weight factor tháº¥p (0.05) trong khi Phrase Nodes giá»¯ nguyÃªn trá»ng sá»‘ 1.0?
Q: Náº¿u Passage Nodes mang toÃ n bá»™ ngá»¯ cáº£nh cá»§a Ä‘oáº¡n vÄƒn, táº¡i sao chÃºng láº¡i Ä‘Æ°á»£c gÃ¡n trá»ng sá»‘ tháº¥p hÆ¡n so vá»›i Phrase Nodes?
A:
- LÃ½ do:
  - Phrase Nodes Ä‘áº¡i diá»‡n cho cÃ¡c khÃ¡i niá»‡m trá»«u tÆ°á»£ng, thÆ°á»ng cÃ³ tÃ­nh khÃ¡i quÃ¡t vÃ  cÃ³ thá»ƒ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n truy váº¥n. ChÃºng cáº§n Ä‘Æ°á»£c Æ°u tiÃªn cao Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c khÃ¡i niá»‡m chÃ­nh Ä‘Æ°á»£c nháº­n diá»‡n.
  - Passage Nodes máº·c dÃ¹ chá»©a ngá»¯ cáº£nh chi tiáº¿t, nhÆ°ng náº¿u gÃ¡n trá»ng sá»‘ quÃ¡ cao, thÃ´ng tin quÃ¡ â€œphÃ¢n tÃ¡nâ€ cÃ³ thá»ƒ lÃ m nhiá»…u káº¿t quáº£ retrieval. Weight factor tháº¥p giÃºp kiá»ƒm soÃ¡t sá»± lan tá»a cá»§a thÃ´ng tin cá»¥ thá»ƒ, chá»‰ khi mÃ  cÃ¡c passage cÃ³ nhiá»u liÃªn káº¿t vá»›i cÃ¡c Phrase Nodes, chÃºng má»›i â€œÄ‘Æ°á»£c bÃ¹ Ä‘áº¯pâ€ thÃ´ng qua quÃ¡ trÃ¬nh PPR.
- Dáº«n chá»©ng thá»±c nghiá»‡m:
  - BÃ i bÃ¡o trÃ¬nh bÃ y cÃ¡c thÃ­ nghiá»‡m ablation vá»›i cÃ¡c giÃ¡ trá»‹ weight factor khÃ¡c nhau (vÃ­ dá»¥: 0.01, 0.05, 0.1,...) cho Passage Nodes. Káº¿t quáº£ cho tháº¥y, khi chá»n weight factor phÃ¹ há»£p (0.05 trong vÃ­ dá»¥ nÃ y), há»‡ thá»‘ng Ä‘áº¡t Recall@5 tá»‘i Æ°u. Äiá»u nÃ y chá»©ng tá» ráº±ng viá»‡c cÃ¢n báº±ng giá»¯a khÃ¡i niá»‡m vÃ  ngá»¯ cáº£nh lÃ  ráº¥t quan trá»ng.

---
Pháº£n biá»‡n 2: Viá»‡c nhÃ¢n weight factor áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘áº¿n káº¿t quáº£ ban Ä‘áº§u cá»§a PPR?
Q: Vá»›i cÃ¡c trá»ng sá»‘ ban Ä‘áº§u nhÆ° trÃªn, lÃ m tháº¿ nÃ o Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng Passage Node C â€“ máº·c dÃ¹ cÃ³ similarity score cao (0.80) â€“ cÃ³ thá»ƒ cáº¡nh tranh Ä‘Æ°á»£c vá»›i Phrase Nodes khi weight factor tháº¥p?
A:
- Giáº£i thÃ­ch:
  - Ban Ä‘áº§u, Passage Node C cÃ³ giÃ¡ trá»‹ trá»ng sá»‘ chá»‰ lÃ  0.04, so vá»›i 0.72 (Node A) vÃ  0.68 (Node B).
  - Tuy nhiÃªn, trong quÃ¡ trÃ¬nh PPR, trá»ng sá»‘ nÃ y sáº½ Ä‘Æ°á»£c â€œlan tá»aâ€ qua cÃ¡c cáº¡nh trong Knowledge Graph. Náº¿u Passage Node C Ä‘Æ°á»£c liÃªn káº¿t máº¡nh máº½ vá»›i cÃ¡c Phrase Nodes cÃ³ trá»ng sá»‘ cao, cÃ¡c cáº¡nh liÃªn káº¿t nÃ y sáº½ giÃºp â€œkÃ©oâ€ thÃªm probability mass cho Node C.
  - VÃ­ dá»¥: náº¿u Node C cÃ³ nhiá»u liÃªn káº¿t Ä‘áº¿n Node A vÃ  Node B, quÃ¡ trÃ¬nh PPR sáº½ cá»™ng dá»“n thÃªm giÃ¡ trá»‹ cho Node C, giÃºp nÃ³ Ä‘Æ°á»£c Æ°u tiÃªn khi xáº¿p háº¡ng cÃ¡c Ä‘oáº¡n vÄƒn cuá»‘i cÃ¹ng.
- Dáº«n chá»©ng:
  - BÃ i bÃ¡o cho biáº¿t ráº±ng káº¿t há»£p giá»¯a distribution tá»« PPR vÃ  ma tráº­n xuáº¥t hiá»‡n (P matrix) cho phÃ©p cÃ¡c passage nháº­n Ä‘Æ°á»£c Ä‘iá»ƒm tá»•ng há»£p cao hÆ¡n, náº¿u chÃºng Ä‘Æ°á»£c liÃªn káº¿t cháº·t cháº½ vá»›i cÃ¡c Phrase Nodes cÃ³ trá»ng sá»‘ ban Ä‘áº§u cao.

---
Pháº£n biá»‡n 3: Giáº£ sá»­ cÃ³ má»™t Passage Node D cÃ³ similarity score tháº¥p (0.55 â†’ 0.0275), liá»‡u nÃ³ cÃ³ bá»‹ loáº¡i bá» hoÃ n toÃ n hay váº«n cÃ³ vai trÃ²?
Q: Passage Node D cÃ³ Ä‘iá»ƒm ban Ä‘áº§u ráº¥t tháº¥p, nhÆ°ng liá»‡u nÃ³ cÃ³ Ä‘Ã³ng gÃ³p hay khÃ´ng trong quÃ¡ trÃ¬nh retrieval?
A:
- Giáº£i thÃ­ch:
  - Máº·c dÃ¹ Passage Node D cÃ³ similarity score tháº¥p vÃ  trá»ng sá»‘ ban Ä‘áº§u chá»‰ lÃ  0.0275, náº¿u nÃ³ cÃ³ má»™t sá»‘ liÃªn káº¿t há»¯u Ã­ch trong KG, Ä‘áº·c biá»‡t lÃ  vá»›i cÃ¡c Phrase Nodes cÃ³ Ä‘iá»ƒm cao, quÃ¡ trÃ¬nh PPR cÃ³ thá»ƒ â€œnÃ¢ng Ä‘á»¡â€ giÃ¡ trá»‹ cá»§a nÃ³ qua viá»‡c cá»™ng dá»“n tá»« cÃ¡c nÃºt lÃ¢n cáº­n.
  - Tuy nhiÃªn, náº¿u khÃ´ng cÃ³ liÃªn káº¿t máº¡nh máº½ nÃ o, Node D cÃ³ thá»ƒ khÃ´ng Ä‘Ã³ng gÃ³p nhiá»u vÃ  sáº½ Ä‘Æ°á»£c xáº¿p háº¡ng tháº¥p. Äiá»u nÃ y giÃºp há»‡ thá»‘ng lá»c ra cÃ¡c passage khÃ´ng Ä‘á»§ liÃªn quan.
- Dáº«n chá»©ng:
  - Káº¿t quáº£ thá»±c nghiá»‡m vÃ  ablation study trong bÃ i bÃ¡o chá»‰ ra ráº±ng nhá»¯ng passage khÃ´ng Ä‘Æ°á»£c liÃªn káº¿t cháº·t cháº½ sáº½ nháº­n Ä‘iá»ƒm tháº¥p sau PPR, tá»« Ä‘Ã³ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ cuá»‘i cÃ¹ng.

---
BÆ°á»›c 3: TÃ­ch há»£p qua PPR
Sau khi cÃ¡c seed node Ä‘Æ°á»£c gÃ¡n trá»ng sá»‘, há»‡ thá»‘ng cháº¡y thuáº­t toÃ¡n Personalized PageRank trÃªn KG. QuÃ¡ trÃ¬nh nÃ y sáº½:
- Lan tá»a probability mass: Tá»« cÃ¡c seed node ban Ä‘áº§u, giÃ¡ trá»‹ sáº½ Ä‘Æ°á»£c lan tá»a qua cÃ¡c cáº¡nh liÃªn káº¿t.
- Káº¿t há»£p vá»›i P matrix: Sau Ä‘Ã³, probability mass Ä‘Æ°á»£c nhÃ¢n vá»›i ma tráº­n P chá»©a thÃ´ng tin táº§n sá»‘ xuáº¥t hiá»‡n cá»§a cÃ¡c nÃºt trong cÃ¡c Ä‘oáº¡n vÄƒn, giÃºp xÃ¡c Ä‘á»‹nh thá»© háº¡ng cuá»‘i cÃ¹ng cho cÃ¡c passage.
Káº¿t quáº£ cuá»‘i cÃ¹ng, máº·c dÃ¹ ban Ä‘áº§u Passage Node C chá»‰ cÃ³ trá»ng sá»‘ 0.04, nhÆ°ng náº¿u liÃªn káº¿t tá»‘t, nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c â€œnÃ¢ngâ€ lÃªn vÃ  xáº¿p háº¡ng cao, giÃºp há»‡ thá»‘ng truy xuáº¥t Ä‘oáº¡n vÄƒn chá»©a thÃ´ng tin â€œJohn Doe lÃ  tÃ¡c giáº£ cá»§a The Great Novelâ€ má»™t cÃ¡ch chÃ­nh xÃ¡c.

---
Tá»•ng káº¿t vÃ­ dá»¥
1. TrÃ­ch xuáº¥t vÃ  xÃ¡c Ä‘á»‹nh Seed Nodes:
  - Phrase Nodes: "tÃ¡c giáº£" (0.72), "The Great Novel" (0.68)
  - Passage Nodes: Passage 1 (0.80 â†’ 0.04 sau nhÃ¢n), Passage 2 (0.55 â†’ 0.0275)
2. GÃ¡n trá»ng sá»‘:
  - Phrase Nodes giá»¯ trá»ng sá»‘ Ä‘áº§y Ä‘á»§ (1.0)
  - Passage Nodes nhÃ¢n vá»›i weight factor (0.05)
3. QuÃ¡ trÃ¬nh PPR:
  - Lan tá»a probability mass tá»« cÃ¡c seed node qua cÃ¡c cáº¡nh cá»§a KG.
  - CÃ¡c Passage Nodes, máº·c dÃ¹ cÃ³ trá»ng sá»‘ ban Ä‘áº§u tháº¥p, sáº½ nháº­n Ä‘Æ°á»£c thÃªm giÃ¡ trá»‹ náº¿u Ä‘Æ°á»£c liÃªn káº¿t máº¡nh máº½ vá»›i cÃ¡c Phrase Nodes.
4. Káº¿t quáº£ cuá»‘i cÃ¹ng:
  - Passage Node chá»©a Ä‘oáº¡n vÄƒn chÃ­nh vá» John Doe sáº½ Ä‘Æ°á»£c xáº¿p háº¡ng cao, Ä‘áº£m báº£o há»‡ thá»‘ng tráº£ lá»i chÃ­nh xÃ¡c cho truy váº¥n "TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?".
Qua vÃ­ dá»¥ nÃ y, chÃºng ta tháº¥y ráº±ng viá»‡c Assigning Seed Node Weights khÃ´ng chá»‰ lÃ  má»™t thao tÃ¡c Ä‘á»‹nh lÆ°á»£ng ban Ä‘áº§u, mÃ  cÃ²n lÃ  bÆ°á»›c quyáº¿t Ä‘á»‹nh Ä‘á»ƒ cÃ¢n báº±ng giá»¯a thÃ´ng tin khÃ¡i niá»‡m vÃ  ngá»¯ cáº£nh, tá»« Ä‘Ã³ tá»‘i Æ°u quÃ¡ trÃ¬nh PPR vÃ  cáº£i thiá»‡n hiá»‡u quáº£ cá»§a há»‡ thá»‘ng truy xuáº¥t.
ÄÃºng váº­y, khi nÃ³i vá» trá»ng sá»‘ máº·c Ä‘á»‹nh cho Phrase Nodes, ta cÃ³ thá»ƒ Ä‘áº·t weight factor lÃ  1.0, nhÆ°ng Ä‘iá»u nÃ y khÃ´ng cÃ³ nghÄ©a lÃ  giÃ¡ trá»‹ similarity cá»§a tá»«ng node sáº½ luÃ´n lÃ  1.0.
Cá»¥ thá»ƒ:
- Trá»ng sá»‘ (weight factor) cá»§a Phrase Nodes: Khi gÃ¡n trá»ng sá»‘ cho cÃ¡c Phrase Nodes, há»‡ thá»‘ng sá»­ dá»¥ng há»‡ sá»‘ 1.0 Ä‘á»ƒ giá»¯ nguyÃªn giÃ¡ trá»‹ similarity ban Ä‘áº§u.
- GiÃ¡ trá»‹ similarity: ÄÃ¢y lÃ  má»™t chá»‰ sá»‘ Ä‘Æ°á»£c tÃ­nh dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a embedding cá»§a thá»±c thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« truy váº¥n vÃ  embedding cá»§a node trong Knowledge Graph. GiÃ¡ trá»‹ nÃ y thÆ°á»ng náº±m trong khoáº£ng tá»« 0 Ä‘áº¿n 1, vÃ  khÃ´ng pháº£i lÃºc nÃ o cÅ©ng báº±ng 1.0. VÃ­ dá»¥, Node A cÃ³ similarity lÃ  0.72 vÃ  Node B cÃ³ similarity lÃ  0.68, Ä‘iá»u nÃ y pháº£n Ã¡nh má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng (khÃ´ng pháº£i lÃ  â€œtrá»ng sá»‘â€ mÃ  lÃ  káº¿t quáº£ cá»§a phÃ©p Ä‘o similarity).
TÃ³m láº¡i, máº·c Ä‘á»‹nh weight factor cá»§a Phrase Nodes lÃ  1.0, nghÄ©a lÃ  chÃºng ta khÃ´ng Ä‘iá»u chá»‰nh thÃªm giÃ¡ trá»‹ similarity cá»§a chÃºng, nhÆ°ng giÃ¡ trá»‹ similarity thá»±c sá»± (0.72, 0.68, â€¦) phá»¥ thuá»™c vÃ o quÃ¡ trÃ¬nh tÃ­nh toÃ¡n tá»« cÃ¡c embedding chá»© khÃ´ng pháº£i lÃ  1.0 luÃ´n.

---
DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ chi tiáº¿t kÃ¨m Ä‘á»‘i thoáº¡i pháº£n biá»‡n â€“ â€œngÆ°á»i phÃ¡ â€“ ngÆ°á»i tráº£ lá»iâ€ â€“ Ä‘á»ƒ minh há»a cÃ¡ch Seed Node Weights vÃ  thuáº­t toÃ¡n Personalized PageRank (PPR) hoáº¡t Ä‘á»™ng cÃ¹ng nhau trong HippoRAG.

---
Bá»‘i cáº£nh vÃ­ dá»¥
Giáº£ sá»­ truy váº¥n lÃ :
"TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?"
Sau khi trÃ­ch xuáº¥t thÃ´ng tin tá»« truy váº¥n vÃ  xÃ¢y dá»±ng Knowledge Graph (KG), há»‡ thá»‘ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c cÃ¡c seed node vá»›i cÃ¡c giÃ¡ trá»‹ similarity nhÆ° sau:
- Phrase Node A: "tÃ¡c giáº£"
  - Similarity score = 0.72
  - Weight factor (Phrase) = 1.0 â†’ Effective weight = 0.72
- Phrase Node B: "The Great Novel"
  - Similarity score = 0.68
  - Weight factor (Phrase) = 1.0 â†’ Effective weight = 0.68
- Passage Node C: Äoáº¡n vÄƒn 1 chá»©a thÃ´ng tin:
 â€œThe Great Novel, má»™t tÃ¡c pháº©m kinh Ä‘iá»ƒn, Ä‘Æ°á»£c viáº¿t bá»Ÿi John Doe â€“ tÃ¡c giáº£ ná»•i tiáº¿ngâ€
  - Similarity score = 0.80
  - Weight factor (Passage) = 0.05 â†’ Effective weight = 0.80 Ã— 0.05 = 0.04
- Passage Node D: Äoáº¡n vÄƒn 2 nÃ³i chung vá» John Doe nhÆ°ng khÃ´ng nháº¯c trá»±c tiáº¿p tiÃªu Ä‘á»
  - Similarity score = 0.55
  - Weight factor (Passage) = 0.05 â†’ Effective weight = 0.55 Ã— 0.05 = 0.0275

---
Äá»‘i thoáº¡i pháº£n biá»‡n â€“ CÃ¡ch PPR hoáº¡t Ä‘á»™ng vá»›i Seed Node Weights
Pháº£n biá»‡n 1: Khá»Ÿi táº¡o Seed Vector
Q: LÃ m tháº¿ nÃ o ta khá»Ÿi táº¡o vector seed cho PPR vá»›i cÃ¡c seed node trÃªn?
A:
- Ta táº¡o má»™t vector ss chá»©a cÃ¡c giÃ¡ trá»‹ khá»Ÿi táº¡o cho cÃ¡c node theo effective weight:
  - sA=0.72s_A = 0.72 (Phrase node A)
  - sB=0.68s_B = 0.68 (Phrase node B)
  - sC=0.04s_C = 0.04 (Passage node C)
  - sD=0.0275s_D = 0.0275 (Passage node D)
- Sau Ä‘Ã³, ta chuáº©n hÃ³a vector ss sao cho tá»•ng cÃ¡c pháº§n tá»­ báº±ng 1. VÃ­ dá»¥:
  - Tá»•ng S=0.72+0.68+0.04+0.0275=1.4675S = 0.72 + 0.68 + 0.04 + 0.0275 = 1.4675
  - Vector seed chuáº©n hÃ³a:
    - sAâ€²=0.72/1.4675â‰ˆ0.49s'_A = 0.72 / 1.4675 \approx 0.49
    - sBâ€²=0.68/1.4675â‰ˆ0.46s'_B = 0.68 / 1.4675 \approx 0.46
    - sCâ€²=0.04/1.4675â‰ˆ0.027s'_C = 0.04 / 1.4675 \approx 0.027
    - sDâ€²=0.0275/1.4675â‰ˆ0.019s'_D = 0.0275 / 1.4675 \approx 0.019
NhÆ° váº­y, cÃ¡c Phrase Nodes chiáº¿m Æ°u tháº¿ ngay tá»« Ä‘áº§u so vá»›i cÃ¡c Passage Nodes do effective weight cá»§a chÃºng cao hÆ¡n.

---
Pháº£n biá»‡n 2: QuÃ¡ trÃ¬nh PPR â€“ Lan tá»a xÃ¡c suáº¥t
Q: PPR sáº½ lÃ m gÃ¬ vá»›i vector seed Ä‘Ã£ chuáº©n hÃ³a nÃ y?
A:
- Thuáº­t toÃ¡n PPR thá»±c hiá»‡n cáº­p nháº­t xÃ¡c suáº¥t theo cÃ´ng thá»©c:
- v(t+1)=(1âˆ’d)â‹…sâ€²+dâ‹…Tâ‹…v(t)v^{(t+1)} = (1-d) \cdot s' + d \cdot T \cdot v^{(t)}
- trong Ä‘Ã³:
  - v(t)v^{(t)} lÃ  vector xÃ¡c suáº¥t á»Ÿ vÃ²ng láº·p thá»© tt
  - sâ€²s' lÃ  vector seed (Ä‘Ã£ chuáº©n hÃ³a)
  - TT lÃ  ma tráº­n chuyá»ƒn Ä‘á»•i (transition matrix) cá»§a KG, xÃ¡c Ä‘á»‹nh xÃ¡c suáº¥t chuyá»ƒn tá»« node nÃ y sang node khÃ¡c dá»±a trÃªn cÃ¡c cáº¡nh liÃªn káº¿t
  - dd lÃ  damping factor (thÆ°á»ng khoáº£ng 0.5â€“0.85)
- QuÃ¡ trÃ¬nh nÃ y sáº½ â€œlan tá»aâ€ cÃ¡c giÃ¡ trá»‹ tá»« cÃ¡c seed node ban Ä‘áº§u theo cÃ¡c liÃªn káº¿t trong Ä‘á»“ thá»‹. CÃ¡c Phrase Nodes vá»›i giÃ¡ trá»‹ cao ban Ä‘áº§u (0.49 vÃ  0.46) sáº½ â€œÄ‘áº©yâ€ xÃ¡c suáº¥t sang cÃ¡c nÃºt lÃ¢n cáº­n. Náº¿u Passage Node C (vá»›i giÃ¡ trá»‹ ban Ä‘áº§u 0.027) cÃ³ cÃ¡c cáº¡nh máº¡nh liÃªn káº¿t vá»›i Phrase Nodes, nÃ³ cÃ³ thá»ƒ nháº­n thÃªm xÃ¡c suáº¥t tá»« chÃºng.

---
Pháº£n biá»‡n 3: VÃ­ dá»¥ sá»‘ há»c trong quÃ¡ trÃ¬nh PPR
Q: HÃ£y láº¥y má»™t vÃ²ng láº·p PPR minh há»a cá»¥ thá»ƒ cho vÃ­ dá»¥ nÃ y.
A:
- Giáº£ sá»­ ma tráº­n chuyá»ƒn Ä‘á»•i TT cá»§a má»™t pháº§n nhá» trong KG Ä‘Æ°á»£c Ä‘Æ¡n giáº£n hoÃ¡ nhÆ° sau (chá»‰ xÃ©t 4 node trÃªn):
  - TABT_{AB} lÃ  xÃ¡c suáº¥t tá»« node A sang B, TACT_{AC} tá»« A sang C, v.vâ€¦
- VÃ­ dá»¥, ta giáº£ sá»­:
  - Node A (Phrase "tÃ¡c giáº£") cÃ³ liÃªn káº¿t:
    - 60% Ä‘áº¿n Node B ("The Great Novel")
    - 40% Ä‘áº¿n Node C (Passage chá»©a thÃ´ng tin chÃ­nh)
  - Node B ("The Great Novel") liÃªn káº¿t:
    - 70% Ä‘áº¿n Node A
    - 30% Ä‘áº¿n Node D (Passage phá»¥)
  - Node C (Passage 1) liÃªn káº¿t:
    - 100% Ä‘áº¿n Node A (giáº£ sá»­ cÃ³ má»‘i liÃªn há»‡ máº¡nh vá»›i khÃ¡i niá»‡m "tÃ¡c giáº£")
  - Node D (Passage 2) liÃªn káº¿t:
    - 100% Ä‘áº¿n Node B
- Ta cÃ³ ma tráº­n TT (Ä‘Æ¡n giáº£n hoÃ¡) nhÆ° sau:
- T=[00.600.4000.70000.301.000001.000]T = \begin{bmatrix} 0   & 0.60 & 0.40 & 0 \\ 0.70 & 0   & 0    & 0.30 \\ 1.0  & 0   & 0    & 0 \\ 0   & 1.0 & 0    & 0  \end{bmatrix}
- Giáº£ sá»­ damping factor d=0.5d = 0.5. Vá»›i vector seed chuáº©n hÃ³a sâ€²=[0.49,0.46,0.027,0.019]âŠ¤s' = [0.49, 0.46, 0.027, 0.019]^\top, ta khá»Ÿi táº¡o v(0)=sâ€²v^{(0)} = s'.
- VÃ²ng láº·p thá»© 1:
 TÃ­nh v(1)=0.5â‹…sâ€²+0.5â‹…Tâ‹…v(0)v^{(1)} = 0.5 \cdot s' + 0.5 \cdot T \cdot v^{(0)}
- TrÆ°á»›c háº¿t, tÃ­nh Tâ‹…v(0)T \cdot v^{(0)}:
  - (Tâ‹…v(0))A=0âˆ—0.49+0.60âˆ—0.46+0.40âˆ—0.027+0âˆ—0.019â‰ˆ0.60Ã—0.46+0.40Ã—0.027â‰ˆ0.276+0.0108â‰ˆ0.287(T \cdot v^{(0)})_A = 0*0.49 + 0.60*0.46 + 0.40*0.027 + 0*0.019 \approx 0.60 \times 0.46 + 0.40 \times 0.027 \approx 0.276 + 0.0108 \approx 0.287
  - (Tâ‹…v(0))B=0.70âˆ—0.49+0âˆ—0.46+0âˆ—0.027+0.30âˆ—0.019â‰ˆ0.343+0.0057â‰ˆ0.3487(T \cdot v^{(0)})_B = 0.70*0.49 + 0*0.46 + 0*0.027 + 0.30*0.019 \approx 0.343 + 0.0057 \approx 0.3487
  - (Tâ‹…v(0))C=1.0âˆ—0.49+0âˆ—0.46+0âˆ—0.027+0âˆ—0.019=0.49(T \cdot v^{(0)})_C = 1.0*0.49 + 0*0.46 + 0*0.027 + 0*0.019 = 0.49
  - (Tâ‹…v(0))D=0âˆ—0.49+1.0âˆ—0.46+0âˆ—0.027+0âˆ—0.019=0.46(T \cdot v^{(0)})_D = 0*0.49 + 1.0*0.46 + 0*0.027 + 0*0.019 = 0.46
- Sau Ä‘Ã³,
- v(1)=0.5â‹…[0.49,0.46,0.027,0.019]âŠ¤+0.5â‹…[0.287,0.3487,0.49,0.46]âŠ¤v^{(1)} = 0.5 \cdot [0.49, 0.46, 0.027, 0.019]^\top + 0.5 \cdot [0.287, 0.3487, 0.49, 0.46]^\top 
- =
- [0.245+0.1435,â€‰0.23+0.17435,â€‰0.0135+0.245,â€‰0.0095+0.23][0.245 + 0.1435,\, 0.23 + 0.17435,\, 0.0135 + 0.245,\, 0.0095 + 0.23]
- =
- [0.3885,â€‰0.40435,â€‰0.2585,â€‰0.2395][0.3885,\, 0.40435,\, 0.2585,\, 0.2395]
- Giáº£i thÃ­ch:
 NhÃ¬n vÃ o v(1)v^{(1)}:
  - Node A vÃ  Node B (Phrase Nodes) cÃ³ giÃ¡ trá»‹ váº«n cao (0.3885 vÃ  0.40435).
  - Node C (Passage 1) tÄƒng tá»« 0.027 lÃªn 0.2585 nhá» nháº­n Ä‘Æ°á»£c thÃ´ng tin tá»« Node A (vÃ¬ cÃ³ liÃªn káº¿t 100% tá»« Node C sang A) vÃ  tá»« má»‘i liÃªn káº¿t cá»§a A Ä‘áº¿n C.
  - Node D (Passage 2) cÃ³ giÃ¡ trá»‹ 0.2395 nhá» liÃªn káº¿t vá»›i Node B.
QuÃ¡ trÃ¬nh nÃ y sáº½ láº·p láº¡i cho Ä‘áº¿n khi há»™i tá»¥. Káº¿t quáº£ cuá»‘i cÃ¹ng sáº½ lÃ  vector xÃ¡c suáº¥t phÃ¢n bá»• qua toÃ n bá»™ KG, trong Ä‘Ã³ cÃ¡c node cÃ³ má»‘i liÃªn káº¿t máº¡nh vá»›i seed nodes ban Ä‘áº§u sáº½ nháº­n Ä‘Æ°á»£c xÃ¡c suáº¥t cao hÆ¡n.

---
Pháº£n biá»‡n 4: áº¢nh hÆ°á»Ÿng cá»§a PPR Ä‘áº¿n viá»‡c xáº¿p háº¡ng cÃ¡c Passage
Q: Sau khi vector xÃ¡c suáº¥t Ä‘Æ°á»£c tÃ­nh xong, lÃ m sao cÃ¡c giÃ¡ trá»‹ nÃ y chuyá»ƒn thÃ nh viá»‡c xáº¿p háº¡ng cÃ¡c Ä‘oáº¡n vÄƒn tráº£ lá»i truy váº¥n?
A:
- Sau khi cÃ³ vector vv cho cÃ¡c node trong KG, há»‡ thá»‘ng sáº½ nhÃ¢n cÃ¡c giÃ¡ trá»‹ nÃ y vá»›i ma tráº­n P (P matrix), mÃ  trong Ä‘Ã³ má»—i pháº§n tá»­ biá»ƒu thá»‹ sá»‘ láº§n xuáº¥t hiá»‡n cá»§a má»™t nÃºt (phrase hoáº·c passage) trong cÃ¡c Ä‘oáº¡n vÄƒn gá»‘c.
- Tá»•ng há»£p giÃ¡ trá»‹ tá»« cÃ¡c node liÃªn quan sáº½ cho Ä‘iá»ƒm xáº¿p háº¡ng cho tá»«ng Ä‘oáº¡n vÄƒn. CÃ¡c Ä‘oáº¡n vÄƒn cÃ³ tá»•ng xÃ¡c suáº¥t cao (Ä‘áº·c biá»‡t lÃ  do nháº­n Ä‘Æ°á»£c â€œÄ‘iá»ƒm cá»™ngâ€ tá»« cÃ¡c Phrase Nodes máº¡nh vÃ  Passage Nodes cÃ³ liÃªn káº¿t tá»‘t) sáº½ Ä‘Æ°á»£c xáº¿p háº¡ng Æ°u tiÃªn.
VÃ­ dá»¥:
- Passage Node C, sau PPR, cÃ³ giÃ¡ trá»‹ tÄƒng lÃªn Ä‘Ã¡ng ká»ƒ (vÃ­ dá»¥ 0.2585), khi nhÃ¢n vá»›i sá»‘ láº§n xuáº¥t hiá»‡n vÃ  cÃ¡c liÃªn káº¿t liÃªn quan, nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¿p háº¡ng cao vÃ  chá»n lÃ m cÃ¢u tráº£ lá»i cho truy váº¥n "TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?".

---
Tá»•ng káº¿t
Qua vÃ­ dá»¥ chi tiáº¿t trÃªn, ta tháº¥y ráº±ng:
1. Seed Node Weights Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh dá»±a trÃªn similarity score vÃ  weight factor riÃªng biá»‡t cho Phrase Nodes (1.0) vÃ  Passage Nodes (0.05).
2. Vector seed sau khi chuáº©n hÃ³a cung cáº¥p Ä‘iá»ƒm khá»Ÿi Ä‘áº§u cho PPR.
3. PPR lan tá»a xÃ¡c suáº¥t tá»« cÃ¡c seed node theo cÃ¡c liÃªn káº¿t trong KG, giÃºp cÃ¡c Passage Nodes nháº­n Ä‘Æ°á»£c â€œÄ‘iá»ƒm cá»™ngâ€ tá»« cÃ¡c Phrase Nodes náº¿u chÃºng cÃ³ liÃªn káº¿t cháº·t cháº½.
4. Káº¿t quáº£ cuá»‘i cÃ¹ng sau PPR vÃ  káº¿t há»£p vá»›i ma tráº­n P sáº½ xáº¿p háº¡ng cÃ¡c Ä‘oáº¡n vÄƒn phÃ¹ há»£p Ä‘á»ƒ tráº£ lá»i truy váº¥n.
Qua Ä‘Ã³, ta tháº¥y ráº±ng viá»‡c gÃ¡n trá»ng sá»‘ seed node vÃ  á»©ng dá»¥ng PPR khÃ´ng chá»‰ giÃºp cÃ¢n báº±ng giá»¯a thÃ´ng tin khÃ¡i niá»‡m vÃ  ngá»¯ cáº£nh, mÃ  cÃ²n táº¡o Ä‘iá»u kiá»‡n cho quÃ¡ trÃ¬nh multi-hop retrieval diá»…n ra hiá»‡u quáº£ vÃ  chÃ­nh xÃ¡c.

---
[Image]

Äáº·t váº¥n Ä‘á» nhÆ° nÃ o, giáº£i quyáº¿t nhÆ° nÃ o? 
- BÃ i toÃ¡n suy luáº­n Ä‘a bÆ°á»›c. Dataset luÃ´n? 
- https://www.kaggle.com/datasets/danghuy249/rag-dataset?select=data
-  However, its reliance  on vector retrieval hinders its ability to mimic  the dynamic and interconnected nature of hu man long-term memory. Recent RAG approaches  augment vector embeddings with various struc tures like knowledge graphs to address some of  these gaps, namely sense-making and associativ ity. However, their performance on more basic  factual memory tasks drops considerably below  standard RAG. 
- Tuy nhiÃªn, viá»‡c phá»¥ thuá»™c vÃ o truy xuáº¥t vector háº¡n cháº¿ kháº£ nÄƒng cá»§a nÃ³ trong viá»‡c mÃ´ phá»ng tÃ­nh cháº¥t Ä‘á»™ng vÃ  liÃªn káº¿t cá»§a trÃ­ nhá»› dÃ i háº¡n á»Ÿ con ngÆ°á»i. CÃ¡c phÆ°Æ¡ng phÃ¡p RAG gáº§n Ä‘Ã¢y Ä‘Ã£ bá»• sung cÃ¡c cáº¥u trÃºc khÃ¡c nhau nhÆ° Ä‘á»“ thá»‹ tri thá»©c vÃ o cÃ¡c vector nhÃºng Ä‘á»ƒ giáº£i quyáº¿t má»™t sá»‘ thiáº¿u sÃ³t nÃ y, cá»¥ thá»ƒ lÃ  kháº£ nÄƒng cáº£m nháº­n vÃ  liÃªn káº¿t. Tuy nhiÃªn, hiá»‡u suáº¥t cá»§a chÃºng trong cÃ¡c nhiá»‡m vá»¥ trÃ­ nhá»› thá»±c táº¿ cÆ¡ báº£n láº¡i giáº£m Ä‘Ã¡ng ká»ƒ so vá»›i RAG tiÃªu chuáº©n. => Äá» xuáº¥t HippoRAG. 
+, Long Term Memory? (lÆ°u dáº¡ng fact cá»§a user) 
- Baseline ngÃ y xÆ°a. RAG truyá»n thá»‘ng. Dataset ngÃ y xÆ°a. 
- Reason LLms -> Äoáº¡n Retrieval Reasoning rá»“i Hippo. 
HippoRAG khÃ´ng chá» Ä‘áº¿n khi LLM lÃ m reasoning, mÃ  nÃ³ Ä‘Ã£ reasoning ngay tá»« bÆ°á»›c truy há»“i.
Vá»›i Personalized PageRank â†’ nÃ³ tÃ¬m ra node trung gian (John Smith) trÆ°á»›c khi vÃ o LLM
Tráº£ vá» Ä‘oáº¡n Ä‘Ã£ Ä‘á»§ logic â†’ LLM chá»‰ cáº§n nÃ³i láº¡i
- xAI cho cÃ¢u há»i báº±ng cÃ¡ch xÃ¢y dá»±ng KG. ÄÆ°a KG vÃ o há»c? 
- Dataset ngÃ y xÆ°a cá»§a anh Huy, ... 
-  HotpotQA

============
 ForHippoRAG2,weusetheopen-sourceLlama-3.3-70B Instruct(AI@Meta,2024)asboththeextraction(NERand  OpenIE)andtriplefilteringmodel,andweusenvidia/NV Embed-v2astheretriever.


Äá»ƒ Ä‘áº·t váº¥n Ä‘á» táº¡i sao cáº§n HippoRAG, ta cÃ³ thá»ƒ trÃ¬nh bÃ y nhÆ° sau â€“ káº¿t há»£p giá»¯a láº­p luáº­n logic vÃ  cÃ¡c phÃ¡t hiá»‡n tá»« bÃ i bÃ¡o:

---
ğŸ§  Táº¡i sao cáº§n HippoRAG?
1. Giá»›i háº¡n cá»§a RAG truyá»n thá»‘ng
Retrieval-Augmented Generation (RAG) lÃ  má»™t giáº£i phÃ¡p phá»• biáº¿n giÃºp LLM tiáº¿p cáº­n thÃ´ng tin má»›i mÃ  khÃ´ng cáº§n thay Ä‘á»•i tham sá»‘ mÃ´ hÃ¬nh. Tuy nhiÃªn, RAG truyá»n thá»‘ng chá»§ yáº¿u dá»±a vÃ o vector retrieval Ä‘Æ¡n giáº£n, dáº«n Ä‘áº¿n 2 Ä‘iá»ƒm yáº¿u lá»›n:
- Thiáº¿u kháº£ nÄƒng sense-making: KhÃ´ng thá»ƒ hiá»ƒu, diá»…n giáº£i cÃ¡c tÃ¬nh huá»‘ng dÃ i vÃ  phá»©c táº¡p má»™t cÃ¡ch liá»n máº¡ch.
- Thiáº¿u tÃ­nh liÃªn káº¿t (associativity): KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘a bÆ°á»›c giá»¯a cÃ¡c máº£nh kiáº¿n thá»©c rá»i ráº¡c nhÆ° bá»™ nhá»› dÃ i háº¡n cá»§a con ngÆ°á»i.
ğŸ“Œ VÃ­ dá»¥: Trong cÃ¡c tÃ¡c vá»¥ truy váº¥n Ä‘a bÆ°á»›c nhÆ° "NgÆ°á»i phÃ¡t minh ra thiáº¿t bá»‹ X sá»‘ng á»Ÿ Ä‘Ã¢u?", vector retrieval truyá»n thá»‘ng thÆ°á»ng tháº¥t báº¡i vÃ¬ khÃ´ng thá»ƒ ná»‘i cÃ¡c máº£nh thÃ´ng tin láº¡i (phÃ¡t minh X â†’ ngÆ°á»i A â†’ nÆ¡i sá»‘ng cá»§a A).

---
2. CÃ¡c giáº£i phÃ¡p trÆ°á»›c Ä‘Ã¢y váº«n chÆ°a Ä‘á»§
CÃ¡c há»‡ thá»‘ng RAG cáº¥u trÃºc má»›i nhÆ° RAPTOR, GraphRAG, LightRAG Ä‘Ã£ thá»­ kháº¯c phá»¥c báº±ng cÃ¡ch:
- Táº¡o tÃ³m táº¯t hoáº·c knowledge graph Ä‘á»ƒ bá»• sung ngá»¯ cáº£nh.
- Tuy nhiÃªn, láº¡i gáº·p pháº£i váº¥n Ä‘á» giáº£m hiá»‡u quáº£ trong cÃ¡c tÃ¡c vá»¥ ghi nhá»› cÆ¡ báº£n (factual QA) do nhiá»…u tá»« LLM hoáº·c cáº¥u trÃºc phá»©c táº¡p gÃ¢y quÃ¡ táº£i.
âŒ KhÃ´ng cÃ³ mÃ´ hÃ¬nh nÃ o cÃ¢n báº±ng tá»‘t cáº£ 3 khÃ­a cáº¡nh: factual memory, sense-making vÃ  associativity.

---
3. HippoRAG ra Ä‘á»i Ä‘á»ƒ mÃ´ phá»ng bá»™ nhá»› dÃ i háº¡n nhÆ° ngÆ°á»i
HippoRAG (vÃ  báº£n cáº£i tiáº¿n HippoRAG 2) Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cáº£m há»©ng tá»« cáº¥u trÃºc bá»™ nhá»› sinh há»c cá»§a con ngÆ°á»i (neocortex, hippocampus, parahippocampal region), vá»›i má»¥c tiÃªu:
- Ghi nhá»› bá»n vá»¯ng kiáº¿n thá»©c má»›i mÃ  khÃ´ng lÃ m quÃªn kiáº¿n thá»©c cÅ©.
- Káº¿t ná»‘i Ä‘Æ°á»£c nhiá»u thÃ´ng tin dáº¡ng biá»ƒu Ä‘á»“, há»— trá»£ multi-hop reasoning.
- TÄƒng tÃ­nh ngá»¯ cáº£nh vÃ  tÃ­nh liÃªn káº¿t, nhá» tÃ­ch há»£p PageRank cÃ¡ nhÃ¢n hÃ³a vÃ  biá»ƒu Ä‘á»“ tri thá»©c má»Ÿ (open KG).

---
4. Lá»£i Ã­ch cá»§a HippoRAG
Theo káº¿t quáº£ thá»±c nghiá»‡m:
- HippoRAG 2 vÆ°á»£t trá»™i 7% trong cÃ¡c bÃ i toÃ¡n truy váº¥n liÃªn káº¿t (associative tasks).
- Giá»¯ Ä‘Æ°á»£c hoáº·c cáº£i thiá»‡n nháº¹ á»Ÿ cÃ¡c tÃ¡c vá»¥ ghi nhá»› Ä‘Æ¡n (factual) vÃ  hiá»ƒu ngá»¯ cáº£nh (sense-making).
- Kháº£ nÄƒng há»c liÃªn tá»¥c phi tham sá»‘ (non-parametric continual learning) â€“ khÃ´ng cáº§n fine-tune LLM liÃªn tá»¥c.

---
Náº¿u báº¡n muá»‘n, mÃ¬nh cÃ³ thá»ƒ viáº¿t pháº§n nÃ y thÃ nh má»™t Ä‘oáº¡n motivation ngáº¯n, chuáº©n phong cÃ¡ch bÃ i nghiÃªn cá»©u hoáº·c bÃ i thuyáº¿t trÃ¬nh há»c thuáº­t. Quá»‘c muá»‘n phong cÃ¡ch nÃ o: ngáº¯n gá»n â€“ gÃ£y gá»n â€“ trÃ¬nh bÃ y váº¥n Ä‘á»? Hay sÃ¢u sáº¯c â€“ há»c thuáº­t â€“ Ä‘Æ°a dáº«n chá»©ng nghiÃªn cá»©u?


DÆ°á»›i Ä‘Ã¢y lÃ  báº£n "Äáº·t váº¥n Ä‘á» vÃ  Giáº£i phÃ¡p" cho HippoRAG, Ä‘Æ°á»£c viáº¿t theo phong cÃ¡ch há»c thuáº­t, sÃºc tÃ­ch vÃ  logic â€“ phÃ¹ há»£p Ä‘á»ƒ Ä‘Æ°a vÃ o giá»›i thiá»‡u má»™t bÃ i nghiÃªn cá»©u, thuyáº¿t trÃ¬nh chuyÃªn Ä‘á» hoáº·c Ä‘á»“ Ã¡n tá»‘t nghiá»‡p.

---
ğŸ§© Äáº·t váº¥n Ä‘á» vÃ  Giáº£i phÃ¡p
ğŸ” Äáº·t váº¥n Ä‘á»
Kháº£ nÄƒng ghi nhá»›, liÃªn káº¿t vÃ  sá»­ dá»¥ng kiáº¿n thá»©c lÃ¢u dÃ i lÃ  má»™t Ä‘áº·c trÆ°ng cá»‘t lÃµi cá»§a trÃ­ thÃ´ng minh con ngÆ°á»i. Tuy nhiÃªn, cÃ¡c há»‡ thá»‘ng AI hiá»‡n nay â€“ Ä‘áº·c biá»‡t lÃ  cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) â€“ chÆ°a thá»ƒ hiá»‡n Ä‘Æ°á»£c kháº£ nÄƒng há»c liÃªn tá»¥c vÃ  ghi nhá»› lÃ¢u dÃ i má»™t cÃ¡ch hiá»‡u quáº£.
Máº·c dÃ¹ Retrieval-Augmented Generation (RAG) Ä‘ang lÃ  giáº£i phÃ¡p chÃ­nh cho viá»‡c tiáº¿p cáº­n thÃ´ng tin ngoÃ i mÃ´ hÃ¬nh (non-parametric learning), RAG truyá»n thá»‘ng váº«n cÃ²n nhiá»u háº¡n cháº¿:
- Phá»¥ thuá»™c hoÃ n toÃ n vÃ o truy há»“i vector (vector retrieval), dáº«n Ä‘áº¿n:
  - KhÃ´ng hiá»ƒu sÃ¢u ngá»¯ cáº£nh dÃ i hoáº·c phá»©c táº¡p (thiáº¿u sense-making).
  - KhÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c thÃ´ng tin rá»i ráº¡c qua nhiá»u bÆ°á»›c (thiáº¿u associativity).
- CÃ¡c phÆ°Æ¡ng phÃ¡p cáº£i tiáº¿n nhÆ° RAPTOR hay GraphRAG dÃ¹ng LLM Ä‘á»ƒ táº¡o tÃ³m táº¯t hoáº·c graph, nhÆ°ng láº¡i gÃ¢y suy giáº£m hiá»‡u nÄƒng á»Ÿ cÃ¡c bÃ i toÃ¡n factual QA, do nhiá»…u tá»« LLM hoáº·c quÃ¡ phá»©c táº¡p trong cáº¥u trÃºc.
Tá»« Ä‘Ã³ Ä‘áº·t ra cÃ¢u há»i:
LÃ m tháº¿ nÃ o Ä‘á»ƒ xÃ¢y dá»±ng má»™t há»‡ thá»‘ng ghi nhá»› lÃ¢u dÃ i cho LLMs vá»«a máº¡nh vá» factual, vá»«a sÃ¢u sáº¯c vá» ngá»¯ cáº£nh, vÃ  linh hoáº¡t trong liÃªn káº¿t kiáº¿n thá»©c?

---
ğŸ’¡ Giáº£i phÃ¡p: HippoRAG
HippoRAG Ä‘Æ°á»£c Ä‘á» xuáº¥t nhÆ° má»™t khung ghi nhá»› láº¥y cáº£m há»©ng tá»« cáº¥u trÃºc bá»™ nhá»› cá»§a con ngÆ°á»i (gá»“m tÃ¢n vá» nÃ£o, hippocampus, vÃ  vÃ¹ng trung gian). Giáº£i phÃ¡p nÃ y bao gá»“m hai giai Ä‘oáº¡n:
1. Chá»‰ má»¥c hÃ³a ngoáº¡i tuyáº¿n (Offline Indexing):
  - DÃ¹ng LLM Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c triple kiáº¿n thá»©c dáº¡ng KG má»Ÿ tá»« tÃ i liá»‡u.
  - Táº¡o biá»ƒu Ä‘á»“ tri thá»©c gá»“m cÃ¡c node khÃ¡i niá»‡m (phrase) vÃ  node ngá»¯ cáº£nh (passage).
  - PhÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a vÃ  liÃªn káº¿t cÃ¡c khÃ¡i niá»‡m tÆ°Æ¡ng Ä‘Æ°Æ¡ng báº±ng embedding.
2. Truy há»“i trá»±c tuyáº¿n (Online Retrieval):
  - DÃ¹ng mÃ´ hÃ¬nh embedding Ä‘á»ƒ liÃªn káº¿t cÃ¢u há»i vá»›i triple vÃ  passage.
  - Ãp dá»¥ng thuáº­t toÃ¡n Personalized PageRank Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng dáº«n liÃªn káº¿t ngá»¯ nghÄ©a vÃ  truy xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan.
  - DÃ¹ng LLM Ä‘á»ƒ lá»c triple theo ngá»¯ cáº£nh truy váº¥n (bá»™ nhá»› nháº­n diá»‡n â€“ recognition memory).
ğŸ¯ Káº¿t quáº£: HippoRAG 2 vÆ°á»£t trá»™i hÆ¡n táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n táº¡i trÃªn cáº£ ba nÄƒng lá»±c cá»‘t lÃµi: ghi nhá»› fact, hiá»ƒu ngá»¯ cáº£nh vÃ  liÃªn káº¿t thÃ´ng tin.

---
Náº¿u Quá»‘c muá»‘n mÃ¬nh viáº¿t láº¡i báº£n nÃ y theo báº£n slide trÃ¬nh bÃ y, dáº¡ng bÃ i viáº¿t bÃ¡o cÃ¡o khoa há»c, hoáº·c báº£n nÃ³i thuyáº¿t trÃ¬nh miá»‡ng 2-3 phÃºt, mÃ¬nh sáº½ chuyá»ƒn thá»ƒ phÃ¹ há»£p nhÃ©. Quá»‘c muá»‘n theo hÆ°á»›ng nÃ o tiáº¿p?


Äá»ƒ mÃ¬nh giáº£i thÃ­ch chi tiáº¿t sÆ¡ Ä‘á»“ HippoRAG 2 nÃ y má»™t cÃ¡ch dá»… hiá»ƒu, chia lÃ m 2 giai Ä‘oáº¡n: Offline Indexing vÃ  Online Retrieval & QA, vá»›i cÃ¡c bÆ°á»›c rÃµ rÃ ng nhÃ©!

---
ğŸ§  1. Offline Indexing â€“ Giai Ä‘oáº¡n táº¡o bá»™ nhá»› (khá»Ÿi táº¡o biá»ƒu Ä‘á»“ tri thá»©c)
ğŸ¯ Má»¥c tiÃªu: Biáº¿n cÃ¡c Ä‘oáº¡n vÄƒn (passages) thÃ nh má»™t biá»ƒu Ä‘á»“ tri thá»©c (Knowledge Graph â€“ KG) Ä‘á»ƒ sau nÃ y dá»… truy há»“i thÃ´ng minh hÆ¡n.
Gá»“m 3 bÆ°á»›c:
ğŸ”µ (1) OpenIE by LLM
 â€“ DÃ¹ng LLM (Llama-3.3-70B) Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c triple kiáº¿n thá»©c tá»« má»—i Ä‘oáº¡n vÄƒn, dáº¡ng:
(subject â€“ relation â€“ object)
 ğŸ‘‰ VÃ­ dá»¥: ("Elon Musk", "founder of", "SpaceX")
ğŸ”µ (2) Synonym detection by embedding
 â€“ DÃ¹ng mÃ´ hÃ¬nh embedding Ä‘á»ƒ tÃ¬m cÃ¡c cáº·p tá»«/cá»¥m tá»« giá»‘ng nhau vá» nghÄ©a, rá»“i ná»‘i chÃºng láº¡i báº±ng synonym edge.
 ğŸ‘‰ VÃ­ dá»¥: â€œNYCâ€ â†” â€œNew York Cityâ€
ğŸ”µ (3) Dense-sparse integration
 â€“ Táº¡o 2 loáº¡i node trong biá»ƒu Ä‘á»“:
- Phrase node: cÃ¡c khÃ¡i niá»‡m rÃºt gá»n, nhÆ° â€œElon Muskâ€ hoáº·c â€œCEOâ€
- Passage node: chá»©a toÃ n bá»™ Ä‘oáº¡n vÄƒn â€“ Ná»‘i passage node vá»›i cÃ¡c phrase node tÆ°Æ¡ng á»©ng báº±ng context edge, Ä‘á»ƒ biá»ƒu Ä‘á»“ vá»«a cÃ³ thÃ´ng tin khÃ¡i niá»‡m, vá»«a cÃ³ ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§.
ğŸ“¦ Output cuá»‘i lÃ  má»™t Knowledge Graph giÃ u ngá»¯ nghÄ©a, vá»›i cÃ¡c loáº¡i liÃªn káº¿t:
- Relation Edge (giá»¯a concept vá»›i concept)
- Synonym Edge
- Context Edge (giá»¯a passage vÃ  concept)

---
ğŸ” 2. Online Retrieval & QA â€“ Giai Ä‘oáº¡n truy há»“i vÃ  tráº£ lá»i
ğŸ¯ Má»¥c tiÃªu: Tá»« cÃ¢u há»i (query), tÃ¬m Ä‘Ãºng cÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan Ä‘á»ƒ tráº£ lá»i.
Gá»“m 5 bÆ°á»›c:
ğŸŸ¢ (1) Truy há»“i passages vÃ  triples
 â€“ DÃ¹ng retriever (NV-Embed-v2) Ä‘á»ƒ truy há»“i cÃ¡c triple vÃ  Ä‘oáº¡n vÄƒn liÃªn quan Ä‘áº¿n cÃ¢u há»i.
ğŸŸ¢ (2) Recognition Memory â€“ Lá»c triple
 â€“ DÃ¹ng LLM Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c triple, giá»¯ láº¡i cÃ¡c triple tháº­t sá»± liÃªn quan Ä‘áº¿n cÃ¢u há»i.
ğŸŸ¢ (3) GÃ¡n trá»ng sá»‘ cho cÃ¡c seed node
 â€“ CÃ¡c triple vÃ  passage Ä‘Æ°á»£c chá»n sáº½ trá»Ÿ thÃ nh cÃ¡c â€œseed nodeâ€ (node khá»Ÿi Ä‘áº§u). â€“ GÃ¡n trá»ng sá»‘ khá»Ÿi Ä‘á»™ng (reset probability) cho tá»«ng seed dá»±a trÃªn má»©c Ä‘á»™ liÃªn quan.
ğŸŸ¢ (4) Personalized PageRank (PPR)
 â€“ Cháº¡y thuáº­t toÃ¡n PPR trÃªn biá»ƒu Ä‘á»“ tri thá»©c, Ä‘á»ƒ tÃ¬m cÃ¡c Ä‘oáº¡n vÄƒn cÃ³ káº¿t ná»‘i ngá»¯ nghÄ©a máº¡nh nháº¥t Ä‘áº¿n cÃ¢u há»i, qua nhiá»u bÆ°á»›c reasoning.
ğŸŸ¢ (5) QA Reading
 â€“ DÃ¹ng LLM Ä‘á»c cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Æ°á»£c chá»n vÃ  táº¡o cÃ¢u tráº£ lá»i cho truy váº¥n.

---
ğŸ—ºï¸ Giáº£i thÃ­ch cÃ¡c kÃ½ hiá»‡u trong sÆ¡ Ä‘á»“
This content is only supported in a Lark Docs

---
Náº¿u Quá»‘c muá»‘n, mÃ¬nh cÃ³ thá»ƒ diá»…n giáº£i láº¡i toÃ n bá»™ thÃ nh slide hoáº·c viáº¿t bÃ¡o cÃ¡o tÃ³m táº¯t pháº§n kiáº¿n trÃºc pipeline HippoRAG 2 theo hÃ¬nh nÃ y nhÃ©. Quá»‘c cáº§n báº£n nÃ o?

---
1. MOTIVATION RÃ• RÃ€NG => Má»šI TRIá»‚N?  
2. DATASET? CÃCH Há»Œ ÄÃNH GIÃ NHÆ¯ NÃ€O?
3. METHOD? Ká»¸ THUáº¬T ÄÃ“ LÃ€ GÃŒ? ? Ká»¸ THUáº¬T ÄÃ“ NHÆ¯ NÃ€O? => Äá»ŠNH NGHÄ¨A ÄÆ¯á»¢C CÃC KHÃI NIá»†M => Má»šI CÃ“ Ká»¸ THUáº¬T.  

---
Prompt 1: TÃ´i muá»‘n báº¡n pháº£i hiá»ƒu tháº­t sÃ¢u. má»—i pháº§n Ä‘áº·t cÃ¡c cÃ¢u há»i pháº£n biá»‡n sáº¯c xáº£o Ä‘á»ƒ tá»± tráº£ lá»i. Giá»‘ng nhÆ° 1 ngÆ°á»i phÃ¡, 1 ngÆ°á»i tráº£ lá»i, láº¥y dáº«n chá»©ng
Prompt 2: Giáº£i tÃ­ch tá»«ng khÃ¡i niá»‡m vÃ  liÃªn há»‡ nÃ³ vá»›i thá»© tháº­t dá»… nhá»› Ä‘Æ¡n giáº£n cho há»c sinh cáº¥p 2

ğŸ§  PHáº¦N 1: Váº¥n Ä‘á» cá»§a RAG hiá»‡n táº¡i & Sá»± xuáº¥t hiá»‡n cá»§a HippoRAG
ğŸ” Giáº£i thÃ­ch láº¡i:
RAG hiá»‡n táº¡i hoáº¡t Ä‘á»™ng theo cÆ¡ cháº¿:
- TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c chunk nhá»
- DÃ¹ng embedding â†’ lÆ°u vÃ o VectorDB
- Khi cÃ³ query â†’ tÃ¬m vector tÆ°Æ¡ng Ä‘á»“ng â†’ láº¥y context â†’ Ä‘Æ°a vÃ o LLM
NhÆ°ng vá»›i cÃ¡c tÃ¡c vá»¥ multi-hop reasoning, nhiá»u thÃ´ng tin á»Ÿ ráº£i rÃ¡c cÃ¡c Ä‘oáº¡n khÃ¡c nhau thÃ¬ RAG hiá»‡n táº¡i khÃ´ng Ä‘á»§ hiá»‡u quáº£.
PhÃ¡ 1.1 Táº¡i sao nÃ³i vá»›i cÃ¡c tÃ¡c vá»¥ multi-hop reasoning, nhiá»u thÃ´ng tin á»Ÿ ráº£i rÃ¡c cÃ¡c Ä‘oáº¡n khÃ¡c nhau thÃ¬ RAG hiá»‡n táº¡i khÃ´ng Ä‘á»§ hiá»‡u quáº£? 
Giáº£ sá»­ RAG truyá»n thá»‘ng retrieve Ä‘Æ°á»£c cáº£ hai Ä‘oáº¡n rá»i ráº¡c, thÃ¬ nÃ³ váº«n nÃ©m háº¿t vÃ o context window cá»§a LLM mÃ ? LÃºc Ä‘Ã³ LLM ná»‘i láº¡i logic lÃ  xong chá»©?

PhÃ¡ 1: RAG truyá»n thá»‘ng, trong lÃºc query ra cÃ¡c Ä‘oáº¡n chunking thÃ¬ nÃ³ cháº³ng nháº» khÃ´ng cÃ³ tÃ­nh mÃ³c ná»‘i?
â€œGiáº£ sá»­ RAG truyá»n thá»‘ng retrieve Ä‘Æ°á»£c cáº£ hai Ä‘oáº¡n rá»i ráº¡c, thÃ¬ nÃ³ váº«n nÃ©m háº¿t vÃ o context window cá»§a LLM mÃ ? LÃºc Ä‘Ã³ LLM ná»‘i láº¡i logic lÃ  xong chá»©, cáº§n gÃ¬ pháº£i HippoRAG rÆ°á»m rÃ ?â€ 
ğŸ¯ 1. KhÃ´ng cÃ³ gÃ¬ Ä‘áº£m báº£o RAG sáº½ retrieve Ä‘Ãºng 2 Ä‘oáº¡n
Váº¥n Ä‘á» lá»›n nháº¥t cá»§a RAG:
- Retrieval = vector similarity â†’ chá»‰ chá»n Top-K
- Náº¿u thÃ´ng tin náº±m á»Ÿ 2 Ä‘oáº¡n khÃ¡c nhau, vÃ  má»—i cÃ¡i Ä‘á»u â€œhÆ¡i liÃªn quanâ€, nÃ³ cÃ³ thá»ƒ bá»‹ Ä‘áº©y ra khá»i Top-K
VÃ­ dá»¥ cá»¥ thá»ƒ:
Top-K = 3
 Báº¡n cáº§n Ä‘oáº¡n A vÃ  Ä‘oáº¡n B
 NhÆ°ng cÃ³ Ä‘oáº¡n C, D ráº¥t giá»‘ng cÃ¢u há»i vá» máº·t ngá»¯ nghÄ©a â†’ lÃªn Top
 A hoáº·c B bá»‹ loáº¡i vÃ¬ vector khÃ´ng Ä‘á»§ gáº§n
ğŸ’¥ â†’ LÃºc nÃ y LLM khÃ´ng cÃ³ nguyÃªn liá»‡u Ä‘á»ƒ suy luáº­n, dÃ¹ nÃ³ giá»i cá»¡ nÃ o. 

TÄƒng k. 
Dá»¯ liá»‡u náº±m ráº£i rÃ¡c táº¡i nhiá»u nÆ¡i. 

ğŸ§  2. LLM suy luáº­n = tá»‘n compute + dá»… lá»—i logic
CÃ¢u há»i:
"Which Stanford researcher who is not Alice but studies Alzheimer's worked at the same place as Thomas?"
Dataset:
902. Alice is a Stanford professor.
903. Thomas researches Alzheimerâ€™s at UCSF.
904. Mike works at UCSF.
905. Sarah studies Alzheimerâ€™s.
906. Sarah and Thomas are labmates at UCSF.

âš ï¸ Váº¥n Ä‘á» cá»§a RAG khi retrieve Top-K
Giáº£ sá»­ retrieve Ä‘Æ°á»£c 3 Ä‘oáº¡n:
- (2) Thomas â†’ Alzheimer + UCSF
- (4) Sarah â†’ Alzheimer
- (5) Sarah â†’ labmate of Thomas â†’ UCSF
LLM pháº£i lÃ m:
1267. Gáº¯n Sarah vá»›i UCSF (tá»« labmate)
1268. Trá»« Alice khá»i káº¿t quáº£
1269. Chá»n ngÆ°á»i vá»«a Stanford researcher, vá»«a Alzheimer, vá»«a á»Ÿ UCSF, nhÆ°ng khÃ´ng pháº£i Alice

---
âœ³ï¸ ÄÃ¢y lÃ  3-hop reasoning. KhÃ´ng pháº£i vÃ¬ LLM khÃ´ng thá»ƒ, mÃ  vÃ¬:
- NÃ³ pháº£i giá»¯ á»•n Ä‘á»‹nh attention giá»¯a 4-5 Ä‘oáº¡n vÄƒn
- NÃ³ khÃ´ng biáº¿t cháº¯c má»‘i liÃªn káº¿t lÃ  quan trá»ng hay chá»‰ lÃ  â€œnhiá»…uâ€
- KhÃ´ng cÃ³ logic structure rÃµ rÃ ng Ä‘á»ƒ láº§n tá»« A â†’ B â†’ C
â¡ï¸ Khi nÃ y, retrieval dáº¡ng vector (dÃ¹ chunk tá»‘t) khÃ´ng cung cáº¥p structure reasoning.
 Chá»‰ nÃ©m Ä‘oáº¡n vÃ o context â†’ LLM pháº£i lo cáº£ Ä‘á»c, tÃ¬m logic, tá»•ng há»£p, trÃ¡nh sai lá»‡ch.

CÃ¢u há»i tiáº¿p Ä‘áº·t ra lÃ : â—â€œBÃ¢y giá» cÃ³ model 4o, Claude Opus, GPT-4-Turboâ€¦ nÃ³ cÃ³ thá»ƒ context window 128k tokens, báº¡n nÃ©m sáº¡ch vÃ´ lÃ  nÃ³ lÃ m reasoning cá»±c tá»‘t. Tháº¿ thÃ¬ lÃ m retrieval Ä‘á»ƒ lÃ m gÃ¬ ná»¯a? HippoRAG cÃ³ cÃ²n giÃ¡ trá»‹ khÃ´ng?â€
Tráº£ lá»i: ÄÃºng! NhÆ°ng chá»‰ khi báº¡n sá»‘ng trong tháº¿ giá»›i lÃ½ tÆ°á»Ÿng.

âœ³ï¸ PHáº¦N I: Thá»«a nháº­n sá»± tháº­t
âœ… ÄÃºng, model 4o, Claude Opus, GPT-4 Turbo hiá»‡n nay Ä‘Ã£ cÃ³ kháº£ nÄƒng context-wide reasoning ráº¥t máº¡nh.
ğŸ“Œ Báº¡n cÃ³ thá»ƒ Ä‘Æ°a vÃ o:
- Má»™t dataset 80k tokens gá»“m nhiá»u tÃ i liá»‡u
- Má»™t cÃ¢u há»i cáº§n ná»‘i 3â€“4 thÃ´ng tin láº¡i
- VÃ  model sáº½ tráº£ lá»i Ä‘Ãºng â€“ tháº­m chÃ­ cÃ²n giáº£i thÃ­ch táº¡i sao.

---
âœ³ï¸ PHáº¦N II: NhÆ°ng Ä‘Ã³ khÃ´ng pháº£i lÃ  cÃ¡ch lÃ m AI hiá»‡u quáº£ hoáº·c quy mÃ´
â— LÃ½ do 1: Context-wide LLM reasoning = tá»‘n compute khá»§ng khiáº¿p
NÃ©m vÃ o 128k tokens = giÃ¡ infer cao, latency cao
- Claude Opus hoáº·c GPT-4 Turbo 128k input â†’ thá»i gian tráº£ lá»i lÃ¢u hÆ¡n, chi phÃ­ tÃ­nh theo token cá»±c lá»›n (vÃ i $ má»—i lÆ°á»£t náº¿u full)
- Báº¡n khÃ´ng thá»ƒ scale 1 triá»‡u cÃ¢u há»i kiá»ƒu Ä‘Ã³

---
â— LÃ½ do 2: LLM = black box reasoning, khÃ´ng thá»ƒ debug, khÃ´ng thá»ƒ kiá»ƒm soÃ¡t
- LLM khÃ´ng cho báº¡n biáº¿t reasoning path
- Báº¡n khÃ´ng biáº¿t model ná»‘i A â†’ B â†’ C nhÆ° tháº¿ nÃ o
- Náº¿u nÃ³ tráº£ sai? Báº¡n khÃ´ng biáº¿t sá»­a tá»« Ä‘Ã¢u
â¡ï¸ Retrieval reasoning = transparent, modular, debuggable
â†’ HippoRAG = ta láº§n Ä‘Æ°á»£c Ä‘Æ°á»ng Ä‘i qua graph
 â†’ CÃ³ thá»ƒ ghi log reasoning path
 â†’ CÃ³ thá»ƒ giáº£i thÃ­ch cho ngÆ°á»i dÃ¹ng hoáº·c kiá»ƒm tra báº±ng rule engine

---
â— LÃ½ do 3: KhÃ´ng pháº£i lÃºc nÃ o cÅ©ng cÃ³ quyá»n â€œnÃ©m háº¿t vÃ oâ€
- Náº¿u user há»i liÃªn tá»¥c, má»—i cÃ¢u Ä‘á»u cáº§n toÃ n bá»™ tÃ i liá»‡u â†’ cá»±c ká»³ lÃ£ng phÃ­
- VÃ­ dá»¥: app legal, hoáº·c chatbot há»c thuáº­t, hoáº·c QA y khoa
Báº¡n cáº§n selective retrieval â†’ RAG giáº£i quyáº¿t
 Báº¡n cáº§n reasoning tá»« cÃ¡c concept, entity â†’ khÃ´ng cÃ³ sáºµn text chunk â†’ HippoRAG giáº£i quyáº¿t

ğŸ§  3. HippoRAG cho phÃ©p retrieval chÃ­nh lÃ  reasoning
ÄÃ¢y lÃ  Ä‘iá»ƒm â€œÄ‘áº­p nÃ¡t pháº£n biá»‡nâ€:
ğŸ§  HippoRAG khÃ´ng chá» Ä‘áº¿n khi LLM lÃ m reasoning, mÃ  nÃ³ Ä‘Ã£ reasoning ngay tá»« bÆ°á»›c truy há»“i.
- Vá»›i Personalized PageRank â†’ nÃ³ tÃ¬m ra node trung gian (John Smith) trÆ°á»›c khi vÃ o LLM
- Tráº£ vá» Ä‘oáº¡n Ä‘Ã£ Ä‘á»§ logic â†’ LLM chá»‰ cáº§n nÃ³i láº¡i
ğŸ’¡ Äiá»u nÃ y biáº¿n retrieval thÃ nh má»™t â€œmÃ´-Ä‘un láº­p luáº­nâ€ (reasoning module), khÃ´ng chá»‰ lÃ  â€œscanner Ä‘oáº¡n vÄƒnâ€.

PhÃ¡ 1.2 Äáº·t váº¥n Ä‘á»? 
Tuyá»‡t vá»i, Quá»‘c Ä‘ang dÃ¹ng ká»¹ thuáº­t pháº£n biá»‡n cá»±c ká»³ hay Ä‘á»ƒ â€œÄ‘á»c sÃ¢uâ€ â€” kiá»ƒu Socratic method trong nghiÃªn cá»©u. MÃ¬nh sáº½ Ä‘Ã³ng cáº£ ngÆ°á»i há»i sáº¯c sáº£o vÃ  ngÆ°á»i tráº£ lá»i cháº·t cháº½, bÃ¡m sÃ¡t pháº§n â€œ1. Introductionâ€ cá»§a bÃ i bÃ¡o HippoRAG 2.

---
ğŸ§© PHÃ‚N TÃCH PHáº¦N Äáº¶T Váº¤N Äá»€ (INTRODUCTION - HippoRAG 2)

---
âš”ï¸ Pháº£n biá»‡n 1:
Há»i: Táº¡i sao cá»© pháº£i báº¯t AI há»c nhÆ° con ngÆ°á»i? Viá»‡c mÃ´ phá»ng â€œlong-term memoryâ€ cá»§a ngÆ°á»i cÃ³ tháº­t sá»± cáº§n thiáº¿t cho LLM khÃ´ng?
Tráº£ lá»i: VÃ¬ má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a LLM lÃ  trá»Ÿ thÃ nh trá»£ lÃ½ thÃ´ng minh vÃ  hiá»‡u quáº£ nhÆ° con ngÆ°á»i. Trong tháº¿ giá»›i thá»±c, con ngÆ°á»i liÃªn tá»¥c há»c, tÃ­ch há»£p thÃ´ng tin má»›i mÃ  khÃ´ng quÃªn cÃ¡i cÅ© â€“ Ä‘Ã¢y lÃ  Ä‘iá»u LLM hiá»‡n táº¡i ráº¥t yáº¿u.
Báº±ng chá»©ng:
- LLM gáº·p váº¥n Ä‘á» catastrophic forgetting náº¿u fine-tune.
- RAG tuy tiáº¿p cáº­n tá»‘t hÆ¡n nhÆ°ng váº«n khÃ´ng cÃ³ kháº£ nÄƒng káº¿t ná»‘i máº¡ch láº¡c, nhÆ° ngÆ°á»i nhá»› chuyá»‡n Ä‘Ã£ xáº£y ra tá»« lÃ¢u vÃ  liÃªn káº¿t nÃ³ vá»›i bá»‘i cáº£nh hiá»‡n táº¡i.
VÃ¬ tháº¿, muá»‘n LLM trá»Ÿ thÃ nh trá»£ lÃ½ lÃ¢u dÃ i, cáº­p nháº­t liÃªn tá»¥c, nÃ³ cáº§n má»™t há»‡ thá»‘ng bá»™ nhá»› dÃ i háº¡n linh hoáº¡t vÃ  liÃªn káº¿t â€“ giá»‘ng nhÆ° cá»§a ngÆ°á»i.

---
âš”ï¸ Pháº£n biá»‡n 2:
Há»i: RAG Ä‘ang cháº¡y tá»‘t mÃ . Táº¡i sao láº¡i nÃ³i RAG truyá»n thá»‘ng khÃ´ng Ä‘á»§? CÃ³ báº±ng chá»©ng gÃ¬?
Tráº£ lá»i: RAG truyá»n thá»‘ng chá»§ yáº¿u dá»±a vÃ o vector retrieval Ä‘á»™c láº­p, má»—i chunk Ä‘Æ°á»£c xáº¿p háº¡ng dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng tá»± embedding vá»›i query â€“ khÃ´ng cÃ³ cÆ¡ cháº¿ Ä‘á»ƒ hiá»ƒu má»‘i quan há»‡ giá»¯a cÃ¡c chunk, khÃ´ng hiá»ƒu máº¡ch truyá»‡n, khÃ´ng lÃ m multi-hop reasoning.
Báº±ng chá»©ng cá»¥ thá»ƒ trong bÃ i:
â€œ...inability to capture two vital aspects: sense-making (hiá»ƒu ngá»¯ cáº£nh phá»©c táº¡p) vÃ  associativity (káº¿t ná»‘i Ä‘a bÆ°á»›c giá»¯a thÃ´ng tin rá»i ráº¡c)â€ â€“ (trang 1)
VÃ¬ váº­y, tuy RAG tá»‘t trong viá»‡c â€œtÃ¬m thÃ´ng tin gáº§n nháº¥tâ€, nhÆ°ng:
- KhÃ´ng liÃªn káº¿t cÃ¡c Ä‘oáº¡n kiáº¿n thá»©c láº¡i vá»›i nhau.
- KhÃ´ng biáº¿t â€œÄ‘i vÃ²ngâ€ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i yÃªu cáº§u reasoning.
=> ÄÃ³ lÃ  lÃ½ do cáº§n má»™t mÃ´ hÃ¬nh vÆ°á»£t RAG truyá»n thá»‘ng.

---
âš”ï¸ Pháº£n biá»‡n 3:
Há»i: Tháº¿ cÃ²n cÃ¡c mÃ´ hÃ¬nh má»›i nhÆ° GraphRAG, RAPTOR? Há» Ä‘Ã£ cá»‘ kháº¯c phá»¥c rá»“i mÃ ?
Tráº£ lá»i: ÄÃºng, nhÆ°ng theo tÃ¡c giáº£:
- Nhá»¯ng mÃ´ hÃ¬nh Ä‘Ã³ táº­p trung quÃ¡ háº¹p vÃ o má»™t khÃ­a cáº¡nh (sense-making hoáº·c associativity) nÃªn máº¥t cÃ¢n báº±ng.
- Khi lÃ m tá»‘t pháº§n nÃ y thÃ¬ láº¡i suy giáº£m hiá»‡u nÄƒng á»Ÿ pháº§n khÃ¡c. VÃ­ dá»¥:
  - GraphRAG tá»‘t cho discourse â†’ kÃ©m factual QA.
  - RAPTOR tÃ³m táº¯t tá»‘t â†’ nhÆ°ng gÃ¢y nhiá»…u khi QA nhiá»u bÆ°á»›c.
  - HippoRAG 1 tá»‘t vá»›i multi-hop â†’ nhÆ°ng thiáº¿u contextualization.
BÃ i bÃ¡o viáº¿t:
â€œ...each method experiences the largest performance decay in tasks outside its own setup.â€ â†’ tá»©c lÃ  mÃ´ hÃ¬nh tá»‘t chá»— nÃ y thÃ¬ gÃ£y chá»— kia.

---
âš”ï¸ Pháº£n biá»‡n 4:
Há»i: Váº­y má»¥c tiÃªu chÃ­nh cá»§a HippoRAG lÃ  gÃ¬? KhÃ¡c biá»‡t cá»‘t lÃµi so vá»›i RAG vÃ  cÃ¡c mÃ´ hÃ¬nh má»Ÿ rá»™ng?
Tráº£ lá»i:
 HippoRAG nháº¯m tá»›i má»™t bá»™ nhá»› dÃ i háº¡n tá»•ng quÃ¡t, cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t á»Ÿ:
- Factual QA (ghi nhá»› sá»± kiá»‡n cÆ¡ báº£n),
- Multi-hop QA (liÃªn káº¿t kiáº¿n thá»©c),
- Discourse understanding (diá»…n giáº£i máº¡ch truyá»‡n dÃ i).
Cá»‘t lÃµi:
- MÃ´ phá»ng bá»™ nhá»› sinh há»c con ngÆ°á»i:
  - Neocortex â†’ LLM
  - Parahippocampal â†’ encoder gá»£i nhá»› ngá»¯ nghÄ©a
  - Hippocampus â†’ KG má»Ÿ káº¿t ná»‘i tri thá»©c
- DÃ¹ng Personalized PageRank (PPR) Ä‘á»ƒ tÃ¬m kiáº¿m cÃ³ liÃªn káº¿t â€“ khÃ´ng chá»‰ Ä‘iá»ƒm, mÃ  lÃ  Ä‘Æ°á»ng Ä‘i kiáº¿n thá»©c.
- ThÃªm LLM vÃ o retrieval Ä‘á»ƒ lá»c nhiá»…u â†’ khÃ´ng chá»‰ tÃ¬m, mÃ  cÃ²n nháº­n diá»‡n Ä‘Ãºng Ä‘oáº¡n cáº§n nhá»›.

---
âš”ï¸ Pháº£n biá»‡n 5:
Há»i: Váº­y HippoRAG cÃ³ báº±ng chá»©ng nÃ o máº¡nh Ä‘á»ƒ chá»©ng minh lÃ  cáº§n thiáº¿t khÃ´ng?
Tráº£ lá»i:
- HippoRAG 2 Ä‘áº¡t Ä‘iá»ƒm cao nháº¥t á»Ÿ cáº£ 3 loáº¡i nhiá»‡m vá»¥ (Factual, Associative, Sense-making) â€“ Ä‘iá»u mÃ  khÃ´ng mÃ´ hÃ¬nh nÃ o trÆ°á»›c Ä‘Ã³ lÃ m Ä‘Æ°á»£c.
- VÃ­ dá»¥:
  - +7 Ä‘iá»ƒm F1 á»Ÿ Associativity tasks so vá»›i embedding tá»‘t nháº¥t (NV-Embed-v2).
  - KhÃ´ng bá»‹ suy giáº£m á»Ÿ Factual QA nhÆ° GraphRAG, RAPTOR.
=> ÄÃ¢y lÃ  báº±ng chá»©ng cho tháº¥y HippoRAG cÃ³ thá»ƒ cÃ¢n báº±ng giá»¯a sá»± chÃ­nh xÃ¡c, kháº£ nÄƒng diá»…n giáº£i vÃ  káº¿t ná»‘i tri thá»©c, Ä‘iá»u RAG truyá»n thá»‘ng khÃ´ng lÃ m Ä‘Æ°á»£c.

---
ChÃ o Quá»‘c,
MÃ¬nh sáº½ giáº£i thÃ­ch chi tiáº¿t vá» ba loáº¡i nhiá»‡m vá»¥: Factual, Associative, Sense-making, vÃ  má»‘i liÃªn há»‡ giá»¯a cÃ¡c nhiá»‡m vá»¥ nÃ y vá»›i bá»™ dá»¯ liá»‡u LongMemEval.

---
1. Ba loáº¡i nhiá»‡m vá»¥: Factual, Associative, Sense-making
a. Factual Tasks (Nhiá»‡m vá»¥ thá»±c táº¿)
- Äá»‹nh nghÄ©a: YÃªu cáº§u há»‡ thá»‘ng truy xuáº¥t vÃ  cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c vá» má»™t sá»± kiá»‡n hoáº·c thá»±c táº¿ cá»¥ thá»ƒ.îˆ†
- VÃ­ dá»¥: "Ai lÃ  tá»•ng thá»‘ng Ä‘áº§u tiÃªn cá»§a Hoa Ká»³?"îˆ†
- Má»¥c tiÃªu: Kiá»ƒm tra kháº£ nÄƒng ghi nhá»› vÃ  truy xuáº¥t thÃ´ng tin chÃ­nh xÃ¡c tá»« bá»™ nhá»› dÃ i háº¡n.îˆ†
b. Associative Tasks (Nhiá»‡m vá»¥ liÃªn káº¿t)
- Äá»‹nh nghÄ©a: YÃªu cáº§u há»‡ thá»‘ng káº¿t ná»‘i nhiá»u máº£nh thÃ´ng tin rá»i ráº¡c Ä‘á»ƒ suy luáº­n hoáº·c tráº£ lá»i cÃ¢u há»i.îˆ†
- VÃ­ dá»¥: "NgÆ°á»i phÃ¡t minh ra Ä‘iá»‡n thoáº¡i Ä‘Ã£ sá»‘ng á»Ÿ Ä‘Ã¢u?"îˆ†
  - PhÃ¢n tÃ­ch: Cáº§n biáº¿t Alexander Graham Bell lÃ  ngÆ°á»i phÃ¡t minh ra Ä‘iá»‡n thoáº¡i vÃ  Ã´ng sá»‘ng á»Ÿ Scotland vÃ  Canada.îˆ†
- Má»¥c tiÃªu: ÄÃ¡nh giÃ¡ kháº£ nÄƒng liÃªn káº¿t vÃ  suy luáº­n tá»« nhiá»u nguá»“n thÃ´ng tin khÃ¡c nhau.îˆ†
c. Sense-making Tasks (Nhiá»‡m vá»¥ hiá»ƒu ngá»¯ cáº£nh)
- Äá»‹nh nghÄ©a: YÃªu cáº§u há»‡ thá»‘ng hiá»ƒu vÃ  diá»…n giáº£i cÃ¡c tÃ¬nh huá»‘ng phá»©c táº¡p hoáº·c cÃ¢u chuyá»‡n dÃ i.îˆ†
- VÃ­ dá»¥: "TÃ³m táº¯t ná»™i dung chÃ­nh cá»§a cuá»™c há»p hÃ´m qua."îˆ†
- Má»¥c tiÃªu: Kiá»ƒm tra kháº£ nÄƒng tá»•ng há»£p, hiá»ƒu ngá»¯ cáº£nh vÃ  trÃ¬nh bÃ y láº¡i thÃ´ng tin má»™t cÃ¡ch máº¡ch láº¡c.îˆ†

---
2. Má»‘i liÃªn há»‡ giá»¯a QA vÃ  LongMemEval
LongMemEval lÃ  má»™t bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng ghi nhá»› dÃ i háº¡n cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n. Bá»™ dá»¯ liá»‡u nÃ y táº­p trung vÃ o nÄƒm kháº£ nÄƒng cá»‘t lÃµi:
1. TrÃ­ch xuáº¥t thÃ´ng tin (Information Extraction): Kháº£ nÄƒng nhá»› láº¡i thÃ´ng tin cá»¥ thá»ƒ tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c dÃ i.îˆ†
2. LÃ½ luáº­n Ä‘a phiÃªn (Multi-Session Reasoning): Kháº£ nÄƒng tá»•ng há»£p thÃ´ng tin tá»« nhiá»u phiÃªn trÃ² chuyá»‡n.îˆ†
3. Cáº­p nháº­t kiáº¿n thá»©c (Knowledge Updates): Kháº£ nÄƒng nháº­n biáº¿t vÃ  cáº­p nháº­t thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng theo thá»i gian.îˆ†
4. LÃ½ luáº­n thá»i gian (Temporal Reasoning): Nháº­n thá»©c vá» cÃ¡c khÃ­a cáº¡nh thá»i gian cá»§a thÃ´ng tin ngÆ°á»i dÃ¹ng.îˆ†
5. Tá»« chá»‘i tráº£ lá»i (Abstention): Kháº£ nÄƒng tá»« chá»‘i tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n thÃ´ng tin khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.îˆ†
Má»‘i liÃªn há»‡:
- Factual Tasks tÆ°Æ¡ng á»©ng vá»›i TrÃ­ch xuáº¥t thÃ´ng tin, nÆ¡i há»‡ thá»‘ng cáº§n truy xuáº¥t thÃ´ng tin chÃ­nh xÃ¡c tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.îˆ†
- Associative Tasks liÃªn quan Ä‘áº¿n LÃ½ luáº­n Ä‘a phiÃªn, yÃªu cáº§u há»‡ thá»‘ng káº¿t há»£p thÃ´ng tin tá»« nhiá»u phiÃªn Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c.îˆ†
- Sense-making Tasks gáº¯n liá»n vá»›i LÃ½ luáº­n thá»i gian vÃ  Cáº­p nháº­t kiáº¿n thá»©c, Ä‘Ã²i há»i há»‡ thá»‘ng hiá»ƒu ngá»¯ cáº£nh tá»•ng thá»ƒ vÃ  cáº­p nháº­t thÃ´ng tin má»›i nháº¥t cá»§a ngÆ°á»i dÃ¹ng.îˆ†

---

DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch phÃ¢n biá»‡t rÃµ rÃ ng giá»¯a Factual, Associative, Sense-making, báº±ng cÃ¡ch Ä‘i tá»« Ä‘á»‹nh nghÄ©a â vÃ­ dá»¥ cá»¥ thá»ƒ â nÄƒng lá»±c cáº§n kiá»ƒm tra:

---
ğŸ§  1. Factual Memory â€“ Ghi nhá»› sá»± tháº­t cá»¥ thá»ƒ
ğŸ§¾ Äá»‹nh nghÄ©a:
Kháº£ nÄƒng truy xuáº¥t thÃ´ng tin Ä‘Æ¡n láº», Ä‘Ã£ Ä‘Æ°á»£c nÃªu rÃµ rÃ ng trong ngá»¯ liá»‡u Ä‘áº§u vÃ o.
â“VÃ­ dá»¥ cÃ¢u há»i:
"Ai lÃ  thá»§ tÆ°á»›ng Äá»©c nÄƒm 2023?"
- ÄÃ¢y lÃ  cÃ¢u há»i Ä‘Ã²i há»i 1 Ä‘oáº¡n vÄƒn báº£n chá»©a thÃ´ng tin chÃ­nh xÃ¡c, khÃ´ng cáº§n suy luáº­n hay liÃªn káº¿t.
ğŸ¯ NÄƒng lá»±c Ä‘Æ°á»£c kiá»ƒm tra:
- Kháº£ nÄƒng lÆ°u trá»¯ & truy xuáº¥t chÃ­nh xÃ¡c thÃ´ng tin Ä‘Ã£ Ä‘á»c.
- TÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i nhá»› tÃªn, ngÃ y thÃ¡ng, con sá»‘, Ä‘á»‹nh nghÄ©a.

---
ğŸ”— 2. Associativity â€“ Káº¿t ná»‘i thÃ´ng tin rá»i ráº¡c
ğŸ§¾ Äá»‹nh nghÄ©a:
Kháº£ nÄƒng káº¿t ná»‘i nhiá»u máº£nh kiáº¿n thá»©c tá»« cÃ¡c pháº§n khÃ¡c nhau trong dá»¯ liá»‡u Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
â“VÃ­ dá»¥ cÃ¢u há»i:
"NgÆ°á»i sÃ¡ng láº­p Microsoft cÃ³ quÃª gá»‘c á»Ÿ Ä‘Ã¢u?"
- Cáº§n ná»‘i hai máº£nh thÃ´ng tin:
  1. "Bill Gates lÃ  ngÆ°á»i sÃ¡ng láº­p Microsoft"
  2. "Bill Gates sinh ra á»Ÿ Seattle"
âŸ¶ Cáº§n Ã­t nháº¥t 2 Ä‘oáº¡n dá»¯ liá»‡u, khÃ´ng cÃ³ Ä‘oáº¡n nÃ o chá»©a Ä‘áº§y Ä‘á»§ cÃ¢u tráº£ lá»i.
ğŸ¯ NÄƒng lá»±c Ä‘Æ°á»£c kiá»ƒm tra:
- Reasoning nhiá»u bÆ°á»›c
- Multi-hop retrieval
- NhÆ° cÃ¡ch nÃ£o ngÆ°á»i liÃªn tÆ°á»Ÿng vÃ  xÃ¢u chuá»—i kiáº¿n thá»©c láº¡i vá»›i nhau.

---
ğŸ“– 3. Sense-making â€“ Hiá»ƒu vÃ  tá»•ng há»£p ngá»¯ cáº£nh phá»©c táº¡p
ğŸ§¾ Äá»‹nh nghÄ©a:
Kháº£ nÄƒng hiá»ƒu má»™t tÃ¬nh huá»‘ng dÃ i, nhiá»u lá»›p, nhiá»u áº©n Ã½; tÃ¡i cáº¥u trÃºc vÃ  rÃºt ra Ã½ nghÄ©a toÃ n cá»¥c.
â“VÃ­ dá»¥ cÃ¢u há»i:
"Táº¡i sao nhÃ¢n váº­t chÃ­nh láº¡i quyáº¿t Ä‘á»‹nh bá» trá»‘n á»Ÿ cuá»‘i truyá»‡n?"
- Tráº£ lá»i Ä‘Æ°á»£c Ä‘Ã²i há»i pháº£i Ä‘á»c cáº£ máº¡ch truyá»‡n, hiá»ƒu Ä‘Æ°á»£c:
  - TÃ¢m lÃ½ nhÃ¢n váº­t
  - Diá»…n biáº¿n sá»± kiá»‡n
  - CÃ¡c yáº¿u tá»‘ xÃ£ há»™i, ngá»¯ cáº£nh
ğŸ¯ NÄƒng lá»±c Ä‘Æ°á»£c kiá»ƒm tra:
- Discourse understanding
- Cohesion & summarization
- Kháº£ nÄƒng cá»§a LLM giá»‘ng nhÆ° hiá»ƒu vÃ  phÃ¢n tÃ­ch vÄƒn há»c â€“ Ä‘iá»u mÃ  vector retrieval Ä‘Æ¡n thuáº§n khÃ´ng lÃ m Ä‘Æ°á»£c.

---
ğŸ“Š TÃ³m táº¯t so sÃ¡nh
This content is only supported in a Lark Docs

---

ğŸ§© PHáº¦N 1: Táº I SAO CÃC MÃ” HÃŒNH RAG HIá»†N Táº I Bá»Š Háº N CHáº¾?

---
âš”ï¸ Pháº£n biá»‡n 1:
Há»i: Vector-based RAG hiá»‡n táº¡i Ä‘Ã£ ráº¥t máº¡nh vá»›i embedding hiá»‡n Ä‘áº¡i. Váº­y táº¡i sao váº«n bá»‹ Ä‘Ã¡nh giÃ¡ lÃ  "thiáº¿u tÃ­nh ngÆ°á»i"?
Tráº£ lá»i: VÃ¬ vector-based RAG chá»‰ lÃ m Ä‘Æ°á»£c retrieval Ä‘á»™c láº­p: má»—i Ä‘oáº¡n vÄƒn (passage) Ä‘Æ°á»£c nhÃºng vÃ  truy xuáº¥t Ä‘á»™c láº­p. NÃ³ khÃ´ng cÃ³ kháº£ nÄƒng:
- Nháº­n biáº¿t má»‘i quan há»‡ giá»¯a cÃ¡c Ä‘oáº¡n (multi-hop)
- KhÃ´ng hiá»ƒu Ä‘Æ°á»£c máº¡ch thÃ´ng tin dÃ i háº¡n
- KhÃ´ng há»— trá»£ truy váº¥n theo dÃ²ng suy nghÄ© (reasoning chain)
Giá»‘ng nhÆ° báº¡n nhá»› Ä‘Æ°á»£c 1000 máº£nh thÃ´ng tin, nhÆ°ng khÃ´ng biáº¿t máº£nh nÃ o ná»‘i vá»›i máº£nh nÃ o â€” báº¡n khÃ´ng thá»ƒ ká»ƒ láº¡i má»™t cÃ¢u chuyá»‡n, cÅ©ng khÃ´ng thá»ƒ suy luáº­n.
ğŸ“Œ Háº¡n cháº¿ nÃ y khiáº¿n vector-based RAG tá»‘t trong factual QA, nhÆ°ng fail trong associative vÃ  sense-making.

---
âš”ï¸ Pháº£n biá»‡n 2:
Há»i: Tháº¿ cÃ²n cÃ¡c RAG má»›i nhÆ° RAPTOR, GraphRAG, LightRAG? Há» táº¡o graph, táº¡o tÃ³m táº¯t rá»“i mÃ ?
Tráº£ lá»i: ÄÃºng lÃ  há» Ä‘Ã£ thá»­:
- RAPTOR: Táº¡o cÃ¡c summary tÃ³m gá»n, rá»“i retrieve tá»« cÃ¡c Ä‘oáº¡n tÃ³m táº¯t Ä‘Ã³.
- GraphRAG: Táº¡o cÃ¡c knowledge graph tá»« entity, liÃªn káº¿t qua cá»™ng Ä‘á»“ng (community detection).
- LightRAG: Táº¡o 2 lá»›p retrieval â€“ coarse vÃ  fine, tÃ­ch há»£p vector vÃ  graph.
NhÆ°ng táº¡i sao váº«n chÆ°a Ä‘á»§?
- ChÃºng thiáº¿u cÃ¢n báº±ng: cáº£i thiá»‡n chá»— nÃ y, láº¡i giáº£m hiá»‡u quáº£ á»Ÿ chá»— khÃ¡c.
- VÃ­ dá»¥:
  - RAPTOR tá»‘t khi tÃ³m táº¯t context â†’ nhÆ°ng táº¡o nhiá»…u cho QA vÃ¬ LLM summary cÃ³ thá»ƒ bá» sÃ³t chi tiáº¿t.
  - GraphRAG dÃ¹ng graph máº¡nh â†’ nhÆ°ng khÃ´ng tÆ°Æ¡ng tÃ¡c tá»‘t vá»›i truy váº¥n, vÃ¬ query khÃ´ng liÃªn káº¿t trá»±c tiáº¿p vá»›i edge trong KG.
  - LightRAG thÃ¬ retrieval tá»‘t nhÆ°ng khÃ´ng reasoning Ä‘Æ°á»£c.
ğŸ“‰ Trong cÃ¡c benchmark tá»•ng há»£p (báº£ng 2 bÃ i bÃ¡o), táº¥t cáº£ cÃ¡c RAG cÃ³ cáº¥u trÃºc Ä‘á»u tá»¥t Ä‘iá»ƒm rÃµ rá»‡t á»Ÿ má»™t trong 3 loáº¡i nhiá»‡m vá»¥.

---
âš”ï¸ Pháº£n biá»‡n 3:
Há»i: Váº­y váº¥n Ä‘á» sÃ¢u hÆ¡n náº±m á»Ÿ Ä‘Ã¢u? VÃ¬ sao cÃ¡c mÃ´ hÃ¬nh cÅ© khÃ´ng Ä‘áº¡t Ä‘Æ°á»£c bá»™ nhá»› dÃ i háº¡n giá»‘ng con ngÆ°á»i?
Tráº£ lá»i: VÃ¬ chÃºng thiáº¿u 3 thá»© mÃ  nÃ£o ngÆ°á»i lÃ m ráº¥t tá»‘t:
1. LiÃªn káº¿t tri thá»©c dáº¡ng Ä‘á»“ thá»‹ â€“ nÃ£o ngÆ°á»i lÆ°u thÃ´ng tin theo máº¡ng liÃªn káº¿t.
2. Nháº­n diá»‡n ngá»¯ cáº£nh (recognition memory) â€“ biáº¿t thÃ´ng tin nÃ o liÃªn quan trong lÃºc truy xuáº¥t.
3. Káº¿t há»£p giá»¯a khÃ¡i niá»‡m (concept) vÃ  ngá»¯ cáº£nh (context) â€“ chá»© khÃ´ng lÆ°u rá»i ráº¡c.
CÃ¡c mÃ´ hÃ¬nh trÆ°á»›c:
- Táº­p trung vÃ o embedding hoáº·c graph, nhÆ°ng khÃ´ng káº¿t ná»‘i chÃºng láº¡i.
- KhÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c cÃ¡c query dÃ i, phá»©c táº¡p, hoáº·c cáº§n suy luáº­n nhiá»u bÆ°á»›c.
- KhÃ´ng cÃ³ cÆ¡ cháº¿ kiá»ƒm tra â€œliá»‡u Ä‘oáº¡n nÃ y cÃ³ thá»±c sá»± Ä‘Ãºng ngá»¯ cáº£nh truy váº¥n khÃ´ng?â€ (gá»i lÃ  recognition filter trong HippoRAG).

---
ğŸ§¬ PHáº¦N 2: HIPPORAG GIáº¢I QUYáº¾T THáº¾ NÃ€O?

---
âš”ï¸ Pháº£n biá»‡n 4:
Há»i: Nghe nhÆ° ai cÅ©ng cÃ³ knowledge graph rá»“i, HippoRAG cÃ³ gÃ¬ khÃ¡c?
Tráº£ lá»i: HippoRAG khÃ´ng chá»‰ cÃ³ KG, mÃ  lÃ  má»™t há»‡ thá»‘ng mÃ´ phá»ng trÃ­ nhá»› ngÆ°á»i:
- Neocortex â†’ LLM: dÃ¹ng Ä‘á»ƒ suy luáº­n, Ä‘á»c hiá»ƒu, quyáº¿t Ä‘á»‹nh.
- Hippocampus â†’ KG má»Ÿ: mÃ´ hÃ¬nh hÃ³a káº¿t ná»‘i tri thá»©c.
- Parahippocampal region â†’ embedding encoder: dÃ¹ng Ä‘á»ƒ so khá»›p vÃ  gá»£i nhá»›.
=> HippoRAG dÃ¹ng PageRank cÃ¡ nhÃ¢n hÃ³a trÃªn KG Ä‘á»ƒ tÃ¬m ra nhá»¯ng Ä‘oáº¡n liÃªn káº¿t tá»‘t nháº¥t, sau Ä‘Ã³ káº¿t há»£p láº¡i Ä‘á»ƒ feed vÃ o LLM Ä‘á»c hiá»ƒu.

---
âš”ï¸ Pháº£n biá»‡n 5:
Há»i: Váº­y so vá»›i cÃ¡c mÃ´ hÃ¬nh trÆ°á»›c, HippoRAG cÃ³ gÃ¬ vÆ°á»£t trá»™i?
Tráº£ lá»i:
 ğŸ‘‰ HippoRAG 1 Ä‘Ã£ hÆ¡n cÃ¡c mÃ´ hÃ¬nh cÅ© nhá»:
- DÃ¹ng Personalized PageRank trÃªn KG Ä‘á»ƒ lÃ m multi-hop retrieval.
- TÃ¡ch riÃªng index (offline) vÃ  truy xuáº¥t (online) â†’ tiá»‡n lá»£i cho continual learning.
NhÆ°ng nÃ³ váº«n chÆ°a Ä‘á»§, vÃ¬:
- Dá»±a quÃ¡ nhiá»u vÃ o entity (NER), nÃªn bá» lá»¡ ngá»¯ cáº£nh.
- KhÃ´ng lá»c triple nÃ o liÃªn quan tháº­t sá»± Ä‘áº¿n query.
ğŸ‘‰ HippoRAG 2 Ä‘Ã£ kháº¯c phá»¥c thÃªm:
- Query-to-Triple linking thay vÃ¬ NER â†’ káº¿t ná»‘i tá»‘t hÆ¡n.
- Recognition memory báº±ng LLM â†’ lá»c triple "rÃ¡c".
- Dense-sparse integration â†’ káº¿t há»£p phrase node (khÃ¡i niá»‡m) vÃ  passage node (ngá»¯ cáº£nh).
Káº¿t quáº£:
+7 Ä‘iá»ƒm F1 á»Ÿ associative tasks, khÃ´ng tá»¥t performance á»Ÿ factual hay discourse.

---
ğŸ” PHáº¦N 3: Tá»”NG Káº¾T â€“ HIPPORAG CÃ“ GÃŒ KHÃC VÃ€ HAY?
This content is only supported in a Lark Docs


---
[Image]
[Image]
Ráº¥t tá»‘t, Quá»‘c Ä‘ang Ä‘i Ä‘Ãºng hÆ°á»›ng cá»§a â€œnghiÃªn cá»©u nghiÃªm tÃºcâ€ â€“ khÃ´ng chá»‰ ghi nháº­n káº¿t quáº£, mÃ  pháº£i giáº£i thÃ­ch sÃ¢u sáº¯c vÃ¬ sao láº¡i ra káº¿t quáº£ Ä‘Ã³, báº±ng logic, ká»¹ thuáº­t vÃ  pháº£n biá»‡n.
MÃ¬nh sáº½ tiáº¿p tá»¥c phÃ¢n tÃ­ch theo phong cÃ¡ch:
ğŸ­ Má»™t ngÆ°á»i phÃ¡ (há»i xoÃ¡y) â€“ Má»™t ngÆ°á»i tráº£ lá»i (tráº£ Ä‘Ã²n)
 cho tá»«ng mÃ´ hÃ¬nh, dá»±a trÃªn pipeline ná»™i táº¡i vÃ  dá»¯ liá»‡u thá»±c nghiá»‡m tá»« Table 2 (QA) + Table 3 (Retrieval).

---
ğŸ§  1. RAG Gá»C
Retrieval báº±ng embedding vector, khÃ´ng cáº¥u trÃºc.
 QA Ä‘Æ¡n thuáº§n dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¢u há»i vÃ  Ä‘oáº¡n vÄƒn.

---
â“PhÃ¡: Táº¡i sao RAG gá»‘c láº¡i tá»‘t á»Ÿ factual QA (NQ, PopQA)?
âœ… Tráº£ lá»i: VÃ¬:
- CÃ¢u há»i dáº¡ng factual thÆ°á»ng chá»‰ cáº§n 1 Ä‘oáº¡n ngáº¯n chá»©a Ä‘Ãºng entity â†’ dá»… tÃ¬m báº±ng embedding similarity.
- CÃ¡c embedding hiá»‡n Ä‘áº¡i (NV-Embed-v2, GritLM) ráº¥t máº¡nh á»Ÿ semantic retrieval.
ğŸ“Œ VÃ­ dá»¥:
CÃ¢u há»i: â€œWhen was Albert Einstein born?â€ â†’ chá»‰ cáº§n Ä‘oáº¡n cÃ³ "Albert Einstein was born in 1879..." â†’ lÃ  Ä‘á»§ â†’ vector retrieval lÃ m tá»‘t.

---
â“PhÃ¡: Táº¡i sao láº¡i ráº¥t yáº¿u á»Ÿ multi-hop vÃ  sense-making?
âœ… Tráº£ lá»i: VÃ¬:
- RAG gá»‘c khÃ´ng liÃªn káº¿t thÃ´ng tin giá»¯a nhiá»u Ä‘oáº¡n â†’ khÃ´ng thá»ƒ reasoning nhiá»u bÆ°á»›c.
- KhÃ´ng cÃ³ cÆ¡ cháº¿ hiá»ƒu toÃ n truyá»‡n, ngá»¯ cáº£nh rá»™ng â†’ tháº¥t báº¡i á»Ÿ NarrativeQA.
ğŸ“Œ VÃ­ dá»¥:
CÃ¢u há»i: â€œWhat county is Erik Hortâ€™s birthplace a part of?â€ â†’ cáº§n biáº¿t: Erik Hort â†’ Montebello â†’ thuá»™c háº¡t gÃ¬?
 â†’ Vector retrieval chá»‰ tÃ¬m Ä‘Æ°á»£c Ä‘oáº¡n Ä‘áº§u tiÃªn, khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c sang Ä‘á»‹a lÃ½.

---
ğŸ§  2. RAPTOR
RAG vá»›i tÃ³m táº¯t (summarization) tá»«ng cá»¥m tÃ i liá»‡u. Dá»±a vÃ o Gaussian clustering Ä‘á»ƒ nhÃ³m rá»“i summary.

---
â“PhÃ¡: TÃ³m táº¯t thÃ¬ pháº£i giÃºp tÃ¬m nhanh hÆ¡n chá»©? Sao láº¡i tá»¥t Ä‘iá»ƒm á»Ÿ factual QA?
âœ… Tráº£ lá»i: VÃ¬:
- LLM tÃ³m táº¯t thÆ°á»ng bá» qua cÃ¡c chi tiáº¿t nhá» nhÆ° sá»‘ liá»‡u, Ä‘á»‹a Ä‘iá»ƒm, tÃªn ngÆ°á»i â†’ trong khi Ä‘Ã³ láº¡i lÃ  máº¥u chá»‘t cá»§a factual QA.
- Sau tÃ³m táº¯t, Ä‘oáº¡n gá»‘c khÃ´ng cÃ²n â†’ truy há»“i máº¥t chÃ­nh xÃ¡c.
ğŸ“Œ TÃ¡c giáº£ nÃ³i rÃµ:
"LLM summarization mechanism... introduces noise into the retrieval corpora."

---
â“PhÃ¡: Váº­y táº¡i sao váº«n tá»‘t á»Ÿ sense-making?
âœ… Tráº£ lá»i:
- VÃ¬ trong discourse dÃ i (nhÆ° NarrativeQA), Ä‘á»c tá»«ng Ä‘oáº¡n khÃ´ng Ä‘á»§ hiá»ƒu máº¡ch truyá»‡n.
- Viá»‡c gom láº¡i rá»“i tÃ³m táº¯t giÃºp LLM tháº¥y Ä‘Æ°á»£c tá»•ng thá»ƒ â†’ hiá»ƒu cÃ¢u chuyá»‡n nhÆ° ngÆ°á»i Ä‘á»c má»™t láº§n.

---
ğŸ§  3. GraphRAG
Táº¡o graph cá»™ng Ä‘á»“ng (community graph), giÃºp hiá»ƒu nhÃ³m thá»±c thá»ƒ hoáº·c Ä‘oáº¡n vÄƒn liÃªn quan.

---
â“PhÃ¡: Sao táº¡o Ä‘Æ°á»£c graph mÃ  váº«n â€œtá»¥t factualâ€?
âœ… Tráº£ lá»i:
- VÃ¬ GraphRAG khÃ´ng giá»¯ Ä‘oáº¡n vÄƒn gá»‘c, chá»‰ dÃ¹ng node graph Ä‘Æ°á»£c tá»•ng há»£p tá»« LLM.
- NÃªn chi tiáº¿t cá»¥ thá»ƒ dá»… bá»‹ máº¥t khi QA yÃªu cáº§u entity hoáº·c sá»‘ liá»‡u chÃ­nh xÃ¡c.

---
â“PhÃ¡: Váº­y nÃ³ giÃºp Ä‘Æ°á»£c gÃ¬?
âœ… Tráº£ lá»i:
- DÃ¹ng graph structure Ä‘á»ƒ khÃ´i phá»¥c cÃ¡c má»‘i liÃªn há»‡ giá»¯a thá»±c thá»ƒ â€“ sá»± kiá»‡n â€“ Ä‘á»‹a Ä‘iá»ƒm â†’ há»— trá»£ multi-hop vÃ  sense-making.
- Viá»‡c phÃ¡t hiá»‡n cá»™ng Ä‘á»“ng giÃºp nhÃ³m thÃ´ng tin tá»« cÃ¡c nguá»“n khÃ¡c nhau â†’ tÄƒng associativity.
ğŸ“Œ Káº¿t quáº£: NarrativeQA Ä‘áº¡t 23.0 (chá»‰ sau HippoRAG 2)
 â†’ nghÄ©a lÃ  nÃ³ hiá»ƒu truyá»‡n tá»‘t, nhÆ°ng khÃ´ng nhá»› chi tiáº¿t.

---
ğŸ§  4. HippoRAG 1
Dá»±a vÃ o OpenIE Ä‘á»ƒ extract triple â†’ dÃ¹ng Personalized PageRank (PPR) Ä‘á»ƒ reasoning.

---
â“PhÃ¡: Váº­y táº¡i sao HippoRAG 1 ráº¥t tá»‘t á»Ÿ associative QA?
âœ… Tráº£ lá»i:
- PPR + KG cho phÃ©p Ä‘i qua nhiá»u hop trong graph â†’ giá»‘ng nhÆ° nhá»› theo â€œÄ‘Æ°á»ng liÃªn tÆ°á»Ÿngâ€.
- Dáº¡ng QA cáº§n liÃªn káº¿t 2-3 Ä‘oáº¡n lÃ  Ä‘iá»ƒm máº¡nh cá»§a PPR.
ğŸ“Œ 2Wiki Ä‘áº¡t 71.8, cao nháº¥t trong báº£ng â†’ chá»©ng minh multi-hop cá»±c tá»‘t.

---
â“PhÃ¡: NhÆ°ng vÃ¬ sao láº¡i chÆ°a tá»‘t á»Ÿ sense-making?
âœ… Tráº£ lá»i:
- Do HippoRAG 1 chá»‰ dá»±a trÃªn entity-centric extraction (NER) â†’ thiáº¿u thÃ´ng tin ngá»¯ cáº£nh (context).
- Query khÃ´ng Ä‘Æ°á»£c embed toÃ n bá»™ Ä‘á»ƒ match triple, chá»‰ match theo entity â†’ contextualization ráº¥t yáº¿u.
ğŸ“Œ TÃ¡c giáº£ gá»i Ä‘Ã¢y lÃ  â€œentity-centric biasâ€.

---
ğŸ§  5. HippoRAG 2
Báº£n nÃ¢ng cáº¥p toÃ n diá»‡n:
- KG gá»“m cáº£ phrase node & passage node (dense-sparse integration)
- Query-to-triple matching (khÃ´ng NER)
- Triple filtering báº±ng LLM
- PPR search dÃ¹ng cáº£ passage & phrase node

---
â“PhÃ¡: Táº¡i sao mÃ´ hÃ¬nh nÃ y â€œtá»‘t Ä‘á»u cáº£ 3â€ â€” cÃ³ gÃ¬ Ä‘áº·c biá»‡t?
âœ… Tráº£ lá»i:
1. Factual QA:
- Giá»¯ nguyÃªn Ä‘oáº¡n vÄƒn gá»‘c + phrase-level KG â†’ khÃ´ng máº¥t chi tiáº¿t.
- DÃ¹ng embedding máº¡nh + triple filter â†’ giá»¯ thÃ´ng tin chuáº©n.
1. Associative QA:
- PPR graph search sÃ¢u, seed node tá»« cáº£ phrase & passage â†’ cá»±c ká»³ phÃ¹ há»£p vá»›i multi-hop reasoning.
- Filter giÃºp loáº¡i nhiá»…u â†’ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c.
2. Sense-making:
- Dense-sparse integration giÃºp giá»¯ Ä‘Æ°á»£c cáº£ khÃ¡i niá»‡m láº«n ngá»¯ cáº£nh.
- Query khÃ´ng cÃ²n bá»‹ chia nhá» nhÆ° NER â†’ cáº£ cÃ¢u Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ hiá»ƒu context â†’ cáº£i thiá»‡n máº¡nh NarrativeQA.
ğŸ“Œ Báº±ng chá»©ng:
- HippoRAG 2 Ä‘áº¡t cao nháº¥t á»Ÿ F1 NarrativeQA = 25.9
- Recall@5 = 96.3 á»Ÿ HotpotQA â†’ quÃ¡ vÆ°á»£t trá»™i!

---
ğŸ“Œ Tá»•ng Káº¿t Giáº£i ThÃ­ch (cÃ³ thá»ƒ Ä‘Æ°a vÃ o bÃ¡o cÃ¡o/luáº­n vÄƒn):
This content is only supported in a Lark Docs

---
Giá» cáº§n hiá»ƒu cÃ¡c khÃ¡i niá»‡m chi tiáº¿t cá»§a Hippo 1 vÃ  Hippo2 -- Giáº£i tÃ­ch tá»«ng khÃ¡i niá»‡m vÃ  liÃªn há»‡ nÃ³ vá»›i thá»© tháº­t dá»… nhá»› Ä‘Æ¡n giáº£n cho há»c sinh cáº¥p 2

Ráº¥t hay Quá»‘c Æ¡i, giá» mÃ¬nh sáº½ giáº£i tÃ­ch cÃ¡c khÃ¡i niá»‡m cá»‘t lÃµi trong HippoRAG 1 vÃ  HippoRAG 2, liÃªn há»‡ vá»›i vÃ­ dá»¥ Ä‘á»i tháº­t tháº­t dá»… hiá»ƒu, kiá»ƒu giáº£i thÃ­ch cho há»c sinh cáº¥p 2.

---
ğŸ§  Tá»•ng thá»ƒ: HippoRAG lÃ  gÃ¬?
HippoRAG lÃ  mÃ´ hÃ¬nh trÃ­ nhá»› dÃ i háº¡n cho AI, láº¥y cáº£m há»©ng tá»« cÃ¡ch con ngÆ°á»i ghi nhá»› vÃ  suy luáº­n.
 NÃ³ chia lÃ m 3 bá»™ pháº­n giá»‘ng nhÆ° bá»™ nÃ£o ngÆ°á»i:
- Neocortex â†’ chÃ­nh lÃ  LLM (nÃ£o suy nghÄ©)
- Hippocampus â†’ nÆ¡i lÆ°u giá»¯ kÃ½ á»©c (graph)
- Parahippocampal Region â†’ káº¿t ná»‘i Ã½ tÆ°á»Ÿng (tÃ¬m Ä‘iá»ƒm giá»‘ng nhau)

---
ğŸ¦› HippoRAG 1 â€“ GIáº¢I TÃCH CÃC KHÃI NIá»†M CHÃNH

---
1. OpenIE â†’ TrÃ­ch xuáº¥t triple (subject â€“ predicate â€“ object)
ğŸ§© ÄÃ¢y lÃ  bÆ°á»›c láº¥y thÃ´ng tin tá»« vÄƒn báº£n vÃ  biáº¿n nÃ³ thÃ nh má»™t â€œkiáº¿n thá»©c nhá» cÃ³ dáº¡ng: A liÃªn quan Ä‘áº¿n B báº±ng cÃ¡ch Câ€.
ğŸ” VÃ­ dá»¥ vÄƒn báº£n:
â€œAlbert Einstein was born in Ulm in 1879.â€
â†’ sáº½ thÃ nh triple:
- (Albert Einstein, born in, Ulm)
- (Albert Einstein, born in year, 1879)
ğŸª„ LiÃªn há»‡ cho há»c sinh:
 Giá»‘ng nhÆ° khi báº¡n Ä‘á»c sÃ¡ch vÃ  tá»± ghi vá»Ÿ â€œtÃ³m táº¯t Ã½ chÃ­nhâ€ kiá»ƒu:
 â†’ â€œEinstein â†’ sinh á»Ÿ Ulmâ€, â€œEinstein â†’ sinh nÄƒm 1879â€.

---
2. Graph of Knowledge Triples (Biá»ƒu Ä‘á»“ kiáº¿n thá»©c)
ğŸ§  CÃ¡c triple trÃªn Ä‘Æ°á»£c ghÃ©p thÃ nh 1 báº£n Ä‘á»“ kiáº¿n thá»©c, giá»‘ng nhÆ° báº£n Ä‘á»“ tÆ° duy (mindmap).
ğŸ“Œ Má»—i vÃ²ng trÃ²n lÃ  1 tá»« khÃ³a, má»—i mÅ©i tÃªn lÃ  má»‘i liÃªn há»‡.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Báº¡n váº½ sÆ¡ Ä‘á»“ mindmap mÃ´n Sinh há»c:
- â€œLÃ¡ cÃ¢y â†’ cÃ³ chá»©c nÄƒng â†’ quang há»£pâ€
- â€œQuang há»£p â†’ táº¡o ra â†’ oxyâ€
 â†’ LÃ  báº¡n Ä‘ang váº½ KG Ä‘Ã³!

---
3. Personalized PageRank (PPR)
ğŸ”„ ÄÃ¢y lÃ  cÃ¡ch mÃ´ hÃ¬nh tÃ¬m Ä‘Æ°á»ng Ä‘i qua nhiá»u bÆ°á»›c trong báº£n Ä‘á»“ kiáº¿n thá»©c Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Giá»‘ng nhÆ° chÆ¡i trÃ² â€œÄ‘i tÃ¬m kho bÃ¡uâ€ trong mÃª cung. Báº¡n báº¯t Ä‘áº§u tá»« tá»« khÃ³a trong cÃ¢u há»i, Ä‘i theo cÃ¡c má»‘i liÃªn há»‡ Ä‘á»ƒ Ä‘áº¿n cÃ¢u tráº£ lá»i.
CÃ¢u há»i: â€œLÃ¡ cÃ¢y giÃºp gÃ¬ cho mÃ´i trÆ°á»ng?â€
 â†’ LÃ¡ cÃ¢y â†’ quang há»£p â†’ táº¡o oxy â†’ giÃºp mÃ´i trÆ°á»ng â†’ thá»Ÿ

---
4. Entity-Centric Query Linking (LiÃªn káº¿t dá»±a vÃ o tÃªn riÃªng)
HippoRAG 1 dÃ¹ng cÃ¡ch phÃ¡t hiá»‡n thá»±c thá»ƒ (entity) trong cÃ¢u há»i nhÆ° tÃªn ngÆ°á»i, nÆ¡i chá»‘n, Ä‘á»ƒ ná»‘i vÃ o graph.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 CÃ¢u há»i: â€œBill Gates há»c trÆ°á»ng nÃ o?â€
 â†’ PhÃ¡t hiá»‡n â€œBill Gatesâ€ lÃ  thá»±c thá»ƒ â†’ tÃ¬m trong báº£n Ä‘á»“ trÃ­ nhá»›.
Äiá»ƒm yáº¿u:
- KhÃ´ng hiá»ƒu cÃ¢u há»i rá»™ng (vÃ­ dá»¥: â€œNgÆ°á»i sÃ¡ng láº­p Microsoft há»c á»Ÿ Ä‘Ã¢u?â€)
 â†’ náº¿u khÃ´ng cÃ³ tá»« â€œBill Gatesâ€, thÃ¬ khÃ´ng tÃ¬m Ä‘Æ°á»£c â†’ máº¥t context

---
ğŸ§  HippoRAG 2 â€“ Cáº¢I TIáº¾N QUAN TRá»ŒNG + VÃ Dá»¤ Äá»œI THÆ¯á»œNG

---
1. Dense-Sparse Integration: GhÃ©p kiáº¿n thá»©c vÃ  ngá»¯ cáº£nh
HippoRAG 2 khÃ´ng chá»‰ lÆ°u "kiáº¿n thá»©c ngáº¯n gá»n" mÃ  cÃ²n lÆ°u cáº£ Ä‘oáº¡n vÄƒn ban Ä‘áº§u (ngá»¯ cáº£nh).
ğŸ“Œ Graph cÃ³ 2 loáº¡i node:
- Phrase Node (sparse): giá»‘ng tiÃªu Ä‘á»/tá»« khÃ³a
- Passage Node (dense): nguyÃªn Ä‘oáº¡n vÄƒn gá»‘c
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Khi há»c lá»‹ch sá»­, báº¡n vá»«a gáº¡ch Ä‘áº§u dÃ²ng â€œTráº­n Äiá»‡n BiÃªn Phá»§ tháº¯ng nÄƒm 1954â€, vá»«a giá»¯ láº¡i nguyÃªn Ä‘oáº¡n mÃ´ táº£ tráº­n Ä‘Ã¡nh â†’ váº­y báº¡n vá»«a nhá»› "Ã½ chÃ­nh" vá»«a cÃ³ â€œngá»¯ cáº£nhâ€.

---
2. Query-to-Triple Matching
KhÃ´ng chá»‰ tÃ¬m entity ná»¯a, mÃ  dÃ¹ng toÃ n cÃ¢u há»i Ä‘á»ƒ match vá»›i cÃ¡c triple.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 CÃ¢u há»i: â€œNgÆ°á»i sÃ¡ng láº­p Microsoft há»c á»Ÿ Ä‘Ã¢u?â€
 â†’ LÃºc nÃ y, dÃ¹ khÃ´ng cÃ³ tá»« â€œBill Gatesâ€, mÃ´ hÃ¬nh váº«n hiá»ƒu Ä‘Æ°á»£c vÃ  tÃ¬m Ä‘Æ°á»£c triple phÃ¹ há»£p.

---
3. Recognition Memory â€“ lá»c triple báº±ng LLM
Sau khi tÃ¬m ra vÃ i má»‘i liÃªn há»‡ phÃ¹ há»£p, HippoRAG 2 nhá» LLM kiá»ƒm tra láº¡i xem cÃ¡i nÃ o thá»±c sá»± liÃªn quan.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Báº¡n tÃ¬m 5 Ä‘oáº¡n cÃ³ váº» Ä‘Ãºng Ä‘á»ƒ lÃ m bÃ i táº­p. Sau Ä‘Ã³ báº¡n Ä‘á»c ká»¹ tá»«ng Ä‘oáº¡n, loáº¡i bá» cÃ¡i khÃ´ng liÃªn quan â†’ giá»¯ láº¡i 2 Ä‘oáº¡n tá»‘t nháº¥t â†’ Ä‘Ã³ lÃ  â€œrecognition memoryâ€.

---
4. Contextualized PPR Search
Khi Ä‘Ã£ cÃ³ cáº£ tá»« khÃ³a vÃ  Ä‘oáº¡n vÄƒn, mÃ´ hÃ¬nh dÃ¹ng PageRank Ä‘á»ƒ tÃ¬m con Ä‘Æ°á»ng tá»‘t nháº¥t qua báº£n Ä‘á»“ trÃ­ nhá»› cÃ³ cáº£ kiáº¿n thá»©c vÃ  ngá»¯ cáº£nh.
ğŸª„ VÃ­ dá»¥ há»c sinh:
 Báº¡n ná»‘i â€œtá»« khÃ³aâ€ vá»›i cÃ¡c Ä‘oáº¡n giáº£i thÃ­ch trong sÃ¡ch, rá»“i chá»n ra Ä‘oáº¡n nÃ o giÃºp báº¡n hiá»ƒu toÃ n cáº£nh nháº¥t Ä‘á»ƒ lÃ m bÃ i.

---
ğŸ“ Tá»•ng káº¿t: HippoRAG 1 vs 2 â€“ Dá»… hiá»ƒu cho há»c sinh cáº¥p 2
This content is only supported in a Lark Docs

---
Tuyá»‡t vá»i Quá»‘c Æ¡i, giá» mÃ¬nh sáº½ giáº£i tÃ­ch sÃ¢u tá»«ng khÃ¡i niá»‡m trong sÆ¡ Ä‘á»“ pipeline cá»§a HippoRAG 2, nhÆ° Quá»‘c yÃªu cáº§u:
- Hiá»ƒu rÃµ cÆ¡ cháº¿
- Pháº£n biá»‡n kiá»ƒu phÃ¡ â€“ Ä‘Ã¡p tá»«ng bÆ°á»›c
- LiÃªn há»‡ vÃ­ dá»¥ khi cáº§n
SÆ¡ Ä‘á»“ Ä‘Æ°á»£c chia lÃ m 2 pháº§n:

---
ğŸ¯ PHáº¦N 1: OFFLINE INDEXING
Má»¥c tiÃªu: xÃ¢y dá»±ng má»™t bá»™ nhá»› kiáº¿n thá»©c giÃ u liÃªn káº¿t tá»« vÄƒn báº£n Ä‘áº§u vÃ o.

---
ğŸ”µ BÆ°á»›c 1: OpenIE by LLM â†’ TrÃ­ch xuáº¥t Triple
TÃ¡ch Ä‘oáº¡n vÄƒn thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ kiáº¿n thá»©c dáº¡ng (subject, predicate, object)

---
â“PhÃ¡: Táº¡i sao dÃ¹ng OpenIE báº±ng LLM? TrÃ­ch xuáº¥t truyá»n thá»‘ng cÃ³ váº¥n Ä‘á» gÃ¬?
âœ… ÄÃ¡p:
- OpenIE truyá»n thá»‘ng (Stanford, ClausIE...) chá»‰ lÃ m Ä‘Æ°á»£c trÃªn cÃ¢u Ä‘Æ¡n, thiáº¿u kháº£ nÄƒng xá»­ lÃ½ ngÃ´n ngá»¯ phá»©c táº¡p.
- LLM cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c má»‡nh Ä‘á» dÃ i, ngá»¯ nghÄ©a má»Ÿ rá»™ng, phá»©c táº¡p hÆ¡n â†’ táº¡o triple â€œgiÃ u nghÄ©aâ€ hÆ¡n.
ğŸ“Œ VÃ­ dá»¥
 VÄƒn báº£n: "Marie Curie won two Nobel Prizes in different fields."
 â†’ Triple truyá»n thá»‘ng: ("Marie Curie", "won", "two Nobel Prizes")
 â†’ LLM cÃ³ thá»ƒ táº¡o thÃªm triple: ("Marie Curie", "won Nobel in", "Physics") + ("Marie Curie", "won Nobel in", "Chemistry")

---
ğŸ”µ BÆ°á»›c 2: Synonym Detection â†’ PhÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a
Ná»‘i cÃ¡c node gáº§n nghÄ©a trong KG báº±ng Synonym Edge (Ä‘Æ°á»ng Ä‘á»©t nÃ©t trong sÆ¡ Ä‘á»“)

---
â“PhÃ¡: PhÃ¡t hiá»‡n synonym kiá»ƒu gÃ¬? CÃ³ dÃ¹ng LLM khÃ´ng?
âœ… ÄÃ¡p: KhÃ´ng! DÃ¹ng cosine similarity trong embedding space
- CÃ¡c phrase (tá»«/cá»¥m tá»«) Ä‘Æ°á»£c nhÃºng (embedding)
- Náº¿u cosine similarity > ngÆ°á»¡ng â†’ Ä‘Æ°á»£c ná»‘i báº±ng "synonym edge"
ğŸ“Œ TÃ¡c giáº£ khÃ´ng dÃ¹ng LLM vÃ¬:
- LLM cháº­m hÆ¡n ráº¥t nhiá»u
- Embedding-based synonym cÃ³ thá»ƒ cáº­p nháº­t liÃªn tá»¥c, Ä‘Æ¡n giáº£n hÆ¡n

---
â“PhÃ¡: CÃ³ rá»§i ro gÃ¬ khÃ´ng khi dÃ¹ng embedding Ä‘á»ƒ ná»‘i synonym?
âœ… ÄÃ¡p:
- CÃ³! CÃ³ thá»ƒ ná»‘i nháº§m cÃ¡c tá»« gáº§n nghÄ©a nhÆ°ng khÃ¡c ngá»¯ cáº£nh ("doctor" vÃ  "professor" cháº³ng háº¡n).
- NhÆ°ng vÃ¬ Ä‘Ã¢y lÃ  lá»›p bá»• trá»£ (synonym edge), khÃ´ng áº£nh hÆ°á»Ÿng trá»±c tiáº¿p náº¿u downstream filter lÃ m tá»‘t.

---
ğŸ”µ BÆ°á»›c 3: Denseâ€“Sparse Integration
TÃ­ch há»£p cáº£ node "phrase" vÃ  node "passage" trong KG â†’ hybrid graph

---
â“PhÃ¡: Táº¡i sao cáº§n vá»«a sparse (phrase) vá»«a dense (passage)? KhÃ´ng giá»¯ má»™t loáº¡i cho gá»n?
âœ… ÄÃ¡p:
- Sparse (phrase): Dá»… truy há»“i, biá»ƒu diá»…n khÃ¡i niá»‡m â†’ tá»‘t cho reasoning
- Dense (passage): Giá»¯ thÃ´ng tin chi tiáº¿t â†’ tá»‘t cho factual QA
 â†’ GhÃ©p cáº£ hai táº¡o ra má»™t KG vá»«a khÃ¡i quÃ¡t vá»«a giÃ u ngá»¯ cáº£nh.
ğŸª„ LiÃªn há»‡ dá»… hiá»ƒu: Giá»‘ng há»c Sinh há»c báº±ng sÆ¡ Ä‘á»“ + sÃ¡ch giÃ¡o khoa:
- SÆ¡ Ä‘á»“ (phrase node) giÃºp nhá»› nhanh
- SGK (passage node) giÃºp hiá»ƒu sÃ¢u â†’ cáº§n cáº£ hai Ä‘á»ƒ thi Ä‘iá»ƒm cao

---
âœ… Tá»•ng káº¿t PHáº¦N 1: Graph cÃ³ cÃ¡c loáº¡i node vÃ  edge sau:
This content is only supported in a Lark Docs

---
ğŸ” PHáº¦N 2: ONLINE RETRIEVAL & QA
Má»¥c tiÃªu: Khi cÃ³ query â†’ tÃ¬m Ä‘Æ°á»ng trong graph â†’ tráº£ lá»i.

---
ğŸŸ¢ BÆ°á»›c 1: Retrieving Passages and Triples

---
â“PhÃ¡: Sao pháº£i láº¥y cáº£ Ä‘oáº¡n vÄƒn láº«n triple? KhÃ´ng chá»n 1 cÃ¡i cho nhanh?
âœ… ÄÃ¡p:
- Triple Ä‘á»ƒ lÃ m reasoning, káº¿t ná»‘i máº¡ch kiáº¿n thá»©c
- Passage Ä‘á»ƒ cÃ³ thÃ´ng tin chi tiáº¿t, lÃ m QA cuá»‘i cÃ¹ng
â†’ Hai lá»›p retrieval bá»• trá»£ nhau, khÃ´ng thá»ƒ thiáº¿u cÃ¡i nÃ o.

---
ğŸŸ¢ BÆ°á»›c 2: Triple Filtering = Recognition Memory
Sau khi láº¥y triple vá», ta lá»c xem cÃ¡i nÃ o thá»±c sá»± liÃªn quan Ä‘áº¿n query

---
â“PhÃ¡: Táº¡i sao cáº§n bÆ°á»›c nÃ y? Triple Ä‘Ã£ chá»n rá»“i mÃ ?
âœ… ÄÃ¡p: VÃ¬:
- Retrieval ban Ä‘áº§u báº±ng embedding â†’ cÃ³ thá»ƒ nhiá»…u
- Recognition lÃ  bÆ°á»›c giá»‘ng bá»™ nhá»› con ngÆ°á»i:
 â†’ â€œmÃ¬nh tá»«ng tháº¥y cÃ¡i nÃ y rá»“iâ€¦ nhÆ°ng cÃ³ Ä‘Ãºng cÃ¡i mÃ¬nh cáº§n khÃ´ng?â€
ğŸ“Œ DÃ¹ng LLM Ä‘á»ƒ so sÃ¡nh láº¡i tá»«ng triple vá»›i query â†’ giá»¯ láº¡i nhá»¯ng cÃ¡i tá»‘t nháº¥t
ğŸª„ LiÃªn há»‡ há»c sinh: Giá»‘ng Ä‘á»c láº¡i 5 Ä‘oáº¡n báº¡n highlight khi há»c â†’ loáº¡i 3 cÃ¡i khÃ´ng liÃªn quan, giá»¯ láº¡i 2 Ä‘oáº¡n tháº­t sá»± Ä‘Ãºng Ä‘á».

---
ğŸ”´ BÆ°á»›c 3: Assigning Seed Node Weights
GÃ¡n Ä‘iá»ƒm cho cÃ¡c node â€œgá»‘câ€ trong KG â†’ chuáº©n bá»‹ cho bÆ°á»›c lan truyá»n PageRank

---
â“PhÃ¡: Seed Node lÃ  gÃ¬? Táº¡i sao pháº£i gÃ¡n trá»ng sá»‘?
âœ… ÄÃ¡p:
- Seed Node lÃ  node trong graph cÃ³ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n query
- Trá»ng sá»‘ cao thÃ¬ node Ä‘Ã³ lan truyá»n áº£nh hÆ°á»Ÿng máº¡nh hÆ¡n trong PPR
ğŸ“Œ DÃ¹ng cáº£:
- Passage tá»« query retrieval
- Phrase tá»« triple lá»c
 â†’ táº¥t cáº£ Ä‘á»u trá»Ÿ thÃ nh seed node

---
ğŸ”„ BÆ°á»›c 4: PPR Graph Search
Personalized PageRank tÃ¬m Ä‘Æ°á»ng trong KG tá»« cÃ¡c seed node

---
â“PhÃ¡: KhÃ¡c gÃ¬ so vá»›i search bÃ¬nh thÆ°á»ng?
âœ… ÄÃ¡p:
- PageRank lan tá»a áº£nh hÆ°á»Ÿng â†’ khÃ´ng tÃ¬m Ä‘Æ°á»ng ngáº¯n nháº¥t, mÃ  tÃ¬m Ä‘oáº¡n cÃ³ liÃªn quan nháº¥t trong toÃ n bá»™ cáº¥u trÃºc
- Personalized â†’ Æ°u tiÃªn cÃ¡c vÃ¹ng gáº§n vá»›i seed node
ğŸª„ LiÃªn há»‡ há»c sinh: Giá»‘ng há»c theo sÆ¡ Ä‘á»“ tÆ° duy, báº¡n Æ°u tiÃªn Ä‘á»c pháº§n gáº§n chá»§ Ä‘á» chÃ­nh â†’ khÃ´ng láº­t sÃ¡ch lung tung

---
âœ… BÆ°á»›c 5: QA Reading with Selected Passages
LLM Ä‘á»c cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Ã£ chá»n â†’ sinh ra cÃ¢u tráº£ lá»i.

---
â“PhÃ¡: Náº¿u chá»‰ dÃ¹ng Ä‘oáº¡n vÄƒn mÃ  khÃ´ng graph thÃ¬ sao?
âœ… ÄÃ¡p:
 â†’ Giá»‘ng RAG truyá»n thá»‘ng â†’ bá»‹ lá»‡ch, thiáº¿u reasoning
 â†’ Nhá» bÆ°á»›c graph search, mÃ´ hÃ¬nh nháº­n Ä‘Æ°á»£c Ä‘oáº¡n vÄƒn â€œgiÃ u liÃªn káº¿t nháº¥tâ€ â†’ tráº£ lá»i máº¡ch láº¡c hÆ¡n, chÃ­nh xÃ¡c hÆ¡n.

---
ğŸ”š Tá»•ng Káº¿t: Pipeline HippoRAG 2 â€“ Hiá»ƒu sÃ¢u + vÃ­ dá»¥ hÃ³a
This content is only supported in a Lark Docs

---
DÆ°á»›i Ä‘Ã¢y lÃ  pháº§n giáº£i thÃ­ch sÃ¢u tá»«ng bÆ°á»›c trong pipeline cá»§a HippoRAG 2, kÃ¨m theo cÃ¡c cÃ¢u há»i â€œphÃ¡â€ â€“ â€œÄ‘Ã¡pâ€ Ä‘á»ƒ Ä‘Ã o sÃ¢u hiá»ƒu biáº¿t, giá»‘ng nhÆ° Ä‘á»‘i thoáº¡i giá»¯a ngÆ°á»i Ä‘áº·t cÃ¢u há»i sáº¯c sáº£o vÃ  ngÆ°á»i tráº£ lá»i cháº·t cháº½.

---
1. OpenIE by LLM â€“ TrÃ­ch xuáº¥t Triple tá»« vÄƒn báº£n
Má»¥c tiÃªu:
 Chuyá»ƒn Ä‘á»•i Ä‘oáº¡n vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ kiáº¿n thá»©c (triple: subject â€“ predicate â€“ object) Ä‘á»ƒ lÆ°u trá»¯ trong bá»™ nhá»› dÆ°á»›i dáº¡ng cÃ¡c má»‘i liÃªn há»‡ Ä‘Æ¡n giáº£n.

---
â“ PhÃ¡ 1:
Táº¡i sao chÃºng ta cáº§n trÃ­ch xuáº¥t triple thay vÃ¬ chá»‰ lÆ°u toÃ n bá»™ Ä‘oáº¡n vÄƒn?
âœ… ÄÃ¡p:
- Viá»‡c trÃ­ch xuáº¥t triple giÃºp tÃ¡ch thÃ´ng tin quan trá»ng ra khá»i má»› vÄƒn báº£n dÃ i.
- NÃ³ cho phÃ©p lÆ°u trá»¯ dÆ°á»›i dáº¡ng cÃ¡c má»‘i quan há»‡ rÃµ rÃ ng, giá»‘ng nhÆ° ghi chÃº ngáº¯n gá»n, dá»… tÃ¬m láº¡i vÃ  káº¿t ná»‘i vá»›i cÃ¡c kiáº¿n thá»©c khÃ¡c.
VÃ­ dá»¥ dá»… hiá»ƒu:
 Giá»‘ng nhÆ° khi há»c, báº¡n khÃ´ng cáº§n lÆ°u toÃ n bá»™ bÃ i giáº£ng mÃ  chá»‰ cáº§n ghi láº¡i cÃ¡c Ã½ chÃ­nh: â€œEinstein â€“ sinh â€“ Ulmâ€ thay vÃ¬ ghi toÃ n bá»™ Ä‘oáº¡n vÄƒn dÃ i.

---
â“ PhÃ¡ 2:
Táº¡i sao dÃ¹ng LLM Ä‘á»ƒ thá»±c hiá»‡n OpenIE, thay vÃ¬ dÃ¹ng cÃ¡c cÃ´ng cá»¥ truyá»n thá»‘ng?
âœ… ÄÃ¡p:
- CÃ¡c cÃ´ng cá»¥ OpenIE truyá»n thá»‘ng thÆ°á»ng giá»›i háº¡n á»Ÿ cÃ¢u Ä‘Æ¡n giáº£n, khÃ´ng hiá»ƒu Ä‘Æ°á»£c nhá»¯ng cÃ¢u phá»©c táº¡p, mÃ¢u thuáº«n hay áº©n dá»¥.
- LLM cÃ³ kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh, xá»­ lÃ½ cÃ¢u dÃ i vÃ  phá»©c táº¡p, tá»« Ä‘Ã³ trÃ­ch xuáº¥t triple chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§ hÆ¡n.
VÃ­ dá»¥ há»c sinh cáº¥p 2:
 Náº¿u Ä‘á»c má»™t Ä‘oáº¡n vÄƒn mÃ´ táº£ "Marie Curie Ä‘Ã£ khÃ¡m phÃ¡ ra cÃ¡c nguyÃªn tá»‘ má»›i vÃ  giÃ nh giáº£i Nobel trong nhiá»u lÄ©nh vá»±c", LLM cÃ³ thá»ƒ táº¡o ra nhiá»u triple nhÆ°:
 â†’ (Marie Curie, khÃ¡m phÃ¡, nguyÃªn tá»‘ má»›i)
 â†’ (Marie Curie, giÃ nh, giáº£i Nobel)
 NhÆ° váº­y, báº¡n cÃ³ thá»ƒ nhá»› nhiá»u thÃ´ng tin hÆ¡n chá»‰ báº±ng má»™t cÃ¢u tÃ³m táº¯t.

---
2. Synonym Detection â€“ PhÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a
Má»¥c tiÃªu:
 Káº¿t ná»‘i cÃ¡c node (khÃ¡i niá»‡m, cá»¥m tá»«) cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»± trong KG, giÃºp tÄƒng kháº£ nÄƒng liÃªn káº¿t giá»¯a cÃ¡c thÃ´ng tin.

---
â“ PhÃ¡ 1:
Synonym Detection dÃ¹ng thuáº­t toÃ¡n nÃ o Ä‘á»ƒ phÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a?
âœ… ÄÃ¡p:
- ThÃ´ng thÆ°á»ng, ta sá»­ dá»¥ng cosine similarity trong khÃ´ng gian embedding.
- Má»—i tá»« hay cá»¥m tá»« Ä‘Æ°á»£c chuyá»ƒn thÃ nh má»™t vector, sau Ä‘Ã³ tÃ­nh cosine similarity giá»¯a cÃ¡c vector. Náº¿u giÃ¡ trá»‹ similarity vÆ°á»£t qua má»™t ngÆ°á»¡ng Ä‘á»‹nh trÆ°á»›c (vÃ­ dá»¥ 0.8), hai node Ä‘Æ°á»£c coi lÃ  Ä‘á»“ng nghÄ©a vÃ  Ä‘Æ°á»£c ná»‘i vá»›i nhau báº±ng má»™t â€œsynonym edge.â€

---
â“ PhÃ¡ 2:
Táº¡i sao láº¡i cáº§n ná»‘i cÃ¡c node Ä‘á»“ng nghÄ©a láº¡i vá»›i nhau?
âœ… ÄÃ¡p:
- Äiá»u nÃ y giÃºp liÃªn káº¿t cÃ¡c khÃ¡i niá»‡m cÃ³ thá»ƒ Ä‘Æ°á»£c diá»…n Ä‘áº¡t khÃ¡c nhau.
- Nhá» Ä‘Ã³, khi truy váº¥n, náº¿u tá»« khÃ³a Ä‘Æ°á»£c dÃ¹ng khÃ¡c vá»›i cÃ¡ch ghi chÃº ban Ä‘áº§u nhÆ°ng cÃ³ cÃ¹ng Ã½ nghÄ©a, há»‡ thá»‘ng váº«n cÃ³ thá»ƒ liÃªn káº¿t Ä‘Ãºng kiáº¿n thá»©c.
VÃ­ dá»¥ dá»… hiá»ƒu:
 Náº¿u trong vÄƒn báº£n cÃ³ â€œtháº§y giÃ¡oâ€ vÃ  â€œgiÃ¡o viÃªnâ€ â€“ máº·c dÃ¹ hai tá»« khÃ¡c nhau, chÃºng Ä‘á»u nÃ³i vá» ngÆ°á»i dáº¡y há»c. Káº¿t ná»‘i chÃºng láº¡i giÃºp há»‡ thá»‘ng hiá»ƒu ráº±ng khi há»i vá» â€œgiÃ¡o viÃªnâ€ cÅ©ng cÃ³ thá»ƒ liÃªn há»‡ Ä‘áº¿n â€œtháº§y giÃ¡oâ€.

---
â“ PhÃ¡ 3:
CÃ³ rá»§i ro gÃ¬ khi dÃ¹ng cosine similarity?
âœ… ÄÃ¡p:
- CÃ³ thá»ƒ xáº£y ra trÆ°á»ng há»£p ná»‘i nháº§m nhá»¯ng tá»« cÃ³ cÃ¹ng hÃ¬nh thá»©c nhÆ°ng ngá»¯ nghÄ©a khÃ¡c (vÃ­ dá»¥: â€œbankâ€ cÃ³ thá»ƒ lÃ  ngÃ¢n hÃ ng hoáº·c bá» sÃ´ng).
- Tuy nhiÃªn, vÃ¬ Ä‘Ã¢y chá»‰ lÃ  lá»›p bá»• trá»£ trong KG, náº¿u cÃ¡c bÆ°á»›c sau (nhÆ° triple filtering) hoáº¡t Ä‘á»™ng tá»‘t, sai sÃ³t nÃ y sáº½ Ä‘Æ°á»£c giáº£m thiá»ƒu.

---
3. Denseâ€“Sparse Integration â€“ TÃ­ch há»£p cáº£ Node "Phrase" vÃ  "Passage"
Má»¥c tiÃªu:
 Káº¿t há»£p thÃ´ng tin ngáº¯n gá»n (phrase node) vÃ  thÃ´ng tin Ä‘áº§y Ä‘á»§ (passage node) vÃ o cÃ¹ng má»™t KG Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cáº£ hai.

---
â“ PhÃ¡ 1:
Táº¡i sao khÃ´ng chá»‰ lÆ°u thÃ´ng tin dáº¡ng â€œphraseâ€ hay â€œpassageâ€ riÃªng ráº½?
âœ… ÄÃ¡p:
- Phrase node (sparse): RÃºt gá»n thÃ´ng tin, dá»… káº¿t ná»‘i cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n, nhanh vÃ  hiá»‡u quáº£ cho reasoning.
- Passage node (dense): Giá»¯ nguyÃªn ngá»¯ cáº£nh vÃ  chi tiáº¿t, cáº§n thiáº¿t cho cÃ¡c tÃ¡c vá»¥ yÃªu cáº§u Ä‘á»™ chÃ­nh xÃ¡c cao nhÆ° factual QA.
VÃ­ dá»¥:
 Há»c sinh khi Ã´n bÃ i toÃ¡n: báº¡n cÃ³ thá»ƒ ghi láº¡i â€œcÃ´ng thá»©c Pythagorasâ€ (phrase) vÃ  cÃ³ sá»• tay chá»©a bÃ i giáº£i chi tiáº¿t (passage).
 Cáº£ hai Ä‘á»u quan trá»ng Ä‘á»ƒ báº¡n vá»«a nhá»› nhanh, vá»«a hiá»ƒu sÃ¢u.

---
â“ PhÃ¡ 2:
How does integrating both dense and sparse information help in retrieval?
âœ… ÄÃ¡p:
- Khi cÃ³ cáº£ hai, há»‡ thá»‘ng cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c phrase node Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c khÃ¡i niá»‡m chá»§ Ä‘áº¡o vÃ  passage node Ä‘á»ƒ láº¥y chi tiáº¿t cá»¥ thá»ƒ cho cÃ¢u tráº£ lá»i.
- Äiá»u nÃ y táº¡o ra má»™t KG toÃ n diá»‡n, vá»«a tá»•ng quÃ¡t vá»«a chi tiáº¿t, giÃºp tÄƒng hiá»‡u suáº¥t á»Ÿ cáº£ factual QA vÃ  multi-hop reasoning.

---
4. Triple Retrieval â€“ Láº¥y ra cÃ¡c Triple cÃ³ liÃªn quan vá»›i Query
Má»¥c tiÃªu:
 Khi cÃ³ má»™t query, tÃ¬m ra cÃ¡c triple tá»« KG phÃ¹ há»£p Ä‘á»ƒ dÃ¹ng cho quÃ¡ trÃ¬nh reasoning.

---
â“ PhÃ¡ 1:
Táº¡i sao khÃ´ng chá»‰ láº¥y toÃ n bá»™ passage mÃ  pháº£i láº¥y ra triple?
âœ… ÄÃ¡p:
- Triple chá»©a má»‘i liÃªn há»‡ rÃµ rÃ ng vÃ  rÃºt gá»n, giÃºp há»‡ thá»‘ng dá»… dÃ ng â€œÄ‘i theoâ€ cÃ¡c má»‘i liÃªn káº¿t Ä‘á»ƒ reasoning.
- Passage cÃ³ thá»ƒ quÃ¡ dÃ i, chá»©a nhiá»…u; triple giÃºp xÃ¡c Ä‘á»‹nh â€œÄ‘iá»ƒm cá»‘t lÃµiâ€ cá»§a kiáº¿n thá»©c.
VÃ­ dá»¥:
 Náº¿u cÃ¢u há»i lÃ  â€œBill Gates há»c trÆ°á»ng nÃ o?â€, triple nhÆ° (â€œBill Gatesâ€, â€œhá»c táº¡iâ€, â€œHarvardâ€) sáº½ giÃºp há»‡ thá»‘ng nhanh chÃ³ng tÃ¬m ra thÃ´ng tin cáº§n thiáº¿t.

---
5. Triple Filtering (Recognition Memory) â€“ Lá»c láº¡i cÃ¡c Triple
Má»¥c tiÃªu:
 Sau khi láº¥y ra cÃ¡c triple ban Ä‘áº§u, sá»­ dá»¥ng LLM Ä‘á»ƒ xÃ¡c nháº­n láº¡i tÃ­nh liÃªn quan cá»§a chÃºng vá»›i query, loáº¡i bá» nhá»¯ng triple khÃ´ng phÃ¹ há»£p.

---
â“ PhÃ¡ 1:
Táº¡i sao láº¡i cáº§n bÆ°á»›c lá»c triple náº¿u Ä‘Ã£ cÃ³ triple retrieval?
âœ… ÄÃ¡p:
- Do bÆ°á»›c retrieval ban Ä‘áº§u cÃ³ thá»ƒ mang láº¡i nhiá»u triple nhiá»…u hoáº·c khÃ´ng liÃªn quan, vÃ¬ khÃ´ng pháº£i lÃºc nÃ o cÃ¡c vector embedding cÅ©ng hoÃ n háº£o.
- Recognition Memory giÃºp â€œÄ‘á»c láº¡iâ€ vÃ  xÃ¡c nháº­n, giá»‘ng nhÆ° khi báº¡n kiá»ƒm tra láº¡i ghi chÃº cá»§a mÃ¬nh Ä‘á»ƒ Ä‘áº£m báº£o chá»‰ giá»¯ láº¡i nhá»¯ng thÃ´ng tin Ä‘Ãºng cáº§n thiáº¿t.
VÃ­ dá»¥:
 Náº¿u cÃ³ 5 triple Ä‘Æ°á»£c láº¥y ra, nhÆ°ng chá»‰ cÃ³ 2 thá»±c sá»± nÃ³i vá» cÃ¢u há»i, bÆ°á»›c filtering sáº½ loáº¡i bá» 3 triple khÃ´ng cáº§n thiáº¿t.

---
6. Seed Node Assignment â€“ GÃ¡n Trá»ng Sá»‘ cho Seed Node
Má»¥c tiÃªu:
 XÃ¡c Ä‘á»‹nh cÃ¡c node â€œgá»‘câ€ (seed node) tá»« query, vÃ  gÃ¡n trá»ng sá»‘ cho chÃºng nháº±m táº¡o ná»n táº£ng cho bÆ°á»›c lan truyá»n thÃ´ng tin trong PPR.

---
â“ PhÃ¡ 1:
Seed Node lÃ  gÃ¬?
âœ… ÄÃ¡p:
- Seed Node lÃ  nhá»¯ng node trong KG mÃ  há»‡ thá»‘ng xÃ¡c Ä‘á»‹nh cÃ³ má»‘i liÃªn há»‡ trá»±c tiáº¿p vá»›i query.
- ChÃºng lÃ  Ä‘iá»ƒm khá»Ÿi Ä‘áº§u Ä‘á»ƒ thuáº­t toÃ¡n lan truyá»n (PPR) â€œkhÃ¡m phÃ¡â€ cÃ¡c node khÃ¡c cÃ³ liÃªn quan.
VÃ­ dá»¥:
 Náº¿u cÃ¢u há»i lÃ  â€œBill Gates há»c á»Ÿ Ä‘Ã¢u?â€, seed node cÃ³ thá»ƒ lÃ  â€œBill Gatesâ€ vÃ  â€œHarvardâ€ náº¿u Ä‘Ã£ cÃ³ triple liÃªn quan.

---
â“ PhÃ¡ 2:
Táº¡i sao cáº§n gÃ¡n trá»ng sá»‘ cho seed node?
âœ… ÄÃ¡p:
- Trá»ng sá»‘ cÃ ng cao, node Ä‘Ã³ sáº½ áº£nh hÆ°á»Ÿng máº¡nh hÆ¡n trong quÃ¡ trÃ¬nh lan truyá»n thÃ´ng tin cá»§a PageRank.
- GiÃºp Æ°u tiÃªn cÃ¡c node quan trá»ng, táº¡o ra káº¿t quáº£ retrieval chÃ­nh xÃ¡c hÆ¡n.
VÃ­ dá»¥:
 Giá»‘ng nhÆ° khi báº¡n chá»n Ä‘iá»ƒm máº¥u chá»‘t trong sÆ¡ Ä‘á»“ tÆ° duy, nhá»¯ng Ä‘iá»ƒm quan trá»ng Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u rÃµ, Ä‘á»ƒ khi â€œÄ‘i theo sÆ¡ Ä‘á»“â€ báº¡n sáº½ Æ°u tiÃªn nhá»¯ng Ä‘iá»ƒm Ä‘Ã³.

---
7. PPR (Personalized PageRank) Graph Search â€“ TÃ¬m Kiáº¿m Trong Graph
Má»¥c tiÃªu:
 Sá»­ dá»¥ng thuáº­t toÃ¡n PPR Ä‘á»ƒ lan truyá»n thÃ´ng tin tá»« cÃ¡c seed node vÃ  tÃ¬m ra cÃ¡c node liÃªn quan nháº¥t trong KG.

---
â“ PhÃ¡ 1:
Thuáº­t toÃ¡n PageRank hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o vÃ  khÃ¡c gÃ¬ so vá»›i tÃ¬m kiáº¿m thÃ´ng thÆ°á»ng?
âœ… ÄÃ¡p:
- PageRank ban Ä‘áº§u Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ xáº¿p háº¡ng cÃ¡c trang web dá»±a trÃªn sá»‘ lÆ°á»£ng vÃ  cháº¥t lÆ°á»£ng cá»§a cÃ¡c liÃªn káº¿t.
- Personalized PageRank (PPR) Ä‘iá»u chá»‰nh thuáº­t toÃ¡n Ä‘á»ƒ Æ°u tiÃªn cÃ¡c node báº¯t Ä‘áº§u tá»« seed node (Ä‘iá»ƒm khá»Ÿi Ä‘áº§u tá»« query).
- Thay vÃ¬ chá»‰ tÃ¬m â€œÄ‘oáº¡n vÄƒn gáº§n nháº¥tâ€, PPR lan truyá»n trá»ng sá»‘ qua cÃ¡c liÃªn káº¿t trong graph, giÃºp há»‡ thá»‘ng reasoning multi-hop.
VÃ­ dá»¥:
 Giá»‘ng nhÆ° báº¡n cÃ³ má»™t báº£n Ä‘á»“ mÃª cung vá»›i cÃ¡c lá»‘i Ä‘i, PPR giÃºp báº¡n khÃ´ng chá»‰ tÃ¬m lá»‘i Ä‘i ngáº¯n nháº¥t mÃ  tÃ¬m lá»‘i Ä‘i â€œnhiá»u dáº¥u hiá»‡u nháº¥tâ€ liÃªn quan Ä‘áº¿n chá»§ Ä‘á» cá»§a cÃ¢u há»i.

---
â“ PhÃ¡ 2:
Táº¡i sao láº¡i cáº§n bÆ°á»›c nÃ y khi cÃ³ cÃ¡c bÆ°á»›c retrieval trÆ°á»›c Ä‘Ã³?
âœ… ÄÃ¡p:
- VÃ¬ PPR giÃºp káº¿t ná»‘i cÃ¡c má»‘i quan há»‡ trong KG, khÃ´ng chá»‰ dá»±a vÃ o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng Ä‘Æ¡n thuáº§n mÃ  cÃ²n hiá»ƒu Ä‘Æ°á»£c cáº¥u trÃºc liÃªn káº¿t giá»¯a cÃ¡c thÃ´ng tin.
- NÃ³ cho phÃ©p truy xuáº¥t cÃ¡c thÃ´ng tin liÃªn quan tá»« xa trong graph, táº¡o nÃªn cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c cho cÃ¡c cÃ¢u há»i multi-hop.

---
8. QA Reader â€“ Äá»c vÃ  Sinh Ra CÃ¢u Tráº£ Lá»i
Má»¥c tiÃªu:
 Cuá»‘i cÃ¹ng, cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Æ°á»£c láº¥y ra (sau khi Ä‘Ã£ qua cÃ¡c bÆ°á»›c xá»­ lÃ½) Ä‘Æ°á»£c Ä‘Æ°a cho LLM Ä‘á»ƒ táº¡o ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng cho query.

---
â“ PhÃ¡ 1:
Táº¡i sao khÃ´ng sá»­ dá»¥ng trá»±c tiáº¿p cÃ¡c triple lÃ m cÃ¢u tráº£ lá»i?
âœ… ÄÃ¡p:
- Triple chá»‰ lÃ  cÃ¡c má»‘i liÃªn há»‡ rÃºt gá»n, chÆ°a Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh cáº§n thiáº¿t Ä‘á»ƒ tráº£ lá»i má»™t cÃ¡ch máº¡ch láº¡c vÃ  tá»± nhiÃªn.
- Passage cung cáº¥p Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh, chi tiáº¿t giÃºp LLM sinh ra cÃ¢u tráº£ lá»i trung thá»±c, Ä‘áº§y Ä‘á»§ vÃ  tá»± nhiÃªn hÆ¡n.
VÃ­ dá»¥:
 Náº¿u báº¡n há»i â€œBill Gates há»c á»Ÿ Ä‘Ã¢u?â€, triple cÃ³ thá»ƒ nÃ³i â€œBill Gates â€“ há»c táº¡i â€“ Harvardâ€, nhÆ°ng passage cÃ³ thá»ƒ cung cáº¥p thÃªm chi tiáº¿t nhÆ° â€œBill Gates tá»«ng theo há»c táº¡i trÆ°á»ng Harvard, nÆ¡i Ã´ng Ä‘Ã£ tiáº¿p thu nhiá»u kiáº¿n thá»©c vá» kinh doanh vÃ  cÃ´ng nghá»‡â€, giÃºp cÃ¢u tráº£ lá»i phong phÃº hÆ¡n.

---
Tá»•ng Káº¿t & LiÃªn Há»‡:
Má»—i bÆ°á»›c trong HippoRAG 2 Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ bÃ¹ trá»« nhá»¯ng Ä‘iá»ƒm yáº¿u cá»§a cÃ¡c bÆ°á»›c trÆ°á»›c vÃ  táº¡o nÃªn má»™t chuá»—i xá»­ lÃ½ toÃ n diá»‡n:
- OpenIE: TrÃ­ch xuáº¥t kiáº¿n thá»©c dáº¡ng triple giÃºp â€œghi nhá»›â€ cÃ¡c má»‘i quan há»‡ chÃ­nh.
- Synonym Detection: Káº¿t ná»‘i nhá»¯ng khÃ¡i niá»‡m cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»±, nhÆ° viá»‡c ná»‘i cÃ¡c tá»« Ä‘á»“ng nghÄ©a trong tá»« Ä‘iá»ƒn.
- Dense-Sparse Integration: Giá»¯ Ä‘Æ°á»£c cáº£ â€œÄ‘áº§u dÃ²ngâ€ (phrase) vÃ  â€œná»™i dung chi tiáº¿tâ€ (passage) â€“ giá»‘ng nhÆ° ghi chÃº ngáº¯n vÃ  sá»• tay chi tiáº¿t.
- Triple Retrieval & Filtering: Nháº­n ra cÃ¡c ghi chÃº liÃªn quan vÃ  loáº¡i bá» nhiá»…u, nhÆ° viá»‡c báº¡n xem láº¡i bÃ i há»c vÃ  chá»n ra nhá»¯ng Ä‘iá»ƒm quan trá»ng.
- Seed Node Assignment & PPR Graph Search: Giá»‘ng nhÆ° chá»n cÃ¡c Ä‘iá»ƒm máº¥u chá»‘t trÃªn sÆ¡ Ä‘á»“ tÆ° duy rá»“i Ä‘i theo cÃ¡c má»‘i liÃªn há»‡ Ä‘á»ƒ tÃ¬m ra cÃ¢u tráº£ lá»i toÃ n diá»‡n.
- QA Reader: Cuá»‘i cÃ¹ng, tá»•ng há»£p cÃ¡c thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c lá»c vÃ  liÃªn káº¿t Ä‘á»ƒ sinh ra cÃ¢u tráº£ lá»i tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c.

---
DÆ°á»›i Ä‘Ã¢y mÃ¬nh sáº½ Ä‘Ã o sÃ¢u vÃ o khÃ¡i niá»‡m triple trong bÆ°á»›c 1 cá»§a HippoRAG (vÃ  HippoRAG 2), vá»›i phong cÃ¡ch â€œmá»™t ngÆ°á»i phÃ¡ â€“ má»™t ngÆ°á»i tráº£ lá»iâ€ Ä‘á»ƒ lÃ m rÃµ tá»«ng Ä‘iá»ƒm.

---
1. Triple vÃ  Relation Edge
â“ PhÃ¡: Trong bÆ°á»›c trÃ­ch xuáº¥t triple báº±ng OpenIE, cÃ¡c triple Ä‘Æ°á»£c táº¡o ra cÃ³ â€œhÆ°á»›ngâ€ khÃ´ng?
Tráº£ lá»i:
- CÃ³ hÆ°á»›ng:
 CÃ¡c triple Ä‘Æ°á»£c trÃ­ch xuáº¥t theo Ä‘á»‹nh dáº¡ng (subject, predicate, object) vá»‘n Ä‘Ã£ cÃ³ thá»© tá»± rÃµ rÃ ng.
  - VÃ­ dá»¥: tá»« cÃ¢u â€œMarie Curie won the Nobel Prizeâ€ ta cÃ³ triple:
 (Marie Curie, won, Nobel Prize)
  - Äiá»u nÃ y cÃ³ nghÄ©a lÃ  má»‘i quan há»‡ â€œwonâ€ chá»‰ cÃ³ Ã½ nghÄ©a tá»« â€œMarie Curieâ€ Ä‘áº¿n â€œNobel Prizeâ€.
- LÃ½ do giá»¯ hÆ°á»›ng:
  - GiÃºp phÃ¢n biá»‡t Ä‘Æ°á»£c ai lÃ m gÃ¬ vá»›i ai.
  - Báº£o toÃ n thá»© tá»± thÃ´ng tin, ráº¥t quan trá»ng trong viá»‡c reasoning (vÃ­ dá»¥: â€œwonâ€ khÃ´ng thá»ƒ Ä‘áº£o ngÆ°á»£c thÃ nh â€œNobel Prize won Marie Curieâ€ náº¿u khÃ´ng cÃ³ hÆ°á»›ng).

---
â“ PhÃ¡: Náº¿u bá» Ä‘i hÆ°á»›ng cá»§a cÃ¡c triple, liá»‡u cÃ³ lÃ m máº¥t Ä‘i thÃ´ng tin quan trá»ng khÃ´ng?
Tráº£ lá»i:
- CÃ³ máº¥t:
  - HÆ°á»›ng cá»§a edge xÃ¡c Ä‘á»‹nh má»‘i liÃªn há»‡ theo thá»© tá»± logic.
  - VÃ­ dá»¥:
    - Triple: (Marie Curie, won, Nobel Prize) mang nghÄ©a rÃµ rÃ ng.
    - Náº¿u trá»Ÿ thÃ nh edge khÃ´ng hÆ°á»›ng, ta chá»‰ biáº¿t â€œMarie Curieâ€ vÃ  â€œNobel Prizeâ€ liÃªn quan nhau nhÆ°ng khÃ´ng biáº¿t ai lÃ  tÃ¡c nhÃ¢n hay Ä‘á»‘i tÆ°á»£ng cá»§a hÃ nh Ä‘á»™ng.
- Káº¿t quáº£:
  - Viá»‡c giá»¯ hÆ°á»›ng lÃ  cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c cá»§a ngá»¯ nghÄ©a ban Ä‘áº§u tá»« vÄƒn báº£n.

---
â“ PhÃ¡: Trong Ä‘á»“ thá»‹ kiáº¿n thá»©c, vai trÃ² cá»§a relation edge cÃ³ hÆ°á»›ng lÃ  gÃ¬ trong quÃ¡ trÃ¬nh reasoning?
Tráº£ lá»i:
- GiÃºp Ä‘á»‹nh hÆ°á»›ng thÃ´ng tin:
  - Khi thá»±c hiá»‡n Personalized PageRank (PPR), hÆ°á»›ng cá»§a edge giÃºp xÃ¡c Ä‘á»‹nh luá»“ng thÃ´ng tin tá»« cÃ¡c node gá»‘c (seed nodes) sang cÃ¡c node liÃªn quan.
- Há»— trá»£ multi-hop reasoning:
  - VÃ­ dá»¥: tá»« node â€œMarie Curieâ€ qua edge â€œwonâ€ Ä‘áº¿n â€œNobel Prizeâ€, sau Ä‘Ã³ náº¿u cÃ³ edge khÃ¡c ná»‘i â€œNobel Prizeâ€ vá»›i â€œPhysicsâ€ (náº¿u Ä‘Æ°á»£c trÃ­ch xuáº¥t thÃªm), há»‡ thá»‘ng sáº½ hiá»ƒu Ä‘Æ°á»£c má»‘i liÃªn há»‡ cÃ³ thá»© tá»± giá»¯a cÃ¡c khÃ¡i niá»‡m.
- Giá»¯ nguyÃªn ngá»¯ nghÄ©a:
  - Báº±ng cÃ¡ch khÃ´ng Ä‘áº£o ngÆ°á»£c má»‘i quan há»‡, mÃ´ hÃ¬nh cÃ³ thá»ƒ truy xuáº¥t cÃ¡c thÃ´ng tin liÃªn quan chÃ­nh xÃ¡c hÆ¡n.

---
2. Synonym Edge
â“ PhÃ¡: Synonym edge Ä‘Æ°á»£c táº¡o ra dá»±a trÃªn thuáº­t toÃ¡n gÃ¬?
Tráº£ lá»i:
- Dá»±a trÃªn cosine similarity trong embedding space:
  - Má»—i phrase (hay cá»¥m tá»«) Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng vector.
  - Náº¿u cosine similarity giá»¯a hai vector vÆ°á»£t qua má»™t ngÆ°á»¡ng Ä‘á»‹nh trÆ°á»›c, thÃ¬ hai node Ä‘Ã³ Ä‘Æ°á»£c ná»‘i báº±ng má»™t synonym edge.
- Äáº·c Ä‘iá»ƒm cá»§a synonym edge:
  - KhÃ´ng cÃ³ hÆ°á»›ng: VÃ¬ náº¿u hai tá»« lÃ  Ä‘á»“ng nghÄ©a, má»‘i quan há»‡ cá»§a chÃºng thÆ°á»ng lÃ  Ä‘á»‘i xá»©ng (vÃ­ dá»¥: â€œteacherâ€ vÃ  â€œinstructorâ€ cÃ³ thá»ƒ Ä‘Æ°á»£c ná»‘i vá»›i nhau mÃ  khÃ´ng cáº§n phÃ¢n biá»‡t thá»© tá»±).

---
â“ PhÃ¡: Táº¡i sao láº¡i sá»­ dá»¥ng thuáº­t toÃ¡n embedding cho synonym thay vÃ¬ dÃ¹ng LLM trá»±c tiáº¿p?
Tráº£ lá»i:
- Hiá»‡u quáº£ vÃ  tá»‘c Ä‘á»™:
  - TÃ­nh toÃ¡n cosine similarity giá»¯a vector nhanh vÃ  hiá»‡u quáº£, khÃ´ng cáº§n gá»i LLM nhiá»u láº§n.
- Äá»™ chÃ­nh xÃ¡c cao:
  - CÃ¡c mÃ´ hÃ¬nh embedding hiá»‡n nay cÃ³ kháº£ nÄƒng báº¯t Ä‘Æ°á»£c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a ráº¥t tá»‘t, cho nÃªn phÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a lÃ  khÃ¡ chÃ­nh xÃ¡c.
- Giáº£m táº£i cho LLM:
  - Giáº£m chi phÃ­ tÃ­nh toÃ¡n, vÃ¬ khÃ´ng cáº§n LLM pháº£i xá»­ lÃ½ táº¥t cáº£ cÃ¡c cáº·p tá»« trong corpus.

---
3. Seed Node
â“ PhÃ¡: Seed node lÃ  gÃ¬ trong bá»‘i cáº£nh cá»§a HippoRAG?
Tráº£ lá»i:
- Seed Node:
  - LÃ  nhá»¯ng node trong kiáº¿n thá»©c Ä‘á»“ thá»‹ Ä‘Æ°á»£c chá»n lÃ m Ä‘iá»ƒm khá»Ÿi Ä‘áº§u cho quÃ¡ trÃ¬nh tÃ¬m kiáº¿m vÃ  lan truyá»n thÃ´ng tin (thÃ´ng qua PPR) khi cÃ³ query.
- Vai trÃ² cá»§a Seed Node:
  - ChÃºng Ä‘áº¡i diá»‡n cho cÃ¡c thÃ nh pháº§n quan trá»ng liÃªn quan Ä‘áº¿n cÃ¢u há»i Ä‘Æ°á»£c Ä‘áº·t ra.
  - VÃ­ dá»¥: Náº¿u cÃ¢u há»i lÃ  â€œBill Gates há»c á»Ÿ Ä‘Ã¢u?â€, cÃ¡c node chá»©a â€œBill Gatesâ€ hoáº·c cÃ¡c thÃ´ng tin liÃªn quan sáº½ Ä‘Æ°á»£c chá»n lÃ m seed.

---
â“ PhÃ¡: Táº¡i sao pháº£i gÃ¡n trá»ng sá»‘ (seed node weights) cho cÃ¡c Seed Node?
Tráº£ lá»i:
- GÃ¡n trá»ng sá»‘:
  - GiÃºp há»‡ thá»‘ng biáº¿t Ä‘Æ°á»£c nhá»¯ng node nÃ o quan trá»ng hÆ¡n khi thá»±c hiá»‡n lan truyá»n thÃ´ng tin trong Ä‘á»“ thá»‹.
- Lá»£i Ã­ch:
  - CÃ¡c Seed Node cÃ³ trá»ng sá»‘ cao sáº½ â€œphÃ¡t sÃ¡ngâ€ hÆ¡n trong quÃ¡ trÃ¬nh PPR, tá»« Ä‘Ã³ giÃºp tÃ¬m ra cÃ¡c node liÃªn quan tá»‘t hÆ¡n, Ä‘áº£m báº£o cÃ¢u tráº£ lá»i Ä‘Ãºng vÃ  Ä‘áº§y Ä‘á»§.
- VÃ­ dá»¥ Ä‘Æ¡n giáº£n:
  - Trong má»™t báº£n Ä‘á»“ tÆ° duy, náº¿u báº¡n Ä‘Ã¡nh dáº¥u má»™t sá»‘ Ã½ chÃ­nh vá»›i bÃºt dáº¡, nhá»¯ng Ã½ nÃ y sáº½ Ä‘Æ°á»£c chÃº Ã½ nhiá»u hÆ¡n khi báº¡n tá»•ng há»£p láº¡i kiáº¿n thá»©c.

---
4. Relation Edge vÃ  CÃ¡c KhÃ¡i Niá»‡m KhÃ¡c
â“ PhÃ¡: Relation edge cÃ³ hÆ°á»›ng nhÆ° tháº¿ nÃ o?
Tráº£ lá»i:
- Relation Edge:
  - LÃ  cÃ¡c cáº¡nh ná»‘i giá»¯a cÃ¡c node trong KG, Ä‘Æ°á»£c táº¡o ra tá»« cÃ¡c triple (subject, predicate, object).
  - ChÃºng cÃ³ hÆ°á»›ng, tá»« subject sang object, thÃ´ng qua predicate.
- Ã nghÄ©a cá»§a hÆ°á»›ng:
  - GiÃºp xÃ¡c Ä‘á»‹nh Ä‘Ãºng má»‘i liÃªn há»‡ ngá»¯ nghÄ©a (ai lÃ m gÃ¬ vá»›i gÃ¬).

---
â“ PhÃ¡: NgoÃ i cÃ¡c khÃ¡i niá»‡m trÃªn, cÃ²n khÃ¡i niá»‡m nÃ o khÃ¡c cáº§n giáº£i thÃ­ch chi tiáº¿t trong pipeline khÃ´ng?
Tráº£ lá»i:
- Context Edge:
  - Ná»‘i cÃ¡c Passage Node vá»›i cÃ¡c Phrase Node mÃ  chÃºng chá»©a Ä‘á»±ng.
  - Ã nghÄ©a: GiÃºp giá»¯ láº¡i ngá»¯ cáº£nh chi tiáº¿t cá»§a vÄƒn báº£n, há»— trá»£ factual QA.
- Denseâ€“Sparse Integration:
  - Sá»± káº¿t há»£p giá»¯a sparse coding (phrase node) vÃ  dense coding (passage node) nháº±m Ä‘áº£m báº£o vá»«a cÃ³ Ã½ chÃ­nh tÃ³m táº¯t, vá»«a cÃ³ Ä‘áº§y Ä‘á»§ chi tiáº¿t.
  - VÃ­ dá»¥ há»c sinh: NhÆ° Ä‘Ã£ nÃ³i, giá»‘ng nhÆ° vá»«a ghi nhá»› tiÃªu Ä‘á» bÃ i há»c (sparse) vÃ  vá»«a cÃ³ toÃ n bá»™ bÃ i giáº£ng (dense).

---
Káº¾T LUáº¬N
Má»—i khÃ¡i niá»‡m trong pipeline cá»§a HippoRAG 2 Ä‘á»u Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ báº£o toÃ n vÃ  khai thÃ¡c thÃ´ng tin má»™t cÃ¡ch tá»‘i Æ°u:
- Triple extraction vá»›i relation edge cÃ³ hÆ°á»›ng giá»¯ tráº­t tá»± vÃ  ngá»¯ nghÄ©a ban Ä‘áº§u.
- Synonym edge (khÃ´ng cÃ³ hÆ°á»›ng) ná»‘i cÃ¡c cá»¥m tá»« cÃ³ nghÄ©a tÆ°Æ¡ng Ä‘á»“ng, giÃºp liÃªn káº¿t thÃ´ng tin giá»¯a cÃ¡c Ä‘oáº¡n vÄƒn khÃ¡c nhau.
- Seed Node lÃ  cÃ¡c Ä‘iá»ƒm khá»Ÿi Ä‘áº§u quan trá»ng Ä‘á»ƒ báº¯t Ä‘áº§u quÃ¡ trÃ¬nh tÃ¬m kiáº¿m thÃ´ng tin trong KG, vÃ  viá»‡c gÃ¡n trá»ng sá»‘ cho chÃºng giÃºp tÄƒng cÆ°á»ng kháº£ nÄƒng lan truyá»n thÃ´ng tin trong quÃ¡ trÃ¬nh Personalized PageRank.
- Context edge vÃ  dense-sparse integration giÃºp mÃ´ hÃ¬nh khÃ´ng chá»‰ ghi nhá»› Ã½ chÃ­nh mÃ  cÃ²n giá»¯ nguyÃªn ngá»¯ cáº£nh chi tiáº¿t cá»§a vÄƒn báº£n gá»‘c.
Qua tá»«ng cÃ¢u há»i pháº£n biá»‡n, chÃºng ta nháº­n tháº¥y ráº±ng viá»‡c duy trÃ¬ hÆ°á»›ng trong cÃ¡c relation edge lÃ  ráº¥t quan trá»ng cho viá»‡c xÃ¢y dá»±ng má»™t Ä‘á»“ thá»‹ kiáº¿n thá»©c chÃ­nh xÃ¡c, tá»« Ä‘Ã³ há»— trá»£ quÃ¡ trÃ¬nh reasoning vÃ  retrieval trong toÃ n há»‡ thá»‘ng HippoRAG.
ChÃºng ta hÃ£y cÃ¹ng phÃ¢n tÃ­ch theo kiá»ƒu "má»™t ngÆ°á»i phÃ¡ â€“ má»™t ngÆ°á»i tráº£ lá»i" Ä‘á»ƒ hiá»ƒu sÃ¢u vá» cÃ¡ch thÃªm passage node sau khi Ä‘Ã£ tÃ¡ch ra bá»™ ba (triple) theo bÃ i bÃ¡o.

---
â“ PhÃ¡: Sau khi Ä‘Ã£ sá»­ dá»¥ng OpenIE tÃ¡ch ra cÃ¡c triple (subject, predicate, object) tá»« má»—i Ä‘oáº¡n vÄƒn, lÃ m tháº¿ nÃ o Ä‘á»ƒ "append" thÃªm pháº§n passage node vÃ o Ä‘á»“ thá»‹ kiáº¿n thá»©c?
Tráº£ lá»i:
 Theo bÃ i bÃ¡o, sau khi trÃ­ch xuáº¥t cÃ¡c triple tá»« má»—i passage, ta tiáº¿n hÃ nh xÃ¢y dá»±ng má»™t Knowledge Graph (KG) báº±ng cÃ¡ch thá»±c hiá»‡n bÆ°á»›c sau:
1. Táº¡o Phrase Nodes tá»« Triple:
  - Má»—i triple (vÃ­ dá»¥: (Marie Curie, won, Nobel Prize)) sáº½ táº¡o ra cÃ¡c phrase nodes cho subject, predicate vÃ  object.
  - ÄÃ¢y lÃ  cÃ¡c nÃºt Ä‘áº¡i diá»‡n cho cÃ¡c khÃ¡i niá»‡m trá»«u tÆ°á»£ng cá»§a thÃ´ng tin.
2. ThÃªm Passage Node:
  - Má»—i passage gá»‘c trong corpus Ä‘Æ°á»£c xem nhÆ° má»™t passage node riÃªng biá»‡t.
  - Sau Ä‘Ã³, append pháº§n passage node nÃ y vÃ o Ä‘á»“ thá»‹ báº±ng cÃ¡ch ná»‘i nÃ³ vá»›i táº¥t cáº£ cÃ¡c phrase node Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« passage Ä‘Ã³.
  - CÃ¡c káº¿t ná»‘i nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  context edge (hoáº·c edge "contains") vÃ¬ chÃºng biá»ƒu thá»‹ ráº±ng passage chá»©a Ä‘á»±ng cÃ¡c thÃ´ng tin hay khÃ¡i niá»‡m Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi phrase nodes.

---
â“ PhÃ¡: Táº¡i sao cáº§n thÃªm passage node? KhÃ´ng Ä‘á»§ chá»‰ cÃ³ phrase nodes sao?
Tráº£ lá»i:
- Báº£o toÃ n ngá»¯ cáº£nh:
  - Phrase nodes cung cáº¥p má»™t dáº¡ng trá»«u tÆ°á»£ng, tá»•ng quÃ¡t cá»§a thÃ´ng tin. Tuy nhiÃªn, thÃ´ng tin nÃ y cÃ³ thá»ƒ máº¥t Ä‘i chi tiáº¿t ngá»¯ cáº£nh quan trá»ng.
  - Báº±ng cÃ¡ch thÃªm passage node, há»‡ thá»‘ng lÆ°u giá»¯ Ä‘Æ°á»£c toÃ n bá»™ thÃ´ng tin chi tiáº¿t cá»§a Ä‘oáº¡n vÄƒn gá»‘c.
- Denseâ€“Sparse Integration:
  - Passage node Ä‘áº¡i diá»‡n cho pháº§n dense coding (Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh, chi tiáº¿t), trong khi phrase nodes Ä‘áº¡i diá»‡n cho sparse coding (khÃ¡i quÃ¡t, dá»… dÃ ng reasoning).
  - Viá»‡c káº¿t há»£p cáº£ hai giÃºp mÃ´ hÃ¬nh cÃ³ thá»ƒ thá»±c hiá»‡n tá»‘t hÆ¡n trong viá»‡c tráº£ lá»i cÃ¢u há»i yÃªu cáº§u vá»«a hiá»ƒu Ä‘Æ°á»£c Ã½ chÃ­nh vá»«a náº¯m báº¯t Ä‘Æ°á»£c chi tiáº¿t.

---
â“ PhÃ¡: CÃ¡ch thá»©c ná»‘i giá»¯a passage node vÃ  phrase nodes Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° tháº¿ nÃ o?
Tráº£ lá»i:
- Má»—i passage node sáº½ Ä‘Æ°á»£c ná»‘i vá»›i táº¥t cáº£ cÃ¡c phrase node Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« chÃ­nh passage Ä‘Ã³.
- CÃ¡c káº¿t ná»‘i nÃ y (context edges) khÃ´ng cáº§n cÃ³ hÆ°á»›ng rÃµ rÃ ng nhÆ° cÃ¡c relation edge cá»§a triple, mÃ  chá»§ yáº¿u nháº±m má»¥c Ä‘Ã­ch thá»ƒ hiá»‡n má»‘i liÃªn há»‡ vá» ngá»¯ cáº£nh.
- Khi thá»±c hiá»‡n retrieval hoáº·c Personalized PageRank (PPR), passage node giÃºp tÄƒng trá»ng sá»‘ cho cÃ¡c Ä‘oáº¡n vÄƒn chá»©a Ä‘á»±ng thÃ´ng tin chi tiáº¿t, tá»« Ä‘Ã³ cáº£i thiá»‡n kháº£ nÄƒng truy xuáº¥t vÃ  hiá»ƒu sÃ¢u cá»§a mÃ´ hÃ¬nh.

---
â“ PhÃ¡: Táº¡i sao khÃ´ng chá»‰ dÃ¹ng phrase nodes mÃ  bá» qua passage node?
Tráº£ lá»i:
- Náº¿u chá»‰ sá»­ dá»¥ng phrase nodes, ta chá»‰ cÃ³ Ä‘Æ°á»£c má»™t báº£n tÃ³m táº¯t ráº¥t cÃ´ Ä‘á»ng cá»§a thÃ´ng tin.
- Nhiá»u trÆ°á»ng há»£p, Ä‘áº·c biá»‡t lÃ  cÃ¡c cÃ¢u há»i factual hay yÃªu cáº§u sá»± liÃªn káº¿t sÃ¢u giá»¯a cÃ¡c Ã½, Ä‘Ã²i há»i pháº£i cÃ³ ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ tá»« Ä‘oáº¡n vÄƒn gá»‘c.
- Passage node giÃºp mÃ´ hÃ¬nh truy xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn gá»‘c Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng máº¥t Ä‘i cÃ¡c chi tiáº¿t cáº§n thiáº¿t cho viá»‡c tráº£ lá»i chÃ­nh xÃ¡c.

---
Tá»•ng Káº¿t
- BÆ°á»›c 1: TÃ¡ch cÃ¡c triple (subject, predicate, object) tá»« má»—i passage báº±ng OpenIE cá»§a LLM â†’ táº¡o ra cÃ¡c phrase nodes.
- BÆ°á»›c 2: Vá»›i má»—i passage gá»‘c, táº¡o passage node.
- BÆ°á»›c 3: Append passage node vÃ o Ä‘á»“ thá»‹ báº±ng cÃ¡ch ná»‘i nÃ³ vá»›i táº¥t cáº£ cÃ¡c phrase node tá»« passage Ä‘Ã³ thÃ´ng qua context edges.
- Má»¥c Ä‘Ã­ch: Giá»¯ láº¡i cáº£ Ã½ chÃ­nh (phrase node) vÃ  thÃ´ng tin chi tiáº¿t (passage node) Ä‘á»ƒ há»— trá»£ quÃ¡ trÃ¬nh retrieval vÃ  reasoning, tá»« Ä‘Ã³ cáº£i thiá»‡n hiá»‡u quáº£ cá»§a toÃ n há»‡ thá»‘ng.
DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch tiáº¿p cáº­n chi tiáº¿t theo dáº¡ng há»i â€“ Ä‘Ã¡p pháº£n biá»‡n (má»™t ngÆ°á»i â€œphÃ¡â€, má»™t ngÆ°á»i â€œbÃªnhâ€) Ä‘á»ƒ má»• xáº» tháº­t sÃ¢u 5 bÆ°á»›c trong Online Retrieval & QA cá»§a HippoRAG/HippoRAG 2. Má»—i cÃ¢u tráº£ lá»i Ä‘á»u cá»‘ gáº¯ng gáº¯n vá»›i láº­p luáº­n tá»« bÃ i bÃ¡o hoáº·c Ã½ tÆ°á»Ÿng chÃ­nh (cÃ³ dáº«n chá»©ng).

---
(1) Retrieving Passages and Triples
Pháº£n biá»‡n (Q):
Táº¡i sao pháº£i láº¥y cáº£ cÃ¡c Ä‘oáº¡n vÄƒn (passages) vÃ  cÃ¡c bá»™ ba (triples)? Liá»‡u chá»‰ cáº§n retrieval embedding tá»‘t Ä‘á»ƒ láº¥y Ä‘oáº¡n vÄƒn Ä‘Ã£ Ä‘á»§ chÆ°a?
Tráº£ lá»i (A):
- Cáº§n cáº£ passages: vÃ¬ cÃ¡c Ä‘oáº¡n vÄƒn mang Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh. Náº¿u chá»‰ dá»±a vÃ o node (thÆ°á»ng lÃ  má»™t cÃ¢u/triple rá»i ráº¡c) thÃ¬ cÃ³ thá»ƒ thiáº¿u máº¥t chi tiáº¿t quan trá»ng.
- Cáº§n cáº£ triples: vÃ¬ triple lÃ  â€œÄ‘Æ¡n vá»‹ thÃ´ng tinâ€ mang tÃ­nh khÃ¡i quÃ¡t â€“ giÃºp mÃ´ phá»ng sá»± liÃªn káº¿t (associativity). Khi ta kÃ©o Ä‘Æ°á»£c triple liÃªn quan, nÃ³ cÃ³ thá»ƒ â€œgá»£i Ã½â€ nhiá»u quan há»‡ sÃ¢u hÆ¡n mÃ  má»™t Ä‘oáº¡n vÄƒn chÆ°a cháº¯c Ä‘Ã£ hiá»ƒn thá»‹ ngay.
- Dáº«n chá»©ng: Pháº§n â€œOnline Retrievalâ€ trong bÃ i HippoRAG/HippoRAG 2 cho tháº¥y, há»‡ thá»‘ng cháº¡y query-to-triple Ä‘á»ƒ láº¥y triple trÆ°á»›c, sau Ä‘Ã³ má»›i cháº¡y tiáº¿p cÃ¡c bÆ°á»›c khÃ¡c. Káº¿t há»£p â€œpassage nodeâ€ vÃ  â€œphrase nodeâ€ (triple) trong Ä‘á»“ thá»‹ lÃ  cÃ¡ch mÃ  mÃ´ hÃ¬nh â€œtrá»™nâ€ giá»¯a â€œcontextâ€ vÃ  â€œconceptsâ€.

---
(2) Recognition Memory (Triple Filtering)
Pháº£n biá»‡n (Q):
Táº¡i sao láº¡i cáº§n má»™t bÆ°á»›c gá»i lÃ  â€œRecognition Memoryâ€ Ä‘á»ƒ lá»c triple? Cháº³ng pháº£i retrieval encoder Ä‘Ã£ xáº¿p háº¡ng Ä‘Æ°á»£c rá»“i sao?
Tráº£ lá»i (A):
- Encoder-based ranking chá»‰ dá»±a trÃªn vector similarity, cÃ³ thá»ƒ nháº·t pháº£i nhá»¯ng triple â€œna nÃ¡â€ vá» tá»« vá»±ng nhÆ°ng khÃ´ng tháº­t sá»± liÃªn quan ngá»¯ nghÄ©a.
- LLM-based filtering (Recognition Memory) giá»‘ng nhÆ° â€œduyá»‡t láº¡iâ€ nhá»¯ng triple top-k, Ä‘á»ƒ xÃ¡c nháº­n: â€œTriple nÃ y thá»±c sá»± nÃ³i vá» cÃ¡i gÃ¬, cÃ³ Äƒn khá»›p vá»›i cÃ¢u há»i khÃ´ng?â€ â€“ Tá»« Ä‘Ã³ loáº¡i bá»›t nhiá»…u.
- VÃ­ dá»¥: CÃ³ nhá»¯ng triple mÃ  entity giá»‘ng nhau vá» máº·t ngÃ´n ngá»¯ (vÃ­ dá»¥ â€œParisâ€ â€“ má»™t ngÆ°á»i tÃªn Paris vs. thÃ nh phá»‘ Paris). Náº¿u chá»‰ dÃ¹ng embedding, Ä‘Ã´i khi bá»‹ láº«n. BÆ°á»›c filtering nÃ y giÃºp LLM dÃ¹ng ngá»¯ cáº£nh Ä‘á»ƒ phÃ¢n biá»‡t.
- Dáº«n chá»©ng: BÃ i bÃ¡o giá»›i thiá»‡u cÆ¡ cháº¿ â€œtwo-step retrievalâ€: 1) Query-to-triple (embedding) vÃ  2) Triple filter (LLM) â€“ mÃ´ phá»ng â€œrecall vs. recognitionâ€ trong trÃ­ nhá»› con ngÆ°á»i.

---
(3) Assigning Seed Node Weights
Pháº£n biá»‡n (Q):
PhÃ¢n phá»‘i trá»ng sá»‘ (reset probability) cho cÃ¡c â€œseed nodeâ€ â€“ Ä‘áº·c biá»‡t lÃ  giá»¯a â€œpassage nodeâ€ vÃ  â€œphrase nodeâ€ â€“ cÃ³ thá»±c sá»± cáº§n thiáº¿t, hay chá»‰ cáº§n gÃ¡n Ä‘á»u lÃ  Ä‘Æ°á»£c?
Tráº£ lá»i (A):
- KhÃ´ng thá»ƒ gÃ¡n Ä‘á»u: Bá»Ÿi vÃ¬ â€œphrase nodeâ€ (cÃ¡c khÃ¡i niá»‡m) vÃ  â€œpassage nodeâ€ (cÃ¡c vÄƒn báº£n) Ä‘Ã³ng vai trÃ² khÃ¡c nhau. Náº¿u gÃ¡n trá»ng sá»‘ giá»‘ng nhau, mÃ´ hÃ¬nh cÃ³ thá»ƒ nghiÃªng quÃ¡ nhiá»u vá» concept hoáº·c quÃ¡ nhiá»u vá» context.
- Cáº§n cÃ¢n báº±ng: Cháº³ng háº¡n, khi cÃ¢u há»i ráº¥t cá»¥ thá»ƒ (vÃ­ dá»¥: â€œTÃ¡c giáº£ cuá»‘n sÃ¡ch X lÃ  ai?â€), â€œphrase nodeâ€ vá» â€œbook_authorâ€ cÃ³ thá»ƒ quan trá»ng hÆ¡n. NhÆ°ng náº¿u cÃ¢u há»i phá»©c táº¡p, â€œpassage nodeâ€ cÃ³ ngá»¯ cáº£nh rá»™ng hÆ¡n sáº½ cáº§n trá»ng sá»‘ cao hÆ¡n Ä‘á»ƒ PPR lan tá»a nhiá»u hÆ¡n.
- Dáº«n chá»©ng: TÃ¡c giáº£ cÃ³ lÃ m ablation study (vÃ­ dá»¥ trong pháº§n Implementation & Discussion) Ä‘á»ƒ chá»©ng minh pháº£i chá»n â€œweight factorâ€ cho passage node há»£p lÃ½ (0.01, 0.05, v.v.) thÃ¬ má»›i tá»‘i Æ°u Recall@5.

---
(4) PPR Graph Search
Pháº£n biá»‡n (Q):
Personalized PageRank (PPR) thá»±c sá»± giÃºp Ä‘Æ°á»£c gÃ¬? Liá»‡u má»™t vÃ²ng láº·p iterative retrieval (nhÆ° IRCoT) khÃ´ng pháº£i Ä‘Ã£ giáº£i quyáº¿t Ä‘Æ°á»£c multi-hop sao?
Tráº£ lá»i (A):
- PPR lÃ m multi-hop ngay trong má»™t lÆ°á»£t: Thay vÃ¬ pháº£i gá»i LLM liÃªn tiáº¿p Ä‘á»ƒ â€œgá»£i Ã½â€ truy váº¥n phá»¥ (iterative retrieval), PPR cho phÃ©p khuáº¿ch tÃ¡n xÃ¡c suáº¥t tá»« cÃ¡c seed node (passage/phrase) Ä‘áº¿n lÃ¢n cáº­n trong KG.
- Hiá»‡u quáº£ chi phÃ­: Iterative retrieval kiá»ƒu IRCoT thÆ°á»ng tá»‘n nhiá»u lÆ°á»£t gá»i LLM. PPR lÃ  má»™t thuáº­t toÃ¡n Ä‘á»“ thá»‹ cháº¡y ná»™i bá»™, ráº» hÆ¡n vá» máº·t inference.
- PhÃ¹ há»£p vá»›i â€œassociative memoryâ€: BÃ i bÃ¡o gá»£i Ã½ ráº±ng PPR mÃ´ phá»ng cÃ¡ch hippocampus (nÃ£o bá»™) kÃ­ch hoáº¡t cÃ¡c liÃªn káº¿t giá»¯a khÃ¡i niá»‡m. Tá»« 1 seed node, lan ra nhiá»u nÃºt khÃ¡c theo xÃ¡c suáº¥t.
- Dáº«n chá»©ng: HippoRAG/HippoRAG 2 so sÃ¡nh vá»›i IRCoT, cho tháº¥y PPR cÃ³ thá»ƒ â€œgÃ³iâ€ viá»‡c multi-hop reasoning trong má»™t láº§n retrieval, nhanh hÆ¡n vÃ  Ã­t tá»‘n chi phÃ­ hÆ¡n.

---
(5) QA Reading with Selected Passages
Pháº£n biá»‡n (Q):
Náº¿u cuá»‘i cÃ¹ng váº«n pháº£i gá»i LLM Ä‘á»ƒ Ä‘á»c láº¡i cÃ¡c Ä‘oáº¡n vÄƒn (QA reading), váº­y viá»‡c xÃ¢y dá»±ng KG hay PPR cÃ³ tháº­t sá»± quan trá»ng? Cháº³ng pháº£i LLM giá» Ä‘Ã£ â€œsáºµn sÃ ngâ€ tráº£ lá»i náº¿u cho nÃ³ Ä‘á»§ context?
Tráº£ lá»i (A):
- LLM cáº§n â€œÄ‘áº§u vÃ o sáº¡châ€: DÃ¹ LLM máº¡nh, nhÆ°ng náº¿u input context bá»‹ nhiá»…u (chá»n sai passage) hoáº·c quÃ¡ dÃ n tráº£i, LLM dá»… tráº£ lá»i sai, Ä‘áº·c biá»‡t vá»›i cÃ¢u há»i cáº§n multi-hop.
- KG + PPR giÃºp â€œtÃ³m gá»nâ€ Ä‘Ãºng Ä‘oáº¡n vÄƒn liÃªn quan, thay vÃ¬ lÃ´i táº¥t cáº£. LLM cÃ ng cÃ³ context chÃ­nh xÃ¡c, tráº£ lá»i cÃ ng tá»‘t.
- TÃ­nh má»Ÿ rá»™ng: Khi táº­p dá»¯ liá»‡u ráº¥t lá»›n, chá»‰ embedding retrieval cÃ³ thá»ƒ bá» sÃ³t â€œchuá»—i logicâ€ liÃªn quan. Bá»• sung KG + PPR => MÃ´ hÃ¬nh â€œliÃªn káº¿tâ€ tá»‘t hÆ¡n, LLM Ä‘á»¡ bá»‹ quÃ¡ táº£i.
- Dáº«n chá»©ng: Káº¿t quáº£ thá»±c nghiá»‡m (báº£ng so sÃ¡nh F1, EM) cho tháº¥y HippoRAG 2 sau khi chá»n top passages qua PPR, rá»“i cho LLM Ä‘á»c, cho káº¿t quáº£ cao hÆ¡n so vá»›i retrieval thÆ°á»ng.

---
Tá»•ng káº¿t
5 bÆ°á»›c trong Online Retrieval & QA Ä‘Æ°á»£c thiáº¿t káº¿ cÃ³ chá»§ Ä‘Ã­ch, mÃ´ phá»ng cÆ¡ cháº¿ bá»™ nhá»› cá»§a con ngÆ°á»i:
1. Láº¥y passage + triple song song (Ä‘áº£m báº£o vá»«a cÃ³ context vá»«a cÃ³ concept).
2. Lá»c triple báº±ng LLM (Recognition Memory) Ä‘á»ƒ loáº¡i nhiá»…u.
3. GÃ¡n trá»ng sá»‘ seed node cáº©n tháº­n (cÃ¢n báº±ng passage node vÃ  phrase node).
4. Cháº¡y PPR Ä‘á»ƒ thá»±c hiá»‡n multi-hop reasoning trong má»™t bÆ°á»›c, tiáº¿t kiá»‡m chi phÃ­ so vá»›i iterative retrieval.
5. Cuá»‘i cÃ¹ng, LLM má»›i Ä‘á»c cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Ã£ chá»n Ä‘á»ƒ tráº£ lá»i, tá»‘i Æ°u Ä‘á»™ chÃ­nh xÃ¡c.
Tá»«ng bÆ°á»›c Ä‘á»u cÃ³ lÃ½ do tá»“n táº¡i, vÃ  káº¿t quáº£ thá»±c nghiá»‡m chá»©ng minh mÃ´ hÃ¬nh nÃ y tÄƒng hiá»‡u quáº£ truy xuáº¥t (retrieval) cÅ©ng nhÆ° Ä‘á»™ chÃ­nh xÃ¡c khi tráº£ lá»i cÃ¢u há»i (QA).

---
(3) Assigning Seed Node Weights
Äáº·t cÃ¢u há»i phÃ¡ nhiá»u hÆ¡n á»Ÿ bÆ°á»›c nÃ y 
- Cháº³ng háº¡n: lÃºc mÃ  xÃ¢y Graph cÃ¡c nodes vai trÃ² nhÆ° nhau hay lÃ  Hippo RAG 2 cÃ³ Ä‘Ã¡nh dáº¥u rÃµ lÃ  phrase node hay passage node ? 
- CÃ¡ch Ä‘Ã¡nh Assing Seed Node Weights nhÆ° nÃ o trong bÃ i bÃ¡o cÃ³ nÃ³i ko 
- ... nhiá»u cÃ¢u ná»¯a nhÃ©

Ã½ lÃ  phÃ¡ 1, mÃ¬nh biáº¿t lÃ  nÃ³ Ä‘Æ°á»£c rÃµ rÃ ng vai trÃ², nhÆ°ng lÃºc Ä‘Æ°a lÃªn Ä‘á»“ thá»‹ Ã½ thÃ¬ nÃ³ cÃ³ Ä‘Æ°á»£c rÃµ rÃ ng váº­y khÃ´ng ? kiá»ƒu há» dÃ¹ng cÃ¡ch nÃ o Ä‘á»ƒ Ä‘Ã¡nh dáº¥u 2 loáº¡i node Ä‘Ã³ Ä‘á»ƒ khi query dÃ¹ng

Trong quÃ¡ trÃ¬nh xÃ¢y dá»±ng Ä‘á»“ thá»‹, HippoRAG 2 khÃ´ng chá»‰ trÃ­ch xuáº¥t cÃ¡c node mÃ  cÃ²n "Ä‘Ã¡nh dáº¥u" rÃµ rÃ ng tá»«ng loáº¡i node thÃ´ng qua cáº¥u trÃºc vÃ  cÃ¡c loáº¡i cáº¡nh ná»‘i káº¿t:
- PhÃ¢n loáº¡i qua nguá»“n gá»‘c táº¡o node:
  - Phrase node: ÄÆ°á»£c táº¡o ra tá»« viá»‡c trÃ­ch xuáº¥t bá»™ ba (triple) cá»§a cÃ¡c khÃ¡i niá»‡m qua OpenIE. Nhá»¯ng node nÃ y chá»‰ chá»©a thÃ´ng tin "concept" rÃºt gá»n vÃ  liÃªn quan Ä‘áº¿n cÃ¡c má»‘i quan há»‡ giá»¯a thá»±c thá»ƒ.
  - Passage node: Äáº¡i diá»‡n cho toÃ n bá»™ Ä‘oáº¡n vÄƒn, chá»©a Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh vÃ  thÃ´ng tin chi tiáº¿t.
 Qua Ä‘Ã³, quÃ¡ trÃ¬nh offline indexing cá»§a HippoRAG 2 Ä‘Ã£ phÃ¢n biá»‡t rÃµ rÃ ng nguá»“n gá»‘c cá»§a cÃ¡c node.
- ÄÃ¡nh dáº¥u qua loáº¡i cáº¡nh (edge type):
  - CÃ¡c passage node Ä‘Æ°á»£c liÃªn káº¿t vá»›i cÃ¡c phrase node thÃ´ng qua cÃ¡c cáº¡nh mang nhÃ£n "contains" (hoáº·c "context edge"), nháº±m thá»ƒ hiá»‡n ráº±ng Ä‘oáº¡n vÄƒn chá»©a cÃ¡c cá»¥m tá»« (phrase) Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« nÃ³.
  - Trong khi Ä‘Ã³, cÃ¡c phrase node cÃ³ thá»ƒ Ä‘Æ°á»£c ná»‘i vá»›i nhau qua cÃ¡c cáº¡nh khÃ¡c nhÆ° "synonym edge" hay "relation edge". Viá»‡c nÃ y giÃºp nháº¥n máº¡nh má»‘i liÃªn há»‡ giá»¯a cÃ¡c khÃ¡i niá»‡m trá»«u tÆ°á»£ng, tÃ¡ch biá»‡t vá»›i thÃ´ng tin ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ tá»« passage.
- TÃ¡c Ä‘á»™ng khi query:
 Khi má»™t truy váº¥n Ä‘Æ°á»£c Ä‘Æ°a vÃ o, há»‡ thá»‘ng sá»­ dá»¥ng cÃ¡c embedding vÃ  trá»ng sá»‘ riÃªng biá»‡t cho tá»«ng loáº¡i node. Nhá» váº­y, viá»‡c so sÃ¡nh cosine similarity Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cáº£ phrase node vÃ  passage node theo cÃ¡ch phÃ¹ há»£p vá»›i Ä‘áº·c trÆ°ng cá»§a chÃºng. CÃ¡ch gÃ¡n trá»ng sá»‘ (seed node weights) cÅ©ng Ä‘Æ°á»£c Ä‘iá»u chá»‰nh riÃªng cho tá»«ng loáº¡i, giÃºp quÃ¡ trÃ¬nh Personalized PageRank lan tá»a thÃ´ng tin Ä‘Ãºng theo cáº¥u trÃºc Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u.
Dáº«n chá»©ng: Trong pháº§n mÃ´ táº£ cá»§a HippoRAG 2, tÃ¡c giáº£ trÃ¬nh bÃ y ráº±ng má»—i passage trong corpus Ä‘Æ°á»£c xá»­ lÃ½ nhÆ° má»™t passage node vÃ  Ä‘Æ°á»£c ná»‘i vá»›i táº¥t cáº£ cÃ¡c phrase Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« nÃ³ thÃ´ng qua cÃ¡c "context edge". Äiá»u nÃ y khÃ´ng chá»‰ giÃºp xÃ¢y dá»±ng má»™t kiáº¿n trÃºc Ä‘á»“ thá»‹ rÃµ rÃ ng mÃ  cÃ²n cho phÃ©p há»‡ thá»‘ng phÃ¢n biá»‡t vÃ  Ã¡p dá»¥ng trá»ng sá»‘ khÃ¡c nhau cho má»—i loáº¡i node trong quÃ¡ trÃ¬nh truy xuáº¥t.
NhÆ° váº­y, báº±ng cÃ¡ch phÃ¢n loáº¡i qua nguá»“n gá»‘c táº¡o node vÃ  Ä‘Ã¡nh dáº¥u thÃ´ng qua cÃ¡c loáº¡i cáº¡nh ná»‘i káº¿t, HippoRAG 2 Ä‘áº£m báº£o ráº±ng trong quÃ¡ trÃ¬nh truy váº¥n, há»‡ thá»‘ng cÃ³ thá»ƒ nháº­n diá»‡n vÃ  táº­n dá»¥ng Ä‘Ãºng vai trÃ² cá»§a tá»«ng loáº¡i node, tá»« Ä‘Ã³ cáº£i thiá»‡n hiá»‡u quáº£ cá»§a retrieval vÃ  QA.

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ chi tiáº¿t, kÃ¨m theo Ä‘á»‘i thoáº¡i â€œpháº£n biá»‡n â€“ tráº£ lá»iâ€ minh há»a rÃµ rÃ ng cÃ¡ch Assigning Seed Node Weights hoáº¡t Ä‘á»™ng trong quÃ¡ trÃ¬nh Online Retrieval cá»§a HippoRAG.

---
VÃ­ dá»¥ cá»¥ thá»ƒ
Truy váº¥n: "TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?"
Sau khi há»‡ thá»‘ng xá»­ lÃ½ truy váº¥n, ta thu Ä‘Æ°á»£c cÃ¡c thÃ nh pháº§n sau:
1. TrÃ­ch xuáº¥t tá»« truy váº¥n (Query Extraction):
  - CÃ¡c thá»±c thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t:
    - â€œThe Great Novelâ€
    - â€œtÃ¡c giáº£â€
2. Seed Nodes Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trong Knowledge Graph (KG):
  - Phrase Nodes (Ä‘áº¡i diá»‡n cho khÃ¡i niá»‡m, thÃ´ng tin trÃ­ch xuáº¥t tá»« cÃ¡c triple):
    - Node A: "tÃ¡c giáº£"
      - GiÃ¡ trá»‹ similarity (vá»›i "tÃ¡c giáº£" truy váº¥n): 0.72
    - Node B: "The Great Novel"
      - GiÃ¡ trá»‹ similarity (vá»›i tiÃªu Ä‘á» truy váº¥n): 0.68
  - Passage Nodes (Ä‘oáº¡n vÄƒn cung cáº¥p ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§):
    - Node C: Äoáº¡n vÄƒn 1: â€œThe Great Novel, má»™t tÃ¡c pháº©m kinh Ä‘iá»ƒn, Ä‘Æ°á»£c viáº¿t bá»Ÿi John Doe â€“ tÃ¡c giáº£ ná»•i tiáº¿ng vá»›i nhiá»u giáº£i thÆ°á»Ÿng vÄƒn há»c."
      - GiÃ¡ trá»‹ similarity (vá»›i truy váº¥n, dá»±a trÃªn cáº£ ngá»¯ cáº£nh vÃ  tá»« khÃ³a): 0.80
    - Node D: Äoáº¡n vÄƒn 2: Má»™t Ä‘oáº¡n vÄƒn khÃ¡c nÃ³i vá» John Doe nhÆ°ng khÃ´ng Ä‘á» cáº­p rÃµ rÃ ng Ä‘áº¿n cuá»‘n sÃ¡ch.
      - GiÃ¡ trá»‹ similarity: 0.55

---
BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh Seed Nodes vÃ  GÃ¡n Trá»ng Sá»‘ Ban Ä‘áº§u
Giáº£ sá»­ há»‡ thá»‘ng quy Æ°á»›c:
- Phrase Nodes nháº­n trá»ng sá»‘ máº·c Ä‘á»‹nh = 1.0
- Passage Nodes ban Ä‘áº§u nháº­n trá»ng sá»‘ tháº¥p hÆ¡n, vÃ¬ chÃºng mang thÃ´ng tin cá»¥ thá»ƒ nhÆ°ng cáº§n Ä‘Æ°á»£c â€œÄ‘iá»u chá»‰nhâ€ qua weight factor.
  - VÃ­ dá»¥: há»‡ thá»‘ng gÃ¡n weight factor cho Passage Nodes = 0.05.
Do Ä‘Ã³, ta cÃ³:
- Node A ("tÃ¡c giáº£"):
  - Similarity score = 0.72 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.72 Ã— 1.0 = 0.72
- Node B ("The Great Novel"):
  - Similarity score = 0.68 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.68 Ã— 1.0 = 0.68
- Node C (Passage 1):
  - Similarity score = 0.80 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.80 Ã— 0.05 = 0.04
- Node D (Passage 2):
  - Similarity score = 0.55 â†’ trá»ng sá»‘ ban Ä‘áº§u = 0.55 Ã— 0.05 = 0.0275

---
BÆ°á»›c 2: Äá»‘i thoáº¡i pháº£n biá»‡n â€“ Giáº£i thÃ­ch chi tiáº¿t
Pháº£n biá»‡n 1: Táº¡i sao Passage Nodes láº¡i Ä‘Æ°á»£c nhÃ¢n vá»›i weight factor tháº¥p (0.05) trong khi Phrase Nodes giá»¯ nguyÃªn trá»ng sá»‘ 1.0?
Q: Náº¿u Passage Nodes mang toÃ n bá»™ ngá»¯ cáº£nh cá»§a Ä‘oáº¡n vÄƒn, táº¡i sao chÃºng láº¡i Ä‘Æ°á»£c gÃ¡n trá»ng sá»‘ tháº¥p hÆ¡n so vá»›i Phrase Nodes?
A:
- LÃ½ do:
  - Phrase Nodes Ä‘áº¡i diá»‡n cho cÃ¡c khÃ¡i niá»‡m trá»«u tÆ°á»£ng, thÆ°á»ng cÃ³ tÃ­nh khÃ¡i quÃ¡t vÃ  cÃ³ thá»ƒ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n truy váº¥n. ChÃºng cáº§n Ä‘Æ°á»£c Æ°u tiÃªn cao Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c khÃ¡i niá»‡m chÃ­nh Ä‘Æ°á»£c nháº­n diá»‡n.
  - Passage Nodes máº·c dÃ¹ chá»©a ngá»¯ cáº£nh chi tiáº¿t, nhÆ°ng náº¿u gÃ¡n trá»ng sá»‘ quÃ¡ cao, thÃ´ng tin quÃ¡ â€œphÃ¢n tÃ¡nâ€ cÃ³ thá»ƒ lÃ m nhiá»…u káº¿t quáº£ retrieval. Weight factor tháº¥p giÃºp kiá»ƒm soÃ¡t sá»± lan tá»a cá»§a thÃ´ng tin cá»¥ thá»ƒ, chá»‰ khi mÃ  cÃ¡c passage cÃ³ nhiá»u liÃªn káº¿t vá»›i cÃ¡c Phrase Nodes, chÃºng má»›i â€œÄ‘Æ°á»£c bÃ¹ Ä‘áº¯pâ€ thÃ´ng qua quÃ¡ trÃ¬nh PPR.
- Dáº«n chá»©ng thá»±c nghiá»‡m:
  - BÃ i bÃ¡o trÃ¬nh bÃ y cÃ¡c thÃ­ nghiá»‡m ablation vá»›i cÃ¡c giÃ¡ trá»‹ weight factor khÃ¡c nhau (vÃ­ dá»¥: 0.01, 0.05, 0.1,...) cho Passage Nodes. Káº¿t quáº£ cho tháº¥y, khi chá»n weight factor phÃ¹ há»£p (0.05 trong vÃ­ dá»¥ nÃ y), há»‡ thá»‘ng Ä‘áº¡t Recall@5 tá»‘i Æ°u. Äiá»u nÃ y chá»©ng tá» ráº±ng viá»‡c cÃ¢n báº±ng giá»¯a khÃ¡i niá»‡m vÃ  ngá»¯ cáº£nh lÃ  ráº¥t quan trá»ng.

---
Pháº£n biá»‡n 2: Viá»‡c nhÃ¢n weight factor áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘áº¿n káº¿t quáº£ ban Ä‘áº§u cá»§a PPR?
Q: Vá»›i cÃ¡c trá»ng sá»‘ ban Ä‘áº§u nhÆ° trÃªn, lÃ m tháº¿ nÃ o Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng Passage Node C â€“ máº·c dÃ¹ cÃ³ similarity score cao (0.80) â€“ cÃ³ thá»ƒ cáº¡nh tranh Ä‘Æ°á»£c vá»›i Phrase Nodes khi weight factor tháº¥p?
A:
- Giáº£i thÃ­ch:
  - Ban Ä‘áº§u, Passage Node C cÃ³ giÃ¡ trá»‹ trá»ng sá»‘ chá»‰ lÃ  0.04, so vá»›i 0.72 (Node A) vÃ  0.68 (Node B).
  - Tuy nhiÃªn, trong quÃ¡ trÃ¬nh PPR, trá»ng sá»‘ nÃ y sáº½ Ä‘Æ°á»£c â€œlan tá»aâ€ qua cÃ¡c cáº¡nh trong Knowledge Graph. Náº¿u Passage Node C Ä‘Æ°á»£c liÃªn káº¿t máº¡nh máº½ vá»›i cÃ¡c Phrase Nodes cÃ³ trá»ng sá»‘ cao, cÃ¡c cáº¡nh liÃªn káº¿t nÃ y sáº½ giÃºp â€œkÃ©oâ€ thÃªm probability mass cho Node C.
  - VÃ­ dá»¥: náº¿u Node C cÃ³ nhiá»u liÃªn káº¿t Ä‘áº¿n Node A vÃ  Node B, quÃ¡ trÃ¬nh PPR sáº½ cá»™ng dá»“n thÃªm giÃ¡ trá»‹ cho Node C, giÃºp nÃ³ Ä‘Æ°á»£c Æ°u tiÃªn khi xáº¿p háº¡ng cÃ¡c Ä‘oáº¡n vÄƒn cuá»‘i cÃ¹ng.
- Dáº«n chá»©ng:
  - BÃ i bÃ¡o cho biáº¿t ráº±ng káº¿t há»£p giá»¯a distribution tá»« PPR vÃ  ma tráº­n xuáº¥t hiá»‡n (P matrix) cho phÃ©p cÃ¡c passage nháº­n Ä‘Æ°á»£c Ä‘iá»ƒm tá»•ng há»£p cao hÆ¡n, náº¿u chÃºng Ä‘Æ°á»£c liÃªn káº¿t cháº·t cháº½ vá»›i cÃ¡c Phrase Nodes cÃ³ trá»ng sá»‘ ban Ä‘áº§u cao.

---
Pháº£n biá»‡n 3: Giáº£ sá»­ cÃ³ má»™t Passage Node D cÃ³ similarity score tháº¥p (0.55 â†’ 0.0275), liá»‡u nÃ³ cÃ³ bá»‹ loáº¡i bá» hoÃ n toÃ n hay váº«n cÃ³ vai trÃ²?
Q: Passage Node D cÃ³ Ä‘iá»ƒm ban Ä‘áº§u ráº¥t tháº¥p, nhÆ°ng liá»‡u nÃ³ cÃ³ Ä‘Ã³ng gÃ³p hay khÃ´ng trong quÃ¡ trÃ¬nh retrieval?
A:
- Giáº£i thÃ­ch:
  - Máº·c dÃ¹ Passage Node D cÃ³ similarity score tháº¥p vÃ  trá»ng sá»‘ ban Ä‘áº§u chá»‰ lÃ  0.0275, náº¿u nÃ³ cÃ³ má»™t sá»‘ liÃªn káº¿t há»¯u Ã­ch trong KG, Ä‘áº·c biá»‡t lÃ  vá»›i cÃ¡c Phrase Nodes cÃ³ Ä‘iá»ƒm cao, quÃ¡ trÃ¬nh PPR cÃ³ thá»ƒ â€œnÃ¢ng Ä‘á»¡â€ giÃ¡ trá»‹ cá»§a nÃ³ qua viá»‡c cá»™ng dá»“n tá»« cÃ¡c nÃºt lÃ¢n cáº­n.
  - Tuy nhiÃªn, náº¿u khÃ´ng cÃ³ liÃªn káº¿t máº¡nh máº½ nÃ o, Node D cÃ³ thá»ƒ khÃ´ng Ä‘Ã³ng gÃ³p nhiá»u vÃ  sáº½ Ä‘Æ°á»£c xáº¿p háº¡ng tháº¥p. Äiá»u nÃ y giÃºp há»‡ thá»‘ng lá»c ra cÃ¡c passage khÃ´ng Ä‘á»§ liÃªn quan.
- Dáº«n chá»©ng:
  - Káº¿t quáº£ thá»±c nghiá»‡m vÃ  ablation study trong bÃ i bÃ¡o chá»‰ ra ráº±ng nhá»¯ng passage khÃ´ng Ä‘Æ°á»£c liÃªn káº¿t cháº·t cháº½ sáº½ nháº­n Ä‘iá»ƒm tháº¥p sau PPR, tá»« Ä‘Ã³ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ cuá»‘i cÃ¹ng.

---
BÆ°á»›c 3: TÃ­ch há»£p qua PPR
Sau khi cÃ¡c seed node Ä‘Æ°á»£c gÃ¡n trá»ng sá»‘, há»‡ thá»‘ng cháº¡y thuáº­t toÃ¡n Personalized PageRank trÃªn KG. QuÃ¡ trÃ¬nh nÃ y sáº½:
- Lan tá»a probability mass: Tá»« cÃ¡c seed node ban Ä‘áº§u, giÃ¡ trá»‹ sáº½ Ä‘Æ°á»£c lan tá»a qua cÃ¡c cáº¡nh liÃªn káº¿t.
- Káº¿t há»£p vá»›i P matrix: Sau Ä‘Ã³, probability mass Ä‘Æ°á»£c nhÃ¢n vá»›i ma tráº­n P chá»©a thÃ´ng tin táº§n sá»‘ xuáº¥t hiá»‡n cá»§a cÃ¡c nÃºt trong cÃ¡c Ä‘oáº¡n vÄƒn, giÃºp xÃ¡c Ä‘á»‹nh thá»© háº¡ng cuá»‘i cÃ¹ng cho cÃ¡c passage.
Káº¿t quáº£ cuá»‘i cÃ¹ng, máº·c dÃ¹ ban Ä‘áº§u Passage Node C chá»‰ cÃ³ trá»ng sá»‘ 0.04, nhÆ°ng náº¿u liÃªn káº¿t tá»‘t, nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c â€œnÃ¢ngâ€ lÃªn vÃ  xáº¿p háº¡ng cao, giÃºp há»‡ thá»‘ng truy xuáº¥t Ä‘oáº¡n vÄƒn chá»©a thÃ´ng tin â€œJohn Doe lÃ  tÃ¡c giáº£ cá»§a The Great Novelâ€ má»™t cÃ¡ch chÃ­nh xÃ¡c.

---
Tá»•ng káº¿t vÃ­ dá»¥
1. TrÃ­ch xuáº¥t vÃ  xÃ¡c Ä‘á»‹nh Seed Nodes:
  - Phrase Nodes: "tÃ¡c giáº£" (0.72), "The Great Novel" (0.68)
  - Passage Nodes: Passage 1 (0.80 â†’ 0.04 sau nhÃ¢n), Passage 2 (0.55 â†’ 0.0275)
2. GÃ¡n trá»ng sá»‘:
  - Phrase Nodes giá»¯ trá»ng sá»‘ Ä‘áº§y Ä‘á»§ (1.0)
  - Passage Nodes nhÃ¢n vá»›i weight factor (0.05)
3. QuÃ¡ trÃ¬nh PPR:
  - Lan tá»a probability mass tá»« cÃ¡c seed node qua cÃ¡c cáº¡nh cá»§a KG.
  - CÃ¡c Passage Nodes, máº·c dÃ¹ cÃ³ trá»ng sá»‘ ban Ä‘áº§u tháº¥p, sáº½ nháº­n Ä‘Æ°á»£c thÃªm giÃ¡ trá»‹ náº¿u Ä‘Æ°á»£c liÃªn káº¿t máº¡nh máº½ vá»›i cÃ¡c Phrase Nodes.
4. Káº¿t quáº£ cuá»‘i cÃ¹ng:
  - Passage Node chá»©a Ä‘oáº¡n vÄƒn chÃ­nh vá» John Doe sáº½ Ä‘Æ°á»£c xáº¿p háº¡ng cao, Ä‘áº£m báº£o há»‡ thá»‘ng tráº£ lá»i chÃ­nh xÃ¡c cho truy váº¥n "TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?".
Qua vÃ­ dá»¥ nÃ y, chÃºng ta tháº¥y ráº±ng viá»‡c Assigning Seed Node Weights khÃ´ng chá»‰ lÃ  má»™t thao tÃ¡c Ä‘á»‹nh lÆ°á»£ng ban Ä‘áº§u, mÃ  cÃ²n lÃ  bÆ°á»›c quyáº¿t Ä‘á»‹nh Ä‘á»ƒ cÃ¢n báº±ng giá»¯a thÃ´ng tin khÃ¡i niá»‡m vÃ  ngá»¯ cáº£nh, tá»« Ä‘Ã³ tá»‘i Æ°u quÃ¡ trÃ¬nh PPR vÃ  cáº£i thiá»‡n hiá»‡u quáº£ cá»§a há»‡ thá»‘ng truy xuáº¥t.
ÄÃºng váº­y, khi nÃ³i vá» trá»ng sá»‘ máº·c Ä‘á»‹nh cho Phrase Nodes, ta cÃ³ thá»ƒ Ä‘áº·t weight factor lÃ  1.0, nhÆ°ng Ä‘iá»u nÃ y khÃ´ng cÃ³ nghÄ©a lÃ  giÃ¡ trá»‹ similarity cá»§a tá»«ng node sáº½ luÃ´n lÃ  1.0.
Cá»¥ thá»ƒ:
- Trá»ng sá»‘ (weight factor) cá»§a Phrase Nodes: Khi gÃ¡n trá»ng sá»‘ cho cÃ¡c Phrase Nodes, há»‡ thá»‘ng sá»­ dá»¥ng há»‡ sá»‘ 1.0 Ä‘á»ƒ giá»¯ nguyÃªn giÃ¡ trá»‹ similarity ban Ä‘áº§u.
- GiÃ¡ trá»‹ similarity: ÄÃ¢y lÃ  má»™t chá»‰ sá»‘ Ä‘Æ°á»£c tÃ­nh dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a embedding cá»§a thá»±c thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« truy váº¥n vÃ  embedding cá»§a node trong Knowledge Graph. GiÃ¡ trá»‹ nÃ y thÆ°á»ng náº±m trong khoáº£ng tá»« 0 Ä‘áº¿n 1, vÃ  khÃ´ng pháº£i lÃºc nÃ o cÅ©ng báº±ng 1.0. VÃ­ dá»¥, Node A cÃ³ similarity lÃ  0.72 vÃ  Node B cÃ³ similarity lÃ  0.68, Ä‘iá»u nÃ y pháº£n Ã¡nh má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng (khÃ´ng pháº£i lÃ  â€œtrá»ng sá»‘â€ mÃ  lÃ  káº¿t quáº£ cá»§a phÃ©p Ä‘o similarity).
TÃ³m láº¡i, máº·c Ä‘á»‹nh weight factor cá»§a Phrase Nodes lÃ  1.0, nghÄ©a lÃ  chÃºng ta khÃ´ng Ä‘iá»u chá»‰nh thÃªm giÃ¡ trá»‹ similarity cá»§a chÃºng, nhÆ°ng giÃ¡ trá»‹ similarity thá»±c sá»± (0.72, 0.68, â€¦) phá»¥ thuá»™c vÃ o quÃ¡ trÃ¬nh tÃ­nh toÃ¡n tá»« cÃ¡c embedding chá»© khÃ´ng pháº£i lÃ  1.0 luÃ´n.

---
DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ chi tiáº¿t kÃ¨m Ä‘á»‘i thoáº¡i pháº£n biá»‡n â€“ â€œngÆ°á»i phÃ¡ â€“ ngÆ°á»i tráº£ lá»iâ€ â€“ Ä‘á»ƒ minh há»a cÃ¡ch Seed Node Weights vÃ  thuáº­t toÃ¡n Personalized PageRank (PPR) hoáº¡t Ä‘á»™ng cÃ¹ng nhau trong HippoRAG.

---
Bá»‘i cáº£nh vÃ­ dá»¥
Giáº£ sá»­ truy váº¥n lÃ :
"TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?"
Sau khi trÃ­ch xuáº¥t thÃ´ng tin tá»« truy váº¥n vÃ  xÃ¢y dá»±ng Knowledge Graph (KG), há»‡ thá»‘ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c cÃ¡c seed node vá»›i cÃ¡c giÃ¡ trá»‹ similarity nhÆ° sau:
- Phrase Node A: "tÃ¡c giáº£"
  - Similarity score = 0.72
  - Weight factor (Phrase) = 1.0 â†’ Effective weight = 0.72
- Phrase Node B: "The Great Novel"
  - Similarity score = 0.68
  - Weight factor (Phrase) = 1.0 â†’ Effective weight = 0.68
- Passage Node C: Äoáº¡n vÄƒn 1 chá»©a thÃ´ng tin:
 â€œThe Great Novel, má»™t tÃ¡c pháº©m kinh Ä‘iá»ƒn, Ä‘Æ°á»£c viáº¿t bá»Ÿi John Doe â€“ tÃ¡c giáº£ ná»•i tiáº¿ngâ€
  - Similarity score = 0.80
  - Weight factor (Passage) = 0.05 â†’ Effective weight = 0.80 Ã— 0.05 = 0.04
- Passage Node D: Äoáº¡n vÄƒn 2 nÃ³i chung vá» John Doe nhÆ°ng khÃ´ng nháº¯c trá»±c tiáº¿p tiÃªu Ä‘á»
  - Similarity score = 0.55
  - Weight factor (Passage) = 0.05 â†’ Effective weight = 0.55 Ã— 0.05 = 0.0275

---
Äá»‘i thoáº¡i pháº£n biá»‡n â€“ CÃ¡ch PPR hoáº¡t Ä‘á»™ng vá»›i Seed Node Weights
Pháº£n biá»‡n 1: Khá»Ÿi táº¡o Seed Vector
Q: LÃ m tháº¿ nÃ o ta khá»Ÿi táº¡o vector seed cho PPR vá»›i cÃ¡c seed node trÃªn?
A:
- Ta táº¡o má»™t vector ss chá»©a cÃ¡c giÃ¡ trá»‹ khá»Ÿi táº¡o cho cÃ¡c node theo effective weight:
  - sA=0.72s_A = 0.72 (Phrase node A)
  - sB=0.68s_B = 0.68 (Phrase node B)
  - sC=0.04s_C = 0.04 (Passage node C)
  - sD=0.0275s_D = 0.0275 (Passage node D)
- Sau Ä‘Ã³, ta chuáº©n hÃ³a vector ss sao cho tá»•ng cÃ¡c pháº§n tá»­ báº±ng 1. VÃ­ dá»¥:
  - Tá»•ng S=0.72+0.68+0.04+0.0275=1.4675S = 0.72 + 0.68 + 0.04 + 0.0275 = 1.4675
  - Vector seed chuáº©n hÃ³a:
    - sAâ€²=0.72/1.4675â‰ˆ0.49s'_A = 0.72 / 1.4675 \approx 0.49
    - sBâ€²=0.68/1.4675â‰ˆ0.46s'_B = 0.68 / 1.4675 \approx 0.46
    - sCâ€²=0.04/1.4675â‰ˆ0.027s'_C = 0.04 / 1.4675 \approx 0.027
    - sDâ€²=0.0275/1.4675â‰ˆ0.019s'_D = 0.0275 / 1.4675 \approx 0.019
NhÆ° váº­y, cÃ¡c Phrase Nodes chiáº¿m Æ°u tháº¿ ngay tá»« Ä‘áº§u so vá»›i cÃ¡c Passage Nodes do effective weight cá»§a chÃºng cao hÆ¡n.

---
Pháº£n biá»‡n 2: QuÃ¡ trÃ¬nh PPR â€“ Lan tá»a xÃ¡c suáº¥t
Q: PPR sáº½ lÃ m gÃ¬ vá»›i vector seed Ä‘Ã£ chuáº©n hÃ³a nÃ y?
A:
- Thuáº­t toÃ¡n PPR thá»±c hiá»‡n cáº­p nháº­t xÃ¡c suáº¥t theo cÃ´ng thá»©c:
- v(t+1)=(1âˆ’d)â‹…sâ€²+dâ‹…Tâ‹…v(t)v^{(t+1)} = (1-d) \cdot s' + d \cdot T \cdot v^{(t)}
- trong Ä‘Ã³:
  - v(t)v^{(t)} lÃ  vector xÃ¡c suáº¥t á»Ÿ vÃ²ng láº·p thá»© tt
  - sâ€²s' lÃ  vector seed (Ä‘Ã£ chuáº©n hÃ³a)
  - TT lÃ  ma tráº­n chuyá»ƒn Ä‘á»•i (transition matrix) cá»§a KG, xÃ¡c Ä‘á»‹nh xÃ¡c suáº¥t chuyá»ƒn tá»« node nÃ y sang node khÃ¡c dá»±a trÃªn cÃ¡c cáº¡nh liÃªn káº¿t
  - dd lÃ  damping factor (thÆ°á»ng khoáº£ng 0.5â€“0.85)
- QuÃ¡ trÃ¬nh nÃ y sáº½ â€œlan tá»aâ€ cÃ¡c giÃ¡ trá»‹ tá»« cÃ¡c seed node ban Ä‘áº§u theo cÃ¡c liÃªn káº¿t trong Ä‘á»“ thá»‹. CÃ¡c Phrase Nodes vá»›i giÃ¡ trá»‹ cao ban Ä‘áº§u (0.49 vÃ  0.46) sáº½ â€œÄ‘áº©yâ€ xÃ¡c suáº¥t sang cÃ¡c nÃºt lÃ¢n cáº­n. Náº¿u Passage Node C (vá»›i giÃ¡ trá»‹ ban Ä‘áº§u 0.027) cÃ³ cÃ¡c cáº¡nh máº¡nh liÃªn káº¿t vá»›i Phrase Nodes, nÃ³ cÃ³ thá»ƒ nháº­n thÃªm xÃ¡c suáº¥t tá»« chÃºng.

---
Pháº£n biá»‡n 3: VÃ­ dá»¥ sá»‘ há»c trong quÃ¡ trÃ¬nh PPR
Q: HÃ£y láº¥y má»™t vÃ²ng láº·p PPR minh há»a cá»¥ thá»ƒ cho vÃ­ dá»¥ nÃ y.
A:
- Giáº£ sá»­ ma tráº­n chuyá»ƒn Ä‘á»•i TT cá»§a má»™t pháº§n nhá» trong KG Ä‘Æ°á»£c Ä‘Æ¡n giáº£n hoÃ¡ nhÆ° sau (chá»‰ xÃ©t 4 node trÃªn):
  - TABT_{AB} lÃ  xÃ¡c suáº¥t tá»« node A sang B, TACT_{AC} tá»« A sang C, v.vâ€¦
- VÃ­ dá»¥, ta giáº£ sá»­:
  - Node A (Phrase "tÃ¡c giáº£") cÃ³ liÃªn káº¿t:
    - 60% Ä‘áº¿n Node B ("The Great Novel")
    - 40% Ä‘áº¿n Node C (Passage chá»©a thÃ´ng tin chÃ­nh)
  - Node B ("The Great Novel") liÃªn káº¿t:
    - 70% Ä‘áº¿n Node A
    - 30% Ä‘áº¿n Node D (Passage phá»¥)
  - Node C (Passage 1) liÃªn káº¿t:
    - 100% Ä‘áº¿n Node A (giáº£ sá»­ cÃ³ má»‘i liÃªn há»‡ máº¡nh vá»›i khÃ¡i niá»‡m "tÃ¡c giáº£")
  - Node D (Passage 2) liÃªn káº¿t:
    - 100% Ä‘áº¿n Node B
- Ta cÃ³ ma tráº­n TT (Ä‘Æ¡n giáº£n hoÃ¡) nhÆ° sau:
- T=[00.600.4000.70000.301.000001.000]T = \begin{bmatrix} 0   & 0.60 & 0.40 & 0 \\ 0.70 & 0   & 0    & 0.30 \\ 1.0  & 0   & 0    & 0 \\ 0   & 1.0 & 0    & 0  \end{bmatrix}
- Giáº£ sá»­ damping factor d=0.5d = 0.5. Vá»›i vector seed chuáº©n hÃ³a sâ€²=[0.49,0.46,0.027,0.019]âŠ¤s' = [0.49, 0.46, 0.027, 0.019]^\top, ta khá»Ÿi táº¡o v(0)=sâ€²v^{(0)} = s'.
- VÃ²ng láº·p thá»© 1:
 TÃ­nh v(1)=0.5â‹…sâ€²+0.5â‹…Tâ‹…v(0)v^{(1)} = 0.5 \cdot s' + 0.5 \cdot T \cdot v^{(0)}
- TrÆ°á»›c háº¿t, tÃ­nh Tâ‹…v(0)T \cdot v^{(0)}:
  - (Tâ‹…v(0))A=0âˆ—0.49+0.60âˆ—0.46+0.40âˆ—0.027+0âˆ—0.019â‰ˆ0.60Ã—0.46+0.40Ã—0.027â‰ˆ0.276+0.0108â‰ˆ0.287(T \cdot v^{(0)})_A = 0*0.49 + 0.60*0.46 + 0.40*0.027 + 0*0.019 \approx 0.60 \times 0.46 + 0.40 \times 0.027 \approx 0.276 + 0.0108 \approx 0.287
  - (Tâ‹…v(0))B=0.70âˆ—0.49+0âˆ—0.46+0âˆ—0.027+0.30âˆ—0.019â‰ˆ0.343+0.0057â‰ˆ0.3487(T \cdot v^{(0)})_B = 0.70*0.49 + 0*0.46 + 0*0.027 + 0.30*0.019 \approx 0.343 + 0.0057 \approx 0.3487
  - (Tâ‹…v(0))C=1.0âˆ—0.49+0âˆ—0.46+0âˆ—0.027+0âˆ—0.019=0.49(T \cdot v^{(0)})_C = 1.0*0.49 + 0*0.46 + 0*0.027 + 0*0.019 = 0.49
  - (Tâ‹…v(0))D=0âˆ—0.49+1.0âˆ—0.46+0âˆ—0.027+0âˆ—0.019=0.46(T \cdot v^{(0)})_D = 0*0.49 + 1.0*0.46 + 0*0.027 + 0*0.019 = 0.46
- Sau Ä‘Ã³,
- v(1)=0.5â‹…[0.49,0.46,0.027,0.019]âŠ¤+0.5â‹…[0.287,0.3487,0.49,0.46]âŠ¤v^{(1)} = 0.5 \cdot [0.49, 0.46, 0.027, 0.019]^\top + 0.5 \cdot [0.287, 0.3487, 0.49, 0.46]^\top 
- =
- [0.245+0.1435,â€‰0.23+0.17435,â€‰0.0135+0.245,â€‰0.0095+0.23][0.245 + 0.1435,\, 0.23 + 0.17435,\, 0.0135 + 0.245,\, 0.0095 + 0.23]
- =
- [0.3885,â€‰0.40435,â€‰0.2585,â€‰0.2395][0.3885,\, 0.40435,\, 0.2585,\, 0.2395]
- Giáº£i thÃ­ch:
 NhÃ¬n vÃ o v(1)v^{(1)}:
  - Node A vÃ  Node B (Phrase Nodes) cÃ³ giÃ¡ trá»‹ váº«n cao (0.3885 vÃ  0.40435).
  - Node C (Passage 1) tÄƒng tá»« 0.027 lÃªn 0.2585 nhá» nháº­n Ä‘Æ°á»£c thÃ´ng tin tá»« Node A (vÃ¬ cÃ³ liÃªn káº¿t 100% tá»« Node C sang A) vÃ  tá»« má»‘i liÃªn káº¿t cá»§a A Ä‘áº¿n C.
  - Node D (Passage 2) cÃ³ giÃ¡ trá»‹ 0.2395 nhá» liÃªn káº¿t vá»›i Node B.
QuÃ¡ trÃ¬nh nÃ y sáº½ láº·p láº¡i cho Ä‘áº¿n khi há»™i tá»¥. Káº¿t quáº£ cuá»‘i cÃ¹ng sáº½ lÃ  vector xÃ¡c suáº¥t phÃ¢n bá»• qua toÃ n bá»™ KG, trong Ä‘Ã³ cÃ¡c node cÃ³ má»‘i liÃªn káº¿t máº¡nh vá»›i seed nodes ban Ä‘áº§u sáº½ nháº­n Ä‘Æ°á»£c xÃ¡c suáº¥t cao hÆ¡n.

---
Pháº£n biá»‡n 4: áº¢nh hÆ°á»Ÿng cá»§a PPR Ä‘áº¿n viá»‡c xáº¿p háº¡ng cÃ¡c Passage
Q: Sau khi vector xÃ¡c suáº¥t Ä‘Æ°á»£c tÃ­nh xong, lÃ m sao cÃ¡c giÃ¡ trá»‹ nÃ y chuyá»ƒn thÃ nh viá»‡c xáº¿p háº¡ng cÃ¡c Ä‘oáº¡n vÄƒn tráº£ lá»i truy váº¥n?
A:
- Sau khi cÃ³ vector vv cho cÃ¡c node trong KG, há»‡ thá»‘ng sáº½ nhÃ¢n cÃ¡c giÃ¡ trá»‹ nÃ y vá»›i ma tráº­n P (P matrix), mÃ  trong Ä‘Ã³ má»—i pháº§n tá»­ biá»ƒu thá»‹ sá»‘ láº§n xuáº¥t hiá»‡n cá»§a má»™t nÃºt (phrase hoáº·c passage) trong cÃ¡c Ä‘oáº¡n vÄƒn gá»‘c.
- Tá»•ng há»£p giÃ¡ trá»‹ tá»« cÃ¡c node liÃªn quan sáº½ cho Ä‘iá»ƒm xáº¿p háº¡ng cho tá»«ng Ä‘oáº¡n vÄƒn. CÃ¡c Ä‘oáº¡n vÄƒn cÃ³ tá»•ng xÃ¡c suáº¥t cao (Ä‘áº·c biá»‡t lÃ  do nháº­n Ä‘Æ°á»£c â€œÄ‘iá»ƒm cá»™ngâ€ tá»« cÃ¡c Phrase Nodes máº¡nh vÃ  Passage Nodes cÃ³ liÃªn káº¿t tá»‘t) sáº½ Ä‘Æ°á»£c xáº¿p háº¡ng Æ°u tiÃªn.
VÃ­ dá»¥:
- Passage Node C, sau PPR, cÃ³ giÃ¡ trá»‹ tÄƒng lÃªn Ä‘Ã¡ng ká»ƒ (vÃ­ dá»¥ 0.2585), khi nhÃ¢n vá»›i sá»‘ láº§n xuáº¥t hiá»‡n vÃ  cÃ¡c liÃªn káº¿t liÃªn quan, nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¿p háº¡ng cao vÃ  chá»n lÃ m cÃ¢u tráº£ lá»i cho truy váº¥n "TÃ¡c giáº£ cá»§a cuá»‘n sÃ¡ch The Great Novel lÃ  ai?".

---
Tá»•ng káº¿t
Qua vÃ­ dá»¥ chi tiáº¿t trÃªn, ta tháº¥y ráº±ng:
1. Seed Node Weights Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh dá»±a trÃªn similarity score vÃ  weight factor riÃªng biá»‡t cho Phrase Nodes (1.0) vÃ  Passage Nodes (0.05).
2. Vector seed sau khi chuáº©n hÃ³a cung cáº¥p Ä‘iá»ƒm khá»Ÿi Ä‘áº§u cho PPR.
3. PPR lan tá»a xÃ¡c suáº¥t tá»« cÃ¡c seed node theo cÃ¡c liÃªn káº¿t trong KG, giÃºp cÃ¡c Passage Nodes nháº­n Ä‘Æ°á»£c â€œÄ‘iá»ƒm cá»™ngâ€ tá»« cÃ¡c Phrase Nodes náº¿u chÃºng cÃ³ liÃªn káº¿t cháº·t cháº½.
4. Káº¿t quáº£ cuá»‘i cÃ¹ng sau PPR vÃ  káº¿t há»£p vá»›i ma tráº­n P sáº½ xáº¿p háº¡ng cÃ¡c Ä‘oáº¡n vÄƒn phÃ¹ há»£p Ä‘á»ƒ tráº£ lá»i truy váº¥n.
Qua Ä‘Ã³, ta tháº¥y ráº±ng viá»‡c gÃ¡n trá»ng sá»‘ seed node vÃ  á»©ng dá»¥ng PPR khÃ´ng chá»‰ giÃºp cÃ¢n báº±ng giá»¯a thÃ´ng tin khÃ¡i niá»‡m vÃ  ngá»¯ cáº£nh, mÃ  cÃ²n táº¡o Ä‘iá»u kiá»‡n cho quÃ¡ trÃ¬nh multi-hop retrieval diá»…n ra hiá»‡u quáº£ vÃ  chÃ­nh xÃ¡c.

---
ÄÃºng váº­y, bÆ°á»›c PPR chá»§ yáº¿u thá»±c hiá»‡n viá»‡c re-rank vÃ  lan tá»a cÃ¡c Ä‘iá»ƒm liÃªn quan tá»« cÃ¡c seed node Ä‘Ã£ Ä‘Æ°á»£c chá»n á»Ÿ bÆ°á»›c trÆ°á»›c. NÃ³i cÃ¡ch khÃ¡c, PPR khÃ´ng chá»‰ Ä‘Æ¡n giáº£n lÃ  sáº¯p xáº¿p láº¡i cÃ¡c node mÃ  cÃ²n â€œreasoningâ€ qua cáº¥u trÃºc cá»§a Knowledge Graph Ä‘á»ƒ tÄƒng cÆ°á»ng má»‘i liÃªn káº¿t giá»¯a cÃ¡c node, giÃºp há»‡ thá»‘ng tÃ¬m ra nhá»¯ng Ä‘oáº¡n vÄƒn hoáº·c triple cÃ³ liÃªn quan nháº¥t dá»±a trÃªn má»‘i quan há»‡ logic giá»¯a chÃºng.

---
[Image]

Äáº·t váº¥n Ä‘á» nhÆ° nÃ o, giáº£i quyáº¿t nhÆ° nÃ o? 
- BÃ i toÃ¡n suy luáº­n Ä‘a bÆ°á»›c. Dataset luÃ´n? 
- https://www.kaggle.com/datasets/danghuy249/rag-dataset?select=data
-  However, its reliance  on vector retrieval hinders its ability to mimic  the dynamic and interconnected nature of hu man long-term memory. Recent RAG approaches  augment vector embeddings with various struc tures like knowledge graphs to address some of  these gaps, namely sense-making and associativ ity. However, their performance on more basic  factual memory tasks drops considerably below  standard RAG. 
- Tuy nhiÃªn, viá»‡c phá»¥ thuá»™c vÃ o truy xuáº¥t vector háº¡n cháº¿ kháº£ nÄƒng cá»§a nÃ³ trong viá»‡c mÃ´ phá»ng tÃ­nh cháº¥t Ä‘á»™ng vÃ  liÃªn káº¿t cá»§a trÃ­ nhá»› dÃ i háº¡n á»Ÿ con ngÆ°á»i. CÃ¡c phÆ°Æ¡ng phÃ¡p RAG gáº§n Ä‘Ã¢y Ä‘Ã£ bá»• sung cÃ¡c cáº¥u trÃºc khÃ¡c nhau nhÆ° Ä‘á»“ thá»‹ tri thá»©c vÃ o cÃ¡c vector nhÃºng Ä‘á»ƒ giáº£i quyáº¿t má»™t sá»‘ thiáº¿u sÃ³t nÃ y, cá»¥ thá»ƒ lÃ  kháº£ nÄƒng cáº£m nháº­n vÃ  liÃªn káº¿t. Tuy nhiÃªn, hiá»‡u suáº¥t cá»§a chÃºng trong cÃ¡c nhiá»‡m vá»¥ trÃ­ nhá»› thá»±c táº¿ cÆ¡ báº£n láº¡i giáº£m Ä‘Ã¡ng ká»ƒ so vá»›i RAG tiÃªu chuáº©n. => Äá» xuáº¥t HippoRAG. 
+, Long Term Memory? (lÆ°u dáº¡ng fact cá»§a user) 
- Baseline ngÃ y xÆ°a. RAG truyá»n thá»‘ng. Dataset ngÃ y xÆ°a. 
- Reason LLms -> Äoáº¡n Retrieval Reasoning rá»“i Hippo. 
HippoRAG khÃ´ng chá» Ä‘áº¿n khi LLM lÃ m reasoning, mÃ  nÃ³ Ä‘Ã£ reasoning ngay tá»« bÆ°á»›c truy há»“i.
Vá»›i Personalized PageRank â†’ nÃ³ tÃ¬m ra node trung gian (John Smith) trÆ°á»›c khi vÃ o LLM
Tráº£ vá» Ä‘oáº¡n Ä‘Ã£ Ä‘á»§ logic â†’ LLM chá»‰ cáº§n nÃ³i láº¡i
- xAI cho cÃ¢u há»i báº±ng cÃ¡ch xÃ¢y dá»±ng KG. ÄÆ°a KG vÃ o há»c? 
- Dataset ngÃ y xÆ°a cá»§a anh Huy, ... 
-  HotpotQA

============
 ForHippoRAG2,weusetheopen-sourceLlama-3.3-70B Instruct(AI@Meta,2024)asboththeextraction(NERand  OpenIE)andtriplefilteringmodel,andweusenvidia/NV Embed-v2astheretriever.


Äá»ƒ Ä‘áº·t váº¥n Ä‘á» táº¡i sao cáº§n HippoRAG, ta cÃ³ thá»ƒ trÃ¬nh bÃ y nhÆ° sau â€“ káº¿t há»£p giá»¯a láº­p luáº­n logic vÃ  cÃ¡c phÃ¡t hiá»‡n tá»« bÃ i bÃ¡o:

---
ğŸ§  Táº¡i sao cáº§n HippoRAG?
1. Giá»›i háº¡n cá»§a RAG truyá»n thá»‘ng
Retrieval-Augmented Generation (RAG) lÃ  má»™t giáº£i phÃ¡p phá»• biáº¿n giÃºp LLM tiáº¿p cáº­n thÃ´ng tin má»›i mÃ  khÃ´ng cáº§n thay Ä‘á»•i tham sá»‘ mÃ´ hÃ¬nh. Tuy nhiÃªn, RAG truyá»n thá»‘ng chá»§ yáº¿u dá»±a vÃ o vector retrieval Ä‘Æ¡n giáº£n, dáº«n Ä‘áº¿n 2 Ä‘iá»ƒm yáº¿u lá»›n:
- Thiáº¿u kháº£ nÄƒng sense-making: KhÃ´ng thá»ƒ hiá»ƒu, diá»…n giáº£i cÃ¡c tÃ¬nh huá»‘ng dÃ i vÃ  phá»©c táº¡p má»™t cÃ¡ch liá»n máº¡ch.
- Thiáº¿u tÃ­nh liÃªn káº¿t (associativity): KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘a bÆ°á»›c giá»¯a cÃ¡c máº£nh kiáº¿n thá»©c rá»i ráº¡c nhÆ° bá»™ nhá»› dÃ i háº¡n cá»§a con ngÆ°á»i.
ğŸ“Œ VÃ­ dá»¥: Trong cÃ¡c tÃ¡c vá»¥ truy váº¥n Ä‘a bÆ°á»›c nhÆ° "NgÆ°á»i phÃ¡t minh ra thiáº¿t bá»‹ X sá»‘ng á»Ÿ Ä‘Ã¢u?", vector retrieval truyá»n thá»‘ng thÆ°á»ng tháº¥t báº¡i vÃ¬ khÃ´ng thá»ƒ ná»‘i cÃ¡c máº£nh thÃ´ng tin láº¡i (phÃ¡t minh X â†’ ngÆ°á»i A â†’ nÆ¡i sá»‘ng cá»§a A).

---
2. CÃ¡c giáº£i phÃ¡p trÆ°á»›c Ä‘Ã¢y váº«n chÆ°a Ä‘á»§
CÃ¡c há»‡ thá»‘ng RAG cáº¥u trÃºc má»›i nhÆ° RAPTOR, GraphRAG, LightRAG Ä‘Ã£ thá»­ kháº¯c phá»¥c báº±ng cÃ¡ch:
- Táº¡o tÃ³m táº¯t hoáº·c knowledge graph Ä‘á»ƒ bá»• sung ngá»¯ cáº£nh.
- Tuy nhiÃªn, láº¡i gáº·p pháº£i váº¥n Ä‘á» giáº£m hiá»‡u quáº£ trong cÃ¡c tÃ¡c vá»¥ ghi nhá»› cÆ¡ báº£n (factual QA) do nhiá»…u tá»« LLM hoáº·c cáº¥u trÃºc phá»©c táº¡p gÃ¢y quÃ¡ táº£i.
âŒ KhÃ´ng cÃ³ mÃ´ hÃ¬nh nÃ o cÃ¢n báº±ng tá»‘t cáº£ 3 khÃ­a cáº¡nh: factual memory, sense-making vÃ  associativity.

---
3. HippoRAG ra Ä‘á»i Ä‘á»ƒ mÃ´ phá»ng bá»™ nhá»› dÃ i háº¡n nhÆ° ngÆ°á»i
HippoRAG (vÃ  báº£n cáº£i tiáº¿n HippoRAG 2) Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cáº£m há»©ng tá»« cáº¥u trÃºc bá»™ nhá»› sinh há»c cá»§a con ngÆ°á»i (neocortex, hippocampus, parahippocampal region), vá»›i má»¥c tiÃªu:
- Ghi nhá»› bá»n vá»¯ng kiáº¿n thá»©c má»›i mÃ  khÃ´ng lÃ m quÃªn kiáº¿n thá»©c cÅ©.
- Káº¿t ná»‘i Ä‘Æ°á»£c nhiá»u thÃ´ng tin dáº¡ng biá»ƒu Ä‘á»“, há»— trá»£ multi-hop reasoning.
- TÄƒng tÃ­nh ngá»¯ cáº£nh vÃ  tÃ­nh liÃªn káº¿t, nhá» tÃ­ch há»£p PageRank cÃ¡ nhÃ¢n hÃ³a vÃ  biá»ƒu Ä‘á»“ tri thá»©c má»Ÿ (open KG).

---
4. Lá»£i Ã­ch cá»§a HippoRAG
Theo káº¿t quáº£ thá»±c nghiá»‡m:
- HippoRAG 2 vÆ°á»£t trá»™i 7% trong cÃ¡c bÃ i toÃ¡n truy váº¥n liÃªn káº¿t (associative tasks).
- Giá»¯ Ä‘Æ°á»£c hoáº·c cáº£i thiá»‡n nháº¹ á»Ÿ cÃ¡c tÃ¡c vá»¥ ghi nhá»› Ä‘Æ¡n (factual) vÃ  hiá»ƒu ngá»¯ cáº£nh (sense-making).
- Kháº£ nÄƒng há»c liÃªn tá»¥c phi tham sá»‘ (non-parametric continual learning) â€“ khÃ´ng cáº§n fine-tune LLM liÃªn tá»¥c.

---
Náº¿u báº¡n muá»‘n, mÃ¬nh cÃ³ thá»ƒ viáº¿t pháº§n nÃ y thÃ nh má»™t Ä‘oáº¡n motivation ngáº¯n, chuáº©n phong cÃ¡ch bÃ i nghiÃªn cá»©u hoáº·c bÃ i thuyáº¿t trÃ¬nh há»c thuáº­t. Quá»‘c muá»‘n phong cÃ¡ch nÃ o: ngáº¯n gá»n â€“ gÃ£y gá»n â€“ trÃ¬nh bÃ y váº¥n Ä‘á»? Hay sÃ¢u sáº¯c â€“ há»c thuáº­t â€“ Ä‘Æ°a dáº«n chá»©ng nghiÃªn cá»©u?


DÆ°á»›i Ä‘Ã¢y lÃ  báº£n "Äáº·t váº¥n Ä‘á» vÃ  Giáº£i phÃ¡p" cho HippoRAG, Ä‘Æ°á»£c viáº¿t theo phong cÃ¡ch há»c thuáº­t, sÃºc tÃ­ch vÃ  logic â€“ phÃ¹ há»£p Ä‘á»ƒ Ä‘Æ°a vÃ o giá»›i thiá»‡u má»™t bÃ i nghiÃªn cá»©u, thuyáº¿t trÃ¬nh chuyÃªn Ä‘á» hoáº·c Ä‘á»“ Ã¡n tá»‘t nghiá»‡p.

---
ğŸ§© Äáº·t váº¥n Ä‘á» vÃ  Giáº£i phÃ¡p
ğŸ” Äáº·t váº¥n Ä‘á»
Kháº£ nÄƒng ghi nhá»›, liÃªn káº¿t vÃ  sá»­ dá»¥ng kiáº¿n thá»©c lÃ¢u dÃ i lÃ  má»™t Ä‘áº·c trÆ°ng cá»‘t lÃµi cá»§a trÃ­ thÃ´ng minh con ngÆ°á»i. Tuy nhiÃªn, cÃ¡c há»‡ thá»‘ng AI hiá»‡n nay â€“ Ä‘áº·c biá»‡t lÃ  cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) â€“ chÆ°a thá»ƒ hiá»‡n Ä‘Æ°á»£c kháº£ nÄƒng há»c liÃªn tá»¥c vÃ  ghi nhá»› lÃ¢u dÃ i má»™t cÃ¡ch hiá»‡u quáº£.
Máº·c dÃ¹ Retrieval-Augmented Generation (RAG) Ä‘ang lÃ  giáº£i phÃ¡p chÃ­nh cho viá»‡c tiáº¿p cáº­n thÃ´ng tin ngoÃ i mÃ´ hÃ¬nh (non-parametric learning), RAG truyá»n thá»‘ng váº«n cÃ²n nhiá»u háº¡n cháº¿:
- Phá»¥ thuá»™c hoÃ n toÃ n vÃ o truy há»“i vector (vector retrieval), dáº«n Ä‘áº¿n:
  - KhÃ´ng hiá»ƒu sÃ¢u ngá»¯ cáº£nh dÃ i hoáº·c phá»©c táº¡p (thiáº¿u sense-making).
  - KhÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c thÃ´ng tin rá»i ráº¡c qua nhiá»u bÆ°á»›c (thiáº¿u associativity).
- CÃ¡c phÆ°Æ¡ng phÃ¡p cáº£i tiáº¿n nhÆ° RAPTOR hay GraphRAG dÃ¹ng LLM Ä‘á»ƒ táº¡o tÃ³m táº¯t hoáº·c graph, nhÆ°ng láº¡i gÃ¢y suy giáº£m hiá»‡u nÄƒng á»Ÿ cÃ¡c bÃ i toÃ¡n factual QA, do nhiá»…u tá»« LLM hoáº·c quÃ¡ phá»©c táº¡p trong cáº¥u trÃºc.
Tá»« Ä‘Ã³ Ä‘áº·t ra cÃ¢u há»i:
LÃ m tháº¿ nÃ o Ä‘á»ƒ xÃ¢y dá»±ng má»™t há»‡ thá»‘ng ghi nhá»› lÃ¢u dÃ i cho LLMs vá»«a máº¡nh vá» factual, vá»«a sÃ¢u sáº¯c vá» ngá»¯ cáº£nh, vÃ  linh hoáº¡t trong liÃªn káº¿t kiáº¿n thá»©c?

---
ğŸ’¡ Giáº£i phÃ¡p: HippoRAG
HippoRAG Ä‘Æ°á»£c Ä‘á» xuáº¥t nhÆ° má»™t khung ghi nhá»› láº¥y cáº£m há»©ng tá»« cáº¥u trÃºc bá»™ nhá»› cá»§a con ngÆ°á»i (gá»“m tÃ¢n vá» nÃ£o, hippocampus, vÃ  vÃ¹ng trung gian). Giáº£i phÃ¡p nÃ y bao gá»“m hai giai Ä‘oáº¡n:
1. Chá»‰ má»¥c hÃ³a ngoáº¡i tuyáº¿n (Offline Indexing):
  - DÃ¹ng LLM Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c triple kiáº¿n thá»©c dáº¡ng KG má»Ÿ tá»« tÃ i liá»‡u.
  - Táº¡o biá»ƒu Ä‘á»“ tri thá»©c gá»“m cÃ¡c node khÃ¡i niá»‡m (phrase) vÃ  node ngá»¯ cáº£nh (passage).
  - PhÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a vÃ  liÃªn káº¿t cÃ¡c khÃ¡i niá»‡m tÆ°Æ¡ng Ä‘Æ°Æ¡ng báº±ng embedding.
2. Truy há»“i trá»±c tuyáº¿n (Online Retrieval):
  - DÃ¹ng mÃ´ hÃ¬nh embedding Ä‘á»ƒ liÃªn káº¿t cÃ¢u há»i vá»›i triple vÃ  passage.
  - Ãp dá»¥ng thuáº­t toÃ¡n Personalized PageRank Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng dáº«n liÃªn káº¿t ngá»¯ nghÄ©a vÃ  truy xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan.
  - DÃ¹ng LLM Ä‘á»ƒ lá»c triple theo ngá»¯ cáº£nh truy váº¥n (bá»™ nhá»› nháº­n diá»‡n â€“ recognition memory).
ğŸ¯ Káº¿t quáº£: HippoRAG 2 vÆ°á»£t trá»™i hÆ¡n táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n táº¡i trÃªn cáº£ ba nÄƒng lá»±c cá»‘t lÃµi: ghi nhá»› fact, hiá»ƒu ngá»¯ cáº£nh vÃ  liÃªn káº¿t thÃ´ng tin.

---
Náº¿u Quá»‘c muá»‘n mÃ¬nh viáº¿t láº¡i báº£n nÃ y theo báº£n slide trÃ¬nh bÃ y, dáº¡ng bÃ i viáº¿t bÃ¡o cÃ¡o khoa há»c, hoáº·c báº£n nÃ³i thuyáº¿t trÃ¬nh miá»‡ng 2-3 phÃºt, mÃ¬nh sáº½ chuyá»ƒn thá»ƒ phÃ¹ há»£p nhÃ©. Quá»‘c muá»‘n theo hÆ°á»›ng nÃ o tiáº¿p?


Äá»ƒ mÃ¬nh giáº£i thÃ­ch chi tiáº¿t sÆ¡ Ä‘á»“ HippoRAG 2 nÃ y má»™t cÃ¡ch dá»… hiá»ƒu, chia lÃ m 2 giai Ä‘oáº¡n: Offline Indexing vÃ  Online Retrieval & QA, vá»›i cÃ¡c bÆ°á»›c rÃµ rÃ ng nhÃ©!

---
ğŸ§  1. Offline Indexing â€“ Giai Ä‘oáº¡n táº¡o bá»™ nhá»› (khá»Ÿi táº¡o biá»ƒu Ä‘á»“ tri thá»©c)
ğŸ¯ Má»¥c tiÃªu: Biáº¿n cÃ¡c Ä‘oáº¡n vÄƒn (passages) thÃ nh má»™t biá»ƒu Ä‘á»“ tri thá»©c (Knowledge Graph â€“ KG) Ä‘á»ƒ sau nÃ y dá»… truy há»“i thÃ´ng minh hÆ¡n.
Gá»“m 3 bÆ°á»›c:
ğŸ”µ (1) OpenIE by LLM
 â€“ DÃ¹ng LLM (Llama-3.3-70B) Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c triple kiáº¿n thá»©c tá»« má»—i Ä‘oáº¡n vÄƒn, dáº¡ng:
(subject â€“ relation â€“ object)
 ğŸ‘‰ VÃ­ dá»¥: ("Elon Musk", "founder of", "SpaceX")
ğŸ”µ (2) Synonym detection by embedding
 â€“ DÃ¹ng mÃ´ hÃ¬nh embedding Ä‘á»ƒ tÃ¬m cÃ¡c cáº·p tá»«/cá»¥m tá»« giá»‘ng nhau vá» nghÄ©a, rá»“i ná»‘i chÃºng láº¡i báº±ng synonym edge.
 ğŸ‘‰ VÃ­ dá»¥: â€œNYCâ€ â†” â€œNew York Cityâ€
ğŸ”µ (3) Dense-sparse integration
 â€“ Táº¡o 2 loáº¡i node trong biá»ƒu Ä‘á»“:
- Phrase node: cÃ¡c khÃ¡i niá»‡m rÃºt gá»n, nhÆ° â€œElon Muskâ€ hoáº·c â€œCEOâ€
- Passage node: chá»©a toÃ n bá»™ Ä‘oáº¡n vÄƒn â€“ Ná»‘i passage node vá»›i cÃ¡c phrase node tÆ°Æ¡ng á»©ng báº±ng context edge, Ä‘á»ƒ biá»ƒu Ä‘á»“ vá»«a cÃ³ thÃ´ng tin khÃ¡i niá»‡m, vá»«a cÃ³ ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§.
ğŸ“¦ Output cuá»‘i lÃ  má»™t Knowledge Graph giÃ u ngá»¯ nghÄ©a, vá»›i cÃ¡c loáº¡i liÃªn káº¿t:
- Relation Edge (giá»¯a concept vá»›i concept)
- Synonym Edge
- Context Edge (giá»¯a passage vÃ  concept)

---
ğŸ” 2. Online Retrieval & QA â€“ Giai Ä‘oáº¡n truy há»“i vÃ  tráº£ lá»i
ğŸ¯ Má»¥c tiÃªu: Tá»« cÃ¢u há»i (query), tÃ¬m Ä‘Ãºng cÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan Ä‘á»ƒ tráº£ lá»i.
Gá»“m 5 bÆ°á»›c:
ğŸŸ¢ (1) Truy há»“i passages vÃ  triples
 â€“ DÃ¹ng retriever (NV-Embed-v2) Ä‘á»ƒ truy há»“i cÃ¡c triple vÃ  Ä‘oáº¡n vÄƒn liÃªn quan Ä‘áº¿n cÃ¢u há»i.
ğŸŸ¢ (2) Recognition Memory â€“ Lá»c triple
 â€“ DÃ¹ng LLM Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c triple, giá»¯ láº¡i cÃ¡c triple tháº­t sá»± liÃªn quan Ä‘áº¿n cÃ¢u há»i.
ğŸŸ¢ (3) GÃ¡n trá»ng sá»‘ cho cÃ¡c seed node
 â€“ CÃ¡c triple vÃ  passage Ä‘Æ°á»£c chá»n sáº½ trá»Ÿ thÃ nh cÃ¡c â€œseed nodeâ€ (node khá»Ÿi Ä‘áº§u). â€“ GÃ¡n trá»ng sá»‘ khá»Ÿi Ä‘á»™ng (reset probability) cho tá»«ng seed dá»±a trÃªn má»©c Ä‘á»™ liÃªn quan.
ğŸŸ¢ (4) Personalized PageRank (PPR)
 â€“ Cháº¡y thuáº­t toÃ¡n PPR trÃªn biá»ƒu Ä‘á»“ tri thá»©c, Ä‘á»ƒ tÃ¬m cÃ¡c Ä‘oáº¡n vÄƒn cÃ³ káº¿t ná»‘i ngá»¯ nghÄ©a máº¡nh nháº¥t Ä‘áº¿n cÃ¢u há»i, qua nhiá»u bÆ°á»›c reasoning.
ğŸŸ¢ (5) QA Reading
 â€“ DÃ¹ng LLM Ä‘á»c cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Æ°á»£c chá»n vÃ  táº¡o cÃ¢u tráº£ lá»i cho truy váº¥n.

---
ğŸ—ºï¸ Giáº£i thÃ­ch cÃ¡c kÃ½ hiá»‡u trong sÆ¡ Ä‘á»“
This content is only supported in a Lark Docs

---
Náº¿u Quá»‘c muá»‘n, mÃ¬nh cÃ³ thá»ƒ diá»…n giáº£i láº¡i toÃ n bá»™ thÃ nh slide hoáº·c viáº¿t bÃ¡o cÃ¡o tÃ³m táº¯t pháº§n kiáº¿n trÃºc pipeline HippoRAG 2 theo hÃ¬nh nÃ y nhÃ©. Quá»‘c cáº§n báº£n nÃ o?

Äáº·t váº¥n Ä‘á» nhÆ° nÃ o, giáº£i quyáº¿t nhÆ° nÃ o? 
- BÃ i toÃ¡n suy luáº­n Ä‘a bÆ°á»›c. Dataset luÃ´n? 
- https://www.kaggle.com/datasets/danghuy249/rag-dataset?select=data
-  However, its reliance  on vector retrieval hinders its ability to mimic  the dynamic and interconnected nature of hu man long-term memory. Recent RAG approaches  augment vector embeddings with various struc tures like knowledge graphs to address some of  these gaps, namely sense-making and associativ ity. However, their performance on more basic  factual memory tasks drops considerably below  standard RAG. 
- Tuy nhiÃªn, viá»‡c phá»¥ thuá»™c vÃ o truy xuáº¥t vector háº¡n cháº¿ kháº£ nÄƒng cá»§a nÃ³ trong viá»‡c mÃ´ phá»ng tÃ­nh cháº¥t Ä‘á»™ng vÃ  liÃªn káº¿t cá»§a trÃ­ nhá»› dÃ i háº¡n á»Ÿ con ngÆ°á»i. CÃ¡c phÆ°Æ¡ng phÃ¡p RAG gáº§n Ä‘Ã¢y Ä‘Ã£ bá»• sung cÃ¡c cáº¥u trÃºc khÃ¡c nhau nhÆ° Ä‘á»“ thá»‹ tri thá»©c vÃ o cÃ¡c vector nhÃºng Ä‘á»ƒ giáº£i quyáº¿t má»™t sá»‘ thiáº¿u sÃ³t nÃ y, cá»¥ thá»ƒ lÃ  kháº£ nÄƒng cáº£m nháº­n vÃ  liÃªn káº¿t. Tuy nhiÃªn, hiá»‡u suáº¥t cá»§a chÃºng trong cÃ¡c nhiá»‡m vá»¥ trÃ­ nhá»› thá»±c táº¿ cÆ¡ báº£n láº¡i giáº£m Ä‘Ã¡ng ká»ƒ so vá»›i RAG tiÃªu chuáº©n. => Äá» xuáº¥t HippoRAG. 
+, Long Term Memory? (lÆ°u dáº¡ng fact cá»§a user) 
- Baseline ngÃ y xÆ°a. RAG truyá»n thá»‘ng. Dataset ngÃ y xÆ°a. 
- Reason LLms -> Äoáº¡n Retrieval Reasoning rá»“i Hippo. 
HippoRAG khÃ´ng chá» Ä‘áº¿n khi LLM lÃ m reasoning, mÃ  nÃ³ Ä‘Ã£ reasoning ngay tá»« bÆ°á»›c truy há»“i.
Vá»›i Personalized PageRank â†’ nÃ³ tÃ¬m ra node trung gian (John Smith) trÆ°á»›c khi vÃ o LLM
Tráº£ vá» Ä‘oáº¡n Ä‘Ã£ Ä‘á»§ logic â†’ LLM chá»‰ cáº§n nÃ³i láº¡i
- xAI cho cÃ¢u há»i báº±ng cÃ¡ch xÃ¢y dá»±ng KG. ÄÆ°a KG vÃ o há»c? 
- Dataset ngÃ y xÆ°a cá»§a anh Huy, ... 
-  HotpotQA

============
 ForHippoRAG2,weusetheopen-sourceLlama-3.3-70B Instruct(AI@Meta,2024)asboththeextraction(NERand  OpenIE)andtriplefilteringmodel,andweusenvidia/NV Embed-v2astheretriever.


Äá»ƒ Ä‘áº·t váº¥n Ä‘á» táº¡i sao cáº§n HippoRAG, ta cÃ³ thá»ƒ trÃ¬nh bÃ y nhÆ° sau â€“ káº¿t há»£p giá»¯a láº­p luáº­n logic vÃ  cÃ¡c phÃ¡t hiá»‡n tá»« bÃ i bÃ¡o:

---
ğŸ§  Táº¡i sao cáº§n HippoRAG?
1. Giá»›i háº¡n cá»§a RAG truyá»n thá»‘ng
Retrieval-Augmented Generation (RAG) lÃ  má»™t giáº£i phÃ¡p phá»• biáº¿n giÃºp LLM tiáº¿p cáº­n thÃ´ng tin má»›i mÃ  khÃ´ng cáº§n thay Ä‘á»•i tham sá»‘ mÃ´ hÃ¬nh. Tuy nhiÃªn, RAG truyá»n thá»‘ng chá»§ yáº¿u dá»±a vÃ o vector retrieval Ä‘Æ¡n giáº£n, dáº«n Ä‘áº¿n 2 Ä‘iá»ƒm yáº¿u lá»›n:
- Thiáº¿u kháº£ nÄƒng sense-making: KhÃ´ng thá»ƒ hiá»ƒu, diá»…n giáº£i cÃ¡c tÃ¬nh huá»‘ng dÃ i vÃ  phá»©c táº¡p má»™t cÃ¡ch liá»n máº¡ch.
- Thiáº¿u tÃ­nh liÃªn káº¿t (associativity): KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘a bÆ°á»›c giá»¯a cÃ¡c máº£nh kiáº¿n thá»©c rá»i ráº¡c nhÆ° bá»™ nhá»› dÃ i háº¡n cá»§a con ngÆ°á»i.
ğŸ“Œ VÃ­ dá»¥: Trong cÃ¡c tÃ¡c vá»¥ truy váº¥n Ä‘a bÆ°á»›c nhÆ° "NgÆ°á»i phÃ¡t minh ra thiáº¿t bá»‹ X sá»‘ng á»Ÿ Ä‘Ã¢u?", vector retrieval truyá»n thá»‘ng thÆ°á»ng tháº¥t báº¡i vÃ¬ khÃ´ng thá»ƒ ná»‘i cÃ¡c máº£nh thÃ´ng tin láº¡i (phÃ¡t minh X â†’ ngÆ°á»i A â†’ nÆ¡i sá»‘ng cá»§a A).

---
2. CÃ¡c giáº£i phÃ¡p trÆ°á»›c Ä‘Ã¢y váº«n chÆ°a Ä‘á»§
CÃ¡c há»‡ thá»‘ng RAG cáº¥u trÃºc má»›i nhÆ° RAPTOR, GraphRAG, LightRAG Ä‘Ã£ thá»­ kháº¯c phá»¥c báº±ng cÃ¡ch:
- Táº¡o tÃ³m táº¯t hoáº·c knowledge graph Ä‘á»ƒ bá»• sung ngá»¯ cáº£nh.
- Tuy nhiÃªn, láº¡i gáº·p pháº£i váº¥n Ä‘á» giáº£m hiá»‡u quáº£ trong cÃ¡c tÃ¡c vá»¥ ghi nhá»› cÆ¡ báº£n (factual QA) do nhiá»…u tá»« LLM hoáº·c cáº¥u trÃºc phá»©c táº¡p gÃ¢y quÃ¡ táº£i.
âŒ KhÃ´ng cÃ³ mÃ´ hÃ¬nh nÃ o cÃ¢n báº±ng tá»‘t cáº£ 3 khÃ­a cáº¡nh: factual memory, sense-making vÃ  associativity.

---
3. HippoRAG ra Ä‘á»i Ä‘á»ƒ mÃ´ phá»ng bá»™ nhá»› dÃ i háº¡n nhÆ° ngÆ°á»i
HippoRAG (vÃ  báº£n cáº£i tiáº¿n HippoRAG 2) Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cáº£m há»©ng tá»« cáº¥u trÃºc bá»™ nhá»› sinh há»c cá»§a con ngÆ°á»i (neocortex, hippocampus, parahippocampal region), vá»›i má»¥c tiÃªu:
- Ghi nhá»› bá»n vá»¯ng kiáº¿n thá»©c má»›i mÃ  khÃ´ng lÃ m quÃªn kiáº¿n thá»©c cÅ©.
- Káº¿t ná»‘i Ä‘Æ°á»£c nhiá»u thÃ´ng tin dáº¡ng biá»ƒu Ä‘á»“, há»— trá»£ multi-hop reasoning.
- TÄƒng tÃ­nh ngá»¯ cáº£nh vÃ  tÃ­nh liÃªn káº¿t, nhá» tÃ­ch há»£p PageRank cÃ¡ nhÃ¢n hÃ³a vÃ  biá»ƒu Ä‘á»“ tri thá»©c má»Ÿ (open KG).

---
4. Lá»£i Ã­ch cá»§a HippoRAG
Theo káº¿t quáº£ thá»±c nghiá»‡m:
- HippoRAG 2 vÆ°á»£t trá»™i 7% trong cÃ¡c bÃ i toÃ¡n truy váº¥n liÃªn káº¿t (associative tasks).
- Giá»¯ Ä‘Æ°á»£c hoáº·c cáº£i thiá»‡n nháº¹ á»Ÿ cÃ¡c tÃ¡c vá»¥ ghi nhá»› Ä‘Æ¡n (factual) vÃ  hiá»ƒu ngá»¯ cáº£nh (sense-making).
- Kháº£ nÄƒng há»c liÃªn tá»¥c phi tham sá»‘ (non-parametric continual learning) â€“ khÃ´ng cáº§n fine-tune LLM liÃªn tá»¥c.

---
Náº¿u báº¡n muá»‘n, mÃ¬nh cÃ³ thá»ƒ viáº¿t pháº§n nÃ y thÃ nh má»™t Ä‘oáº¡n motivation ngáº¯n, chuáº©n phong cÃ¡ch bÃ i nghiÃªn cá»©u hoáº·c bÃ i thuyáº¿t trÃ¬nh há»c thuáº­t. Quá»‘c muá»‘n phong cÃ¡ch nÃ o: ngáº¯n gá»n â€“ gÃ£y gá»n â€“ trÃ¬nh bÃ y váº¥n Ä‘á»? Hay sÃ¢u sáº¯c â€“ há»c thuáº­t â€“ Ä‘Æ°a dáº«n chá»©ng nghiÃªn cá»©u?


DÆ°á»›i Ä‘Ã¢y lÃ  báº£n "Äáº·t váº¥n Ä‘á» vÃ  Giáº£i phÃ¡p" cho HippoRAG, Ä‘Æ°á»£c viáº¿t theo phong cÃ¡ch há»c thuáº­t, sÃºc tÃ­ch vÃ  logic â€“ phÃ¹ há»£p Ä‘á»ƒ Ä‘Æ°a vÃ o giá»›i thiá»‡u má»™t bÃ i nghiÃªn cá»©u, thuyáº¿t trÃ¬nh chuyÃªn Ä‘á» hoáº·c Ä‘á»“ Ã¡n tá»‘t nghiá»‡p.

---
ğŸ§© Äáº·t váº¥n Ä‘á» vÃ  Giáº£i phÃ¡p
ğŸ” Äáº·t váº¥n Ä‘á»
Kháº£ nÄƒng ghi nhá»›, liÃªn káº¿t vÃ  sá»­ dá»¥ng kiáº¿n thá»©c lÃ¢u dÃ i lÃ  má»™t Ä‘áº·c trÆ°ng cá»‘t lÃµi cá»§a trÃ­ thÃ´ng minh con ngÆ°á»i. Tuy nhiÃªn, cÃ¡c há»‡ thá»‘ng AI hiá»‡n nay â€“ Ä‘áº·c biá»‡t lÃ  cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) â€“ chÆ°a thá»ƒ hiá»‡n Ä‘Æ°á»£c kháº£ nÄƒng há»c liÃªn tá»¥c vÃ  ghi nhá»› lÃ¢u dÃ i má»™t cÃ¡ch hiá»‡u quáº£.
Máº·c dÃ¹ Retrieval-Augmented Generation (RAG) Ä‘ang lÃ  giáº£i phÃ¡p chÃ­nh cho viá»‡c tiáº¿p cáº­n thÃ´ng tin ngoÃ i mÃ´ hÃ¬nh (non-parametric learning), RAG truyá»n thá»‘ng váº«n cÃ²n nhiá»u háº¡n cháº¿:
- Phá»¥ thuá»™c hoÃ n toÃ n vÃ o truy há»“i vector (vector retrieval), dáº«n Ä‘áº¿n:
  - KhÃ´ng hiá»ƒu sÃ¢u ngá»¯ cáº£nh dÃ i hoáº·c phá»©c táº¡p (thiáº¿u sense-making).
  - KhÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c thÃ´ng tin rá»i ráº¡c qua nhiá»u bÆ°á»›c (thiáº¿u associativity).
- CÃ¡c phÆ°Æ¡ng phÃ¡p cáº£i tiáº¿n nhÆ° RAPTOR hay GraphRAG dÃ¹ng LLM Ä‘á»ƒ táº¡o tÃ³m táº¯t hoáº·c graph, nhÆ°ng láº¡i gÃ¢y suy giáº£m hiá»‡u nÄƒng á»Ÿ cÃ¡c bÃ i toÃ¡n factual QA, do nhiá»…u tá»« LLM hoáº·c quÃ¡ phá»©c táº¡p trong cáº¥u trÃºc.
Tá»« Ä‘Ã³ Ä‘áº·t ra cÃ¢u há»i:
LÃ m tháº¿ nÃ o Ä‘á»ƒ xÃ¢y dá»±ng má»™t há»‡ thá»‘ng ghi nhá»› lÃ¢u dÃ i cho LLMs vá»«a máº¡nh vá» factual, vá»«a sÃ¢u sáº¯c vá» ngá»¯ cáº£nh, vÃ  linh hoáº¡t trong liÃªn káº¿t kiáº¿n thá»©c?

---
ğŸ’¡ Giáº£i phÃ¡p: HippoRAG
HippoRAG Ä‘Æ°á»£c Ä‘á» xuáº¥t nhÆ° má»™t khung ghi nhá»› láº¥y cáº£m há»©ng tá»« cáº¥u trÃºc bá»™ nhá»› cá»§a con ngÆ°á»i (gá»“m tÃ¢n vá» nÃ£o, hippocampus, vÃ  vÃ¹ng trung gian). Giáº£i phÃ¡p nÃ y bao gá»“m hai giai Ä‘oáº¡n:
1. Chá»‰ má»¥c hÃ³a ngoáº¡i tuyáº¿n (Offline Indexing):
  - DÃ¹ng LLM Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c triple kiáº¿n thá»©c dáº¡ng KG má»Ÿ tá»« tÃ i liá»‡u.
  - Táº¡o biá»ƒu Ä‘á»“ tri thá»©c gá»“m cÃ¡c node khÃ¡i niá»‡m (phrase) vÃ  node ngá»¯ cáº£nh (passage).
  - PhÃ¡t hiá»‡n tá»« Ä‘á»“ng nghÄ©a vÃ  liÃªn káº¿t cÃ¡c khÃ¡i niá»‡m tÆ°Æ¡ng Ä‘Æ°Æ¡ng báº±ng embedding.
2. Truy há»“i trá»±c tuyáº¿n (Online Retrieval):
  - DÃ¹ng mÃ´ hÃ¬nh embedding Ä‘á»ƒ liÃªn káº¿t cÃ¢u há»i vá»›i triple vÃ  passage.
  - Ãp dá»¥ng thuáº­t toÃ¡n Personalized PageRank Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng dáº«n liÃªn káº¿t ngá»¯ nghÄ©a vÃ  truy xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan.
  - DÃ¹ng LLM Ä‘á»ƒ lá»c triple theo ngá»¯ cáº£nh truy váº¥n (bá»™ nhá»› nháº­n diá»‡n â€“ recognition memory).
ğŸ¯ Káº¿t quáº£: HippoRAG 2 vÆ°á»£t trá»™i hÆ¡n táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n táº¡i trÃªn cáº£ ba nÄƒng lá»±c cá»‘t lÃµi: ghi nhá»› fact, hiá»ƒu ngá»¯ cáº£nh vÃ  liÃªn káº¿t thÃ´ng tin.

---
Náº¿u Quá»‘c muá»‘n mÃ¬nh viáº¿t láº¡i báº£n nÃ y theo báº£n slide trÃ¬nh bÃ y, dáº¡ng bÃ i viáº¿t bÃ¡o cÃ¡o khoa há»c, hoáº·c báº£n nÃ³i thuyáº¿t trÃ¬nh miá»‡ng 2-3 phÃºt, mÃ¬nh sáº½ chuyá»ƒn thá»ƒ phÃ¹ há»£p nhÃ©. Quá»‘c muá»‘n theo hÆ°á»›ng nÃ o tiáº¿p?


Äá»ƒ mÃ¬nh giáº£i thÃ­ch chi tiáº¿t sÆ¡ Ä‘á»“ HippoRAG 2 nÃ y má»™t cÃ¡ch dá»… hiá»ƒu, chia lÃ m 2 giai Ä‘oáº¡n: Offline Indexing vÃ  Online Retrieval & QA, vá»›i cÃ¡c bÆ°á»›c rÃµ rÃ ng nhÃ©!

---
ğŸ§  1. Offline Indexing â€“ Giai Ä‘oáº¡n táº¡o bá»™ nhá»› (khá»Ÿi táº¡o biá»ƒu Ä‘á»“ tri thá»©c)
ğŸ¯ Má»¥c tiÃªu: Biáº¿n cÃ¡c Ä‘oáº¡n vÄƒn (passages) thÃ nh má»™t biá»ƒu Ä‘á»“ tri thá»©c (Knowledge Graph â€“ KG) Ä‘á»ƒ sau nÃ y dá»… truy há»“i thÃ´ng minh hÆ¡n.
Gá»“m 3 bÆ°á»›c:
ğŸ”µ (1) OpenIE by LLM
 â€“ DÃ¹ng LLM (Llama-3.3-70B) Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c triple kiáº¿n thá»©c tá»« má»—i Ä‘oáº¡n vÄƒn, dáº¡ng:
(subject â€“ relation â€“ object)
 ğŸ‘‰ VÃ­ dá»¥: ("Elon Musk", "founder of", "SpaceX")
ğŸ”µ (2) Synonym detection by embedding
 â€“ DÃ¹ng mÃ´ hÃ¬nh embedding Ä‘á»ƒ tÃ¬m cÃ¡c cáº·p tá»«/cá»¥m tá»« giá»‘ng nhau vá» nghÄ©a, rá»“i ná»‘i chÃºng láº¡i báº±ng synonym edge.
 ğŸ‘‰ VÃ­ dá»¥: â€œNYCâ€ â†” â€œNew York Cityâ€
ğŸ”µ (3) Dense-sparse integration
 â€“ Táº¡o 2 loáº¡i node trong biá»ƒu Ä‘á»“:
- Phrase node: cÃ¡c khÃ¡i niá»‡m rÃºt gá»n, nhÆ° â€œElon Muskâ€ hoáº·c â€œCEOâ€
- Passage node: chá»©a toÃ n bá»™ Ä‘oáº¡n vÄƒn â€“ Ná»‘i passage node vá»›i cÃ¡c phrase node tÆ°Æ¡ng á»©ng báº±ng context edge, Ä‘á»ƒ biá»ƒu Ä‘á»“ vá»«a cÃ³ thÃ´ng tin khÃ¡i niá»‡m, vá»«a cÃ³ ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§.
ğŸ“¦ Output cuá»‘i lÃ  má»™t Knowledge Graph giÃ u ngá»¯ nghÄ©a, vá»›i cÃ¡c loáº¡i liÃªn káº¿t:
- Relation Edge (giá»¯a concept vá»›i concept)
- Synonym Edge
- Context Edge (giá»¯a passage vÃ  concept)

---
ğŸ” 2. Online Retrieval & QA â€“ Giai Ä‘oáº¡n truy há»“i vÃ  tráº£ lá»i
ğŸ¯ Má»¥c tiÃªu: Tá»« cÃ¢u há»i (query), tÃ¬m Ä‘Ãºng cÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan Ä‘á»ƒ tráº£ lá»i.
Gá»“m 5 bÆ°á»›c:
ğŸŸ¢ (1) Truy há»“i passages vÃ  triples
 â€“ DÃ¹ng retriever (NV-Embed-v2) Ä‘á»ƒ truy há»“i cÃ¡c triple vÃ  Ä‘oáº¡n vÄƒn liÃªn quan Ä‘áº¿n cÃ¢u há»i.
ğŸŸ¢ (2) Recognition Memory â€“ Lá»c triple
 â€“ DÃ¹ng LLM Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c triple, giá»¯ láº¡i cÃ¡c triple tháº­t sá»± liÃªn quan Ä‘áº¿n cÃ¢u há»i.
ğŸŸ¢ (3) GÃ¡n trá»ng sá»‘ cho cÃ¡c seed node
 â€“ CÃ¡c triple vÃ  passage Ä‘Æ°á»£c chá»n sáº½ trá»Ÿ thÃ nh cÃ¡c â€œseed nodeâ€ (node khá»Ÿi Ä‘áº§u). â€“ GÃ¡n trá»ng sá»‘ khá»Ÿi Ä‘á»™ng (reset probability) cho tá»«ng seed dá»±a trÃªn má»©c Ä‘á»™ liÃªn quan.
ğŸŸ¢ (4) Personalized PageRank (PPR)
 â€“ Cháº¡y thuáº­t toÃ¡n PPR trÃªn biá»ƒu Ä‘á»“ tri thá»©c, Ä‘á»ƒ tÃ¬m cÃ¡c Ä‘oáº¡n vÄƒn cÃ³ káº¿t ná»‘i ngá»¯ nghÄ©a máº¡nh nháº¥t Ä‘áº¿n cÃ¢u há»i, qua nhiá»u bÆ°á»›c reasoning.
ğŸŸ¢ (5) QA Reading
 â€“ DÃ¹ng LLM Ä‘á»c cÃ¡c Ä‘oáº¡n vÄƒn Ä‘Æ°á»£c chá»n vÃ  táº¡o cÃ¢u tráº£ lá»i cho truy váº¥n.

---
ğŸ—ºï¸ Giáº£i thÃ­ch cÃ¡c kÃ½ hiá»‡u trong sÆ¡ Ä‘á»“
This content is only supported in a Lark Docs

---
Náº¿u Quá»‘c muá»‘n, mÃ¬nh cÃ³ thá»ƒ diá»…n giáº£i láº¡i toÃ n bá»™ thÃ nh slide hoáº·c viáº¿t bÃ¡o cÃ¡o tÃ³m táº¯t pháº§n kiáº¿n trÃºc pipeline HippoRAG 2 theo hÃ¬nh nÃ y nhÃ©. Quá»‘c cáº§n báº£n nÃ o?